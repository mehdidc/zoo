{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 64, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.019446064977066718, 0.018233589718769094, 0.015010247889692489, 0.019123360478363476, 0.013083079268952672, 0.0260786437339004, 0.01575681984470879, 0.026224339490599043, 0.024991521860579995, 0.02640154929612243, 0.02315321920641369, 0.027211052120047374, 0.029475848616468524, 0.022639731026362624, 0.027645652124945356, 0.02955207726691953, 0.0310239108849338, 0.026134936891715987, 0.023132835777163, 0.019925330954172968, 0.02272851305764488, 0.028922201005178495, 0.024011833161617663, 0.02590272711004606, 0.023087300200171693, 0.020500705879316395, 0.022836820530735713, 0.02426214719223804, 0.02399066670086993, 0.022229705006332166, 0.021603856666854316, 0.021746993879842313, 0.02111629088606188, 0.017711670876698803, 0.024286065519369692, 0.02271573705154396, 0.021139477206470164, 0.02194217962747547, 0.02570020512443182, 0.028657513247784913, 0.01995990303515675, 0.024429083703873953, 0.020856060946919275, 0.024571269458671486, 0.02306056512745239, 0.023289172190124715, 0.022952522968182267, 0.02022269317439258, 0.023296182660698772, 0.02215858284791769, 0.022808200957453367, 0.02210693775746621, 0.02306921810029369, 0.02555791082969705, 0.023127344861352102, 0.024277098908202205, 0.024277098908202198, 0.025078489488657384, 0.026246469738833194, 0.021339370030681124, 0.02120032762451425, 0.02678023953520618, 0.02459046057595696, 0.027888073643575574, 0.02525080767087187, 0.023240820375726234, 0.024317422621544734, 0.02677210829483803, 0.02418124817426631, 0.024681782157427497, 0.02482471317899947, 0.025715731818495097, 0.022676565756170095, 0.0233848000457402, 0.02624370447835679, 0.02269975693204838, 0.024103088788726915, 0.026784304229702568, 0.02257793840491052, 0.024123404505486883, 0.023300855135957664, 0.026770075098837765, 0.027409692547364015, 0.025306791452345656, 0.023254868356013102, 0.025561460095893346, 0.025379099786391128, 0.022616478299369996, 0.02275643556442768, 0.02404732069441961, 0.023259549130663346, 0.024612585578701517, 0.022815359216434454, 0.023107723796598808, 0.024733919327324102, 0.022476456771138097, 0.022331495495120847, 0.02179449738166043, 0.023158703987432604, 0.02398083306494384, 0.022812178045306827, 0.02171526714711018, 0.021108556452903552, 0.023113219373708672, 0.024332340371933492, 0.024788142767613076, 0.022032125570806967, 0.024577176013915136, 0.02525296318819799, 0.025640127283214395, 0.024468415455767105, 0.022529670826567965, 0.023983102723413946, 0.024056372951730416, 0.02349857653310502, 0.02500023223732798, 0.02322520154306445, 0.022751651282746626, 0.02093854262049897, 0.02048831179076675, 0.024801314327799164, 0.02213318524401264, 0.02272851305764488, 0.02436363791635646, 0.022418261014403605, 0.02293987172126923, 0.023145381550318835, 0.025684669044295318, 0.022766001111535462, 0.024670753164447574, 0.025011841354386118, 0.024152719261522532, 0.0254005378584551, 0.025591963465679733, 0.024604475387771656, 0.023513242159514326, 0.022992802360912782, 0.02563305004460123, 0.02549109264784358, 0.023424336126821453, 0.021045716576050928, 0.024896235358414166, 0.02318141282704096, 0.025777041215028506, 0.023895185605496683, 0.02476324403894195, 0.021841897569316357, 0.022504692016805175, 0.023601813293084306, 0.023758118400535398, 0.025117526681361007, 0.026364413395321338, 0.02351632849439137, 0.026387113767210948, 0.025953110676996422, 0.025982455972369582, 0.026208421832385535, 0.023844258204050443, 0.024596362522658035, 0.02505170666330347, 0.023902777404133182, 0.023471536918058408, 0.023316423291055884, 0.025184615560608463, 0.024946469797214623, 0.024521746205988952, 0.025760142852404917, 0.02536980432924292, 0.025003135021922644, 0.024216487442739458, 0.024413481897079554, 0.02378330652407677, 0.0242232295395086, 0.022111040981095557, 0.022189675738224935, 0.023873155736956173, 0.023862513348899297, 0.020334532834704558, 0.022770782378229267, 0.024165486359550244, 0.024262894996864338, 0.02274207969995755, 0.026220879952110504, 0.025028520026380098, 0.023640986454136658, 0.025982455972369582, 0.0246626621016397, 0.022318492221293892, 0.025379814680541676, 0.025441933575480382, 0.02406014372043419, 0.023100656133550296, 0.02444096411002493, 0.025021269795925734, 0.023906572399378836, 0.02331564513018357, 0.02515578205689549, 0.027335454512857373, 0.026025714743703583, 0.02371607874684672, 0.026041047356884856, 0.024867067478493213, 0.02243444164517444, 0.024324882640317506, 0.024535800267378716, 0.025045912016452506, 0.02480350890780209, 0.024268128984042986, 0.02431294551157002, 0.023493171099809206, 0.025449776891371765, 0.025950314156253755, 0.025209817824055316, 0.025192538988698353, 0.02638848891971297, 0.024750052218398776, 0.02433308601938243, 0.023981589641634354, 0.025429093840332335, 0.028292410125346383, 0.02549465121620901, 0.02679107735011168, 0.02520333964899679, 0.024911534824121152, 0.023640986454136658, 0.02515578205689549, 0.026808002664422386, 0.024165486359550244, 0.025810804750166522, 0.023805419540482056, 0.023627935949993295, 0.024446902148040156, 0.025012566745298656, 0.025755916529412528, 0.025178130901166558, 0.0260201370079465, 0.027914734977107558, 0.025219172250206444, 0.024543193909332708, 0.02383664776748238, 0.026086295612086193, 0.024961011605384702, 0.02592583173249894, 0.026802587726698225, 0.024347994172980107, 0.02550034389267076, 0.025385533108976892, 0.02405561872706055, 0.025980360979074215, 0.026172398397742623, 0.026551623551365675, 0.02509150863442505, 0.026444122500096104], "moving_avg_accuracy_train": [0.010055064006024093, 0.01745505459337349, 0.025752847326807226, 0.03276434723268072, 0.03905116552146084, 0.052225303487387034, 0.05951453895190134, 0.07109414680369916, 0.08318183302694371, 0.10725493360979152, 0.1374062286524268, 0.16496361030525641, 0.2000438682506344, 0.2296300311243661, 0.26601143689747164, 0.2982393594727847, 0.3303224265375544, 0.3654024768054857, 0.40215383304059976, 0.43555949642328673, 0.4707897854857773, 0.5008004153709344, 0.5289677382916723, 0.5548124930769629, 0.5821437437692666, 0.6087138196935447, 0.631502076278407, 0.6535363566023735, 0.674819110249365, 0.6926393453087658, 0.7112213256875278, 0.7269850176669678, 0.7424359887918373, 0.7561677287680751, 0.7695922774274122, 0.7823897326665987, 0.793537995845722, 0.8042773815021137, 0.8141028436531071, 0.8227104433239409, 0.8308526143529926, 0.8391994877068499, 0.8468787482735143, 0.854176001458211, 0.8613718237521489, 0.8673162490576569, 0.87354866782659, 0.8788825246885092, 0.8837324122798993, 0.8880196567446804, 0.8923370434798509, 0.8965850785896972, 0.9005589125981973, 0.9044389211877751, 0.9078391555750217, 0.9112970510114954, 0.914413863229623, 0.9172872359428053, 0.918649626806356, 0.9219959781317445, 0.9248382666137508, 0.9274622148017733, 0.9299343668155718, 0.932375794591846, 0.9348366338073603, 0.9366842957278291, 0.9387872328719137, 0.9407904749461681, 0.9426639876925151, 0.9445478148268781, 0.9459749987056361, 0.9476524423591689, 0.9491174315569869, 0.9506830039133364, 0.9519720077388703, 0.9535015577179953, 0.9541769102293283, 0.9554836167967569, 0.9567867234905751, 0.95807011815959, 0.959149872156884, 0.9603546138568583, 0.9613635801820157, 0.9624222522842961, 0.9634268267546616, 0.9642956463382315, 0.9650164017345289, 0.9657474422839676, 0.9666783456459322, 0.9673396714729053, 0.9683701998075425, 0.9691941361520894, 0.9701686419645913, 0.9707280202380116, 0.9712220480334876, 0.971796096995199, 0.9726139458800164, 0.9733335377377978, 0.9738540996266687, 0.9741861218929174, 0.9745625963000112, 0.9749508396820582, 0.9754343889969849, 0.9761566692237924, 0.9764796318194855, 0.977055030836332, 0.9775446519996868, 0.9781312071310434, 0.9785814523817945, 0.9786807619628921, 0.9791631187485306, 0.9797807865423522, 0.9802002041230568, 0.9806859254276186, 0.981337212402929, 0.9816409911626361, 0.9820273438536015, 0.9823279980224582, 0.9826291778888872, 0.982794347449396, 0.9831406657165045, 0.9833676383014806, 0.9835577946520554, 0.9839289541928739, 0.9842512319663574, 0.984604817354059, 0.9848595088114243, 0.9851922702796794, 0.985491755601109, 0.9857401139265403, 0.9859471642808743, 0.985954669238329, 0.986253215868713, 0.9864866103962995, 0.9865837136638985, 0.9870099620264243, 0.9872147451912519, 0.9874484664552592, 0.9877670610747935, 0.987943197587796, 0.987979355991667, 0.98836016662744, 0.9882204978562622, 0.9885254247272625, 0.9886774944533314, 0.9890567329598055, 0.9892850958084032, 0.9894341464685267, 0.9893823922132403, 0.9896393713654102, 0.9899294816686282, 0.9901364582005605, 0.9901650751817093, 0.990292016458719, 0.990606282433329, 0.9908491180454179, 0.9908605917830446, 0.9910685838095594, 0.9912887209105312, 0.9913927177953816, 0.9915616161965664, 0.9915842008118495, 0.9917504230499417, 0.9918694319497667, 0.9920541943270792, 0.9921428260991906, 0.9922061225555365, 0.9924536955409466, 0.9926012100229965, 0.9926539655267209, 0.9928849921668199, 0.9930529123778488, 0.9931852152665699, 0.9931983955471417, 0.9933326222574879, 0.9934275415076427, 0.9935270878086857, 0.9936355047808292, 0.9937636711702161, 0.9938507829688572, 0.9940233100936582, 0.9941385807409189, 0.9942964470644173, 0.9942926306712286, 0.9944397983269974, 0.9945628365665868, 0.994621801403904, 0.9947831152394171, 0.9948459369986079, 0.9948154095638074, 0.9949267714688724, 0.9950458224846358, 0.9951670873747264, 0.9951326828541212, 0.9951417225506368, 0.9951333861389466, 0.9952717794527628, 0.9953092664171251, 0.9954724286308343, 0.9955486797436546, 0.9956290715584457, 0.9957037773544083, 0.9957498341069192, 0.9959230622926127, 0.9959730753404599, 0.9960580908485825, 0.9960616567637243, 0.9961613457560266, 0.9962793038009059, 0.99635722808949, 0.9964532447383724, 0.9964808306561014, 0.9965103643073587, 0.9964945876657796, 0.9964874481763101, 0.9965680896538598, 0.996548893640281, 0.9966069184328794, 0.9967485609269409, 0.9967113177860539, 0.9968189887182919, 0.996857063491041, 0.9969783978045874, 0.9970805391988274, 0.9970618678090652, 0.9971486027149057, 0.9972360767807645, 0.99724656172317, 0.9972889424484435, 0.997371795191551, 0.9974463626603477, 0.9974217000388912, 0.9974183289807852, 0.9974129418658393, 0.997513985781665, 0.9975649215408479, 0.9975731131217028, 0.997618136146882, 0.9976515973815914, 0.9977052441193358, 0.9977323477194504, 0.9977449751463006, 0.9978034030834777, 0.9978889325040456, 0.9979659089825568, 0.9979998903734577, 0.9980563584144252, 0.9980530569103321, 0.9980995019723109, 0.9981342430401401, 0.9981349188867286, 0.9981378803113087, 0.9982040809849971], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.000909938809487172, 0.0013117836747741918, 0.0018002855855200038, 0.0020627072053385795, 0.0022121532425699833, 0.0035529591186203094, 0.0036758597896724764, 0.004515059672717987, 0.005378563129530731, 0.01005633436162482, 0.01723260626019474, 0.022344029186212654, 0.031185246745219674, 0.03594479137301655, 0.04426277240996112, 0.04918424611064837, 0.05352973023012614, 0.05925224654831881, 0.06548298155956925, 0.06897812851794888, 0.0732508750729938, 0.07403152872062943, 0.07376895857325662, 0.07240362486513717, 0.07188623775827342, 0.07105133439404324, 0.06861994269823664, 0.06612753401296922, 0.0635913810368591, 0.060090289931323884, 0.05718887089136175, 0.05370642966562952, 0.050484379277380524, 0.047132987494617584, 0.044041655305518006, 0.04111146352035704, 0.0381188711155203, 0.03534499364245867, 0.03267935163653824, 0.03007823342172447, 0.02766706462114898, 0.025527390812102087, 0.023505391116548424, 0.021634101141267688, 0.019936709753514328, 0.01826106450807777, 0.01678454545067214, 0.01536214116681592, 0.014037619736976398, 0.012799281949185929, 0.011687112208256573, 0.010680813208081293, 0.009754854097817177, 0.008914858887932241, 0.008127427344132944, 0.007422297977365923, 0.006767498845256954, 0.006165055397471003, 0.0055652548375096835, 0.005109511958495079, 0.004671268196980081, 0.004266107314122912, 0.003894500402924576, 0.003558695488912984, 0.003257327506823198, 0.002962319447292032, 0.0027058886042505637, 0.0024714165530980757, 0.002255865347884792, 0.002062218055145772, 0.0018743279340452748, 0.0017122194955377442, 0.0015603132861314818, 0.0014263411087450252, 0.0012986607756306902, 0.0011898504063153933, 0.0010749702748149284, 0.0009828405858136853, 0.0008998393107315832, 0.0008246792965465246, 0.0007527041851439239, 0.0006904963897024431, 0.0006306088681399162, 0.000577635060907245, 0.000528954083615113, 0.0004828523024727542, 0.0004392424672971043, 0.00040012800313170624, 0.00036791443244239, 0.0003350591558429451, 0.0003111111380950614, 0.000286109864184344, 0.0002660458319733095, 0.0002422573852509515, 0.0002202282178901818, 0.00020117118599514194, 0.00018707395858120213, 0.0001730268746991508, 0.00015816304935053848, 0.0001433388934830495, 0.0001302806009475142, 0.00011860913716609294, 0.00010885260290917851, 0.00010266254115259498, 9.333503058128657e-05, 8.698128378045061e-05, 8.044071535484927e-05, 7.549306611845143e-05, 6.97682465790214e-05, 6.28801834571994e-05, 5.868617772934308e-05, 5.62511814881292e-05, 5.2209263302352734e-05, 4.911166364346434e-05, 4.801806979699893e-05, 4.404679663094167e-05, 4.098553258419264e-05, 3.770051568903169e-05, 3.474684792760756e-05, 3.1517691988315055e-05, 2.9445349868681457e-05, 2.6964463870789743e-05, 2.4593452422686215e-05, 2.33739418230834e-05, 2.1971314310309615e-05, 2.089938651684351e-05, 1.939325751125237e-05, 1.845050351292512e-05, 1.741267628139899e-05, 1.6226545373558292e-05, 1.4989719479270688e-05, 1.3491254450821197e-05, 1.2944299820361663e-05, 1.2140126887891594e-05, 1.1010975600307991e-05, 1.1545067039280564e-05, 1.0767985636723208e-05, 1.0182817736293618e-05, 1.0078058747030209e-05, 9.349469513241565e-06, 8.426289433451897e-06, 8.88881115296771e-06, 8.175496328451625e-06, 8.194770265528711e-06, 7.5834200532559716e-06, 8.119474651064673e-06, 7.776873501535377e-06, 7.1991310449312126e-06, 6.503324466900349e-06, 6.44733658206017e-06, 6.560078816153429e-06, 6.289624497474844e-06, 5.668032432217979e-06, 5.246255979276088e-06, 5.6104983065267955e-06, 5.580170686360933e-06, 5.023338437621017e-06, 4.9103507417025736e-06, 4.8554587565506225e-06, 4.4672510494230195e-06, 4.277265973785524e-06, 3.854129960034353e-06, 3.7173854559582256e-06, 3.47311497450066e-06, 3.433037701682052e-06, 3.1604342507622426e-06, 2.8804487981596794e-06, 3.1440353662880273e-06, 3.0254765313893258e-06, 2.7479771668093736e-06, 2.953539226047319e-06, 2.9119600788903465e-06, 2.778300560276958e-06, 2.5020339824128546e-06, 2.413981872104495e-06, 2.2536706613436323e-06, 2.1174887896712884e-06, 2.0115280693429948e-06, 1.958214872725113e-06, 1.8306895746148887e-06, 1.915511096282384e-06, 1.8435458857332317e-06, 1.8834872820142497e-06, 1.6952696375255627e-06, 1.7206675439130905e-06, 1.6848464651331543e-06, 1.5476534869784601e-06, 1.62708752003248e-06, 1.4998979288797008e-06, 1.3582954544712514e-06, 1.3340791741215737e-06, 1.3282295558980063e-06, 1.327753162426454e-06, 1.2056308855264505e-06, 1.0858032419916522e-06, 9.778483796313022e-07, 1.052437925449379e-06, 9.59841585378326e-07, 1.1034545986828127e-06, 1.0454372286713667e-06, 9.990591007729923e-07, 9.493817942494296e-07, 8.735346348912102e-07, 1.0562532102704293e-06, 9.731396338380524e-07, 9.408744000464773e-07, 8.469014017990128e-07, 8.516523182952915e-07, 8.917139896314203e-07, 8.571923434303324e-07, 8.544458808506191e-07, 7.758501384781295e-07, 7.061152536396221e-07, 6.377438500513414e-07, 5.744282158351759e-07, 5.755128253642681e-07, 5.212779252636796e-07, 4.994520217421075e-07, 6.300701846834427e-07, 5.795466301031592e-07, 6.259292339338065e-07, 5.763835054195008e-07, 6.512432956717755e-07, 6.800147458603153e-07, 6.151508584351729e-07, 6.21342267612035e-07, 6.280734506317008e-07, 5.662555117237455e-07, 5.257950934236339e-07, 5.34996777445152e-07, 5.315398663253769e-07, 4.838600837667806e-07, 4.3557635168488774e-07, 3.922799055833709e-07, 4.449407713535674e-07, 4.237967582900572e-07, 3.820210004331842e-07, 3.620625555563754e-07, 3.359331880552281e-07, 3.282416214853252e-07, 3.020289055893464e-07, 2.7326108221011294e-07, 2.766593885741223e-07, 3.1483098576081166e-07, 3.3667629138044634e-07, 3.1340127659047065e-07, 3.10758905787772e-07, 2.79781114572487e-07, 2.712172971552033e-07, 2.5495804358486603e-07, 2.2946635014387892e-07, 2.0659864544938702e-07, 2.2538154367563968e-07], "duration": 99207.693856, "accuracy_train": [0.10055064006024096, 0.08405496987951808, 0.10043298192771084, 0.09586784638554217, 0.09563253012048192, 0.1707925451807229, 0.12511765813253012, 0.17531061746987953, 0.19197100903614459, 0.3239128388554217, 0.40876788403614456, 0.4129800451807229, 0.5157661897590361, 0.49590549698795183, 0.5934440888554217, 0.5882906626506024, 0.6190700301204819, 0.6811229292168675, 0.7329160391566265, 0.7362104668674698, 0.7878623870481928, 0.7708960843373494, 0.7824736445783133, 0.7874152861445783, 0.828125, 0.8478445030120482, 0.8365963855421686, 0.8518448795180723, 0.8663638930722891, 0.8530214608433735, 0.8784591490963856, 0.8688582454819277, 0.8814947289156626, 0.8797533885542169, 0.8904132153614458, 0.8975668298192772, 0.8938723644578314, 0.9009318524096386, 0.9025320030120482, 0.9001788403614458, 0.9041321536144579, 0.9143213478915663, 0.915992093373494, 0.9198512801204819, 0.9261342243975904, 0.9208160768072289, 0.9296404367469879, 0.9268872364457831, 0.9273814006024096, 0.9266048569277109, 0.9311935240963856, 0.9348173945783133, 0.9363234186746988, 0.9393589984939759, 0.938441265060241, 0.942418109939759, 0.9424651731927711, 0.9431475903614458, 0.9309111445783133, 0.952113140060241, 0.9504188629518072, 0.9510777484939759, 0.952183734939759, 0.9543486445783133, 0.9569841867469879, 0.9533132530120482, 0.9577136671686747, 0.9588196536144579, 0.9595256024096386, 0.9615022590361446, 0.9588196536144579, 0.9627494352409639, 0.9623023343373494, 0.9647731551204819, 0.9635730421686747, 0.9672675075301205, 0.9602550828313253, 0.9672439759036144, 0.9685146837349398, 0.9696206701807228, 0.9688676581325302, 0.9711972891566265, 0.9704442771084337, 0.9719503012048193, 0.9724679969879518, 0.9721150225903614, 0.9715032003012049, 0.9723268072289156, 0.9750564759036144, 0.9732916039156626, 0.9776449548192772, 0.9766095632530121, 0.9789391942771084, 0.9757624246987951, 0.9756682981927711, 0.9769625376506024, 0.9799745858433735, 0.9798098644578314, 0.978539156626506, 0.9771743222891566, 0.9779508659638554, 0.9784450301204819, 0.9797863328313253, 0.9826571912650602, 0.9793862951807228, 0.9822336219879518, 0.9819512424698795, 0.983410203313253, 0.9826336596385542, 0.9795745481927711, 0.9835043298192772, 0.985339796686747, 0.9839749623493976, 0.9850574171686747, 0.9871987951807228, 0.984375, 0.9855045180722891, 0.9850338855421686, 0.985339796686747, 0.9842808734939759, 0.9862575301204819, 0.9854103915662651, 0.9852692018072289, 0.987269390060241, 0.9871517319277109, 0.9877870858433735, 0.9871517319277109, 0.9881871234939759, 0.9881871234939759, 0.9879753388554217, 0.9878106174698795, 0.9860222138554217, 0.9889401355421686, 0.9885871611445783, 0.9874576430722891, 0.9908461972891566, 0.9890577936746988, 0.9895519578313253, 0.9906344126506024, 0.9895284262048193, 0.988304781626506, 0.9917874623493976, 0.9869634789156626, 0.9912697665662651, 0.9900461219879518, 0.9924698795180723, 0.9913403614457831, 0.9907756024096386, 0.9889166039156626, 0.9919521837349398, 0.9925404743975904, 0.9919992469879518, 0.9904226280120482, 0.9914344879518072, 0.9934346762048193, 0.9930346385542169, 0.9909638554216867, 0.9929405120481928, 0.9932699548192772, 0.9923286897590361, 0.9930817018072289, 0.9917874623493976, 0.9932464231927711, 0.9929405120481928, 0.9937170557228916, 0.9929405120481928, 0.9927757906626506, 0.9946818524096386, 0.9939288403614458, 0.993128765060241, 0.9949642319277109, 0.9945641942771084, 0.9943759412650602, 0.9933170180722891, 0.9945406626506024, 0.9942818147590361, 0.9944230045180723, 0.9946112575301205, 0.9949171686746988, 0.9946347891566265, 0.9955760542168675, 0.9951760165662651, 0.9957172439759037, 0.9942582831325302, 0.9957643072289156, 0.9956701807228916, 0.995152484939759, 0.9962349397590361, 0.9954113328313253, 0.9945406626506024, 0.9959290286144579, 0.996117281626506, 0.9962584713855421, 0.9948230421686747, 0.9952230798192772, 0.9950583584337349, 0.9965173192771084, 0.9956466490963856, 0.9969408885542169, 0.9962349397590361, 0.9963525978915663, 0.9963761295180723, 0.9961643448795181, 0.9974821159638554, 0.9964231927710844, 0.9968232304216867, 0.99609375, 0.997058546686747, 0.9973409262048193, 0.997058546686747, 0.9973173945783133, 0.9967291039156626, 0.9967761671686747, 0.9963525978915663, 0.9964231927710844, 0.9972938629518072, 0.9963761295180723, 0.9971291415662651, 0.998023343373494, 0.9963761295180723, 0.9977880271084337, 0.9971997364457831, 0.998070406626506, 0.9979998117469879, 0.9968938253012049, 0.9979292168674698, 0.998023343373494, 0.9973409262048193, 0.9976703689759037, 0.9981174698795181, 0.9981174698795181, 0.9971997364457831, 0.9973879894578314, 0.9973644578313253, 0.9984233810240963, 0.998023343373494, 0.9976468373493976, 0.998023343373494, 0.9979527484939759, 0.9981880647590361, 0.9979762801204819, 0.9978586219879518, 0.9983292545180723, 0.9986586972891566, 0.9986586972891566, 0.9983057228915663, 0.9985645707831325, 0.998023343373494, 0.9985175075301205, 0.9984469126506024, 0.9981410015060241, 0.9981645331325302, 0.9987998870481928], "end": "2016-01-18 22:04:48.857000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0], "accuracy_valid": [0.0972521551724138, 0.08230064655172414, 0.09644396551724138, 0.09603987068965517, 0.09469288793103449, 0.1732219827586207, 0.12863685344827586, 0.17739762931034483, 0.19302262931034483, 0.31721443965517243, 0.39991918103448276, 0.40045797413793105, 0.49407327586206895, 0.47979525862068967, 0.5689655172413793, 0.5674838362068966, 0.587957974137931, 0.6491109913793104, 0.6969288793103449, 0.6978717672413793, 0.7471713362068966, 0.7198275862068966, 0.7354525862068966, 0.7361260775862069, 0.7672413793103449, 0.7843480603448276, 0.7751885775862069, 0.7781519396551724, 0.7926993534482759, 0.7790948275862069, 0.7952586206896551, 0.7905441810344828, 0.7974137931034483, 0.7941810344827587, 0.8006465517241379, 0.8050915948275862, 0.8017241379310345, 0.8069773706896551, 0.8076508620689655, 0.8032058189655172, 0.8064385775862069, 0.8112877155172413, 0.8089978448275862, 0.8130387931034483, 0.8160021551724138, 0.8100754310344828, 0.814520474137931, 0.814520474137931, 0.8130387931034483, 0.8107489224137931, 0.8129040948275862, 0.8149245689655172, 0.8169450431034483, 0.8158674568965517, 0.8189655172413793, 0.8209859913793104, 0.8170797413793104, 0.8212553879310345, 0.814520474137931, 0.8236799568965517, 0.8215247844827587, 0.8258351293103449, 0.822332974137931, 0.8235452586206896, 0.8252963362068966, 0.8216594827586207, 0.8244881465517241, 0.8254310344827587, 0.8232758620689655, 0.8252963362068966, 0.822332974137931, 0.8266433189655172, 0.8232758620689655, 0.8271821120689655, 0.8243534482758621, 0.8290678879310345, 0.8196390086206896, 0.8247575431034483, 0.8266433189655172, 0.828125, 0.8278556034482759, 0.828125, 0.8278556034482759, 0.8263739224137931, 0.8292025862068966, 0.8270474137931034, 0.8287984913793104, 0.8248922413793104, 0.8254310344827587, 0.8265086206896551, 0.8308189655172413, 0.8290678879310345, 0.8310883620689655, 0.8296066810344828, 0.8285290948275862, 0.8274515086206896, 0.8292025862068966, 0.8289331896551724, 0.8290678879310345, 0.8282596982758621, 0.8304148706896551, 0.8293372844827587, 0.8305495689655172, 0.8310883620689655, 0.830145474137931, 0.8292025862068966, 0.8306842672413793, 0.83203125, 0.83203125, 0.8286637931034483, 0.8323006465517241, 0.8316271551724138, 0.8302801724137931, 0.8323006465517241, 0.8314924568965517, 0.8323006465517241, 0.8298760775862069, 0.8309536637931034, 0.830145474137931, 0.8309536637931034, 0.8304148706896551, 0.8297413793103449, 0.8309536637931034, 0.8313577586206896, 0.8337823275862069, 0.8327047413793104, 0.8313577586206896, 0.8331088362068966, 0.8297413793103449, 0.8351293103448276, 0.8318965517241379, 0.8316271551724138, 0.8293372844827587, 0.8312230603448276, 0.8306842672413793, 0.8312230603448276, 0.8328394396551724, 0.8312230603448276, 0.8318965517241379, 0.8335129310344828, 0.8309536637931034, 0.83203125, 0.8290678879310345, 0.8344558189655172, 0.8341864224137931, 0.8325700431034483, 0.8324353448275862, 0.8345905172413793, 0.8324353448275862, 0.8328394396551724, 0.8343211206896551, 0.8343211206896551, 0.8343211206896551, 0.8349946120689655, 0.8352640086206896, 0.8343211206896551, 0.8314924568965517, 0.8345905172413793, 0.8309536637931034, 0.8312230603448276, 0.8323006465517241, 0.8343211206896551, 0.833917025862069, 0.8314924568965517, 0.8328394396551724, 0.8344558189655172, 0.8337823275862069, 0.834051724137931, 0.8313577586206896, 0.8345905172413793, 0.8328394396551724, 0.8343211206896551, 0.8341864224137931, 0.8337823275862069, 0.8348599137931034, 0.8337823275862069, 0.8347252155172413, 0.8323006465517241, 0.8353987068965517, 0.8323006465517241, 0.8344558189655172, 0.8318965517241379, 0.8324353448275862, 0.8333782327586207, 0.8332435344827587, 0.8336476293103449, 0.83203125, 0.8355334051724138, 0.8347252155172413, 0.8332435344827587, 0.8347252155172413, 0.8347252155172413, 0.8349946120689655, 0.8356681034482759, 0.8332435344827587, 0.8341864224137931, 0.8355334051724138, 0.8352640086206896, 0.8368803879310345, 0.8358028017241379, 0.8374191810344828, 0.837823275862069, 0.8384967672413793, 0.8345905172413793, 0.8366109913793104, 0.8347252155172413, 0.834051724137931, 0.8349946120689655, 0.8353987068965517, 0.8360721982758621, 0.8360721982758621, 0.8356681034482759, 0.8356681034482759, 0.8347252155172413, 0.8353987068965517, 0.8332435344827587, 0.8349946120689655, 0.8382273706896551, 0.8358028017241379, 0.8349946120689655, 0.8329741379310345, 0.8366109913793104, 0.833917025862069, 0.8364762931034483, 0.8332435344827587, 0.8355334051724138, 0.8362068965517241, 0.8362068965517241, 0.8352640086206896, 0.8375538793103449, 0.8343211206896551, 0.8356681034482759, 0.8343211206896551, 0.8351293103448276, 0.8344558189655172, 0.8351293103448276, 0.8360721982758621, 0.8351293103448276, 0.8335129310344828, 0.8362068965517241, 0.8349946120689655, 0.8366109913793104, 0.8364762931034483, 0.8368803879310345, 0.8355334051724138, 0.8362068965517241, 0.8367456896551724, 0.8348599137931034, 0.8374191810344828, 0.8362068965517241, 0.8362068965517241, 0.8345905172413793, 0.8375538793103449], "accuracy_test": 0.8307291666666666, "start": "2016-01-17 18:31:21.163000", "learning_rate_per_epoch": [0.005347559694200754, 0.002673779847100377, 0.0017825198592618108, 0.0013368899235501885, 0.0010695119854062796, 0.0008912599296309054, 0.0007639370742253959, 0.0006684449617750943, 0.0005941733252257109, 0.0005347559927031398, 0.00048614179831929505, 0.0004456299648154527, 0.0004113507457077503, 0.00038196853711269796, 0.00035650396603159606, 0.00033422248088754714, 0.0003145623195450753, 0.00029708666261285543, 0.0002814505132846534, 0.0002673779963515699, 0.00025464571081101894, 0.00024307089915964752, 0.00023250259982887655, 0.00022281498240772635, 0.00021390238543972373, 0.00020567537285387516, 0.00019805776537396014, 0.00019098426855634898, 0.0001843986101448536, 0.00017825198301579803, 0.00017250192468054593, 0.00016711124044377357, 0.00016204726125579327, 0.00015728115977253765, 0.00015278742648661137, 0.00014854333130642772, 0.00014452864706981927, 0.0001407252566423267, 0.00013711692008655518, 0.00013368899817578495, 0.00013042827777098864, 0.00012732285540550947, 0.0001243618462467566, 0.00012153544957982376, 0.00011883465776918456, 0.00011625129991443828, 0.00011377786722732708, 0.00011140749120386317, 0.00010913387086475268, 0.00010695119271986187, 0.0001048541089403443, 0.00010283768642693758, 0.00010089735587825999, 9.902888268698007e-05, 9.722835966385901e-05, 9.549213427817449e-05, 9.381683776155114e-05, 9.21993050724268e-05, 9.063660399988294e-05, 8.912599150789902e-05, 8.766491373535246e-05, 8.625096234027296e-05, 8.488189632771537e-05, 8.355562022188678e-05, 8.227014768635854e-05, 8.102363062789664e-05, 7.981432281667367e-05, 7.864057988626882e-05, 7.750086660962552e-05, 7.639371324330568e-05, 7.531774463132024e-05, 7.427166565321386e-05, 7.325423939619213e-05, 7.226432353490964e-05, 7.130079757189378e-05, 7.036262832116336e-05, 6.944882625248283e-05, 6.855846004327759e-05, 6.769062747480348e-05, 6.684449908789247e-05, 6.601925269933417e-05, 6.521413888549432e-05, 6.44284300506115e-05, 6.366142770275474e-05, 6.291246972978115e-05, 6.21809231233783e-05, 6.146620580693707e-05, 6.076772478991188e-05, 6.0084941651439294e-05, 5.941732888459228e-05, 5.876439172425307e-05, 5.812564995721914e-05, 5.7500641560181975e-05, 5.688893361366354e-05, 5.62901004741434e-05, 5.570374560193159e-05, 5.512947973329574e-05, 5.456693543237634e-05, 5.401575253927149e-05, 5.347559635993093e-05, 5.2946135838283226e-05, 5.242705447017215e-05, 5.191805394133553e-05, 5.141884321346879e-05, 5.092913852422498e-05, 5.044867793912999e-05, 4.99771922477521e-05, 4.9514441343490034e-05, 4.906018148176372e-05, 4.8614179831929505e-05, 4.8176214477280155e-05, 4.7746067139087245e-05, 4.7323537728516385e-05, 4.690841888077557e-05, 4.650051778298803e-05, 4.60996525362134e-05, 4.570563760353252e-05, 4.531830199994147e-05, 4.49374747404363e-05, 4.456299575394951e-05, 4.4194708607392386e-05, 4.383245686767623e-05, 4.347609501564875e-05, 4.312548117013648e-05, 4.2780477087944746e-05, 4.2440948163857684e-05, 4.210677070659585e-05, 4.177781011094339e-05, 4.1453949961578473e-05, 4.113507384317927e-05, 4.0821065340423957e-05, 4.051181531394832e-05, 4.0207214624388143e-05, 3.990716140833683e-05, 3.961155380238779e-05, 3.932028994313441e-05, 3.903328251908533e-05, 3.875043330481276e-05, 3.8471651350846514e-05, 3.819685662165284e-05, 3.792595452978276e-05, 3.765887231566012e-05, 3.739552266779356e-05, 3.713583282660693e-05, 3.687972275656648e-05, 3.662711969809607e-05, 3.637795816757716e-05, 3.613216176745482e-05, 3.58896613761317e-05, 3.565039878594689e-05, 3.541430123732425e-05, 3.518131416058168e-05, 3.495137207210064e-05, 3.4724413126241416e-05, 3.450038639130071e-05, 3.4279230021638796e-05, 3.406088944757357e-05, 3.384531373740174e-05, 3.3632451959419996e-05, 3.3422249543946236e-05, 3.321465555927716e-05, 3.300962634966709e-05, 3.280711462139152e-05, 3.260706944274716e-05, 3.240945079596713e-05, 3.221421502530575e-05, 3.202131483703852e-05, 3.183071385137737e-05, 3.164236477459781e-05, 3.1456234864890575e-05, 3.127228046650998e-05, 3.109046156168915e-05, 3.091074904659763e-05, 3.0733102903468534e-05, 3.055748311453499e-05, 3.038386239495594e-05, 3.021220072696451e-05, 3.0042470825719647e-05, 2.9874634492443874e-05, 2.970866444229614e-05, 2.954452793346718e-05, 2.9382195862126537e-05, 2.922163730545435e-05, 2.906282497860957e-05, 2.890572795877233e-05, 2.8750320780090988e-05, 2.8596576157724485e-05, 2.844446680683177e-05, 2.8293967261561193e-05, 2.81450502370717e-05, 2.799769390549045e-05, 2.7851872800965793e-05, 2.7707563276635483e-05, 2.756473986664787e-05, 2.7423382562119514e-05, 2.728346771618817e-05, 2.7144973500980996e-05, 2.7007876269635744e-05, 2.6872159651247784e-05, 2.6737798179965466e-05, 2.6604773665894754e-05, 2.6473067919141613e-05, 2.63426591118332e-05, 2.6213527235086076e-05, 2.6085657736985013e-05, 2.5959026970667765e-05, 2.5833622203208506e-05, 2.5709421606734395e-05, 2.5586410629330203e-05, 2.546456926211249e-05, 2.5343884772155434e-05, 2.5224338969564997e-05, 2.5105913664447144e-05, 2.498859612387605e-05, 2.4872369976947084e-05, 2.4757220671745017e-05, 2.4643131837365218e-05, 2.453009074088186e-05, 2.4418081011390314e-05, 2.4307089915964752e-05, 2.419710290268995e-05, 2.4088107238640077e-05, 2.398008837189991e-05, 2.3873033569543622e-05, 2.3766931917634793e-05, 2.3661768864258192e-05, 2.3557531676487997e-05, 2.3454209440387785e-05, 2.3351789423031732e-05, 2.3250258891494013e-05, 2.314960875082761e-05, 2.30498262681067e-05, 2.2950900529394858e-05, 2.285281880176626e-05, 2.2755573809263296e-05, 2.2659150999970734e-05, 2.256354309793096e-05, 2.246873737021815e-05, 2.237472654087469e-05, 2.2281497876974754e-05, 2.218904410256073e-05, 2.2097354303696193e-05, 2.2006417566444725e-05, 2.1916228433838114e-05, 2.1826774172950536e-05, 2.1738047507824376e-05, 2.1650039343512617e-05, 2.156274058506824e-05, 2.1476143956533633e-05, 2.1390238543972373e-05, 2.130501889041625e-05, 2.1220474081928842e-05, 2.113659866154194e-05], "accuracy_train_last": 0.9987998870481928, "error_valid": [0.9027478448275862, 0.9176993534482758, 0.9035560344827587, 0.9039601293103449, 0.9053071120689655, 0.8267780172413793, 0.8713631465517242, 0.8226023706896551, 0.8069773706896551, 0.6827855603448276, 0.6000808189655172, 0.599542025862069, 0.505926724137931, 0.5202047413793103, 0.43103448275862066, 0.4325161637931034, 0.41204202586206895, 0.3508890086206896, 0.30307112068965514, 0.30212823275862066, 0.2528286637931034, 0.2801724137931034, 0.2645474137931034, 0.26387392241379315, 0.23275862068965514, 0.21565193965517238, 0.22481142241379315, 0.22184806034482762, 0.2073006465517241, 0.22090517241379315, 0.20474137931034486, 0.20945581896551724, 0.2025862068965517, 0.20581896551724133, 0.1993534482758621, 0.1949084051724138, 0.19827586206896552, 0.19302262931034486, 0.19234913793103448, 0.19679418103448276, 0.19356142241379315, 0.18871228448275867, 0.1910021551724138, 0.1869612068965517, 0.1839978448275862, 0.18992456896551724, 0.18547952586206895, 0.18547952586206895, 0.1869612068965517, 0.18925107758620685, 0.1870959051724138, 0.18507543103448276, 0.1830549568965517, 0.1841325431034483, 0.18103448275862066, 0.1790140086206896, 0.1829202586206896, 0.17874461206896552, 0.18547952586206895, 0.1763200431034483, 0.17847521551724133, 0.17416487068965514, 0.17766702586206895, 0.1764547413793104, 0.17470366379310343, 0.17834051724137934, 0.1755118534482759, 0.17456896551724133, 0.17672413793103448, 0.17470366379310343, 0.17766702586206895, 0.17335668103448276, 0.17672413793103448, 0.17281788793103448, 0.1756465517241379, 0.17093211206896552, 0.1803609913793104, 0.1752424568965517, 0.17335668103448276, 0.171875, 0.1721443965517241, 0.171875, 0.1721443965517241, 0.17362607758620685, 0.17079741379310343, 0.17295258620689657, 0.1712015086206896, 0.1751077586206896, 0.17456896551724133, 0.17349137931034486, 0.16918103448275867, 0.17093211206896552, 0.16891163793103448, 0.17039331896551724, 0.1714709051724138, 0.1725484913793104, 0.17079741379310343, 0.17106681034482762, 0.17093211206896552, 0.1717403017241379, 0.16958512931034486, 0.17066271551724133, 0.16945043103448276, 0.16891163793103448, 0.16985452586206895, 0.17079741379310343, 0.16931573275862066, 0.16796875, 0.16796875, 0.1713362068965517, 0.1676993534482759, 0.1683728448275862, 0.16971982758620685, 0.1676993534482759, 0.1685075431034483, 0.1676993534482759, 0.17012392241379315, 0.16904633620689657, 0.16985452586206895, 0.16904633620689657, 0.16958512931034486, 0.17025862068965514, 0.16904633620689657, 0.1686422413793104, 0.16621767241379315, 0.1672952586206896, 0.1686422413793104, 0.16689116379310343, 0.17025862068965514, 0.16487068965517238, 0.1681034482758621, 0.1683728448275862, 0.17066271551724133, 0.16877693965517238, 0.16931573275862066, 0.16877693965517238, 0.16716056034482762, 0.16877693965517238, 0.1681034482758621, 0.16648706896551724, 0.16904633620689657, 0.16796875, 0.17093211206896552, 0.16554418103448276, 0.16581357758620685, 0.1674299568965517, 0.1675646551724138, 0.16540948275862066, 0.1675646551724138, 0.16716056034482762, 0.16567887931034486, 0.16567887931034486, 0.16567887931034486, 0.16500538793103448, 0.1647359913793104, 0.16567887931034486, 0.1685075431034483, 0.16540948275862066, 0.16904633620689657, 0.16877693965517238, 0.1676993534482759, 0.16567887931034486, 0.16608297413793105, 0.1685075431034483, 0.16716056034482762, 0.16554418103448276, 0.16621767241379315, 0.16594827586206895, 0.1686422413793104, 0.16540948275862066, 0.16716056034482762, 0.16567887931034486, 0.16581357758620685, 0.16621767241379315, 0.16514008620689657, 0.16621767241379315, 0.16527478448275867, 0.1676993534482759, 0.1646012931034483, 0.1676993534482759, 0.16554418103448276, 0.1681034482758621, 0.1675646551724138, 0.16662176724137934, 0.16675646551724133, 0.16635237068965514, 0.16796875, 0.1644665948275862, 0.16527478448275867, 0.16675646551724133, 0.16527478448275867, 0.16527478448275867, 0.16500538793103448, 0.1643318965517241, 0.16675646551724133, 0.16581357758620685, 0.1644665948275862, 0.1647359913793104, 0.16311961206896552, 0.1641971982758621, 0.16258081896551724, 0.16217672413793105, 0.16150323275862066, 0.16540948275862066, 0.1633890086206896, 0.16527478448275867, 0.16594827586206895, 0.16500538793103448, 0.1646012931034483, 0.1639278017241379, 0.1639278017241379, 0.1643318965517241, 0.1643318965517241, 0.16527478448275867, 0.1646012931034483, 0.16675646551724133, 0.16500538793103448, 0.16177262931034486, 0.1641971982758621, 0.16500538793103448, 0.16702586206896552, 0.1633890086206896, 0.16608297413793105, 0.1635237068965517, 0.16675646551724133, 0.1644665948275862, 0.1637931034482759, 0.1637931034482759, 0.1647359913793104, 0.16244612068965514, 0.16567887931034486, 0.1643318965517241, 0.16567887931034486, 0.16487068965517238, 0.16554418103448276, 0.16487068965517238, 0.1639278017241379, 0.16487068965517238, 0.16648706896551724, 0.1637931034482759, 0.16500538793103448, 0.1633890086206896, 0.1635237068965517, 0.16311961206896552, 0.1644665948275862, 0.1637931034482759, 0.16325431034482762, 0.16514008620689657, 0.16258081896551724, 0.1637931034482759, 0.1637931034482759, 0.16540948275862066, 0.16244612068965514], "accuracy_train_std": [0.01827637972698742, 0.017863279112271577, 0.02031132447509653, 0.017990848005222965, 0.01888872974074679, 0.02196200537132681, 0.020272206014021237, 0.02346021732840812, 0.02530546081429885, 0.029058205046481357, 0.03325365203460041, 0.031512529435825075, 0.0300439436651792, 0.0303086983999443, 0.028197693833474518, 0.0317196898545173, 0.027753236614187885, 0.029523602378699436, 0.027388831996307563, 0.02712664828476452, 0.02668591236242219, 0.02594611266882109, 0.027201705272239862, 0.02517791667318219, 0.024716453730182147, 0.025340098406537472, 0.02691561388352698, 0.025299289300382728, 0.02444274572946785, 0.024505135804769863, 0.02349395751952406, 0.02426348051592333, 0.022880051062378014, 0.022358017399116727, 0.023217846311329695, 0.021994418527862804, 0.0221371454760378, 0.021266810791932194, 0.02119168972901345, 0.02161556342550259, 0.019883876293559498, 0.01991867048299336, 0.020400223770337317, 0.020368549633091683, 0.01976433903921658, 0.02111169874828058, 0.019751152642830665, 0.019852619559296368, 0.019932218336651326, 0.018962008914829714, 0.019101362073985658, 0.018791797652833293, 0.019032940451644775, 0.01816423611312671, 0.01873460361695067, 0.018486410437733063, 0.016474911691050455, 0.016647557233208397, 0.018669109197363247, 0.016388371478129157, 0.016620809419435525, 0.017970907655275526, 0.016584906060509292, 0.016627388010399414, 0.015369881400676078, 0.016803097300073698, 0.01596237144501518, 0.015492676765214028, 0.016265784693886915, 0.014642647953787695, 0.014973734397025602, 0.014869251031377963, 0.014399586596035057, 0.013253487518906641, 0.014848064747906072, 0.013770988503025858, 0.014784473552656796, 0.014613727391772637, 0.013199459650517482, 0.013390818812048344, 0.013113704853057797, 0.012488977093615487, 0.013349630881675194, 0.013077657765351147, 0.012178838708622986, 0.012322149221457277, 0.013300532436411447, 0.01297973460731742, 0.010827207962905752, 0.011563226729723054, 0.011267890287130423, 0.010998411365181157, 0.010895854588518467, 0.01099448359479276, 0.011887361621675547, 0.011470044727995263, 0.010882327825095916, 0.009688973975529905, 0.009775004794641141, 0.0112134064535365, 0.011368503579551896, 0.010929116936233678, 0.010563394054925242, 0.009553405654022037, 0.01029599420482939, 0.008855429740722636, 0.009201467776053641, 0.009180380875624944, 0.00931032006967627, 0.01051703130010586, 0.008997732119551582, 0.00836300233883931, 0.00954551952514807, 0.008973081596552824, 0.007971214741542849, 0.008586045286591704, 0.008629403904131342, 0.009142271605137874, 0.00820768155268803, 0.008983165671590567, 0.008199682963116862, 0.009087108610561431, 0.008667564062427916, 0.0074007994161495085, 0.008090637241206648, 0.007623992784044991, 0.008056481147037145, 0.007462902405940508, 0.007882216784847622, 0.007238299242914818, 0.0078570265006543, 0.008652601785102272, 0.0073597959869537224, 0.0073391895047483086, 0.008087796409454204, 0.006652295504072387, 0.0075397108661909695, 0.007576160551661185, 0.006314373370585019, 0.0073147030508139855, 0.007395410320317065, 0.006012365109802246, 0.008677141695863772, 0.007160770994718857, 0.006973981724681261, 0.006102454483211723, 0.006273727573224043, 0.006754355683750648, 0.007528539284759587, 0.006149458322773691, 0.005860508944565263, 0.006015081439422288, 0.0068810998591533334, 0.006723789113496684, 0.00516373113534716, 0.005093061645151644, 0.006911731833842551, 0.006016738258470213, 0.0054510137913401395, 0.005637776991110193, 0.005417591677931433, 0.006468972098254489, 0.005597016038357741, 0.005783037851316942, 0.005299633666540594, 0.005267301111132414, 0.005698201700731296, 0.004601140080045272, 0.00517203524178699, 0.00580311093441203, 0.004518562504781045, 0.004977648242103981, 0.004915857593718096, 0.005879893806833868, 0.004575555196813492, 0.0049381105607650575, 0.004894634879336018, 0.004917659553078932, 0.004486093371533311, 0.004586434137690235, 0.0042562981902068515, 0.004412729007004475, 0.004439938409288687, 0.005103488464859153, 0.003985676837300476, 0.004309565400301727, 0.004291539054301204, 0.0041546484408797775, 0.004557183660216383, 0.0048486572376466594, 0.0041868444356847065, 0.004298307793355946, 0.0040302333064074335, 0.005022555434322481, 0.0046846852827134135, 0.004719719269029988, 0.003787349310755028, 0.004443740661029188, 0.0036406399860342085, 0.0041101608244870575, 0.0040480553652390595, 0.0038245951908716194, 0.004167490251264714, 0.003349187878278435, 0.003985676837300476, 0.003703447421045019, 0.004351506142023273, 0.0034684212568203304, 0.0034311822801784075, 0.003572857307964645, 0.00331720824297931, 0.0038182919239150206, 0.0039054702598526835, 0.0039094382265961, 0.00396254703685839, 0.0034211618489197407, 0.0038485541708694494, 0.0033534012664389232, 0.0027369618502538985, 0.00382459519087162, 0.0030131510358639155, 0.003758657135239014, 0.002770343119872258, 0.002770043282962471, 0.0036260090896570447, 0.002865839109118661, 0.002993607660728398, 0.0036643661564471305, 0.0032928305347373027, 0.002899264463716385, 0.002633440292458166, 0.0035576364068938588, 0.0031910308514402808, 0.0034625890863923966, 0.002499342339204559, 0.0029627429848526243, 0.0029976743108611034, 0.0029000283326438055, 0.003228638632815381, 0.002864776201982946, 0.003171797900130694, 0.0028620688389623516, 0.002949349341721445, 0.0022571969875978845, 0.0022160995935974204, 0.002550550433892927, 0.0023205811044940726, 0.0028359271263521894, 0.0024460443790404675, 0.002420900065815453, 0.0027688436106312097, 0.0028009555753702015, 0.0022552335669913793], "accuracy_test_std": 0.022988033542791993, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.909208697595186, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.005347559674219184, "patience_threshold": 1, "do_flip": true, "batch_size": 256, "optimization": "rmsprop", "nb_data_augmentation": 3, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 9.893508534420986e-08, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.022875396795521807}, "accuracy_valid_max": 0.8384967672413793, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8375538793103449, "loss_train": [231150403584.0, 12196337664.0, 3492290.25, 1568860.0, 1762.8299560546875, 87.07232666015625, 9.94888687133789, 3.1001086235046387, 2.1113648414611816, 1.7170512676239014, 1.4694230556488037, 1.2934426069259644, 1.1628721952438354, 1.0453790426254272, 0.9612208604812622, 0.8864402770996094, 0.831716775894165, 0.7836018800735474, 0.739791750907898, 0.7027698159217834, 0.6697841882705688, 0.6388458609580994, 0.6150130033493042, 0.5952585935592651, 0.5717813372612, 0.5532431602478027, 0.5357436537742615, 0.5184051990509033, 0.5021058320999146, 0.487602561712265, 0.473445862531662, 0.4627901315689087, 0.4497168958187103, 0.4376170337200165, 0.42869189381599426, 0.41654977202415466, 0.4078657031059265, 0.3967152535915375, 0.3877008259296417, 0.383134126663208, 0.37572601437568665, 0.365649551153183, 0.3600115478038788, 0.3521803319454193, 0.34544965624809265, 0.3397500813007355, 0.3330363631248474, 0.328899621963501, 0.3214491307735443, 0.3179507851600647, 0.31070274114608765, 0.3058529496192932, 0.3019344210624695, 0.29739078879356384, 0.2940106987953186, 0.2877350449562073, 0.28408172726631165, 0.28017449378967285, 0.27740585803985596, 0.2729033827781677, 0.2677166759967804, 0.26392662525177, 0.26133084297180176, 0.25829392671585083, 0.2531563639640808, 0.25057920813560486, 0.2472408562898636, 0.24357549846172333, 0.24224497377872467, 0.23701922595500946, 0.234576016664505, 0.23203085362911224, 0.23087593913078308, 0.2267531454563141, 0.22335729002952576, 0.2213526964187622, 0.21720878779888153, 0.21653451025485992, 0.2144293487071991, 0.2127223014831543, 0.21239128708839417, 0.20818157494068146, 0.2064705640077591, 0.20493735373020172, 0.20272964239120483, 0.19938500225543976, 0.19813966751098633, 0.1972682774066925, 0.1943095326423645, 0.1907803863286972, 0.18956978619098663, 0.18741066753864288, 0.18721985816955566, 0.18339306116104126, 0.18334408104419708, 0.18206778168678284, 0.1784493625164032, 0.17902900278568268, 0.17484593391418457, 0.17554812133312225, 0.17203889787197113, 0.17188960313796997, 0.169715017080307, 0.16989122331142426, 0.1687929481267929, 0.16721300780773163, 0.16520152986049652, 0.16339576244354248, 0.15920570492744446, 0.1605687439441681, 0.16001008450984955, 0.1603260189294815, 0.15732116997241974, 0.15572960674762726, 0.154904305934906, 0.15357470512390137, 0.15139302611351013, 0.14967238903045654, 0.14823651313781738, 0.14667998254299164, 0.14868760108947754, 0.14531424641609192, 0.14430899918079376, 0.14400535821914673, 0.14438503980636597, 0.14294514060020447, 0.14253973960876465, 0.14043700695037842, 0.1378440111875534, 0.1402365118265152, 0.13734164834022522, 0.13656213879585266, 0.135851189494133, 0.1357520967721939, 0.1335509717464447, 0.13437965512275696, 0.13281512260437012, 0.1297815591096878, 0.12974630296230316, 0.12927062809467316, 0.1299378126859665, 0.12810425460338593, 0.12675698101520538, 0.12583760917186737, 0.12431419640779495, 0.12428498268127441, 0.12434200197458267, 0.12413391470909119, 0.1222161129117012, 0.11998163163661957, 0.12064959853887558, 0.12057715654373169, 0.12082309275865555, 0.11912143975496292, 0.11784544587135315, 0.11863401532173157, 0.11672992259263992, 0.1157207116484642, 0.11468124389648438, 0.11575154960155487, 0.11397192627191544, 0.11213990300893784, 0.11268903315067291, 0.11141751706600189, 0.11014079302549362, 0.11245652288198471, 0.11038827896118164, 0.11175856739282608, 0.10910653322935104, 0.10749319940805435, 0.10868190973997116, 0.1071794256567955, 0.10703474283218384, 0.10567376017570496, 0.10718045383691788, 0.10696326941251755, 0.10414224863052368, 0.1028798520565033, 0.10289161652326584, 0.10316259413957596, 0.10152700543403625, 0.1007005125284195, 0.10087756812572479, 0.09968362748622894, 0.0992678552865982, 0.0996147096157074, 0.09976805001497269, 0.10073629766702652, 0.09745072573423386, 0.09611375629901886, 0.097244992852211, 0.09688407927751541, 0.09625520557165146, 0.09641281515359879, 0.09343517571687698, 0.09346351027488708, 0.09450644254684448, 0.09319762885570526, 0.09470457583665848, 0.09279617667198181, 0.09205381572246552, 0.09369970858097076, 0.09136159718036652, 0.08988428115844727, 0.089864082634449, 0.0880768820643425, 0.08962921053171158, 0.08894386142492294, 0.08912298828363419, 0.08873300999403, 0.08861608803272247, 0.08853761851787567, 0.08701243251562119, 0.08730456978082657, 0.0873999148607254, 0.08529502898454666, 0.08579996228218079, 0.08455130457878113, 0.08449012786149979, 0.08487116545438766, 0.08395037055015564, 0.0850665345788002, 0.08443330228328705, 0.08322440832853317, 0.08145152777433395, 0.08158286660909653, 0.08308348804712296, 0.08348497748374939, 0.08118588477373123, 0.0804152712225914, 0.08060062676668167, 0.08185507357120514, 0.0801658034324646, 0.08087880909442902, 0.0801955834031105, 0.07907169312238693, 0.07928827404975891, 0.07918944954872131, 0.0788906067609787, 0.07810185849666595, 0.07777082920074463, 0.0768355280160904, 0.07688209414482117, 0.0771561786532402, 0.07816652208566666, 0.07545167207717896, 0.07588530331850052, 0.0766041949391365, 0.07592306286096573, 0.07436374574899673, 0.07462926208972931, 0.0749867856502533, 0.07461029291152954], "accuracy_train_first": 0.10055064006024096, "model": "residualv2", "loss_std": [1957734252544.0, 126270611456.0, 26580988.0, 13151705.0, 7683.58544921875, 315.27130126953125, 22.3751163482666, 2.9475820064544678, 0.286657452583313, 0.1556846797466278, 0.12392667680978775, 0.12193837761878967, 0.12978875637054443, 0.095423124730587, 0.12244522571563721, 0.10107430070638657, 0.10207033157348633, 0.0956776961684227, 0.08863366395235062, 0.08115439116954803, 0.0798601433634758, 0.0760246142745018, 0.07260496914386749, 0.07660012692213058, 0.07492469996213913, 0.07192672044038773, 0.06547324359416962, 0.06814344227313995, 0.06565865874290466, 0.06431873142719269, 0.06480251997709274, 0.06030961126089096, 0.062060434371232986, 0.06152981147170067, 0.05972468480467796, 0.05994671955704689, 0.057003457099199295, 0.05620824545621872, 0.057704273611307144, 0.05713038146495819, 0.05391724035143852, 0.0537182092666626, 0.051971565932035446, 0.051431428641080856, 0.052385956048965454, 0.04947829246520996, 0.050592321902513504, 0.049161896109580994, 0.04766596853733063, 0.04778148978948593, 0.0490507073700428, 0.04774834215641022, 0.04620285704731941, 0.04490717127919197, 0.0454062856733799, 0.045839909464120865, 0.04450203478336334, 0.04349127784371376, 0.04478034749627113, 0.04310442507266998, 0.04118140786886215, 0.04124000668525696, 0.040401823818683624, 0.04040517657995224, 0.04126256704330444, 0.04050850495696068, 0.04061382636427879, 0.03729924559593201, 0.039526745676994324, 0.0393345020711422, 0.03876552730798721, 0.038446810096502304, 0.03922724351286888, 0.035377707332372665, 0.03742804750800133, 0.03756861016154289, 0.0362122468650341, 0.035358697175979614, 0.03608628734946251, 0.034127067774534225, 0.03529052808880806, 0.034981876611709595, 0.03579187020659447, 0.0348622091114521, 0.03390110656619072, 0.03432023897767067, 0.03375702351331711, 0.03408152237534523, 0.032838061451911926, 0.033275116235017776, 0.03319918364286423, 0.03351178765296936, 0.03227079659700394, 0.034120481461286545, 0.03169947862625122, 0.03219801187515259, 0.03246157988905907, 0.03179122135043144, 0.029950883239507675, 0.03165684640407562, 0.03194937855005264, 0.03014897182583809, 0.030869323760271072, 0.03150152042508125, 0.02952512353658676, 0.030300958082079887, 0.02960379421710968, 0.027294667437672615, 0.03014770708978176, 0.029009182006120682, 0.029504291713237762, 0.028829654678702354, 0.02859386056661606, 0.029123587533831596, 0.02883007936179638, 0.0283820740878582, 0.028496144339442253, 0.027410849928855896, 0.027556775137782097, 0.028070926666259766, 0.02751605585217476, 0.025940118357539177, 0.027112459763884544, 0.024403102695941925, 0.028934717178344727, 0.026881130412220955, 0.027032867074012756, 0.02668960765004158, 0.026107745245099068, 0.025865215808153152, 0.02556549571454525, 0.025719143450260162, 0.024810397997498512, 0.025636347010731697, 0.024625195190310478, 0.025674669072031975, 0.02483801543712616, 0.025387940928339958, 0.023922262713313103, 0.024088146165013313, 0.025372589007019997, 0.02437794953584671, 0.02329906076192856, 0.023841962218284607, 0.023057054728269577, 0.024462111294269562, 0.023773428052663803, 0.02498803101480007, 0.023819394409656525, 0.02457486093044281, 0.02332138642668724, 0.02239939756691456, 0.02429516240954399, 0.023735959082841873, 0.022728364914655685, 0.02300366386771202, 0.022993940860033035, 0.02234647050499916, 0.023587102070450783, 0.023898404091596603, 0.02179393731057644, 0.020319130271673203, 0.022607430815696716, 0.021594645455479622, 0.022680412977933884, 0.02171383425593376, 0.02219831384718418, 0.022877104580402374, 0.020687038078904152, 0.022695237770676613, 0.021151190623641014, 0.020391548052430153, 0.019846757873892784, 0.022087080404162407, 0.022673310711979866, 0.02229900471866131, 0.021323131397366524, 0.02243918552994728, 0.020884625613689423, 0.019944507628679276, 0.021714309230446815, 0.021010663360357285, 0.019849510863423347, 0.019985120743513107, 0.020031198859214783, 0.021265899762511253, 0.022287223488092422, 0.019076401367783546, 0.02063293382525444, 0.02129797451198101, 0.01934017241001129, 0.019081251695752144, 0.01857340894639492, 0.017958421260118484, 0.019929960370063782, 0.018291572108864784, 0.020232945680618286, 0.019755754619836807, 0.01899409294128418, 0.019051268696784973, 0.018946735188364983, 0.017573870718479156, 0.0194966122508049, 0.01775203086435795, 0.018657328560948372, 0.018969010561704636, 0.018567951396107674, 0.01897270232439041, 0.017456768080592155, 0.017920151352882385, 0.018613301217556, 0.02019747719168663, 0.017970463261008263, 0.01830221340060234, 0.017606131732463837, 0.017366597428917885, 0.01805882528424263, 0.017695331946015358, 0.01774854212999344, 0.01828981563448906, 0.017107538878917694, 0.017973504960536957, 0.017660807818174362, 0.01858855038881302, 0.017430905252695084, 0.018175100907683372, 0.018024198710918427, 0.017198029905557632, 0.017492899671196938, 0.01641433872282505, 0.01697501912713051, 0.016509531065821648, 0.017249982804059982, 0.01756514422595501, 0.016076499596238136, 0.017235977575182915, 0.01568174920976162, 0.01707356423139572, 0.017291905358433723, 0.015873752534389496, 0.017264941707253456, 0.01688472554087639, 0.0159752294421196, 0.017679300159215927, 0.015539098531007767, 0.016372203826904297, 0.015176414512097836, 0.016572706401348114, 0.015475346706807613, 0.01653006486594677, 0.015851158648729324, 0.015921669080853462, 0.016700169071555138]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:01 2016", "state": "available"}], "summary": "16af3eb55b8ccf74dca3590e9a050cd0"}