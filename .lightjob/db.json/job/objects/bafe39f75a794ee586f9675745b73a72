{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 32, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.7855831384658813, 1.5105006694793701, 1.3773831129074097, 1.2780920267105103, 1.2013965845108032, 1.1369518041610718, 1.0816293954849243, 1.030303716659546, 0.9870561361312866, 0.9482693672180176, 0.913059651851654, 0.8826536536216736, 0.8557793498039246, 0.8294999599456787, 0.8075760006904602, 0.7862691879272461, 0.7656342387199402, 0.7478052973747253, 0.7332750558853149, 0.7177051901817322, 0.7028692960739136, 0.6922698020935059, 0.6791618466377258, 0.666378378868103, 0.6558290123939514, 0.6458843350410461, 0.6368144154548645, 0.6268098950386047, 0.6181396842002869, 0.6086791753768921, 0.6011264324188232, 0.5931283235549927, 0.5884412527084351, 0.5820521712303162, 0.5754550695419312, 0.5688429474830627, 0.5627694725990295, 0.5580682158470154, 0.5521145462989807, 0.5484791398048401, 0.5414090156555176, 0.538712739944458, 0.5335658192634583, 0.5302320122718811, 0.5242282152175903, 0.520488440990448, 0.5147367119789124, 0.5133174657821655, 0.5103614330291748, 0.5079513192176819, 0.5029462575912476, 0.5011152029037476, 0.49789026379585266, 0.49632570147514343, 0.4906351864337921, 0.4910097122192383, 0.48694083094596863, 0.48460233211517334, 0.4818568825721741, 0.48049888014793396, 0.47713983058929443, 0.4772810935974121, 0.4752574563026428, 0.4726896584033966, 0.4713524878025055, 0.46757203340530396, 0.4666980504989624, 0.466373473405838, 0.4644971191883087, 0.46236535906791687, 0.46086251735687256, 0.458683043718338, 0.45832115411758423, 0.4551393985748291, 0.4555707275867462, 0.45388320088386536, 0.4527345299720764, 0.4512639343738556, 0.4509739875793457, 0.4486214518547058, 0.4476291239261627, 0.44632893800735474, 0.44724592566490173, 0.4448658525943756, 0.44532448053359985, 0.4443899989128113, 0.4444326162338257, 0.44351470470428467, 0.4438060224056244, 0.4430208206176758, 0.44103488326072693, 0.43959668278694153, 0.4420938193798065, 0.44054821133613586, 0.43719467520713806, 0.4372909963130951, 0.4376036524772644, 0.43542495369911194, 0.43755871057510376, 0.4368046224117279, 0.4360930323600769, 0.437774658203125, 0.43445342779159546, 0.4349675476551056, 0.43487152457237244, 0.43392443656921387, 0.4354628026485443, 0.4327692985534668, 0.4320991337299347, 0.43320736289024353, 0.4336288571357727, 0.43203213810920715, 0.4305426776409149, 0.4338110685348511, 0.4312630593776703, 0.4300582706928253, 0.43151551485061646, 0.4292333126068115, 0.43109628558158875, 0.4300118684768677, 0.4307941198348999, 0.4295918047428131, 0.4287101924419403, 0.42961177229881287, 0.4286998212337494, 0.42837613821029663, 0.428724080324173, 0.4281872808933258, 0.4278298318386078, 0.42864978313446045, 0.4286212623119354, 0.4284937083721161, 0.427676796913147, 0.4271884858608246, 0.42780765891075134, 0.4285362660884857, 0.42696326971054077, 0.42769673466682434, 0.42728978395462036, 0.4277476966381073, 0.4276825487613678, 0.4266315996646881, 0.4275393784046173, 0.4268192946910858, 0.42657145857810974, 0.4254336357116699, 0.42511168122291565, 0.42627981305122375, 0.4263342618942261, 0.4261598587036133, 0.4266180098056793, 0.425241619348526, 0.42764291167259216, 0.4253908693790436, 0.42613929510116577, 0.4250126779079437, 0.4268559515476227, 0.42496126890182495, 0.42590510845184326, 0.4264349341392517, 0.4264957904815674, 0.4248051047325134, 0.42702731490135193, 0.4247216284275055, 0.4246990978717804, 0.42536601424217224, 0.4251379668712616, 0.42515894770622253, 0.42518818378448486, 0.4246842563152313, 0.4264880418777466, 0.4272736608982086], "moving_avg_accuracy_train": [0.04661344779554263, 0.09622733070032759, 0.1455945061239733, 0.19399552509790854, 0.2398018870457127, 0.28327105696055815, 0.32419501182075006, 0.36254206353526974, 0.3985098811354619, 0.43215691898860076, 0.463729061766974, 0.49303666716627326, 0.5204855137732893, 0.54640999860051, 0.5704696623271146, 0.5926858654488697, 0.6132195943917088, 0.6323601845557125, 0.6496610123187644, 0.6657291949554741, 0.6806995145760673, 0.6947472942346473, 0.7076993604749423, 0.7195654113864274, 0.7308096881233845, 0.741048083727113, 0.7506369887288018, 0.759471760520835, 0.7678322092265034, 0.7755240237758907, 0.7826488366703115, 0.7891936657086145, 0.7952702400407615, 0.8010457521431822, 0.8063622595758277, 0.8115469697628094, 0.8163527178596642, 0.8208590725098828, 0.8250032915451163, 0.8290609707077881, 0.8329569504815553, 0.8365401342862975, 0.8397418203201077, 0.8428370711969193, 0.845548392224145, 0.8482978259403149, 0.8508631052348954, 0.8533741445464466, 0.8557595658649193, 0.8581180335932115, 0.8602591475926943, 0.8622699636958279, 0.8641260569695637, 0.8658709817266494, 0.867576272638979, 0.86925969978861, 0.8707330036423441, 0.8722728187035433, 0.8736424483145936, 0.8750031062955189, 0.8763507150723439, 0.877654207726239, 0.87885060260284, 0.8799621991751049, 0.8810113921175058, 0.8819789533025806, 0.882859203159661, 0.8838094660524436, 0.884627428177358, 0.8854171806588562, 0.886200001456481, 0.8869232495112759, 0.8876530836248401, 0.8883331858151431, 0.888994069862597, 0.8896120809445821, 0.8902707056124439, 0.8908355299789865, 0.8913810742898273, 0.8919580225778987, 0.8924354233585916, 0.892965173606481, 0.8934465270295631, 0.8939448492770036, 0.8943746660115863, 0.8947964143536724, 0.8951992753984639, 0.8956292436054336, 0.8960000470964959, 0.8963335899943583, 0.8966431152464913, 0.8970355121186214, 0.8972982408416984, 0.8975507924900405, 0.8977967262128435, 0.8980994107228807, 0.8983881028235808, 0.8986735383999345, 0.8988931919888816, 0.8991048311117912, 0.8992302742533806, 0.8994198669427066, 0.8995835970143089, 0.8996727532608755, 0.8998111226030234, 0.8999496419526325, 0.9000998499553666, 0.9002513852971314, 0.900403971048749, 0.9005971017966334, 0.900726813739986, 0.9007877509175747, 0.9008960007023861, 0.9009795467134967, 0.9010896514044578, 0.9011979380751047, 0.9013187196644197, 0.901439012790032, 0.9015054239245116, 0.9016209975169719, 0.9017133519573197, 0.9018499493762518, 0.9019172276771371, 0.9019916929919722, 0.9020284487919813, 0.9020382054262567, 0.90206329848759, 0.9021625761046856, 0.9022356499184049, 0.9022666472650657, 0.9023642632925274, 0.9024289022779663, 0.9024475137862809, 0.9025014304758975, 0.9025522445965433, 0.902598049402762, 0.9026602000676445, 0.9026928481291248, 0.9026990159451805, 0.9026486913105646, 0.9026894296453627, 0.9026959393097946, 0.9027366031422888, 0.9028151253677423, 0.9028787478265846, 0.9028710120193322, 0.9028198719654241, 0.9028110122490405, 0.9028587699780861, 0.9029320870152074, 0.9030026505485982, 0.9030266301988878, 0.9030830170186539, 0.9031036463683758, 0.9031128761390687, 0.9031351338255493, 0.9031272639576677, 0.9031294816718124, 0.9030595421967222, 0.9030522560452947, 0.902999267630457, 0.9030096346797036, 0.9029538969061777, 0.9029734152766525, 0.9030258950910416, 0.9031219190001729, 0.9030990945731623, 0.9030785886376714, 0.9030717229909585, 0.9030074872863163, 0.903061210197358, 0.9030933208244476], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04654438064759035, 0.09576692512236444, 0.1450295939617846, 0.19312179058028986, 0.23796623802828495, 0.2802112710872787, 0.32029493619428884, 0.35774359934255573, 0.39232115488269775, 0.4247013676135545, 0.45507455728292195, 0.48309402173535265, 0.5092626586713807, 0.533881591175854, 0.5566429520130879, 0.5777660309175773, 0.5967492993431388, 0.6141669192958731, 0.6300513262932436, 0.6444631858514192, 0.6581378963043043, 0.6706170636580606, 0.6820405382504624, 0.6925068298696933, 0.7022326976169107, 0.7109595055095871, 0.7189011408490651, 0.7263629069411164, 0.7337082615783301, 0.7399375742570935, 0.7457514751992305, 0.7510694352659039, 0.7559278120047501, 0.7604743380331909, 0.7649721320160164, 0.7692754647481497, 0.773162730255639, 0.7769552674710389, 0.7802119185759682, 0.7835233815564737, 0.7864273675254498, 0.7894214318835976, 0.7920428476184306, 0.7944987485211207, 0.7964984512763128, 0.7985087918132147, 0.8004900262425859, 0.8023697639703604, 0.8041734502239268, 0.8057377917132059, 0.8072290892549877, 0.8084227136502721, 0.8094613840209376, 0.8106912151218558, 0.8116393717064323, 0.81261787147103, 0.813681626727918, 0.8145372321744786, 0.8153703712499525, 0.8161090188952885, 0.8167462991876121, 0.8174673653343629, 0.8181305909150081, 0.8187508784914289, 0.8192358951227077, 0.8199562603355876, 0.8205292878223601, 0.8209819183868862, 0.8215256222560289, 0.8220505473233477, 0.8224873882988443, 0.8228184805118816, 0.8233117760036152, 0.823781185517335, 0.8241415894147732, 0.8245046330335368, 0.8250388918216741, 0.825176898347338, 0.8255330378141855, 0.8259766631555079, 0.8261185487977885, 0.8262696304296813, 0.8264666390546348, 0.8267670466382526, 0.826828864423599, 0.8270808424390704, 0.8273198296842447, 0.8274250549236515, 0.8276062363665273, 0.8276584068752059, 0.8277308039041762, 0.8278692034177495, 0.8278838996987156, 0.8279937530929253, 0.8280183494515545, 0.8282124141204803, 0.8283016231037635, 0.8283819111887185, 0.8285406491925876, 0.8286336557624102, 0.8285942618540909, 0.8286930846803535, 0.8286731914513995, 0.8286919086390909, 0.8288074398666728, 0.8287384605166772, 0.8288228634766812, 0.828937506251754, 0.8290030341469099, 0.8290864233150502, 0.8290760243476265, 0.8290168076432856, 0.8290489618281286, 0.8290911371343971, 0.8291657160037887, 0.8292450440174911, 0.8292187829798232, 0.8293294253896722, 0.8294167965272863, 0.829458809457389, 0.8293857283045718, 0.829577332431946, 0.8295656411691731, 0.8296039471576774, 0.8296150379934909, 0.8295517775582231, 0.8297033922063918, 0.8296180598099243, 0.8296511239343535, 0.8295954324275898, 0.8295697241340024, 0.8296208583659335, 0.8295926074785117, 0.8295427676173321, 0.8295711539297707, 0.8296821508297153, 0.8297210128834155, 0.8297071606067457, 0.8297811722851525, 0.829822339224559, 0.8298349754075247, 0.829773105784694, 0.8297927243289656, 0.8299192147914004, 0.8299709915426821, 0.8299565554625855, 0.8299801840842487, 0.8299404146874956, 0.8300033079890773, 0.8299734332330913, 0.8300462612200231, 0.8299510559846924, 0.8299640570315545, 0.8300235565900707, 0.8301259343177353, 0.8301814531788835, 0.8300971428101668, 0.8301067126970718, 0.8301397396577863, 0.8301206357974293, 0.8299946085505178, 0.8299778107696377, 0.8299626927668456, 0.8299857076580828, 0.8300308351226962, 0.8300236512245078, 0.8299795351137288, 0.8299042390289373, 0.8298486795838749, 0.8298597112395687, 0.8299439114258528, 0.8299698339598489], "moving_var_accuracy_train": [0.019555321638490004, 0.039753625866648434, 0.05771232536376467, 0.07302502056682539, 0.08460652366418205, 0.09315198989553522, 0.0989097216385729, 0.10225321685148002, 0.10367105029261822, 0.10349305366997148, 0.1021149500995362, 0.0996338766977519, 0.09645144164847619, 0.09285500770563974, 0.0887793137028114, 0.08434341946285401, 0.07970378373525051, 0.07503066508816267, 0.07022146635092748, 0.06552299815505468, 0.06098769256543368, 0.05666498432891445, 0.05250829007505991, 0.04852468954565953, 0.04481012442512905, 0.04127253468346215, 0.037972805107298654, 0.03487800333012452, 0.03201927692015309, 0.02934982532769728, 0.02687170942395211, 0.02457005156582242, 0.02244536920976715, 0.02050104114919731, 0.018705324295809962, 0.017076722843735886, 0.01557690749229612, 0.014201981833168422, 0.012936354612559512, 0.011790901992988193, 0.010748419719267798, 0.00978913060294812, 0.00890247468378517, 0.008098452417320285, 0.007354768531002346, 0.006687326149738615, 0.006077819455497587, 0.005526785375765222, 0.005025318951988317, 0.004572848387018044, 0.004156822870745273, 0.0037775310162763416, 0.0034307836548159725, 0.003115108151005391, 0.0028297694897659147, 0.002572297883502359, 0.0023346037133609716, 0.0021224826158291413, 0.001927117321689418, 0.0017510681007899772, 0.0015923057354493614, 0.0014483669997932534, 0.00131641254612074, 0.001195892113963906, 0.0010862101550409703, 0.0009860147113586438, 0.0008943867985207906, 0.0008130751147573059, 0.0007377891616217251, 0.0006696236262978462, 0.0006081765392788083, 0.0005520666750898085, 0.0005016539280807257, 0.0004556513861759474, 0.00041401715707596457, 0.00037605288064547493, 0.0003423516706589724, 0.00031098774267843745, 0.00028256753576640987, 0.000257306606133747, 0.00023362714906902853, 0.0002127901520883761, 0.00019359644694075378, 0.00017647172780732584, 0.00016048723685453982, 0.00014603935814555723, 0.00013289609552369586, 0.00012127033990236954, 0.00011038076297298917, 0.00010034394445812039, 9.117180294768034e-05, 8.344040040022989e-05, 7.571759779757366e-05, 6.871987903353942e-05, 6.239224169429063e-05, 5.6977578738409626e-05, 5.2029909025628526e-05, 4.756017933730052e-05, 4.3238390695806675e-05, 3.9317671691339346e-05, 3.552752835815182e-05, 3.229828401294948e-05, 2.9309723438776742e-05, 2.6450290621615524e-05, 2.3977576233072106e-05, 2.175250710170994e-05, 1.97803183883074e-05, 1.8008953187710973e-05, 1.6417599573310236e-05, 1.5111534987984726e-05, 1.3751808183420858e-05, 1.2410047421591056e-05, 1.1274504822637309e-05, 1.0209873764125997e-05, 9.297993774458235e-06, 8.473728424370447e-06, 7.757649312790347e-06, 7.112118306137477e-06, 6.4406004245695725e-06, 5.916755679580098e-06, 5.401844195489757e-06, 5.029589469670996e-06, 4.567367850634056e-06, 4.160536813592195e-06, 3.7566420317417754e-06, 3.3818345557790667e-06, 3.049318055744872e-06, 2.833090657475915e-06, 2.5978396319916962e-06, 2.3467031882926148e-06, 2.19779286881994e-06, 2.015617367885229e-06, 1.8171731252723617e-06, 1.6616188975180912e-06, 1.5186956814792968e-06, 1.3857088357859467e-06, 1.2819022985154103e-06, 1.163305131929656e-06, 1.0473169963307705e-06, 9.653784163406893e-07, 8.837770820057153e-07, 7.957807553842963e-07, 7.310846053038679e-07, 7.134678037852451e-07, 6.785513788288954e-07, 6.112348253706231e-07, 5.736490888570896e-07, 5.169906311409761e-07, 4.85818774179339e-07, 4.856153881515422e-07, 4.818667595375486e-07, 4.388552962359295e-07, 4.2358502760232366e-07, 3.850566554716276e-07, 3.4731768792785084e-07, 3.17044560602333e-07, 2.858975179263683e-07, 2.573520304379769e-07, 2.7564059897703604e-07, 2.485543311029507e-07, 2.489688469557724e-07, 2.2503924365093798e-07, 2.3049561386448368e-07, 2.1087475355198762e-07, 2.1457445646152717e-07, 2.7610233093916607e-07, 2.531806880605296e-07, 2.3164705976766412e-07, 2.0890658773397257e-07, 2.2515196071846956e-07, 2.2861212518374962e-07, 2.1503074401415907e-07], "duration": 174119.466897, "accuracy_train": [0.46613447795542634, 0.5427522768433923, 0.5898990849367848, 0.629604695863326, 0.6520591445759505, 0.6744935861941675, 0.6925106055624769, 0.7076655289659468, 0.7222202395371908, 0.7349802596668512, 0.747878346772333, 0.7568051157599668, 0.7675251332364341, 0.7797303620454964, 0.787006635866556, 0.7926316935446659, 0.7980231548772609, 0.804625496031746, 0.8053684621862311, 0.8103428386858619, 0.8154323911614065, 0.8211773111618678, 0.824267956637597, 0.8263598695897933, 0.8320081787559985, 0.8331936441606681, 0.8369371337440015, 0.8389847066491326, 0.8430762475775194, 0.8447503547203765, 0.8467721527200996, 0.8480971270533407, 0.8499594090300849, 0.8530253610649685, 0.8542108264696382, 0.8582093614456442, 0.8596044507313585, 0.8614162643618494, 0.8623012628622186, 0.8655800831718347, 0.8680207684454596, 0.8687887885289776, 0.8685569946244001, 0.8706943290882245, 0.8699502814691769, 0.8730427293858435, 0.8739506188861205, 0.8759734983504062, 0.8772283577311739, 0.8793442431478405, 0.87952917358804, 0.8803673086240311, 0.8808308964331857, 0.8815753045404209, 0.8829238908499446, 0.8844105441352897, 0.8839927383259505, 0.8861311542543374, 0.8859691148140458, 0.8872490281238464, 0.8884791940637689, 0.8893856416112956, 0.8896181564922481, 0.889966568325489, 0.890454128599114, 0.890687003968254, 0.890781451873385, 0.8923618320874861, 0.8919890873015872, 0.8925249529923404, 0.8932453886351052, 0.8934324820044297, 0.8942215906469176, 0.89445410552787, 0.8949420262896824, 0.8951741806824474, 0.8961983276232004, 0.89591894927787, 0.8962909730873938, 0.8971505571705426, 0.8967320303848283, 0.8977329258374861, 0.8977787078373015, 0.8984297495039681, 0.8982430166228312, 0.8985921494324474, 0.8988250248015872, 0.8994989574681617, 0.8993372785160576, 0.8993354760751201, 0.8994288425156883, 0.9005670839677926, 0.8996627993493909, 0.8998237573251201, 0.9000101297180694, 0.9008235713132153, 0.900986331729882, 0.9012424585871169, 0.9008700742894058, 0.9010095832179772, 0.9003592625276854, 0.9011262011466408, 0.9010571676587301, 0.900475159479974, 0.9010564466823551, 0.901196316099114, 0.901451721979974, 0.9016152033730158, 0.9017772428133074, 0.9023352785275931, 0.9018942212301587, 0.9013361855158729, 0.9018702487656883, 0.901731460813492, 0.9020805936231081, 0.9021725181109265, 0.902405753968254, 0.9025216509205426, 0.9021031241348283, 0.902661159849114, 0.9025445419204503, 0.9030793261466408, 0.9025227323851052, 0.902661880825489, 0.9023592509920635, 0.902126015134736, 0.9022891360395902, 0.9030560746585455, 0.9028933142418788, 0.9025456233850129, 0.9032428075396824, 0.9030106531469176, 0.902615017361111, 0.9029866806824474, 0.9030095716823551, 0.9030102926587301, 0.9032195560515872, 0.9029866806824474, 0.9027545262896824, 0.9021957695990217, 0.9030560746585455, 0.9027545262896824, 0.903102577634736, 0.9035218253968254, 0.9034513499561646, 0.9028013897540605, 0.902359611480251, 0.9027312748015872, 0.9032885895394979, 0.9035919403492986, 0.903637722349114, 0.9032424470514949, 0.9035904983965486, 0.9032893105158729, 0.9031959440753045, 0.9033354530038759, 0.9030564351467331, 0.903149441099114, 0.9024300869209118, 0.9029866806824474, 0.9025223718969176, 0.9031029381229235, 0.9024522569444444, 0.9031490806109265, 0.9034982134205426, 0.9039861341823551, 0.9028936747300664, 0.902894035218254, 0.9030099321705426, 0.9024293659445367, 0.9035447163967331, 0.903382316468254], "end": "2016-01-31 11:53:17.643000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0], "moving_var_accuracy_valid": [0.019497414328810053, 0.03935340285706908, 0.05725935744200359, 0.07234915607813995, 0.0832134606725699, 0.09095389996871223, 0.09631881184753943, 0.09930855200711675, 0.10013816293058952, 0.09956065022599041, 0.09790736105961369, 0.09518243844746156, 0.09182737263452223, 0.08809946191000836, 0.0839522316434724, 0.07957266864077273, 0.07485868209774775, 0.07010317525133397, 0.0653636871971236, 0.06069663374073245, 0.05630994972039144, 0.05208052130893981, 0.04804693112391507, 0.044228127353852606, 0.040656647149394626, 0.03727639701841628, 0.03411638346336187, 0.031205846695938066, 0.028570850139062187, 0.026063004153204186, 0.023760916735368603, 0.021639351355268333, 0.019687850640570572, 0.01790510366685911, 0.016296664656580674, 0.014833666244353654, 0.013486297118049731, 0.0122671174530165, 0.011135857695487982, 0.010120964009580515, 0.009184765819194555, 0.008346969029701678, 0.007574118510824981, 0.006870989702936988, 0.006219880032625398, 0.005634265251031655, 0.005106166334705626, 0.0046273504265620345, 0.00419389494081757, 0.0037965299253915293, 0.0034368926480754973, 0.0031060260360411093, 0.0028051329576870856, 0.00253823202274945, 0.0022924998286543864, 0.002071867001892808, 0.0018748644789225385, 0.0016939665771519424, 0.0015308170059084806, 0.0013826457084132775, 0.0012480362731108058, 0.001127912073291636, 0.00101907967949987, 0.0009206345216470417, 0.0008306882396758907, 0.0007522897500676482, 0.0006800160195662555, 0.0006138582874611184, 0.0005551329837908933, 0.0005020996023485007, 0.00045360711245450564, 0.00040923299969086025, 0.00037049976370125576, 0.0003354328949552662, 0.0003030586241833372, 0.0002739389677871286, 0.00024911396308273305, 0.0002243739789845922, 0.00020307809896475095, 0.00018454152005944677, 0.00016626855187287033, 0.00014984712762104193, 0.00013521172644369254, 0.0001225027562459793, 0.00011028687356864759, 9.982962249431107e-05, 9.03606943750839e-05, 8.14242760966496e-05, 7.357728892416773e-05, 6.624405588953302e-05, 5.966682226881337e-05, 5.38725298701481e-05, 4.848722070920138e-05, 4.374710855225591e-05, 3.937784252475064e-05, 3.577900813380338e-05, 3.2272731504709e-05, 2.910347394350973e-05, 2.641990633400993e-05, 2.385576769888037e-05, 2.1484157849106388e-05, 1.942363562311048e-05, 1.748483372582339e-05, 1.573950335127672e-05, 1.4285680197068225e-05, 1.2899935533893744e-05, 1.1674056717421236e-05, 1.0624937738566753e-05, 9.601089110102174e-06, 8.703563979360191e-06, 7.834180828135485e-06, 7.082322307978991e-06, 6.383395101607428e-06, 5.761064399576235e-06, 5.2350160294561945e-06, 4.768150830332369e-06, 4.297542526193629e-06, 3.977963959289018e-06, 3.648871004551961e-06, 3.29986968075909e-06, 3.017950406757028e-06, 3.0465646407225884e-06, 2.7431383472773583e-06, 2.4820306513472996e-06, 2.234934645963966e-06, 2.047458125399969e-06, 2.049595326713733e-06, 1.910170355024337e-06, 1.7289924464403623e-06, 1.5840070971267943e-06, 1.4315546346466505e-06, 1.3119315582586235e-06, 1.1879214161937633e-06, 1.091485380435901e-06, 9.895888869969977e-07, 1.0015128044731288e-06, 9.149538569859551e-07, 8.251854414078072e-07, 7.919664541322936e-07, 7.280222608198994e-07, 6.566570928174133e-07, 6.254420355986942e-07, 5.663618175528452e-07, 6.537241695802075e-07, 6.124792403817136e-07, 5.531069200205214e-07, 5.028210338737625e-07, 4.66773374749401e-07, 4.5569614372916124e-07, 4.181590387632786e-07, 4.240783760118538e-07, 4.6324686991991054e-07, 4.1844342790351e-07, 4.0846086228580024e-07, 4.619455681531493e-07, 4.4349210682652223e-07, 4.6311704060213004e-07, 4.176295811602877e-07, 3.8568364425059277e-07, 3.503998971503738e-07, 4.583057101124167e-07, 4.1501462808363737e-07, 3.7557015135106904e-07, 3.4278030318386856e-07, 3.2683066542738874e-07, 2.946120744232671e-07, 2.8266694805340947e-07, 3.054257567125039e-07, 3.0266484846203254e-07, 2.7349364046195596e-07, 3.099513187482568e-07, 2.8500398679241513e-07], "accuracy_test": 0.8289202008928571, "start": "2016-01-29 11:31:18.176000", "learning_rate_per_epoch": [0.00031224655685946345, 0.0002993576053995639, 0.0002870007010642439, 0.00027515387046150863, 0.0002637960424181074, 0.00025290704797953367, 0.00024246753309853375, 0.00023245892953127623, 0.00022286346938926727, 0.00021366409782785922, 0.0002048444584943354, 0.00019638886442407966, 0.0001882823125924915, 0.00018051038205157965, 0.00017305926303379238, 0.00016591571329627186, 0.00015906702901702374, 0.0001525010447949171, 0.0001462060899939388, 0.00014017098874319345, 0.0001343850017292425, 0.00012883784074801952, 0.00012351965415291488, 0.00011842099775094539, 0.00011353280569892377, 0.00010884639050345868, 0.00010435342119308189, 0.00010004590876633301, 9.591620619175956e-05, 9.195696475217119e-05, 8.816115587251261e-05, 8.452203474007547e-05, 8.103312575258315e-05, 7.768822979414836e-05, 7.448140968335792e-05, 7.140696106944233e-05, 6.845941970823333e-05, 6.56335469102487e-05, 6.292432226473466e-05, 6.0326925449771807e-05, 5.783674350823276e-05, 5.5449352657888085e-05, 5.316051101544872e-05, 5.0966147682629526e-05, 4.88623627461493e-05, 4.6845416363794357e-05, 4.491172876441851e-05, 4.305785842007026e-05, 4.128051295992918e-05, 3.957653098041192e-05, 3.794288568315096e-05, 3.6376677599037066e-05, 3.487511639832519e-05, 3.343553908052854e-05, 3.205538450856693e-05, 3.0732200684724376e-05, 2.9463633836712688e-05, 2.824743205565028e-05, 2.7081432563136332e-05, 2.5963563530240208e-05, 2.4891836801543832e-05, 2.3864349714131095e-05, 2.2879274183651432e-05, 2.1934862161288038e-05, 2.1029432900832035e-05, 2.0161378415650688e-05, 1.9329154383740388e-05, 1.8531283785705455e-05, 1.7766347809811123e-05, 1.7032987670972943e-05, 1.6329899153788574e-05, 1.565583261253778e-05, 1.5009589333203621e-05, 1.4390022442967165e-05, 1.3796029634249862e-05, 1.3226555893197656e-05, 1.2680588952207472e-05, 1.215715838043252e-05, 1.1655333764792886e-05, 1.1174223800480831e-05, 1.0712973562476691e-05, 1.0270762686559465e-05, 9.846805369306821e-06, 9.440347639610991e-06, 9.050668268173467e-06, 8.6770742200315e-06, 8.318901564052794e-06, 7.975512744451407e-06, 7.646298399777152e-06, 7.330673724936787e-06, 7.02807756169932e-06, 6.737971943948651e-06, 6.459841188188875e-06, 6.19319143879693e-06, 5.937548394285841e-06, 5.692457762052072e-06, 5.457483894133475e-06, 5.2322093324619345e-06, 5.016233899368672e-06, 4.809173333342187e-06, 4.610659743775614e-06, 4.420340701472014e-06, 4.2378774196549784e-06, 4.062946118210675e-06, 3.895235295203747e-06, 3.7344473184930393e-06, 3.5802963793685194e-06, 3.4325084925512783e-06, 3.290821041446179e-06, 3.1549823233945062e-06, 3.0247506401792634e-06, 2.899894752772525e-06, 2.7801927444670582e-06, 2.6654317935026484e-06, 2.555407945692423e-06, 2.4499256596755004e-06, 2.3487973521696404e-06, 2.2518433979712427e-06, 2.1588916752079967e-06, 2.06977665584418e-06, 1.984340315175359e-06, 1.9024305402126629e-06, 1.8239018118038075e-06, 1.7486146361989086e-06, 1.6764352039899677e-06, 1.6072351627371972e-06, 1.5408916169690201e-06, 1.4772865597478813e-06, 1.4163069863570854e-06, 1.3578445532402839e-06, 1.301795350627799e-06, 1.2480596751629491e-06, 1.1965421435888857e-06, 1.147151124314405e-06, 1.0997989647876238e-06, 1.0544013093749527e-06, 1.0108776677952847e-06, 9.691506193121313e-07, 9.291459264204605e-07, 8.907925348466961e-07, 8.540223461750429e-07, 8.187699336303922e-07, 7.849726557651593e-07, 7.525704859290272e-07, 7.215058417386899e-07, 6.917234713910148e-07, 6.631704536630423e-07, 6.357960273817298e-07, 6.095515914239513e-07, 5.843904773428221e-07, 5.602679493676987e-07, 5.371411475607601e-07, 5.1496897413017e-07, 4.937120365866576e-07, 4.7333253405668074e-07, 4.5379425728242495e-07, 4.3506250335667573e-07, 4.1710396203598066e-07, 3.998867157406494e-07, 3.8338015428962535e-07, 3.675549464787764e-07, 3.5238298323747586e-07, 3.3783729236347426e-07, 3.2389201010118995e-07, 3.105223527199996e-07, 2.9770458809252887e-07, 2.854159220078145e-07, 2.736344981713046e-07, 2.623393982048583e-07, 2.5151052795990836e-07, 2.411286459391704e-07, 2.3117532066407875e-07], "accuracy_train_first": 0.46613447795542634, "accuracy_train_last": 0.903382316468254, "batch_size_eval": 1024, "accuracy_train_std": [0.016134893506879834, 0.016400320502929035, 0.01601832808637895, 0.015167678958204582, 0.014466762022855163, 0.014510136807698769, 0.015695732001632132, 0.015778163404891646, 0.016971318348713457, 0.016309451036280294, 0.018516063595983196, 0.016690414495516857, 0.017088121523722292, 0.01517637484392058, 0.016693163442097463, 0.01627515662214656, 0.015486796853511649, 0.01531995284786929, 0.014939094547046958, 0.014059861974212896, 0.014647438328798605, 0.014455037995607772, 0.014227355187803563, 0.014213455761223903, 0.014927618659339902, 0.014903378170453474, 0.014531993515470156, 0.013843236229013687, 0.013100537597899028, 0.012904883255790568, 0.012880100843961509, 0.013406798611773326, 0.012643040160752822, 0.01292643356793322, 0.012073461051368216, 0.01240170450104273, 0.013043774909186885, 0.012303413112635752, 0.012616904969938598, 0.011264926818976444, 0.011596043006375784, 0.010601229177968245, 0.011596212967997625, 0.011386960673595013, 0.010994794024197547, 0.01142551922398183, 0.01006340052990871, 0.00914831847556715, 0.009350486405005317, 0.009600353529600881, 0.008089971830870353, 0.008504774358543044, 0.008688602209912969, 0.008924530675577441, 0.00902681471516264, 0.009537365642532596, 0.00900933208388573, 0.009209651925824892, 0.008572185528900975, 0.008274911161343544, 0.008989899869847153, 0.009445961263522845, 0.008991523745843376, 0.008767226380639533, 0.009489209414493372, 0.009312084710460244, 0.008239258994442562, 0.009289369714978082, 0.008795730591447365, 0.008490851900600602, 0.008908111502447427, 0.008531119201109858, 0.008952282707850237, 0.008528783499623705, 0.008896073905105578, 0.0091267930391491, 0.00913392237023821, 0.008561273379796976, 0.008761085447539455, 0.008621514716635529, 0.008338818402967657, 0.008191545048108973, 0.008569739268896896, 0.008491364362430994, 0.009023246362432628, 0.008359274550376536, 0.00912101344964991, 0.008837820991669179, 0.008455086376597473, 0.009186511673930793, 0.008885533248925227, 0.008971389567142664, 0.008189802035324918, 0.009011678758316469, 0.008047940472585011, 0.009197352036093833, 0.008649797553691527, 0.008579726834640488, 0.008595160593824968, 0.008948819852650598, 0.008100484332914598, 0.008668099336009085, 0.008446817572010403, 0.008050847800459698, 0.008695959457504676, 0.00856569369532819, 0.008345446710358202, 0.008061022620288892, 0.008497662341780947, 0.008092718899437922, 0.008264747539692965, 0.008084462126441023, 0.008447089317166919, 0.008351370766958847, 0.00823112792324075, 0.00822865707113174, 0.00831231937142786, 0.007871665216510238, 0.007852667243379248, 0.008354393210365345, 0.008263786660709609, 0.008198887230469637, 0.007783424293811105, 0.008057704858713575, 0.008043207722258326, 0.008268851493578823, 0.007976631468882832, 0.00838033252754488, 0.008146833110175318, 0.008287836060009641, 0.008307635185057794, 0.008119282009767328, 0.008068251478551587, 0.007979373599441961, 0.008346153503327175, 0.007858208013314055, 0.007830398674378482, 0.008033259066562312, 0.008036965420979153, 0.008304641989809897, 0.00786022541741829, 0.008257145095625336, 0.00821475438547526, 0.008038815081928427, 0.007763313968396454, 0.008176916808225253, 0.00767294553817192, 0.007468502806714701, 0.007947888884550183, 0.007983878383360845, 0.008182341029939602, 0.007858420255468424, 0.008448865567361638, 0.008195115172695121, 0.007700198637017192, 0.007650900290427449, 0.008205024098387867, 0.007991623667955663, 0.00795162922333976, 0.007882027588644114, 0.008053550139047141, 0.008111766513008064, 0.008045313545775843, 0.008168736478779238, 0.007887079330164018, 0.007908538000410767, 0.00769970470130185, 0.007580575496827752, 0.007950379872813476, 0.007765148176497251, 0.00754750354621116, 0.008222600558919208], "accuracy_test_std": 0.009042514183102748, "error_valid": [0.5345561935240963, 0.46123017460466864, 0.41160638648343373, 0.3740484398531627, 0.35843373493975905, 0.33958343138177716, 0.3189520778426205, 0.30521843232304224, 0.29648084525602414, 0.2838767178087349, 0.2715667356927711, 0.2647307981927711, 0.25521960890436746, 0.24454801628388556, 0.23850480045180722, 0.2321262589420181, 0.23240128482680722, 0.2290745011295181, 0.22698901073042166, 0.225830078125, 0.21878970961972888, 0.21707043015813254, 0.21514819041792166, 0.21329654555722888, 0.21023449265813254, 0.21049922345632532, 0.20962414109563254, 0.20648119823042166, 0.20018354668674698, 0.2039986116340362, 0.2019234163215362, 0.2010689241340362, 0.20034679734563254, 0.19860692771084332, 0.1945477221385542, 0.19199454066265065, 0.19185188017695776, 0.18891189759036142, 0.19047822147966864, 0.18667345161897586, 0.1874367587537651, 0.1836319888930723, 0.1843644107680723, 0.18339814335466864, 0.18550422392695776, 0.18339814335466864, 0.1816788638930723, 0.18071259647966864, 0.17959337349397586, 0.1801831348832832, 0.17934923286897586, 0.18083466679216864, 0.1811905826430723, 0.17824030496987953, 0.17982721903237953, 0.1785756306475903, 0.1767445759600903, 0.17776231880647586, 0.1771313770707832, 0.17724315229668675, 0.17751817818147586, 0.17604303934487953, 0.17590037885918675, 0.1756665333207832, 0.1763989551957832, 0.17356045274849397, 0.17431346479668675, 0.17494440653237953, 0.17358104292168675, 0.1732251270707832, 0.17358104292168675, 0.1742016895707832, 0.1722485645707832, 0.17199412885918675, 0.1726147755082832, 0.1722279743975903, 0.1701527790850903, 0.17358104292168675, 0.17126170698418675, 0.1700307087725903, 0.17260448042168675, 0.1723706348832832, 0.1717602833207832, 0.17052928510918675, 0.1726147755082832, 0.17065135542168675, 0.17052928510918675, 0.17162791792168675, 0.1707631306475903, 0.17187205854668675, 0.1716176228350903, 0.1708852009600903, 0.1719838337725903, 0.17101756635918675, 0.1717602833207832, 0.17004100385918675, 0.17089549604668675, 0.17089549604668675, 0.1700307087725903, 0.17052928510918675, 0.1717602833207832, 0.1704175098832832, 0.17150584760918675, 0.17113963667168675, 0.1701527790850903, 0.1718823536332832, 0.1704175098832832, 0.1700307087725903, 0.17040721479668675, 0.17016307417168675, 0.17101756635918675, 0.1715161426957832, 0.1706616505082832, 0.17052928510918675, 0.17016307417168675, 0.17004100385918675, 0.17101756635918675, 0.16967479292168675, 0.16979686323418675, 0.17016307417168675, 0.1712720020707832, 0.16869823042168675, 0.1705395801957832, 0.1700512989457832, 0.17028514448418675, 0.17101756635918675, 0.1689320759600903, 0.1711499317582832, 0.1700512989457832, 0.1709057911332832, 0.1706616505082832, 0.16991893354668675, 0.1706616505082832, 0.1709057911332832, 0.1701733692582832, 0.1693188770707832, 0.1699292286332832, 0.1704175098832832, 0.16955272260918675, 0.1698071583207832, 0.1700512989457832, 0.1707837208207832, 0.1700307087725903, 0.16894237104668675, 0.1695630176957832, 0.1701733692582832, 0.1698071583207832, 0.1704175098832832, 0.16943065229668675, 0.1702954395707832, 0.1692982868975903, 0.1709057911332832, 0.16991893354668675, 0.1694409473832832, 0.1689526661332832, 0.1693188770707832, 0.1706616505082832, 0.1698071583207832, 0.1695630176957832, 0.1700512989457832, 0.17113963667168675, 0.1701733692582832, 0.1701733692582832, 0.1698071583207832, 0.1695630176957832, 0.17004100385918675, 0.1704175098832832, 0.17077342573418675, 0.17065135542168675, 0.17004100385918675, 0.1692982868975903, 0.16979686323418675], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.041278090277126214, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.00032569042264721167, "optimization": "nesterov_momentum", "nb_data_augmentation": 4, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 9.183833752725553e-06, "rotation_range": [0, 0], "momentum": 0.9481833851976259}, "accuracy_valid_max": 0.8313017695783133, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8302031367658133, "accuracy_valid_std": [0.01905679955319215, 0.012602628401088719, 0.01331708383225752, 0.013001217572006426, 0.012320168964889636, 0.014218978546783084, 0.0076166579325330115, 0.007999962322222448, 0.010686367492491284, 0.009785266470939441, 0.00901072154478002, 0.011578253733576646, 0.010714531778455336, 0.012245379925983622, 0.011848168103769156, 0.012044398212226849, 0.012000085184612905, 0.010634302029200517, 0.01313221409755699, 0.014183288621365052, 0.012375048830591515, 0.011952033348197099, 0.015618367517192238, 0.01328350025046642, 0.01319125656185522, 0.01566805992502419, 0.012140303897242861, 0.015094939726897717, 0.010730460390010254, 0.013496131158045934, 0.011247642762065333, 0.012054157145212758, 0.015829464756929253, 0.013360978508757905, 0.013442652755157236, 0.012039796362358067, 0.011904300226896296, 0.010636555483140643, 0.011929105072589869, 0.013187497718265608, 0.010489012895236233, 0.009314820081652695, 0.011096319079217222, 0.012143358058073818, 0.01264934634439171, 0.012558389380328733, 0.009698855664596254, 0.010240414341614379, 0.00907062363908767, 0.010346859515358362, 0.010615471090544266, 0.01183241632246339, 0.009713061435551012, 0.009775438498599406, 0.011461687433792934, 0.008732768186239375, 0.011401362080308433, 0.00843021738340799, 0.008230942724454736, 0.008720820763290022, 0.011025758191300312, 0.010595586419980086, 0.009256666066521814, 0.008495922886546372, 0.008303690923469921, 0.00851492512757781, 0.010906457900973361, 0.010239128319865183, 0.008932315688485869, 0.010165148138586651, 0.009894749899340029, 0.010533055997461302, 0.011107284091484697, 0.008980873921949569, 0.010910093309393007, 0.00942595311711004, 0.00992593137264159, 0.009563930882997186, 0.010628146832854433, 0.00966641536817874, 0.008985220801359007, 0.010084085235224607, 0.010697581243213043, 0.01045834201222586, 0.01077817763216228, 0.011966134318224037, 0.011857898275284415, 0.010726636155561403, 0.009946989637687191, 0.010883915849660681, 0.009359254131672845, 0.009386836008019714, 0.01114662675468618, 0.010361140489122149, 0.0108963195106867, 0.011862802716202553, 0.010850742991112999, 0.011553105629218968, 0.011666898119437824, 0.01161411665209867, 0.010929091207244833, 0.01173536133255483, 0.011100042353909354, 0.010399216889114805, 0.008913508135760329, 0.011446459782101633, 0.010604071681239122, 0.010719013862725562, 0.01061738954663781, 0.01026045534718754, 0.010700737806729648, 0.010142905599973277, 0.010034970381563365, 0.010401193429082098, 0.010248830438275767, 0.009645847420689909, 0.009756735350574324, 0.009361148250354364, 0.009458575871673607, 0.009374027291256162, 0.010166560962755493, 0.009784251797074891, 0.010410404598012093, 0.009827613418897135, 0.009859824230380992, 0.009521760955738019, 0.009113407422380534, 0.009794134861963035, 0.009431468124318559, 0.009870095463315236, 0.010634772995797102, 0.010181659111667078, 0.010362256065823588, 0.00996624970025177, 0.010856976998072233, 0.010120968956726916, 0.010050344556605289, 0.01028448423626648, 0.009991718043190745, 0.010721184950524772, 0.010984637545251315, 0.009705000144076084, 0.009263379056203634, 0.009652461849690059, 0.010073566779714726, 0.010590179584796099, 0.009304407333638878, 0.010214700413198831, 0.01052691156637306, 0.009977906950297995, 0.009597337882055903, 0.009711814605244257, 0.010122948650007341, 0.009834890572098933, 0.010039191685210025, 0.010073744999086667, 0.009320533942374943, 0.009864118404086766, 0.00868844764864828, 0.010312960159646264, 0.010143889942552425, 0.009975756560073488, 0.009672396521457865, 0.010185191874755499, 0.010536293979831553, 0.010009741746732383, 0.009870459246532506, 0.009621322366209797, 0.00914241356242368, 0.00950893096478847, 0.009060646450451515, 0.009596210808976807], "accuracy_valid": [0.4654438064759036, 0.5387698253953314, 0.5883936135165663, 0.6259515601468373, 0.641566265060241, 0.6604165686182228, 0.6810479221573795, 0.6947815676769578, 0.7035191547439759, 0.7161232821912651, 0.7284332643072289, 0.7352692018072289, 0.7447803910956325, 0.7554519837161144, 0.7614951995481928, 0.7678737410579819, 0.7675987151731928, 0.7709254988704819, 0.7730109892695783, 0.774169921875, 0.7812102903802711, 0.7829295698418675, 0.7848518095820783, 0.7867034544427711, 0.7897655073418675, 0.7895007765436747, 0.7903758589043675, 0.7935188017695783, 0.799816453313253, 0.7960013883659638, 0.7980765836784638, 0.7989310758659638, 0.7996532026543675, 0.8013930722891567, 0.8054522778614458, 0.8080054593373494, 0.8081481198230422, 0.8110881024096386, 0.8095217785203314, 0.8133265483810241, 0.8125632412462349, 0.8163680111069277, 0.8156355892319277, 0.8166018566453314, 0.8144957760730422, 0.8166018566453314, 0.8183211361069277, 0.8192874035203314, 0.8204066265060241, 0.8198168651167168, 0.8206507671310241, 0.8191653332078314, 0.8188094173569277, 0.8217596950301205, 0.8201727809676205, 0.8214243693524097, 0.8232554240399097, 0.8222376811935241, 0.8228686229292168, 0.8227568477033133, 0.8224818218185241, 0.8239569606551205, 0.8240996211408133, 0.8243334666792168, 0.8236010448042168, 0.826439547251506, 0.8256865352033133, 0.8250555934676205, 0.8264189570783133, 0.8267748729292168, 0.8264189570783133, 0.8257983104292168, 0.8277514354292168, 0.8280058711408133, 0.8273852244917168, 0.8277720256024097, 0.8298472209149097, 0.8264189570783133, 0.8287382930158133, 0.8299692912274097, 0.8273955195783133, 0.8276293651167168, 0.8282397166792168, 0.8294707148908133, 0.8273852244917168, 0.8293486445783133, 0.8294707148908133, 0.8283720820783133, 0.8292368693524097, 0.8281279414533133, 0.8283823771649097, 0.8291147990399097, 0.8280161662274097, 0.8289824336408133, 0.8282397166792168, 0.8299589961408133, 0.8291045039533133, 0.8291045039533133, 0.8299692912274097, 0.8294707148908133, 0.8282397166792168, 0.8295824901167168, 0.8284941523908133, 0.8288603633283133, 0.8298472209149097, 0.8281176463667168, 0.8295824901167168, 0.8299692912274097, 0.8295927852033133, 0.8298369258283133, 0.8289824336408133, 0.8284838573042168, 0.8293383494917168, 0.8294707148908133, 0.8298369258283133, 0.8299589961408133, 0.8289824336408133, 0.8303252070783133, 0.8302031367658133, 0.8298369258283133, 0.8287279979292168, 0.8313017695783133, 0.8294604198042168, 0.8299487010542168, 0.8297148555158133, 0.8289824336408133, 0.8310679240399097, 0.8288500682417168, 0.8299487010542168, 0.8290942088667168, 0.8293383494917168, 0.8300810664533133, 0.8293383494917168, 0.8290942088667168, 0.8298266307417168, 0.8306811229292168, 0.8300707713667168, 0.8295824901167168, 0.8304472773908133, 0.8301928416792168, 0.8299487010542168, 0.8292162791792168, 0.8299692912274097, 0.8310576289533133, 0.8304369823042168, 0.8298266307417168, 0.8301928416792168, 0.8295824901167168, 0.8305693477033133, 0.8297045604292168, 0.8307017131024097, 0.8290942088667168, 0.8300810664533133, 0.8305590526167168, 0.8310473338667168, 0.8306811229292168, 0.8293383494917168, 0.8301928416792168, 0.8304369823042168, 0.8299487010542168, 0.8288603633283133, 0.8298266307417168, 0.8298266307417168, 0.8301928416792168, 0.8304369823042168, 0.8299589961408133, 0.8295824901167168, 0.8292265742658133, 0.8293486445783133, 0.8299589961408133, 0.8307017131024097, 0.8302031367658133], "seed": 219624902, "model": "residualv3", "loss_std": [0.29865071177482605, 0.23143135011196136, 0.24142667651176453, 0.24401691555976868, 0.24565249681472778, 0.24521112442016602, 0.24368761479854584, 0.24046730995178223, 0.24307675659656525, 0.24018771946430206, 0.2408909648656845, 0.2383585125207901, 0.23644058406352997, 0.23380567133426666, 0.23435433208942413, 0.23386633396148682, 0.22957906126976013, 0.2292303591966629, 0.227754607796669, 0.22672398388385773, 0.22353829443454742, 0.22154979407787323, 0.2216627150774002, 0.218809574842453, 0.21863272786140442, 0.21600086987018585, 0.2165026068687439, 0.2139585316181183, 0.21307477355003357, 0.21022582054138184, 0.20974405109882355, 0.20794342458248138, 0.20882661640644073, 0.2074057161808014, 0.20602932572364807, 0.20513750612735748, 0.20237381756305695, 0.20302146673202515, 0.2027682214975357, 0.1998458355665207, 0.19897979497909546, 0.1978965550661087, 0.19786256551742554, 0.19625528156757355, 0.19581495225429535, 0.19408908486366272, 0.1936749815940857, 0.19301548600196838, 0.1921321451663971, 0.19239579141139984, 0.19177481532096863, 0.1906081885099411, 0.19113005697727203, 0.19161562621593475, 0.1872835010290146, 0.18942324817180634, 0.18839259445667267, 0.1859980970621109, 0.18651361763477325, 0.18622566759586334, 0.18670722842216492, 0.18629337847232819, 0.18556076288223267, 0.1844295710325241, 0.1851290762424469, 0.1824416220188141, 0.18253007531166077, 0.18359138071537018, 0.18161268532276154, 0.1816563606262207, 0.18172094225883484, 0.18114565312862396, 0.18034134805202484, 0.1801401972770691, 0.18183384835720062, 0.18073219060897827, 0.18059928715229034, 0.17970487475395203, 0.18004226684570312, 0.1788201630115509, 0.17802734673023224, 0.17798760533332825, 0.17862297594547272, 0.1786230504512787, 0.1771063357591629, 0.17768554389476776, 0.17739026248455048, 0.17616163194179535, 0.17762351036071777, 0.1786833107471466, 0.17610186338424683, 0.17787475883960724, 0.17735227942466736, 0.1761271357536316, 0.17787496745586395, 0.17448052763938904, 0.1753472089767456, 0.17455145716667175, 0.177485853433609, 0.1763235628604889, 0.17528609931468964, 0.17711767554283142, 0.1754324734210968, 0.1765097975730896, 0.1750745177268982, 0.17631161212921143, 0.17662584781646729, 0.17502553761005402, 0.17322856187820435, 0.1753072291612625, 0.17475810647010803, 0.17484721541404724, 0.17389285564422607, 0.1757916510105133, 0.17316171526908875, 0.17338602244853973, 0.17472463846206665, 0.174237921833992, 0.17346204817295074, 0.1729620397090912, 0.17400972545146942, 0.17291942238807678, 0.1731196492910385, 0.17426331341266632, 0.17354929447174072, 0.1749182492494583, 0.17443890869617462, 0.1729169338941574, 0.17434875667095184, 0.17401622235774994, 0.17423543334007263, 0.173092320561409, 0.17280733585357666, 0.17423708736896515, 0.1733814775943756, 0.1724206954240799, 0.1728736311197281, 0.17301960289478302, 0.17136752605438232, 0.17397506535053253, 0.17330045998096466, 0.1734369695186615, 0.17270247638225555, 0.17159150540828705, 0.17442628741264343, 0.1730269342660904, 0.17152643203735352, 0.17408891022205353, 0.17274236679077148, 0.17317461967468262, 0.1718727946281433, 0.1723122000694275, 0.17296291887760162, 0.1717732697725296, 0.17284835875034332, 0.1723131537437439, 0.17261220514774323, 0.1719876527786255, 0.172039195895195, 0.1728384643793106, 0.173801451921463, 0.17199885845184326, 0.17153945565223694, 0.1720568984746933, 0.17277862131595612, 0.17214427888393402, 0.1722966432571411, 0.17326369881629944, 0.17327885329723358, 0.17091350257396698, 0.1731541007757187, 0.1728389412164688]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:25 2016", "state": "available"}], "summary": "d00b7afabc0b90cf36a88fa41605f1ba"}