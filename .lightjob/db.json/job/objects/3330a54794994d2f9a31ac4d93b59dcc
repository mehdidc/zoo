{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 64, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.013081402864175407, 0.0157254583135051, 0.01805608858197915, 0.017826903369160735, 0.01625851267607889, 0.014954518092504837, 0.01635244111616161, 0.015245653256960071, 0.014524790355479832, 0.01389340165428794, 0.014051318398043711, 0.013688630952735589, 0.01538554415928333, 0.016143157790507304, 0.01352168779679228, 0.01635750670405088, 0.016866899570891436, 0.018935445265670897, 0.018914189808641912, 0.02337403286430958, 0.02086445380070356, 0.019815610566007207, 0.022958040471864883, 0.023946106759581563, 0.021855690070541438, 0.023732906571699502, 0.026874442274428808, 0.02909383067230622, 0.029655084605579092, 0.028337164895848498, 0.029559804578771425, 0.028532564393684318, 0.02808117432679294, 0.026514422509632388, 0.025426887777214382, 0.023933899065100402, 0.02234911484998304, 0.02107553818577176, 0.021191596436779456, 0.0229125818997739, 0.023733010244171748, 0.025472415737357812, 0.026670959249970795, 0.026896271721426467, 0.025761162641207012, 0.02518355380538181, 0.025221469050685515, 0.025282838895246865, 0.02434974667667189, 0.02458364317466813, 0.02428225972973136, 0.02365200203407765, 0.020559369470352185, 0.019510100341771003, 0.018058624652826472, 0.01822402652015783, 0.018871999294142038, 0.01896081836358598, 0.019352572233284147, 0.017753066114142714, 0.017156504224178387, 0.016850651238123782, 0.0162602759627062, 0.016548332897589696, 0.017248490612031795, 0.016905920393258018, 0.016867088148997626, 0.01677851632056152, 0.01641343698996914, 0.016237874610601395, 0.014559127807584218, 0.014878198191830408, 0.014313974707213049, 0.013839741019580078, 0.013795226630841314, 0.013483405660420607, 0.014241849602381623, 0.014095957426150463, 0.014464451762861328, 0.01463650077584376, 0.015385897876917385, 0.01556005226422502, 0.01500144470052649, 0.015503395552458079, 0.015085479546246413, 0.01517592367647509, 0.014975326499881673, 0.014891562190920572, 0.01522705577464163, 0.01553305874813036, 0.015096900168589765, 0.015002114874042355, 0.014842340755821894, 0.015012929249919923, 0.015257044432873831, 0.015012929249919923, 0.01478080816737003, 0.015564048629428862, 0.015520876507835095, 0.0162908587763306, 0.01627632187016875, 0.016370452160415577, 0.016470378578171498, 0.016786911941435948, 0.016820913593138278, 0.016955131913964656, 0.016506141785025217, 0.015697742390420568, 0.01578016836172422, 0.015750810969906208, 0.016462855653316497, 0.016532094899262485, 0.016397840855346624, 0.01664486737532968, 0.016522666071115397, 0.01660901914764405, 0.0165305744842912, 0.01602422532889234, 0.016040132417810476, 0.01680280939639847, 0.01680280939639847, 0.016742219719358253, 0.0168416086297803, 0.016825146827339033, 0.016010775135543286, 0.01607939306032512, 0.016019972926182464, 0.016019972926182464, 0.016019972926182464, 0.016774235728321218, 0.016774235728321218, 0.016557683947140696, 0.0157959920991298, 0.01582604276510677, 0.016050532641791803, 0.01599018005649758, 0.016010775135543286, 0.016010775135543286, 0.01599018005649758, 0.01599018005649758, 0.01599018005649758, 0.015958571063451145, 0.015958571063451145, 0.016185826221771768, 0.016185826221771768, 0.016140863635591973, 0.016101011394658273, 0.016101011394658273, 0.016101011394658273, 0.016155627999585115, 0.016155627999585115, 0.016155627999585115, 0.016155627999585115, 0.01612320761102518, 0.01612320761102518, 0.016101011394658273, 0.016101011394658273, 0.016101011394658273, 0.016101011394658273], "moving_avg_accuracy_train": [0.038886275349529334, 0.07174696721287835, 0.1034923055598814, 0.1362222966089597, 0.16966136724337566, 0.20321058443192108, 0.236343435411025, 0.2685087520016593, 0.2996775854116816, 0.3297703312079073, 0.3582743602006732, 0.38512311278225775, 0.41054249836518925, 0.43469140525163086, 0.45754807178742196, 0.4791211387089012, 0.4994296118858147, 0.5186231661318955, 0.5367784882057215, 0.5538016555292523, 0.5699616324989368, 0.5851240653061676, 0.5993909614183618, 0.6127031370788513, 0.6251258373982824, 0.636943214264305, 0.6480298602151355, 0.658470582232797, 0.668311227324855, 0.6776163813838516, 0.6863723083928918, 0.6946130047176853, 0.7022853977790471, 0.7095602862438057, 0.716468047878746, 0.7229988202930216, 0.7291508469765748, 0.734992157339364, 0.7405585454087221, 0.7458566131235255, 0.7508294871620865, 0.7555747550098775, 0.7600872755002611, 0.7643973348642255, 0.7684926631798978, 0.7723714460151935, 0.7760437121741024, 0.7795068618361679, 0.7827933602974847, 0.7858837423948126, 0.7887837409693309, 0.791547199507826, 0.7941876999162627, 0.7966780465267036, 0.7990402662141957, 0.8012801241269676, 0.8033936524984624, 0.805418988822075, 0.8073278220192787, 0.8091550538908097, 0.8109018330739878, 0.8125506282007436, 0.8141089485767287, 0.8156020816698679, 0.8170342571084552, 0.8183952946162789, 0.8197295103673679, 0.8210139738516721, 0.822232770005403, 0.8233761895199512, 0.8244796718449494, 0.8255239231624386, 0.8265241311195889, 0.8274708212572146, 0.8283995362429734, 0.8292678957646709, 0.8301214989472938, 0.8309362087390262, 0.8317159505277758, 0.8324711605114508, 0.8331950273241392, 0.833874409241273, 0.8345230553476458, 0.8351533037707531, 0.8357345142932256, 0.8362878306979746, 0.8368276681408202, 0.8373391345251047, 0.8378436320983417, 0.8383232565511598, 0.8387595688563151, 0.8391755014190501, 0.8395847179576545, 0.8399669637352556, 0.8403272609767632, 0.8406817193798252, 0.8410170079842475, 0.8413280683234658, 0.8416359244144765, 0.8419292709380528, 0.8422095228021195, 0.8424989518607318, 0.8427757140551495, 0.8430643275598874, 0.8433264048629611, 0.8435575880892896, 0.8437819290346519, 0.8439815467854872, 0.8441751536540961, 0.844360989531073, 0.8445212663739236, 0.8446631903836797, 0.8448002225876982, 0.8449328521665529, 0.8450452433410937, 0.8451580211422279, 0.8452711469072963, 0.845368309798239, 0.8454511421512869, 0.8455419673106968, 0.845616734507737, 0.8456840249850732, 0.8457329606706283, 0.8457723885388276, 0.8458055484713973, 0.8458400427083292, 0.845875737819187, 0.8459055743189682, 0.8459394026151998, 0.8459675229329987, 0.8459881809213987, 0.8460114234085777, 0.8460253662006103, 0.8460402398622492, 0.8460606016041528, 0.8460766020230563, 0.8460956526976887, 0.846117448602477, 0.8461370649167863, 0.8461570447484742, 0.8461727014481838, 0.846189117626732, 0.8462038921874253, 0.8462125389944303, 0.8462226462695444, 0.846231742817147, 0.8462422548587988, 0.8462563659939044, 0.8462667408666901, 0.8462807285498162, 0.8462886671670106, 0.8462934867736761, 0.8463001495684845, 0.8463061460838122, 0.846311542947607, 0.846318725273832, 0.846327514516244, 0.8463354248344147, 0.8463402189719589], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 882085289, "moving_var_accuracy_train": [0.013609281695034718, 0.021966779153173, 0.028839999798747018, 0.035597270645527035, 0.04210108658501546, 0.04802092769219164, 0.053098907249004054, 0.05710048484648526, 0.060133901947112246, 0.06227067189840736, 0.0633559217279506, 0.06350802919183984, 0.06297253274337956, 0.06192380680333183, 0.06043327096915384, 0.05857851881982607, 0.0564325736828402, 0.054104849035931386, 0.05166090560877741, 0.04910290907942402, 0.04654292187242825, 0.043957724002889505, 0.041393850524685846, 0.0388493916595588, 0.03635336384264049, 0.0339748810223189, 0.03168361638603861, 0.029496332833685468, 0.027418244212767538, 0.02545569282004573, 0.023600119858129896, 0.021851289555574093, 0.020195951137608973, 0.018652672043420183, 0.017216859376324528, 0.015879032333635648, 0.014631755991108431, 0.013475668552787442, 0.012406963782756936, 0.011418893098077013, 0.010499569074099865, 0.009652270269215389, 0.008870308812879037, 0.008150467437079199, 0.007486366119489606, 0.006873134114091096, 0.006307190551358792, 0.005784412146459697, 0.005303180581039865, 0.004858816676503244, 0.004448624934442797, 0.004072492768844352, 0.0037279936736225048, 0.003411010742421465, 0.0031201304048470943, 0.0028532700355870517, 0.0026081460516223687, 0.002384249331473833, 0.0021786171958991726, 0.001990804463120302, 0.001819185154441333, 0.0016617333673273255, 0.001517415292142485, 0.0013857387808326848, 0.0012656250411314488, 0.0011557343448976313, 0.0010561820954419537, 0.0009654125038803563, 0.0008822404300714642, 0.0008057830607405669, 0.0007361638138407619, 0.0006723615797833868, 0.0006141291654229706, 0.0005607822488307757, 0.0005124666276706548, 0.00046800639923389353, 0.00042776350485096106, 0.0003909609227685671, 0.00035733680580581306, 0.00032673620430021314, 0.00029877843233279736, 0.0002730546272034743, 0.00024953584042494123, 0.00022815717405591057, 0.00020838170769321417, 0.0001902989683177725, 0.00017389189166827775, 0.00015885708326172785, 0.00014526203514817366, 0.0001328061881750259, 0.00012123888520619302, 0.00011067199575626383, 0.00010111191975984377, 9.231573429430423e-05, 8.425248778501612e-05, 7.695800584202558e-05, 7.027397129212295e-05, 6.411740097462205e-05, 5.855863923211157e-05, 5.347724495494937e-05, 4.8836390425269796e-05, 4.470667400246589e-05, 4.0925382412549806e-05, 3.7582523967348744e-05, 3.444243218569107e-05, 3.147920012434276e-05, 2.8784239849802895e-05, 2.626444108285946e-05, 2.3975349550726397e-05, 2.1888629354199622e-05, 1.993096441596711e-05, 1.8119149795277467e-05, 1.6476235240193274e-05, 1.4986927162858695e-05, 1.3601920431604595e-05, 1.2356197880302257e-05, 1.1235755040772951e-05, 1.019714518308251e-05, 9.239181453177498e-06, 8.389506194096224e-06, 7.60086677846601e-06, 6.8815321756807405e-06, 6.214931269999361e-06, 5.607429154116067e-06, 5.056582468856769e-06, 4.561632893404706e-06, 4.11693687251655e-06, 3.713255135737631e-06, 3.3522288047972886e-06, 3.0241226947755886e-06, 2.725551197660648e-06, 2.4578579967870057e-06, 2.213821810155283e-06, 1.99443066143468e-06, 1.7987190000913291e-06, 1.621151220728031e-06, 1.462302452490756e-06, 1.3203477604314887e-06, 1.1917761824720828e-06, 1.0761913072933744e-06, 9.70778366776209e-07, 8.761259483617173e-07, 7.904779423186804e-07, 7.121030535292441e-07, 6.4181216126839e-07, 5.783756697461272e-07, 5.21532629948728e-07, 4.711714841596037e-07, 4.2502307761150603e-07, 3.842816673634851e-07, 3.464206954137785e-07, 3.1198768334809125e-07, 2.8118845052521975e-07, 2.533932292373674e-07, 2.283160415630131e-07, 2.0594870969671874e-07, 1.8604909576662602e-07, 1.6800734439202215e-07, 1.5141346374594795e-07], "duration": 53960.59939, "accuracy_train": [0.3888627534952934, 0.3674931939830196, 0.3892003506829088, 0.4307922160506645, 0.47061300295311925, 0.5051535391288299, 0.5345390942229604, 0.557996601317368, 0.5801970861018826, 0.6006050433739387, 0.6148106211355666, 0.626761886016519, 0.6393169686115725, 0.652031567229605, 0.6632580706095423, 0.6732787410022149, 0.6822058704780363, 0.6913651543466224, 0.700176386870155, 0.7070101614410299, 0.7154014252260982, 0.721585960571244, 0.7277930264281101, 0.7325127180232558, 0.7369301402731635, 0.7432996060585086, 0.7478096737726099, 0.7524370803917497, 0.7568770331533776, 0.7613627679148209, 0.7651756514742525, 0.7687792716408268, 0.771336935331303, 0.7750342824266334, 0.7786379025932078, 0.7817757720215025, 0.784519087128553, 0.7875639506044666, 0.7906560380329457, 0.7935392225567552, 0.7955853535091363, 0.7982821656399963, 0.8006999599137136, 0.803187869139904, 0.8053506180209486, 0.8072804915328534, 0.809094107604282, 0.8106752087947582, 0.8123718464493356, 0.8136971812707641, 0.8148837281399963, 0.816418326354282, 0.8179522035921927, 0.8190911660206718, 0.8203002434016242, 0.8214388453419158, 0.8224154078419158, 0.8236470157345883, 0.8245073207941122, 0.8256001407345883, 0.8266228457225913, 0.8273897843415466, 0.8281338319605942, 0.8290402795081212, 0.8299238360557402, 0.8306446321866926, 0.8317374521271688, 0.8325741452104098, 0.8332019353889812, 0.833666965150886, 0.8344110127699336, 0.8349221850198413, 0.8355260027339424, 0.8359910324958472, 0.8367579711148025, 0.8370831314599483, 0.8378039275909007, 0.838268596864618, 0.8387336266265227, 0.8392680503645257, 0.8397098286383352, 0.839988846495478, 0.8403608703050018, 0.8408255395787191, 0.840965408995478, 0.8412676783407161, 0.8416862051264304, 0.8419423319836655, 0.8423841102574751, 0.8426398766265227, 0.8426863796027132, 0.8429188944836655, 0.8432676668050941, 0.8434071757336655, 0.8435699361503323, 0.8438718450073828, 0.8440346054240495, 0.8441276113764304, 0.8444066292335732, 0.84456938965024, 0.8447317895787191, 0.8451038133882429, 0.8452665738049095, 0.8456618491025286, 0.8456851005906239, 0.8456382371262459, 0.8458009975429125, 0.8457781065430048, 0.8459176154715762, 0.8460335124238648, 0.8459637579595791, 0.8459405064714839, 0.8460335124238648, 0.8461265183762459, 0.84605676391196, 0.8461730213524363, 0.8462892787929125, 0.846242775816722, 0.8461966333287191, 0.8463593937453857, 0.8462896392811, 0.8462896392811, 0.8461733818406239, 0.8461272393526209, 0.8461039878645257, 0.8461504908407161, 0.8461969938169066, 0.8461741028169989, 0.8462438572812846, 0.8462206057931894, 0.8461741028169989, 0.8462206057931894, 0.8461508513289037, 0.8461741028169989, 0.8462438572812846, 0.8462206057931894, 0.8462671087693798, 0.8463136117455703, 0.8463136117455703, 0.8463368632336655, 0.8463136117455703, 0.8463368632336655, 0.8463368632336655, 0.8462903602574751, 0.8463136117455703, 0.8463136117455703, 0.8463368632336655, 0.846383366209856, 0.8463601147217608, 0.8464066176979512, 0.8463601147217608, 0.8463368632336655, 0.8463601147217608, 0.8463601147217608, 0.8463601147217608, 0.846383366209856, 0.8464066176979512, 0.8464066176979512, 0.846383366209856], "end": "2016-01-30 01:43:56.938000", "learning_rate_per_epoch": [0.0024892352521419525, 0.002360867103561759, 0.0022391187958419323, 0.002123648766428232, 0.0020141336135566235, 0.001910266000777483, 0.0018117547733709216, 0.0017183237941935658, 0.0016297108959406614, 0.0015456677647307515, 0.0014659586595371366, 0.0013903600629419088, 0.0013186600990593433, 0.0012506576022133231, 0.0011861620005220175, 0.0011249923845753074, 0.0010669772746041417, 0.0010119539219886065, 0.0009597681346349418, 0.0009102735202759504, 0.0008633313118480146, 0.000818809843622148, 0.0007765843183733523, 0.0007365363417193294, 0.0006985536310821772, 0.0006625296664424241, 0.0006283634575083852, 0.000595959136262536, 0.0005652258987538517, 0.0005360775394365191, 0.0005084323929622769, 0.00048221286851912737, 0.00045734544983133674, 0.00043376043322496116, 0.00041139169479720294, 0.00039017648668959737, 0.0003700553497765213, 0.00035097182262688875, 0.00033287244150415063, 0.00031570642022415996, 0.00029942565015517175, 0.0002839844673871994, 0.0002693395654205233, 0.0002554499078541994, 0.0002422765246592462, 0.00022978248307481408, 0.00021793274208903313, 0.0002066940942313522, 0.00019603500550147146, 0.00018592560081742704, 0.0001763375330483541, 0.00016724392480682582, 0.0001586192665854469, 0.00015043937310110778, 0.00014268131053540856, 0.00013532332377508283, 0.000128344792756252, 0.00012172613787697628, 0.00011544879816938192, 0.0001094951803679578, 0.00010384858614997938, 9.84931830316782e-05, 9.341395343653858e-05, 8.859665831550956e-05, 8.40277862153016e-05, 7.969452417455614e-05, 7.558472862001508e-05, 7.168687443481758e-05, 6.799002585466951e-05, 6.448382191592827e-05, 6.115843279985711e-05, 5.8004530728794634e-05, 5.5013275414239615e-05, 5.21762776770629e-05, 4.9485581257613376e-05, 4.693364098784514e-05, 4.451330460142344e-05, 4.2217783629894257e-05, 4.004063885076903e-05, 3.7975769373588264e-05, 3.6017383536091074e-05, 3.415999162825756e-05, 3.239838406443596e-05, 3.072762046940625e-05, 2.9143016945454292e-05, 2.764012970146723e-05, 2.621474595798645e-05, 2.486286757630296e-05, 2.358070560148917e-05, 2.2364663891494274e-05, 2.121133184118662e-05, 2.011747710639611e-05, 1.9080031051998958e-05, 1.80960869329283e-05, 1.7162883523269556e-05, 1.6277805116260424e-05, 1.5438368791365065e-05, 1.4642221685789991e-05, 1.3887131899537053e-05, 1.3170981219445821e-05, 1.2491762390709482e-05, 1.1847570021927822e-05, 1.1236598766117822e-05, 1.065713422576664e-05, 1.0107552952831611e-05, 9.58631335379323e-06, 9.091952961171046e-06, 8.623086614534259e-06, 8.178399184544105e-06, 7.756644663459156e-06, 7.356639343925053e-06, 6.977262273721863e-06, 6.617449344048509e-06, 6.2761919252807274e-06, 5.952532774244901e-06, 5.645564669976011e-06, 5.354426320991479e-06, 5.078301910543814e-06, 4.816417003894458e-06, 4.568037638819078e-06, 4.3324666876287665e-06, 4.109044311917387e-06, 3.897143415088067e-06, 3.696170324474224e-06, 3.505561153360759e-06, 3.3247815736103803e-06, 3.153324769300525e-06, 2.990709845107631e-06, 2.836480916812434e-06, 2.690205519684241e-06, 2.5514732442388777e-06, 2.419895508865011e-06, 2.295103058713721e-06, 2.176745965698501e-06, 2.064492491626879e-06, 1.958027951332042e-06, 1.8570536894912948e-06, 1.7612866258787108e-06, 1.6704582321835915e-06, 1.5843137362026027e-06, 1.5026116670924239e-06, 1.4251229458750458e-06, 1.3516303170035826e-06, 1.2819276662412449e-06, 1.2158194522271515e-06, 1.153120479102654e-06, 1.0936547596429591e-06, 1.0372557426308049e-06, 9.837651759880828e-07, 9.330330499324191e-07, 8.849171422298241e-07, 8.392825066039222e-07, 7.960012453622767e-07, 7.549519978056196e-07, 7.16019599167339e-07, 6.790949100832222e-07, 6.440744186875236e-07, 6.108598995524517e-07, 5.793582431579125e-07], "accuracy_valid": [0.38238010636295183, 0.3609560311558735, 0.3807828972138554, 0.4185349797628012, 0.4577416109751506, 0.4907623658697289, 0.5200401214231928, 0.5406508847891567, 0.5570803722703314, 0.5721362010542168, 0.5850859492658133, 0.592552828501506, 0.6013315959149097, 0.6077101374246988, 0.6143225244728916, 0.6216364481362951, 0.6264074854103916, 0.6337419992469879, 0.6397543298192772, 0.6423884012612951, 0.6476889001317772, 0.6519922463290663, 0.6553999199924698, 0.6585737481174698, 0.661046039627259, 0.6620123070406627, 0.6629579842808735, 0.6652670251317772, 0.6662332925451807, 0.6675966561558735, 0.6685629235692772, 0.6697939217808735, 0.6711469903049698, 0.6709131447665663, 0.6728765648531627, 0.6747282097138554, 0.6757253623870482, 0.6774446418486446, 0.679408061935241, 0.6798860480986446, 0.6790212608245482, 0.6791227409638554, 0.679356586502259, 0.6789800804781627, 0.678990375564759, 0.6800993034638554, 0.6802213737763554, 0.6802213737763554, 0.6810861610504518, 0.6812082313629518, 0.6807199501129518, 0.6813405967620482, 0.6827245505459337, 0.6831010565700302, 0.6838540686182228, 0.6839761389307228, 0.6838437735316265, 0.6850747717432228, 0.6849527014307228, 0.6859498541039157, 0.6854615728539157, 0.6860719244164157, 0.6863263601280121, 0.6864484304405121, 0.6861939947289157, 0.6858277837914157, 0.6864381353539157, 0.6858277837914157, 0.6860719244164157, 0.6869367116905121, 0.6883000753012049, 0.6879338643637049, 0.6884324407003012, 0.6884324407003012, 0.6886765813253012, 0.6890427922628012, 0.6894090032003012, 0.6891648625753012, 0.6890427922628012, 0.6890427922628012, 0.6889104268637049, 0.6892766378012049, 0.6889104268637049, 0.6893987081137049, 0.6892766378012049, 0.6896428487387049, 0.6895207784262049, 0.6893987081137049, 0.6892766378012049, 0.6896428487387049, 0.6902532003012049, 0.6896428487387049, 0.6896428487387049, 0.6898869893637049, 0.6900090596762049, 0.6898869893637049, 0.6900090596762049, 0.6893884130271084, 0.6892663427146084, 0.6888898366905121, 0.6890119070030121, 0.6891339773155121, 0.6892560476280121, 0.6895001882530121, 0.6898663991905121, 0.6899884695030121, 0.6898663991905121, 0.6903649755271084, 0.6906091161521084, 0.6903649755271084, 0.6899884695030121, 0.6896222585655121, 0.6892560476280121, 0.6891339773155121, 0.6891339773155121, 0.6891339773155121, 0.6888898366905121, 0.6887677663780121, 0.6888898366905121, 0.6885133306664157, 0.6885133306664157, 0.6883912603539157, 0.6883912603539157, 0.6880250494164157, 0.6882794851280121, 0.6881574148155121, 0.6881574148155121, 0.6881574148155121, 0.6881574148155121, 0.6877809087914157, 0.6877809087914157, 0.6876588384789157, 0.6880353445030121, 0.6879132741905121, 0.6880353445030121, 0.6881574148155121, 0.6882794851280121, 0.6882794851280121, 0.6881574148155121, 0.6881574148155121, 0.6881574148155121, 0.6882794851280121, 0.6882794851280121, 0.6884015554405121, 0.6884015554405121, 0.6886456960655121, 0.6885236257530121, 0.6885236257530121, 0.6885236257530121, 0.6886456960655121, 0.6886456960655121, 0.6886456960655121, 0.6886456960655121, 0.6885236257530121, 0.6885236257530121, 0.6885236257530121, 0.6885236257530121, 0.6885236257530121, 0.6885236257530121], "accuracy_test": 0.6865752551020409, "start": "2016-01-29 10:44:36.339000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0], "accuracy_train_last": 0.846383366209856, "batch_size_eval": 1024, "accuracy_train_std": [0.015007743373254365, 0.01626557458242902, 0.016896680485795238, 0.015118072138763986, 0.014323284372237245, 0.016092947332421784, 0.01600874813308774, 0.015428715959087421, 0.014279200231708297, 0.015658315424397353, 0.015702270885181887, 0.01554800799385874, 0.014824108903189095, 0.01578107104560371, 0.016978663546129216, 0.016051632624004837, 0.016784264343407886, 0.016591939882847358, 0.01637090936166511, 0.016492429614821242, 0.015450456653337729, 0.014424729952560182, 0.013924517557536908, 0.01377451360280777, 0.013610263482131697, 0.013470536408268183, 0.013449404950461498, 0.013010250002650664, 0.012882873962842059, 0.0123380716618704, 0.011965413321100045, 0.011610558174837119, 0.011434493481902832, 0.011196647739818084, 0.011101278118219341, 0.011094711168781897, 0.011175627108608532, 0.01125331005954363, 0.01137339262287282, 0.011114114246155929, 0.011053481335615421, 0.011264857598684093, 0.011220909473766418, 0.011321855874316902, 0.011262366907910038, 0.011302875705959841, 0.011279677058123692, 0.011429880003597606, 0.011913311534045649, 0.011550022845619992, 0.011665256115688543, 0.011192486412956145, 0.011400587640086135, 0.0112388584685177, 0.011165569614439517, 0.011370060175497536, 0.011565210562862634, 0.011458100056577469, 0.011558458297964539, 0.011256528526943947, 0.011117595777238141, 0.011008992016615359, 0.011156305528443024, 0.011565614324454774, 0.011534541319184374, 0.011357902115184925, 0.011198726322097759, 0.011176703739469385, 0.01103730293685252, 0.010695149970074185, 0.010494139333020383, 0.010340140639451795, 0.01038235642469542, 0.010416561138567166, 0.01044767201536611, 0.010337146736608854, 0.010558161535783102, 0.010516099250387085, 0.010613882987290907, 0.010658222270900864, 0.01048944137003722, 0.010545425635003776, 0.010608915995546387, 0.010540271026518612, 0.0104909371382937, 0.010437897827250808, 0.010330527280234914, 0.010360908535637002, 0.010052508165980288, 0.010217058213659756, 0.010459157464452746, 0.010566506014000138, 0.010324313696121601, 0.010067165873884022, 0.00989053060944597, 0.009909749055715608, 0.009877717804336538, 0.009793786893938691, 0.00970913478351864, 0.009735315933769868, 0.009773699878825766, 0.009663578201310639, 0.009701598033235212, 0.009886281600151295, 0.009873711098320415, 0.009844494334341647, 0.009614338948977034, 0.009443213596498676, 0.009462696056762364, 0.009480697823766995, 0.009505592124182819, 0.009394474641673265, 0.009478302493742971, 0.009512871336492973, 0.009485845204940412, 0.009501132994698132, 0.00952690037957204, 0.009600863293882858, 0.009654283126305075, 0.00951063649002758, 0.009542596414528341, 0.009547354212342872, 0.009500159790829136, 0.009570283421387365, 0.009390464337871227, 0.009402864122046188, 0.009429477198499528, 0.009428184130378133, 0.009461711640714328, 0.009425746109942661, 0.009473832602877692, 0.009521618152212348, 0.00953587429260438, 0.009512103510282281, 0.009476099701256055, 0.0093822840822878, 0.009377182424228558, 0.009313517449155154, 0.009306200518345792, 0.00937151520455643, 0.009286660518516407, 0.009242207793967505, 0.00927898705932732, 0.009272255793365903, 0.009242544479623737, 0.009171022278891753, 0.009155814967159575, 0.009159917191009124, 0.009172760453501367, 0.009117159291102735, 0.009050651593432596, 0.009058578159297894, 0.009085707300111982, 0.009085707300111982, 0.009085707300111982, 0.009085245790046855, 0.009077223393547431, 0.009097213256698605, 0.009137579842713843], "accuracy_test_std": 0.015353109161187291, "error_valid": [0.6176198936370482, 0.6390439688441265, 0.6192171027861446, 0.5814650202371988, 0.5422583890248494, 0.5092376341302711, 0.4799598785768072, 0.4593491152108433, 0.44291962772966864, 0.4278637989457832, 0.41491405073418675, 0.40744717149849397, 0.3986684040850903, 0.3922898625753012, 0.3856774755271084, 0.37836355186370485, 0.3735925145896084, 0.36625800075301207, 0.36024567018072284, 0.35761159873870485, 0.35231109986822284, 0.34800775367093373, 0.3446000800075302, 0.3414262518825302, 0.33895396037274095, 0.3379876929593373, 0.3370420157191265, 0.33473297486822284, 0.3337667074548193, 0.3324033438441265, 0.33143707643072284, 0.3302060782191265, 0.3288530096950302, 0.32908685523343373, 0.3271234351468373, 0.3252717902861446, 0.32427463761295183, 0.3225553581513554, 0.32059193806475905, 0.3201139519013554, 0.32097873917545183, 0.3208772590361446, 0.32064341349774095, 0.3210199195218373, 0.32100962443524095, 0.3199006965361446, 0.3197786262236446, 0.3197786262236446, 0.31891383894954817, 0.31879176863704817, 0.31928004988704817, 0.31865940323795183, 0.31727544945406627, 0.3168989434299698, 0.31614593138177716, 0.31602386106927716, 0.3161562264683735, 0.31492522825677716, 0.31504729856927716, 0.31405014589608427, 0.31453842714608427, 0.31392807558358427, 0.31367363987198793, 0.31355156955948793, 0.31380600527108427, 0.31417221620858427, 0.31356186464608427, 0.31417221620858427, 0.31392807558358427, 0.31306328830948793, 0.31169992469879515, 0.31206613563629515, 0.3115675592996988, 0.3115675592996988, 0.3113234186746988, 0.3109572077371988, 0.3105909967996988, 0.3108351374246988, 0.3109572077371988, 0.3109572077371988, 0.31108957313629515, 0.31072336219879515, 0.31108957313629515, 0.31060129188629515, 0.31072336219879515, 0.31035715126129515, 0.31047922157379515, 0.31060129188629515, 0.31072336219879515, 0.31035715126129515, 0.30974679969879515, 0.31035715126129515, 0.31035715126129515, 0.31011301063629515, 0.30999094032379515, 0.31011301063629515, 0.30999094032379515, 0.3106115869728916, 0.3107336572853916, 0.31111016330948793, 0.31098809299698793, 0.31086602268448793, 0.31074395237198793, 0.31049981174698793, 0.31013360080948793, 0.31001153049698793, 0.31013360080948793, 0.3096350244728916, 0.3093908838478916, 0.3096350244728916, 0.31001153049698793, 0.31037774143448793, 0.31074395237198793, 0.31086602268448793, 0.31086602268448793, 0.31086602268448793, 0.31111016330948793, 0.31123223362198793, 0.31111016330948793, 0.31148666933358427, 0.31148666933358427, 0.31160873964608427, 0.31160873964608427, 0.31197495058358427, 0.31172051487198793, 0.31184258518448793, 0.31184258518448793, 0.31184258518448793, 0.31184258518448793, 0.31221909120858427, 0.31221909120858427, 0.31234116152108427, 0.31196465549698793, 0.31208672580948793, 0.31196465549698793, 0.31184258518448793, 0.31172051487198793, 0.31172051487198793, 0.31184258518448793, 0.31184258518448793, 0.31184258518448793, 0.31172051487198793, 0.31172051487198793, 0.31159844455948793, 0.31159844455948793, 0.31135430393448793, 0.31147637424698793, 0.31147637424698793, 0.31147637424698793, 0.31135430393448793, 0.31135430393448793, 0.31135430393448793, 0.31135430393448793, 0.31147637424698793, 0.31147637424698793, 0.31147637424698793, 0.31147637424698793, 0.31147637424698793, 0.31147637424698793], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.6357491460386316, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0026245835053437107, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "l2_decay": 9.733067871234298e-06, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.05156935677498047}, "accuracy_valid_max": 0.6906091161521084, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop', 'santa_sss'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6885236257530121, "loss_train": [1.958040475845337, 1.5942041873931885, 1.4650046825408936, 1.3658283948898315, 1.294691562652588, 1.2395548820495605, 1.1933497190475464, 1.153093934059143, 1.1167657375335693, 1.0838810205459595, 1.0535502433776855, 1.0256232023239136, 0.9996218681335449, 0.9755380749702454, 0.9527991414070129, 0.9316238760948181, 0.9117099642753601, 0.8928558230400085, 0.8749125599861145, 0.8578056693077087, 0.8414710760116577, 0.8258506655693054, 0.8110297322273254, 0.7968999147415161, 0.7833070755004883, 0.7703235149383545, 0.7579285502433777, 0.7460722327232361, 0.7347162961959839, 0.7238112092018127, 0.7134323120117188, 0.7035133242607117, 0.6939790844917297, 0.6848645806312561, 0.6761378645896912, 0.6677696704864502, 0.6597693562507629, 0.6520918011665344, 0.644754946231842, 0.6377376914024353, 0.631027102470398, 0.6246119737625122, 0.6184985637664795, 0.6126567125320435, 0.6070724725723267, 0.6017671227455139, 0.5966794490814209, 0.5918384194374084, 0.587207555770874, 0.5827974081039429, 0.5785858035087585, 0.5745716691017151, 0.5707560777664185, 0.567119300365448, 0.5636681914329529, 0.5603765249252319, 0.5572409629821777, 0.5542615652084351, 0.5514234900474548, 0.5487297773361206, 0.5461652278900146, 0.5437178611755371, 0.541406512260437, 0.5392080545425415, 0.5371049046516418, 0.5351150631904602, 0.5332219004631042, 0.5314225554466248, 0.5296997427940369, 0.5280659198760986, 0.5265165567398071, 0.5250501036643982, 0.5236531496047974, 0.5223129391670227, 0.5210474729537964, 0.5198463201522827, 0.5187047123908997, 0.517623782157898, 0.5165954232215881, 0.5156175494194031, 0.5146865248680115, 0.5138029456138611, 0.5129584074020386, 0.5121565461158752, 0.511398434638977, 0.5106781125068665, 0.5099954009056091, 0.5093411207199097, 0.5087230801582336, 0.5081346035003662, 0.507574200630188, 0.5070449113845825, 0.5065379738807678, 0.5060561895370483, 0.5055959224700928, 0.5051596760749817, 0.5047461986541748, 0.5043510794639587, 0.5039750933647156, 0.5036186575889587, 0.5032819509506226, 0.502960205078125, 0.5026558637619019, 0.5023646354675293, 0.5020864009857178, 0.5018225312232971, 0.5015724301338196, 0.5013325810432434, 0.5011063814163208, 0.5008899569511414, 0.5006843209266663, 0.5004880428314209, 0.5003011226654053, 0.5001233816146851, 0.49995389580726624, 0.49979403614997864, 0.4996418058872223, 0.499496191740036, 0.49935922026634216, 0.49922820925712585, 0.49910297989845276, 0.4989842176437378, 0.49887120723724365, 0.49876290559768677, 0.49865996837615967, 0.4985620975494385, 0.4984690845012665, 0.4983799457550049, 0.49829554557800293, 0.4982152283191681, 0.4981388747692108, 0.49806639552116394, 0.4979974925518036, 0.4979320466518402, 0.49786972999572754, 0.4978102147579193, 0.4977535605430603, 0.49769994616508484, 0.4976491332054138, 0.4976007044315338, 0.4975546598434448, 0.4975111186504364, 0.49746957421302795, 0.49743038415908813, 0.4973931610584259, 0.49735769629478455, 0.4973241090774536, 0.4972922205924988, 0.4972621202468872, 0.4972335398197174, 0.497206449508667, 0.4971807301044464, 0.4971563518047333, 0.4971332848072052, 0.49711132049560547, 0.4970906972885132, 0.4970710873603821, 0.4970524311065674, 0.4970349371433258], "accuracy_train_first": 0.3888627534952934, "model": "residualv3", "loss_std": [0.39878201484680176, 0.08185956627130508, 0.08327677845954895, 0.0840158462524414, 0.0850401520729065, 0.08619881421327591, 0.0871528834104538, 0.08799291402101517, 0.08856137096881866, 0.08879466354846954, 0.08889849483966827, 0.08887182176113129, 0.08862911909818649, 0.08839790523052216, 0.08808805793523788, 0.08764554560184479, 0.08724351972341537, 0.08664572238922119, 0.08612799644470215, 0.08528444170951843, 0.0845239907503128, 0.08376394212245941, 0.08300936222076416, 0.08224653452634811, 0.0815243273973465, 0.08074098080396652, 0.07995050400495529, 0.0792379304766655, 0.0785353034734726, 0.077862448990345, 0.07723569124937057, 0.07654523104429245, 0.07590342313051224, 0.07532252371311188, 0.07472660392522812, 0.0741521492600441, 0.0736328661441803, 0.07309212535619736, 0.07260891795158386, 0.07207481563091278, 0.0715884193778038, 0.07108521461486816, 0.07062285393476486, 0.07017401605844498, 0.0697707012295723, 0.06937037408351898, 0.06897982954978943, 0.06860891729593277, 0.06825102865695953, 0.06790025532245636, 0.06757254153490067, 0.06726053357124329, 0.06694357097148895, 0.066639244556427, 0.0663532167673111, 0.06608457863330841, 0.06581801921129227, 0.06557410210371017, 0.06533816456794739, 0.06511450558900833, 0.0648937076330185, 0.0646948590874672, 0.06449653208255768, 0.06430744379758835, 0.06413757055997849, 0.06397560983896255, 0.06381967663764954, 0.06367096304893494, 0.06353234499692917, 0.06340383738279343, 0.06327816843986511, 0.06315414607524872, 0.06303589046001434, 0.06292213499546051, 0.06281662732362747, 0.06271466612815857, 0.0626194104552269, 0.06252894550561905, 0.06244231015443802, 0.06235881149768829, 0.06227647513151169, 0.06219836696982384, 0.06212230399250984, 0.062051329761743546, 0.061982665210962296, 0.061916597187519073, 0.061851922422647476, 0.061791639775037766, 0.061735983937978745, 0.061683014035224915, 0.0616314522922039, 0.06158245727419853, 0.06153601408004761, 0.06148969382047653, 0.061446331441402435, 0.0614052452147007, 0.06136750802397728, 0.06133009120821953, 0.06129426881670952, 0.06125771254301071, 0.06122465431690216, 0.06119132414460182, 0.06116198003292084, 0.06113366037607193, 0.061107050627470016, 0.061082083731889725, 0.06105734780430794, 0.06103289872407913, 0.06100911274552345, 0.060986943542957306, 0.06096620112657547, 0.06094607710838318, 0.06092678755521774, 0.06090809777379036, 0.060889266431331635, 0.06087234988808632, 0.06085658818483353, 0.06084175780415535, 0.060827601701021194, 0.060814354568719864, 0.06080107390880585, 0.06078868731856346, 0.060777872800827026, 0.06076682358980179, 0.060756076127290726, 0.06074608862400055, 0.0607365183532238, 0.06072726845741272, 0.060718655586242676, 0.06071024760603905, 0.060702066868543625, 0.06069421395659447, 0.06068681925535202, 0.06067989021539688, 0.06067309528589249, 0.060666508972644806, 0.060660455375909805, 0.06065448746085167, 0.06064904108643532, 0.06064372509717941, 0.060638632625341415, 0.06063365563750267, 0.06062892824411392, 0.06062457337975502, 0.0606204979121685, 0.060616448521614075, 0.06061246618628502, 0.06060874089598656, 0.0606052465736866, 0.06060191988945007, 0.06059862673282623, 0.06059547886252403, 0.06059252470731735, 0.06058972701430321, 0.06058703735470772, 0.060584500432014465, 0.06058212369680405, 0.060579776763916016, 0.06057751923799515]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:23 2016", "state": "available"}], "summary": "0c5515069e83a9d94a5e9b0f9b42aa92"}