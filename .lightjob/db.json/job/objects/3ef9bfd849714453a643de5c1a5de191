{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 64, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [2.237901210784912, 1.9695035219192505, 1.8910677433013916, 1.8458354473114014, 1.809861421585083, 1.7793339490890503, 1.7520736455917358, 1.7275978326797485, 1.7062180042266846, 1.6866066455841064, 1.6685876846313477, 1.6534423828125, 1.6376515626907349, 1.6234122514724731, 1.6119917631149292, 1.6013147830963135, 1.5901947021484375, 1.5804774761199951, 1.5721994638442993, 1.563143014907837, 1.5547465085983276, 1.5482234954833984, 1.5406792163848877, 1.5347079038619995, 1.5283440351486206, 1.5227733850479126, 1.5162320137023926, 1.5130590200424194, 1.5083551406860352, 1.5040009021759033, 1.5011171102523804, 1.4961727857589722, 1.4934720993041992, 1.4902516603469849, 1.4866749048233032, 1.4830646514892578, 1.4811139106750488, 1.4788960218429565, 1.47641122341156, 1.4750300645828247, 1.4742642641067505, 1.4707242250442505, 1.469480037689209, 1.4676214456558228, 1.464561104774475, 1.4633153676986694, 1.4626184701919556, 1.4621020555496216, 1.4600870609283447, 1.4598064422607422, 1.4587385654449463, 1.4573458433151245, 1.4569264650344849, 1.4563407897949219, 1.4546282291412354, 1.4545838832855225, 1.452930212020874, 1.4530391693115234, 1.4523704051971436, 1.4519140720367432, 1.4520727396011353, 1.4510650634765625, 1.4501545429229736, 1.4509379863739014, 1.451369047164917, 1.4497601985931396, 1.4497506618499756, 1.449028730392456, 1.4485217332839966, 1.4468028545379639, 1.4481836557388306, 1.4480222463607788, 1.4473106861114502, 1.447992205619812, 1.4469144344329834, 1.4475526809692383, 1.4461108446121216, 1.4469386339187622, 1.4463906288146973, 1.445631980895996, 1.4450128078460693, 1.445654273033142, 1.4447345733642578, 1.4464161396026611, 1.445319652557373, 1.4463868141174316, 1.4458547830581665, 1.4457881450653076, 1.4456053972244263, 1.445106863975525, 1.4440242052078247, 1.4458980560302734, 1.4444652795791626, 1.4447391033172607, 1.4456244707107544, 1.4443652629852295, 1.4448585510253906, 1.4458619356155396, 1.4441685676574707, 1.4436811208724976, 1.445180892944336, 1.4442867040634155, 1.444640040397644, 1.444738745689392, 1.4456267356872559, 1.4457193613052368, 1.4446568489074707, 1.4454349279403687, 1.4429328441619873, 1.4457241296768188, 1.4437973499298096, 1.4426400661468506, 1.4434734582901, 1.444576382637024, 1.4443509578704834, 1.4447613954544067, 1.444393515586853, 1.445456624031067, 1.4437427520751953, 1.4455949068069458, 1.4438964128494263, 1.445298671722412, 1.4455291032791138, 1.445387363433838, 1.4445903301239014, 1.4437223672866821, 1.4441585540771484, 1.4445586204528809, 1.444582223892212, 1.4443161487579346, 1.4433523416519165, 1.4445364475250244, 1.4441232681274414, 1.4449541568756104, 1.4436523914337158, 1.4447458982467651, 1.4440587759017944, 1.4447952508926392, 1.4436393976211548, 1.4447578191757202, 1.4445234537124634, 1.445468544960022, 1.4435300827026367, 1.444983959197998, 1.4456217288970947, 1.4454796314239502, 1.4446203708648682, 1.4439810514450073, 1.4435465335845947, 1.4441885948181152, 1.4459182024002075, 1.444495439529419, 1.4440828561782837, 1.4452852010726929, 1.4442315101623535, 1.4444570541381836, 1.4449535608291626, 1.444370150566101, 1.4449518918991089, 1.4444472789764404, 1.4435486793518066, 1.4452191591262817, 1.4439457654953003, 1.442905068397522, 1.445386528968811], "moving_avg_accuracy_train": [0.034102651174326315, 0.0700396717250369, 0.10477764115275469, 0.1376718871484519, 0.16893893760165765, 0.19837156106417553, 0.22602356868284104, 0.2519518980110316, 0.27634508477300523, 0.2990495072896139, 0.3202483894152575, 0.34021061543658704, 0.35867409255456545, 0.37593975823450937, 0.39200426887977385, 0.407013388728369, 0.4208794532158588, 0.4338216158676948, 0.44577186764840404, 0.45694097073913764, 0.46723483480171335, 0.4767271409925461, 0.4855352474797625, 0.4936555667182665, 0.5012309577554653, 0.508267445774677, 0.5148536901633869, 0.520951045976321, 0.5265850784853242, 0.5318463338969892, 0.5367394657400791, 0.5413037196667172, 0.5455369980899495, 0.5494330152256297, 0.5530184136096282, 0.5563592404957123, 0.5594775557872264, 0.562381695799589, 0.5650767659702299, 0.5675745168833584, 0.5698736099301647, 0.5720310411805959, 0.5739866801988411, 0.575802775179603, 0.5775975517348708, 0.579177973402469, 0.5806212431937743, 0.5819178969059583, 0.5831477003135999, 0.5843403736423358, 0.5853464584691783, 0.5862890650966515, 0.5871513619542343, 0.5879437772653631, 0.5886709019382361, 0.5894020440545361, 0.5900158220341878, 0.5905961600504073, 0.5911789181340524, 0.5917312300974098, 0.5922259857156219, 0.5926550618279837, 0.5930923475541, 0.5935230710397384, 0.5939270703161171, 0.5942836221207919, 0.5945928569521328, 0.5948851912908343, 0.5951807721813615, 0.5954142068506837, 0.5955872038185777, 0.5957940904123106, 0.5960034677371279, 0.5962198812128152, 0.5964122200456681, 0.5966015289392648, 0.5967674729387953, 0.5969189674430886, 0.5970739497362474, 0.5972064583536617, 0.5973187767117248, 0.5974454398708864, 0.5975268846307985, 0.5976281587980712, 0.5977843376176457, 0.5978109662635962, 0.5978767126258856, 0.5979429318960121, 0.598028069827212, 0.5980816227212904, 0.59816451731401, 0.5982136900058279, 0.5982694269772366, 0.5983242765979423, 0.5984247584815681, 0.5984594967518589, 0.5984767382046262, 0.5984829909656973, 0.5985095087411283, 0.5985798416663879, 0.5985990716181968, 0.5985767789474254, 0.5986287951568265, 0.5986547194548206, 0.598684990720625, 0.5986983200158106, 0.5987312427207633, 0.5987166232302022, 0.5987500407625254, 0.5987662016975777, 0.5988063231760297, 0.5988237592185226, 0.5988301510615283, 0.5988452403642902, 0.5988053783629756, 0.598839148879622, 0.5988859625815454, 0.5988908204346864, 0.5988998428001325, 0.5989498516564242, 0.5989134794187534, 0.5988899729024502, 0.5988967548723103, 0.5988493802225654, 0.598846270567557, 0.5988946251518589, 0.5989055921943972, 0.5989596043112438, 0.5989779161842447, 0.598940954496145, 0.598912411372112, 0.5989262861390628, 0.5989224613388331, 0.5989235972186079, 0.5989270888544898, 0.5989487604196222, 0.5989776014722983, 0.5989872463292213, 0.598993529454005, 0.5990061957615576, 0.5990454972240692, 0.5990064637784249, 0.5990039218094972, 0.5989621425565191, 0.5989500818169248, 0.5989508528953376, 0.5989189947825757, 0.59893915060609, 0.5989850844865109, 0.598987005595584, 0.5989631219080264, 0.5989625889773289, 0.5989667956861391, 0.5989543417312203, 0.5989360856277273, 0.5989358951274315, 0.5989613363628887, 0.5989819443748096, 0.5989400016676719, 0.5989441419586382, 0.5989105937419179, 0.5989525520576024, 0.5989903145417186, 0.5989777617524138, 0.5989571275979827], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.03412718255835843, 0.07028627694371234, 0.10552736286944651, 0.1391721687041133, 0.1708004149361116, 0.20058625493722937, 0.22891233035653355, 0.2555075195723862, 0.2802865045315633, 0.3035225319304401, 0.3249089717908599, 0.34495537223978595, 0.3634752658798887, 0.38072101865936975, 0.39653207638492377, 0.4113479658379223, 0.42512480799659996, 0.4379329750226177, 0.44991713304558184, 0.46112100285472846, 0.47162055425412003, 0.4810558844650032, 0.48987933051586735, 0.4981642878539643, 0.505866949100571, 0.5129092075037669, 0.5193448963166433, 0.5254930791718012, 0.5310651238525127, 0.5360901120790837, 0.5409188067729074, 0.5452534544747583, 0.5492146430540144, 0.5530004688465046, 0.556394475519836, 0.5595610038244037, 0.5624352933610145, 0.5650556865117353, 0.5675106670887244, 0.5698208943839935, 0.5720221692622358, 0.573890364845425, 0.5757080772313644, 0.5772453326200503, 0.5786898976261176, 0.5800622188104184, 0.5814082006661988, 0.5826216433537205, 0.5838225755450803, 0.5848291428211446, 0.585845946159512, 0.5867244480702927, 0.5876127560399953, 0.5883125179454085, 0.5890643739727803, 0.5897186893522343, 0.590453028060083, 0.5909552414908971, 0.5915425404310393, 0.5920843460170769, 0.5926319766921012, 0.5931513173794424, 0.5934701806057301, 0.593805985634389, 0.5941092396688417, 0.5944564399960086, 0.5948167189067993, 0.5952508332077611, 0.5954451940699669, 0.5957198341132715, 0.5959781876748359, 0.5962483564826535, 0.5965515140572797, 0.5966280138657837, 0.5967456918184373, 0.5969136666407352, 0.5970892580433033, 0.5972849409080242, 0.597387813298773, 0.5974926054816969, 0.5976092734915092, 0.5977142747003401, 0.5978220123281978, 0.5978691185596099, 0.5980224069577905, 0.5979518174762435, 0.598022564286601, 0.5980750588933326, 0.5980867124543006, 0.5980829346106025, 0.5982280779435935, 0.5982854647557854, 0.5983727044718483, 0.5983403274263954, 0.5983366316566474, 0.5983821335888743, 0.5983355770918092, 0.5982326410882006, 0.5981755902700433, 0.5981385105822709, 0.5981916175906853, 0.598191615281918, 0.5981519035842985, 0.5982647064487602, 0.5983296079330257, 0.5983524276837743, 0.5983342853483788, 0.5984410570676826, 0.5984628799188962, 0.5984082487888289, 0.5983702582943586, 0.598337096357995, 0.5983449012176775, 0.5984139902563013, 0.598563678627132, 0.5986708955724007, 0.5986320839707329, 0.5985239113417319, 0.5985252417342907, 0.5985264390875936, 0.5986017884017258, 0.5985342959320352, 0.5983982515044943, 0.5984375914519365, 0.598583890194544, 0.5986036367643216, 0.5985725805521214, 0.5985324229298912, 0.5986071738597937, 0.5986754792053656, 0.598612824686561, 0.5986185002845464, 0.598621549305414, 0.5985887018391045, 0.598546932088176, 0.5985927295137711, 0.5985993851203759, 0.59861861170623, 0.5984751652099293, 0.5985312278493279, 0.5985674181762173, 0.5984158544930083, 0.5983293048117797, 0.5981761088938548, 0.5982111900225416, 0.598293650180679, 0.5983922783855027, 0.5983213228549343, 0.5982808474312632, 0.5982077984562092, 0.5982529471685701, 0.598244752884695, 0.5983106202167074, 0.5982966586280186, 0.5983339508318584, 0.5982820645965641, 0.59830655015498, 0.598292995572464, 0.5984283103318592, 0.5985002359816552, 0.5985171704501313, 0.5983462174770309, 0.5982533949574904, 0.5982451558947234, 0.598225533706983], "moving_var_accuracy_train": [0.010466917354060022, 0.021043450633213775, 0.0297996442495419, 0.036557962601216694, 0.041700822337484844, 0.045327254018713584, 0.04767623034492684, 0.048959111666194134, 0.04941844854321544, 0.04911602090520808, 0.04824895224507967, 0.047010471230111525, 0.045377523992675396, 0.04352270049575221, 0.04149304696662332, 0.03937120537762642, 0.037164494539204805, 0.034955541252243545, 0.0327452637856203, 0.030593477181721188, 0.02848780219959816, 0.026449956871023035, 0.024503205842932026, 0.022646341519455788, 0.020898186311808457, 0.0192539771534282, 0.01771898697441569, 0.016281688008159816, 0.014939200108156378, 0.013694407373901716, 0.01254045128961629, 0.011473897885816141, 0.010487793913111967, 0.009575625067494395, 0.008733758294892766, 0.007960832583948528, 0.007252264337869288, 0.0066029441669850095, 0.006008020379308617, 0.005463367177994057, 0.0049646029197355085, 0.004510033214164996, 0.0040934506084756486, 0.00371378935644042, 0.00337140142674643, 0.0030567408778985113, 0.0027698140393231114, 0.002507964433034686, 0.002270779737524238, 0.002056503990793518, 0.0018599634518233895, 0.001681963671928456, 0.0015204593075709867, 0.0013740646750416902, 0.0012414166001466283, 0.001122086059280015, 0.00101326796402676, 0.0009149722975417099, 0.0008265315306440227, 0.0007466238141234299, 0.0006741644808068592, 0.0006084049895179686, 0.0005492854598225579, 0.0004960266183300261, 0.00044789289523485394, 0.00040424776841611984, 0.0003646836272027377, 0.0003289843987727201, 0.00029687227146105134, 0.0002676754700185209, 0.0002411772745747734, 0.00021744476568129353, 0.000196094838890493, 0.00017690686813357545, 0.00015954912935982666, 0.00014391675713859735, 0.00012977291812355928, 0.00011700218157468282, 0.00010551813901794948, 9.512435191935618e-05, 8.572545544944256e-05, 7.729730190749746e-05, 6.962727095700196e-05, 6.275685197391273e-05, 5.670069318967482e-05, 5.103700563377373e-05, 4.597220832778487e-05, 4.1414452420631124e-05, 3.733824338452887e-05, 3.363023025825349e-05, 3.032905085394749e-05, 2.7317907351138305e-05, 2.461407610586082e-05, 2.2179744823298712e-05, 2.0052639821401834e-05, 1.8058236566066922e-05, 1.625508831870195e-05, 1.4629931360020871e-05, 1.3173266955743067e-05, 1.1900460743548955e-05, 1.0713742788613241e-05, 9.646841178282986e-06, 8.706508234818772e-06, 7.841906034375214e-06, 7.06596257673829e-06, 6.3609653500557556e-06, 5.734623955562816e-06, 5.163085125544905e-06, 4.656827196189488e-06, 4.193495058966445e-06, 3.7886331503682965e-06, 3.412505975531828e-06, 3.0716230788917207e-06, 2.7665099545231123e-06, 2.5041597714100368e-06, 2.264007824420071e-06, 2.05733074616799e-06, 1.8518100601854592e-06, 1.6673616818710981e-06, 1.523133485052387e-06, 1.3827265936058089e-06, 1.2494269410236289e-06, 1.1248982029579274e-06, 1.0326075996081838e-06, 9.294338692358118e-07, 8.575339747192999e-07, 7.728630614457157e-07, 7.218325341975269e-07, 6.526672030129345e-07, 5.996959801962054e-07, 5.47058771542713e-07, 4.940854768099059e-07, 4.4480859100008597e-07, 4.003393439058439e-07, 3.604151332054473e-07, 3.2860053050250037e-07, 3.0322673432739746e-07, 2.737412702802433e-07, 2.467224421656432e-07, 2.2349411607223788e-07, 2.150461490650011e-07, 2.072540230682836e-07, 1.8658677521572273e-07, 1.8363765150886028e-07, 1.6658303931403465e-07, 1.4993008643989837e-07, 1.4407153193465008e-07, 1.3332069373504111e-07, 1.389779166962341e-07, 1.2511334096724975e-07, 1.1773588165269554e-07, 1.0596484962358033e-07, 9.552763225234008e-08, 8.73707779651873e-08, 8.163326800139872e-08, 7.347026781452317e-08, 7.194854918741235e-08, 6.857590566661999e-08, 7.755103123827382e-08, 6.99502061980165e-08, 7.30845311841769e-08, 8.162058036151287e-08, 8.629256918492036e-08, 7.90814649403938e-08, 7.50052334081409e-08], "duration": 162260.441194, "accuracy_train": [0.3410265117432632, 0.3934728566814322, 0.41741936600221485, 0.43372010110972686, 0.4503423916805094, 0.46326517222683644, 0.47489163725083056, 0.4853068619647471, 0.4958837656307678, 0.5033893099390919, 0.5110383285460502, 0.519870649628553, 0.5248453866163714, 0.5313307493540051, 0.5365848646871539, 0.5420954673657253, 0.5456740336032668, 0.5503010797342193, 0.5533241336747877, 0.5574628985557402, 0.5598796113648948, 0.5621578967100407, 0.5648082058647103, 0.5667384398648025, 0.5694094770902547, 0.5715958379475821, 0.5741298896617756, 0.575827248292728, 0.5772913710663529, 0.5791976326019749, 0.5807776523278885, 0.5823820050064599, 0.5836365038990403, 0.5844971694467516, 0.5852869990656147, 0.5864266824704688, 0.5875423934108527, 0.5885189559108527, 0.5893323975059985, 0.5900542751015135, 0.5905654473514212, 0.5914479224344776, 0.5915874313630491, 0.5921476300064599, 0.5937505407322813, 0.5934017684108527, 0.5936106713155224, 0.5935877803156147, 0.5942159309823736, 0.5950744336009597, 0.5944012219107604, 0.5947725247439092, 0.5949120336724806, 0.5950755150655224, 0.5952150239940938, 0.5959823231012367, 0.5955398238510521, 0.5958192021963824, 0.5964237408868586, 0.5967020377676264, 0.5966787862795312, 0.5965167468392396, 0.5970279190891473, 0.5973995824104835, 0.5975630638035253, 0.5974925883628645, 0.5973759704342008, 0.5975162003391473, 0.5978410001961055, 0.5975151188745847, 0.5971441765296235, 0.5976560697559062, 0.5978878636604835, 0.5981676024940015, 0.5981432695413437, 0.5983053089816354, 0.5982609689345699, 0.5982824179817277, 0.5984687903746769, 0.5983990359103912, 0.598329641934293, 0.5985854083033407, 0.5982598874700074, 0.5985396263035253, 0.5991899469938169, 0.5980506240771503, 0.5984684298864895, 0.5985389053271503, 0.5987943112080103, 0.5985635987679956, 0.5989105686484865, 0.598656244232189, 0.5987710597199151, 0.598817923184293, 0.5993290954342008, 0.5987721411844776, 0.5986319112795312, 0.5985392658153378, 0.5987481687200074, 0.5992128379937246, 0.5987721411844776, 0.5983761449104835, 0.599096941041436, 0.5988880381367663, 0.5989574321128645, 0.5988182836724806, 0.5990275470653378, 0.5985850478151532, 0.599050798553433, 0.5989116501130491, 0.5991674164820967, 0.5989806836009597, 0.5988876776485788, 0.5989810440891473, 0.5984466203511444, 0.5991430835294389, 0.5993072858988556, 0.5989345411129567, 0.5989810440891473, 0.5993999313630491, 0.5985861292797158, 0.5986784142557217, 0.5989577926010521, 0.5984230083748615, 0.5988182836724806, 0.5993298164105758, 0.5990042955772426, 0.5994457133628645, 0.5991427230412514, 0.5986082993032484, 0.598655523255814, 0.5990511590416205, 0.5988880381367663, 0.5989338201365817, 0.5989585135774271, 0.599143804505814, 0.5992371709463824, 0.5990740500415282, 0.599050077577058, 0.5991201925295312, 0.599399210386674, 0.5986551627676264, 0.5989810440891473, 0.5985861292797158, 0.5988415351605758, 0.5989577926010521, 0.5986322717677187, 0.5991205530177187, 0.599398489410299, 0.5990042955772426, 0.5987481687200074, 0.5989577926010521, 0.5990046560654301, 0.5988422561369509, 0.5987717806962901, 0.5989341806247692, 0.5991903074820044, 0.5991674164820967, 0.598562517303433, 0.5989814045773348, 0.598608659791436, 0.5993301768987633, 0.5993301768987633, 0.598864786648671, 0.5987714202081026], "end": "2016-01-31 12:47:40.910000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0], "moving_var_accuracy_valid": [0.010481981304343715, 0.021201104134829736, 0.030258400956371584, 0.037420317497606284, 0.042681399385252894, 0.04639802582787728, 0.048979522183030526, 0.05044730676957052, 0.050928558953077574, 0.050694919781302, 0.049741846092302, 0.04838438502170012, 0.046632824663496567, 0.0446462860975257, 0.04243156340538034, 0.04016400228739358, 0.03785581447744028, 0.03554667531279357, 0.03328458817318859, 0.031085869644172502, 0.02896944789605145, 0.02687373221214195, 0.024887037792840343, 0.023016098676403092, 0.021248467721282555, 0.01956996157991075, 0.01798572823638531, 0.016527356784530902, 0.015154050243392416, 0.01386589977914777, 0.01268915643324843, 0.011589343326216035, 0.010571628128238295, 0.009643457607794217, 0.008782785378702362, 0.007994748954364776, 0.007269627921990734, 0.006604463272170762, 0.005998259311654231, 0.005446467731891067, 0.004945431458508183, 0.004482299705290789, 0.004063806439623689, 0.003678694182831713, 0.0033296056770593285, 0.0030135944982493247, 0.002728540052829204, 0.0024689380359493823, 0.002235024375508644, 0.0020206405370889703, 0.0018278814846403105, 0.001652039226641488, 0.0014939371234186747, 0.0013489504115952139, 0.0012191429578087502, 0.0011010818195699842, 0.000995826917653592, 0.0008985141908590423, 0.0008117670521789678, 0.0007332323265986254, 0.0006626081881448104, 0.0005987748020760805, 0.0005398123856821799, 0.00048684603226941547, 0.0004389890961271803, 0.0003961751191191264, 0.00035772581524925865, 0.00032364933076102747, 0.0002916243829877413, 0.00026314078906944393, 0.00023742742922745676, 0.00021434160696716942, 0.00019373458690593177, 0.00017441379820164887, 0.00015709705128635058, 0.00014164128602604947, 0.00012775464848934688, 0.00011532380969232052, 0.0001038866732820938, 9.359683856830204e-05, 8.435965753209377e-05, 7.602291906358816e-05, 6.852509372533671e-05, 6.169255532614374e-05, 5.573477579068029e-05, 5.020614408575809e-05, 4.523057567776422e-05, 4.073231926361082e-05, 3.666030958659884e-05, 3.299440707686602e-05, 2.9884565653184937e-05, 2.6925748303788336e-05, 2.4301670385938344e-05, 2.188093780499484e-05, 1.969296695292163e-05, 1.7742304090156844e-05, 1.5987581247911955e-05, 1.4484185510671017e-05, 1.3065060122275791e-05, 1.1770928239255899e-05, 1.0619218604414941e-05, 9.557296744021421e-06, 8.615760239969694e-06, 7.868704592049661e-06, 7.119743956783547e-06, 6.412456230323263e-06, 5.774172906293363e-06, 5.299357416051613e-06, 4.773707805962298e-06, 4.3231980687179804e-06, 3.903867760877022e-06, 3.523378410999758e-06, 3.1715888124117435e-06, 2.8973895884922527e-06, 2.809310104900604e-06, 2.6318383545853632e-06, 2.382211582942995e-06, 2.249302283633594e-06, 2.0243879847694774e-06, 1.8219620891869166e-06, 1.6908635525299761e-06, 1.562774298461477e-06, 1.5730696449996363e-06, 1.4296913636824237e-06, 1.479352126111147e-06, 1.3349262566618392e-06, 1.2101140258416873e-06, 1.103616334866176e-06, 1.0435440150710925e-06, 9.811801956671753e-07, 9.183924746402144e-07, 8.268431388886277e-07, 7.44242493754027e-07, 6.795288487651902e-07, 6.272783727223485e-07, 5.834271731703208e-07, 5.25483129746792e-07, 4.7626177120451597e-07, 6.138276697926218e-07, 5.807320786404453e-07, 5.344465286197215e-07, 6.87745826368873e-07, 6.86388869618845e-07, 8.289708860767813e-07, 7.571499677785646e-07, 7.426320701211284e-07, 7.559165681896756e-07, 7.256370972348184e-07, 6.678177268035682e-07, 6.490613289312195e-07, 6.025008520888151e-07, 5.428550834739627e-07, 5.276161239644835e-07, 4.7660884519647593e-07, 4.4146433688183053e-07, 4.215475359107508e-07, 3.847886654581098e-07, 3.479633392769643e-07, 4.779577623408365e-07, 4.76721677993955e-07, 4.316304961976559e-07, 6.514917176849387e-07, 6.638867271209552e-07, 5.981089938063841e-07, 5.41763366691224e-07], "accuracy_test": 0.5908502072704082, "start": "2016-01-29 15:43:20.469000", "learning_rate_per_epoch": [0.00020851485896855593, 0.00019465827790554613, 0.00018172252748627216, 0.0001696464023552835, 0.0001583727716933936, 0.0001478483172832057, 0.00013802325702272356, 0.0001288511120947078, 0.00012028848141198978, 0.0001122948742704466, 0.00010483247024239972, 9.786596638150513e-05, 9.13624171516858e-05, 8.529105252819136e-05, 7.9623147030361e-05, 7.433189603034407e-05, 6.939227023394778e-05, 6.478089926531538e-05, 6.0475973441498354e-05, 5.645712371915579e-05, 5.270534165902063e-05, 4.9202877562493086e-05, 4.593316771206446e-05, 4.2880739783868194e-05, 4.003115827799775e-05, 3.737094084499404e-05, 3.488750371616334e-05, 3.2569099857937545e-05, 3.0404764402192086e-05, 2.8384256438584998e-05, 2.649801899679005e-05, 2.473712811479345e-05, 2.3093254640116356e-05, 2.1558624212048016e-05, 2.0125975424889475e-05, 1.878853072412312e-05, 1.753996366460342e-05, 1.6374369806726463e-05, 1.52862339746207e-05, 1.4270407518779393e-05, 1.3322086488187779e-05, 1.2436785254976712e-05, 1.161031559604453e-05, 1.0838767593668308e-05, 1.0118491445609834e-05, 9.446081094210967e-06, 8.818354217510205e-06, 8.23234222480096e-06, 7.685272976232227e-06, 7.174558504630113e-06, 6.697782737319358e-06, 6.252690582186915e-06, 5.8371765589981806e-06, 5.449274794955272e-06, 5.087150384497363e-06, 4.749090749101015e-06, 4.433496087585809e-06, 4.1388739191461354e-06, 3.863830443151528e-06, 3.6070648548047757e-06, 3.3673622965579852e-06, 3.1435888558917213e-06, 2.9346858809731202e-06, 2.7396652058087057e-06, 2.5576043753972044e-06, 2.387642325629713e-06, 2.2289748358161887e-06, 2.0808513454539934e-06, 1.942571088875411e-06, 1.813480139389867e-06, 1.6929677713051206e-06, 1.580463845129998e-06, 1.4754361927771242e-06, 1.3773881164524937e-06, 1.2858556601713644e-06, 1.2004059044556925e-06, 1.1206345789105399e-06, 1.0461643569215084e-06, 9.76642922978499e-07, 9.117414379034017e-07, 8.511528903909493e-07, 7.945906759232457e-07, 7.417872325277131e-07, 6.924927333784581e-07, 6.464740636147326e-07, 6.035135129422997e-07, 5.634078092953132e-07, 5.259673230284534e-07, 4.910148732051312e-07, 4.583851307415898e-07, 4.279237657556223e-07, 3.9948668018041644e-07, 3.729393256435287e-07, 3.481561350326956e-07, 3.250198972182261e-07, 3.034211317753943e-07, 2.8325769108050736e-07, 2.644341918767168e-07, 2.4686156052666774e-07, 2.3045670616284042e-07, 2.1514200909678038e-07, 2.0084503660200426e-07, 1.8749814501006767e-07, 1.750382097043257e-07, 1.634062840594197e-07, 1.525473436458924e-07, 1.4241001622394833e-07, 1.3294635436977842e-07, 1.2411157968017505e-07, 1.1586391224227555e-07, 1.0816432904903195e-07, 1.0097641478523656e-07, 9.426616287555589e-08, 8.800183337598355e-08, 8.215378954901098e-08, 7.669436996593504e-08, 7.159774639831085e-08, 6.683981723654142e-08, 6.239806538133053e-08, 5.825148718940909e-08, 5.438046457584278e-08, 5.076668330161738e-08, 4.739305126122417e-08, 4.4243609664817996e-08, 4.1303458431229956e-08, 3.855869223912123e-08, 3.59963259199958e-08, 3.3604237614781596e-08, 3.1371111930411644e-08, 2.9286386649118867e-08, 2.7340199437730917e-08, 2.552334343874918e-08, 2.382722463778464e-08, 2.2243819230993722e-08, 2.076563632158468e-08, 1.938568416903763e-08, 1.8097434661967782e-08, 1.689479312005915e-08, 1.577207164871197e-08, 1.4723959829154865e-08, 1.3745498961270641e-08, 1.2832060747314245e-08, 1.1979323311095413e-08, 1.118325343441029e-08, 1.0440085240759345e-08, 9.746303319957406e-09, 9.09862585274368e-09, 8.493988623570203e-09, 7.92953169792554e-09, 7.402585211480073e-09, 6.910656491498912e-09, 6.451418066433234e-09, 6.022697895957663e-09, 5.622467824650812e-09, 5.2488342561218815e-09, 4.900030159404878e-09, 4.574405298995998e-09, 4.270419129426273e-09, 3.986634133923417e-09, 3.7217078308060536e-09, 3.4743867782793814e-09, 3.24350102332005e-09, 3.0279585505610385e-09, 2.8267397311765308e-09, 2.6388924379006085e-09], "accuracy_train_first": 0.3410265117432632, "accuracy_train_last": 0.5987714202081026, "batch_size_eval": 1024, "accuracy_train_std": [0.01732665659914939, 0.016464989370918726, 0.01665048050631141, 0.014635181231059405, 0.013130537130911371, 0.013142661734201965, 0.012952711314382689, 0.01283821423568273, 0.012546491860454681, 0.013015186935913569, 0.011704289437584217, 0.013396978404990373, 0.01273044024893473, 0.01330833534111492, 0.013804300344053168, 0.013511890185658739, 0.013331334523167478, 0.013370091145897668, 0.012667997771918996, 0.012039979891625337, 0.012711228445430807, 0.012041206660805853, 0.012580478164579823, 0.01199302382882643, 0.012542823152681109, 0.011919055559467621, 0.012372873750068873, 0.011868376401865863, 0.01289580196238661, 0.012001636866636039, 0.012742109924763706, 0.013440066335732713, 0.013135314682491487, 0.01283779634785533, 0.013886990995174148, 0.012678467143893386, 0.013187716286112783, 0.013409642160566738, 0.012703382678030487, 0.012636329842218304, 0.01319561565917736, 0.012889757217370084, 0.01314048694988444, 0.011933326152960904, 0.01251458667091271, 0.012340701207712685, 0.012542023411042361, 0.012512627944553949, 0.012136172472061973, 0.012590139010412182, 0.012401999253578015, 0.012811331443665511, 0.012917744507279248, 0.012520711629045115, 0.012593814142465989, 0.013166804673904818, 0.013541955540099036, 0.013124407321161016, 0.01353303118022939, 0.013276140394879512, 0.013448636673263441, 0.013139484822159337, 0.013427351036449698, 0.013220608167571871, 0.013003820746298564, 0.013106521210471662, 0.013320876426683642, 0.013464101085045966, 0.013520484499798181, 0.013514826701507943, 0.012662190020369939, 0.012916554668420281, 0.013316048162398907, 0.013350919828684511, 0.013373625837408393, 0.013604281052892065, 0.012797133662720912, 0.012927997370792867, 0.01334079096550529, 0.013316751818772954, 0.01321281779657728, 0.013000549721269135, 0.012799867857790937, 0.012751691577081482, 0.01306190989104058, 0.012970974240890097, 0.013204496262097316, 0.01332631946576381, 0.013069133908022527, 0.0123690933468568, 0.013010582997052657, 0.012705484510880938, 0.013026765162882595, 0.012800413984090243, 0.012998401232541274, 0.013237059367118411, 0.013080731641785564, 0.012851937590022459, 0.01304458762299716, 0.01332407362939257, 0.012495849485348817, 0.013051274281596685, 0.012784458671102464, 0.01232756379122738, 0.012977350216066816, 0.012764067187719112, 0.013173465173468917, 0.013216030617033341, 0.012762128525952736, 0.012709739727889044, 0.01276841504644111, 0.013357004250313331, 0.013369296876721953, 0.01281127697676154, 0.012711434492174222, 0.013144219260013128, 0.012185498721754956, 0.01308253241970947, 0.012836066401078818, 0.012742309381125683, 0.012492904441897506, 0.01271088975691399, 0.012830483549231638, 0.012846147296851436, 0.012998513319216443, 0.01288457275840297, 0.013315576519572454, 0.012928738772170571, 0.012981418303774236, 0.013185993221659906, 0.012934150240217311, 0.012556708817731943, 0.012857693006774888, 0.013007447605503096, 0.012152249963415216, 0.012707027394552234, 0.012773305518393846, 0.012949457033474701, 0.01307823550127509, 0.013356042216089225, 0.012926309958496123, 0.012533758882454647, 0.012999539048854909, 0.013125742827485562, 0.012666382027359015, 0.01247879592831958, 0.012952590284601566, 0.012847439147321699, 0.013155716363825471, 0.012977041949617444, 0.013103637296009652, 0.012700635989233955, 0.012630289190374136, 0.01260148699527746, 0.013160541430195323, 0.012940239291803282, 0.01279589479395377, 0.01291165678275839, 0.01265535325301427, 0.012851049353067452, 0.01331213787017495, 0.012491703416253367, 0.013048254398974515, 0.012690156151625985, 0.01312851456772895], "accuracy_test_std": 0.01540042927476725, "error_valid": [0.6587281744164157, 0.6042818735881024, 0.5773028637989458, 0.5580245787838856, 0.5445453689759037, 0.5313411850527108, 0.5161529908697289, 0.5051357774849398, 0.4967026308358433, 0.48735322147966864, 0.4826130694653614, 0.4746270237198795, 0.46984569135918675, 0.4640672063253012, 0.4611684040850903, 0.4553090290850903, 0.4508836125753012, 0.44679352174322284, 0.44222544474774095, 0.43804416886295183, 0.4338834831513554, 0.43402614363704817, 0.4307096550263554, 0.4272710961031627, 0.4248090996799698, 0.4237104668674698, 0.4227339043674698, 0.41917327513177716, 0.41878647402108427, 0.41868499388177716, 0.4156229409826807, 0.41573471620858427, 0.4151346597326807, 0.41292709902108427, 0.4130594644201807, 0.41194024143448793, 0.41169610080948793, 0.41136077513177716, 0.4103945077183735, 0.40938705995858427, 0.40816635683358427, 0.4092958749058735, 0.4079325112951807, 0.40891936888177716, 0.40830901731927716, 0.4075868905308735, 0.40647796263177716, 0.40645737245858427, 0.4053690347326807, 0.40611175169427716, 0.4050028237951807, 0.4053690347326807, 0.4043924722326807, 0.4053896249058735, 0.4041689217808735, 0.4043924722326807, 0.40293792356927716, 0.40452483763177716, 0.4031717691076807, 0.40303940370858427, 0.4024393472326807, 0.40217461643448793, 0.4036600503576807, 0.4031717691076807, 0.40316147402108427, 0.40241875705948793, 0.40194077089608427, 0.40084213808358427, 0.4028055581701807, 0.40180840549698793, 0.40169663027108427, 0.40132012424698793, 0.40072006777108427, 0.4026834878576807, 0.4021952066076807, 0.40157455995858427, 0.40133041933358427, 0.40095391330948793, 0.40168633518448793, 0.40156426487198793, 0.4013407144201807, 0.4013407144201807, 0.40120834902108427, 0.4017069253576807, 0.40059799745858427, 0.4026834878576807, 0.4013407144201807, 0.40145248964608427, 0.40180840549698793, 0.4019510659826807, 0.40046563205948793, 0.40119805393448793, 0.40084213808358427, 0.4019510659826807, 0.40169663027108427, 0.40120834902108427, 0.40208343138177716, 0.40269378294427716, 0.4023378670933735, 0.4021952066076807, 0.40133041933358427, 0.40180840549698793, 0.40220550169427716, 0.40072006777108427, 0.40108627870858427, 0.40144219455948793, 0.4018289956701807, 0.40059799745858427, 0.4013407144201807, 0.40208343138177716, 0.4019716561558735, 0.40196136106927716, 0.4015848550451807, 0.40096420839608427, 0.4000891260353916, 0.4003641519201807, 0.40171722044427716, 0.40244964231927716, 0.4014627847326807, 0.4014627847326807, 0.40072006777108427, 0.4020731362951807, 0.4028261483433735, 0.40120834902108427, 0.40009942112198793, 0.4012186441076807, 0.4017069253576807, 0.4018289956701807, 0.40072006777108427, 0.40070977268448793, 0.4019510659826807, 0.40133041933358427, 0.40135100950677716, 0.4017069253576807, 0.4018289956701807, 0.4009950936558735, 0.4013407144201807, 0.40120834902108427, 0.40281585325677716, 0.40096420839608427, 0.40110686888177716, 0.4029482186558735, 0.40244964231927716, 0.4032026543674698, 0.40147307981927716, 0.40096420839608427, 0.40072006777108427, 0.4023172769201807, 0.40208343138177716, 0.40244964231927716, 0.4013407144201807, 0.4018289956701807, 0.4010965737951807, 0.4018289956701807, 0.40133041933358427, 0.40218491152108427, 0.40147307981927716, 0.4018289956701807, 0.40035385683358427, 0.4008524331701807, 0.40133041933358427, 0.4031923592808735, 0.4025820077183735, 0.4018289956701807, 0.4019510659826807], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.06645365534436601, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.00022335779515412216, "optimization": "nesterov_momentum", "nb_data_augmentation": 4, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 5.465724598085998e-05, "rotation_range": [0, 0], "momentum": 0.7372205670075134}, "accuracy_valid_max": 0.5999108739646084, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.5980489340173193, "accuracy_valid_std": [0.010293161760511664, 0.013803791856218373, 0.01733011976151661, 0.020641106540331102, 0.020391466352618518, 0.017584028326539793, 0.018551772338792818, 0.01715191668523863, 0.015650481876512152, 0.02084097405692199, 0.017045755958441905, 0.02080752471851528, 0.021275153759618515, 0.023778887603335692, 0.020668745758134443, 0.01861334080104929, 0.019868480519288967, 0.02524142477225008, 0.027956293367666134, 0.028988636951897774, 0.02943263040886306, 0.026965660106934405, 0.026757208395536927, 0.028712463380548312, 0.029303151753712533, 0.0294997084283509, 0.02763886265057634, 0.028724896895232523, 0.03040596267675876, 0.02771501149945565, 0.028094736300168066, 0.028932878478938513, 0.028800379958215543, 0.028298927258020846, 0.027720948149658073, 0.029942600214973206, 0.02943022185941272, 0.026248206232231203, 0.024042394881222152, 0.027217648776400514, 0.027094178771724008, 0.02403561734033559, 0.0250299675031014, 0.025307911109860297, 0.02451160187096874, 0.02418599925582216, 0.023970979179103746, 0.02583390142150862, 0.024762391227634885, 0.02388447973136948, 0.024917073066171122, 0.024491316397465487, 0.023425441249184727, 0.022321997085435724, 0.022101629568292716, 0.0239886260252308, 0.022004030616075566, 0.023057087918484355, 0.023556008771290573, 0.02435630017566571, 0.023336554403044052, 0.025538495143814302, 0.02413891394233513, 0.023464739819765535, 0.02454510447378976, 0.025252607409719914, 0.024642645322074247, 0.023799674434248363, 0.023558443713902342, 0.02506459850099536, 0.02430129039220306, 0.02448628000158235, 0.023899142181131178, 0.02368415863785744, 0.022975912268300033, 0.02370922233491399, 0.023785397912404693, 0.02464057490258564, 0.02530547478979756, 0.025482754301828378, 0.023174151957244402, 0.023096862129971925, 0.023372850957688616, 0.02295718246903904, 0.02342443105972195, 0.023041237584305242, 0.023107182377112408, 0.024406162356474782, 0.025432867458979547, 0.023307887433316757, 0.024384318647833525, 0.02490809392093421, 0.023959421898440393, 0.023683339955708968, 0.023740501580373236, 0.024493557156692804, 0.022910141618033265, 0.02282292542870169, 0.021811006233225048, 0.023570089090548665, 0.023765341982286293, 0.02539064749024937, 0.022398156859101053, 0.02357269160177348, 0.024309256783891158, 0.025027916160871817, 0.023066434084906422, 0.023747900107886726, 0.022957084715076282, 0.0224369542683315, 0.021799631781681154, 0.022586132710852642, 0.02291968870262439, 0.023719272361363133, 0.025495537904763797, 0.02302336801837401, 0.02209504428925524, 0.02303332321153285, 0.022756102155528254, 0.023238182226010772, 0.023914101545557346, 0.023353054264629934, 0.02179473519441059, 0.024115889193586804, 0.02489315131466284, 0.023057656788846144, 0.023506233532419023, 0.022910866941700084, 0.02348656368027463, 0.024919133793408915, 0.023338554541332986, 0.023750288920299786, 0.02230453673076792, 0.023556892848413523, 0.022999150850955614, 0.02095736744728551, 0.022743187482673243, 0.02363658029418822, 0.022708138788778226, 0.024558807784576815, 0.02208279178399965, 0.022174000943151775, 0.022264879645264423, 0.020806625700132805, 0.02184799847547313, 0.023879557698662995, 0.023759067253978635, 0.023859579467169732, 0.022029459930454045, 0.02233971185412945, 0.022962276829493422, 0.02335915752766186, 0.022913922820614885, 0.02335915752766186, 0.023684948432905535, 0.024255383827868633, 0.022059767949124877, 0.023849042802070137, 0.023607809952594934, 0.02323524239748022, 0.02388043355666633, 0.02184093513852259, 0.021994760568450407, 0.023251740800228033, 0.02318995449078823], "accuracy_valid": [0.3412718255835843, 0.3957181264118976, 0.4226971362010542, 0.44197542121611444, 0.4554546310240964, 0.46865881494728917, 0.48384700913027107, 0.49486422251506024, 0.5032973691641567, 0.5126467785203314, 0.5173869305346386, 0.5253729762801205, 0.5301543086408133, 0.5359327936746988, 0.5388315959149097, 0.5446909709149097, 0.5491163874246988, 0.5532064782567772, 0.557774555252259, 0.5619558311370482, 0.5661165168486446, 0.5659738563629518, 0.5692903449736446, 0.5727289038968373, 0.5751909003200302, 0.5762895331325302, 0.5772660956325302, 0.5808267248682228, 0.5812135259789157, 0.5813150061182228, 0.5843770590173193, 0.5842652837914157, 0.5848653402673193, 0.5870729009789157, 0.5869405355798193, 0.5880597585655121, 0.5883038991905121, 0.5886392248682228, 0.5896054922816265, 0.5906129400414157, 0.5918336431664157, 0.5907041250941265, 0.5920674887048193, 0.5910806311182228, 0.5916909826807228, 0.5924131094691265, 0.5935220373682228, 0.5935426275414157, 0.5946309652673193, 0.5938882483057228, 0.5949971762048193, 0.5946309652673193, 0.5956075277673193, 0.5946103750941265, 0.5958310782191265, 0.5956075277673193, 0.5970620764307228, 0.5954751623682228, 0.5968282308923193, 0.5969605962914157, 0.5975606527673193, 0.5978253835655121, 0.5963399496423193, 0.5968282308923193, 0.5968385259789157, 0.5975812429405121, 0.5980592291039157, 0.5991578619164157, 0.5971944418298193, 0.5981915945030121, 0.5983033697289157, 0.5986798757530121, 0.5992799322289157, 0.5973165121423193, 0.5978047933923193, 0.5984254400414157, 0.5986695806664157, 0.5990460866905121, 0.5983136648155121, 0.5984357351280121, 0.5986592855798193, 0.5986592855798193, 0.5987916509789157, 0.5982930746423193, 0.5994020025414157, 0.5973165121423193, 0.5986592855798193, 0.5985475103539157, 0.5981915945030121, 0.5980489340173193, 0.5995343679405121, 0.5988019460655121, 0.5991578619164157, 0.5980489340173193, 0.5983033697289157, 0.5987916509789157, 0.5979165686182228, 0.5973062170557228, 0.5976621329066265, 0.5978047933923193, 0.5986695806664157, 0.5981915945030121, 0.5977944983057228, 0.5992799322289157, 0.5989137212914157, 0.5985578054405121, 0.5981710043298193, 0.5994020025414157, 0.5986592855798193, 0.5979165686182228, 0.5980283438441265, 0.5980386389307228, 0.5984151449548193, 0.5990357916039157, 0.5999108739646084, 0.5996358480798193, 0.5982827795557228, 0.5975503576807228, 0.5985372152673193, 0.5985372152673193, 0.5992799322289157, 0.5979268637048193, 0.5971738516566265, 0.5987916509789157, 0.5999005788780121, 0.5987813558923193, 0.5982930746423193, 0.5981710043298193, 0.5992799322289157, 0.5992902273155121, 0.5980489340173193, 0.5986695806664157, 0.5986489904932228, 0.5982930746423193, 0.5981710043298193, 0.5990049063441265, 0.5986592855798193, 0.5987916509789157, 0.5971841467432228, 0.5990357916039157, 0.5988931311182228, 0.5970517813441265, 0.5975503576807228, 0.5967973456325302, 0.5985269201807228, 0.5990357916039157, 0.5992799322289157, 0.5976827230798193, 0.5979165686182228, 0.5975503576807228, 0.5986592855798193, 0.5981710043298193, 0.5989034262048193, 0.5981710043298193, 0.5986695806664157, 0.5978150884789157, 0.5985269201807228, 0.5981710043298193, 0.5996461431664157, 0.5991475668298193, 0.5986695806664157, 0.5968076407191265, 0.5974179922816265, 0.5981710043298193, 0.5980489340173193], "seed": 520440480, "model": "residualv3", "loss_std": [0.29801395535469055, 0.17947350442409515, 0.19206351041793823, 0.20006780326366425, 0.20424425601959229, 0.2089087963104248, 0.21152585744857788, 0.21466144919395447, 0.2160540670156479, 0.21739596128463745, 0.2186562865972519, 0.21898461878299713, 0.2210656851530075, 0.222171351313591, 0.22235362231731415, 0.22375018894672394, 0.2238260805606842, 0.22234447300434113, 0.22406914830207825, 0.2252531796693802, 0.22466951608657837, 0.22532309591770172, 0.22818739712238312, 0.22617223858833313, 0.22781209647655487, 0.2271479368209839, 0.2266775518655777, 0.2281263768672943, 0.22916680574417114, 0.22801779210567474, 0.22795064747333527, 0.22950351238250732, 0.22949078679084778, 0.22952312231063843, 0.22978359460830688, 0.22922518849372864, 0.22895126044750214, 0.2292325645685196, 0.22942540049552917, 0.22793671488761902, 0.22942665219306946, 0.23029638826847076, 0.23025855422019958, 0.2299645096063614, 0.2290908247232437, 0.22944608330726624, 0.2295815497636795, 0.22974799573421478, 0.23022030293941498, 0.23168492317199707, 0.22968178987503052, 0.23076708614826202, 0.23019982874393463, 0.22934624552726746, 0.2311972826719284, 0.23001724481582642, 0.2295430451631546, 0.23067344725131989, 0.22853828966617584, 0.2300419956445694, 0.23147445917129517, 0.22913289070129395, 0.23081481456756592, 0.23098714649677277, 0.2304106503725052, 0.22971111536026, 0.23098841309547424, 0.22940552234649658, 0.23045474290847778, 0.2302865982055664, 0.23062089085578918, 0.23038698732852936, 0.230620339512825, 0.23052264750003815, 0.2319689840078354, 0.2300339788198471, 0.22943052649497986, 0.23066630959510803, 0.23050257563591003, 0.2298474907875061, 0.23090389370918274, 0.23194697499275208, 0.22913631796836853, 0.23130467534065247, 0.23077267408370972, 0.23019185662269592, 0.23053649067878723, 0.22992365062236786, 0.23040924966335297, 0.22939972579479218, 0.23014800250530243, 0.23080898821353912, 0.23102298378944397, 0.23033463954925537, 0.2291022688150406, 0.23129308223724365, 0.2299797385931015, 0.2307787835597992, 0.22998379170894623, 0.23102135956287384, 0.23109927773475647, 0.22994717955589294, 0.23025667667388916, 0.23114974796772003, 0.22982242703437805, 0.23161491751670837, 0.23052464425563812, 0.23061703145503998, 0.23075100779533386, 0.22983916103839874, 0.23025842010974884, 0.23035480082035065, 0.22953927516937256, 0.2296759933233261, 0.23043136298656464, 0.23192788660526276, 0.22959759831428528, 0.2309957593679428, 0.22949130833148956, 0.22976751625537872, 0.22951751947402954, 0.2308972328901291, 0.23155048489570618, 0.2303086221218109, 0.23043762147426605, 0.2300887256860733, 0.23060816526412964, 0.2304983139038086, 0.2316657304763794, 0.22999805212020874, 0.23028653860092163, 0.2306971698999405, 0.22988857328891754, 0.23121728003025055, 0.23011450469493866, 0.2302331179380417, 0.22917427122592926, 0.23025833070278168, 0.23010185360908508, 0.23012366890907288, 0.2295895665884018, 0.2306082397699356, 0.2292049676179886, 0.23117704689502716, 0.22996403276920319, 0.23040539026260376, 0.23014047741889954, 0.22997727990150452, 0.23190420866012573, 0.2312554270029068, 0.22970061004161835, 0.23008236289024353, 0.22979451715946198, 0.2303318977355957, 0.23067128658294678, 0.2314106673002243, 0.23046942055225372, 0.23124225437641144, 0.23032492399215698, 0.23028823733329773, 0.22985517978668213, 0.22999687492847443, 0.23006519675254822, 0.2303459644317627, 0.2297467440366745]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:25 2016", "state": "available"}], "summary": "13984f665a3d72e2541814900f406392"}