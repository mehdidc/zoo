{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 64, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.3753125667572021, 0.99615877866745, 0.8369951248168945, 0.7315740585327148, 0.6479290723800659, 0.575398325920105, 0.5089339017868042, 0.4470122456550598, 0.39442846179008484, 0.3518962860107422, 0.31561869382858276, 0.29246053099632263, 0.2732515335083008, 0.25046268105506897, 0.23558145761489868, 0.22731514275074005, 0.21892762184143066, 0.21304099261760712, 0.2061520367860794, 0.19936354458332062, 0.19711144268512726, 0.19418060779571533, 0.18790429830551147, 0.1891041249036789, 0.1872696727514267, 0.18512123823165894, 0.18069685995578766, 0.18285831809043884, 0.1786949336528778, 0.18551306426525116, 0.1794273555278778, 0.18189941346645355, 0.1777753084897995, 0.18323037028312683, 0.17566591501235962, 0.179124116897583, 0.17894026637077332, 0.18045738339424133, 0.17883415520191193, 0.17904914915561676, 0.17974920570850372, 0.1783062070608139, 0.17764250934123993, 0.17696866393089294, 0.18060636520385742, 0.17942342162132263, 0.1797407567501068, 0.17767561972141266, 0.17939937114715576, 0.17803236842155457, 0.17972736060619354, 0.1783493459224701, 0.1785929799079895, 0.18209688365459442, 0.18021294474601746, 0.17879576981067657, 0.17843036353588104, 0.1788041740655899, 0.17996492981910706, 0.1819092333316803, 0.17751561105251312, 0.1802058070898056, 0.1775684356689453, 0.17719994485378265, 0.17951518297195435, 0.17690308392047882, 0.17791885137557983, 0.178519606590271, 0.1804303228855133, 0.17841391265392303, 0.1780625283718109, 0.177551731467247, 0.17690178751945496, 0.17860111594200134, 0.17681287229061127, 0.17645688354969025, 0.175685316324234, 0.17631769180297852, 0.1774725764989853, 0.17664095759391785, 0.17612934112548828, 0.17795699834823608, 0.17383328080177307, 0.1756892204284668, 0.17438258230686188, 0.1772332340478897, 0.17307250201702118, 0.17396880686283112, 0.17581668496131897, 0.174043670296669, 0.17357945442199707, 0.17321056127548218, 0.17481346428394318, 0.17007534205913544, 0.17666558921337128, 0.17218725383281708, 0.17114631831645966, 0.17110008001327515, 0.1741238385438919, 0.17141318321228027, 0.17254941165447235, 0.16879722476005554, 0.168747216463089, 0.17256760597229004, 0.171169713139534, 0.17129363119602203, 0.16939818859100342, 0.17062406241893768, 0.16775956749916077, 0.17118673026561737, 0.16797614097595215, 0.1691456437110901, 0.16690437495708466, 0.1663704663515091, 0.16959625482559204, 0.16872352361679077, 0.16743223369121552, 0.16602014005184174, 0.1687382310628891, 0.16540420055389404, 0.16639401018619537, 0.16707409918308258, 0.1655653864145279, 0.16610439121723175, 0.16779068112373352, 0.1610431969165802, 0.16578738391399384, 0.16686147451400757, 0.1612011343240738, 0.16642868518829346, 0.1613251268863678, 0.1653338074684143, 0.1614784598350525, 0.1658155918121338, 0.16217568516731262, 0.16298504173755646, 0.1651557981967926, 0.16005131602287292, 0.16479672491550446, 0.16046977043151855, 0.16156333684921265, 0.16403821110725403, 0.1613413244485855, 0.1579902023077011, 0.16261018812656403, 0.15934430062770844, 0.1618041694164276, 0.16062380373477936, 0.1576361507177353, 0.16151951253414154, 0.16230253875255585, 0.15826764702796936, 0.1581963449716568, 0.15925493836402893, 0.15644651651382446, 0.1618136614561081, 0.15706327557563782, 0.15718886256217957, 0.15999957919120789], "moving_avg_accuracy_train": [0.0635978667751015, 0.12722662738787371, 0.18934179903792908, 0.24836021565845326, 0.30324343507751933, 0.3543888812177002, 0.40249338289613323, 0.4468173671828857, 0.4876714204528308, 0.5252583765814587, 0.5606164047697968, 0.5924505442738986, 0.6219870794263814, 0.6497369334242656, 0.6747817006819221, 0.6966733107447746, 0.7191052682108102, 0.7389242591718905, 0.7570032746595096, 0.7742881534793191, 0.7905188015230908, 0.8040546714053887, 0.81677628067681, 0.8289441639544132, 0.8396093376983222, 0.8502101332047451, 0.8601041996819359, 0.8697667859256747, 0.8763056638403516, 0.8836227653861337, 0.890982287135634, 0.8974362289935452, 0.9028449231680648, 0.9082357261631815, 0.9134688093123672, 0.9176763520037772, 0.9215793978665224, 0.9253990227370316, 0.9280787587062225, 0.9319390887868276, 0.9349018891700958, 0.9373313164341032, 0.9402663546931014, 0.942775355644057, 0.9450334564999171, 0.9467751036690006, 0.9480962284938226, 0.9496106535230394, 0.9507807568446003, 0.9514618260244814, 0.9527396726994419, 0.9545315478759724, 0.9557374065908207, 0.9568690742639184, 0.9580828516208876, 0.9585220646707774, 0.9596681631882419, 0.960516037195645, 0.9600607458261174, 0.9608159552447516, 0.962025741601275, 0.9629052138316514, 0.9629295478783035, 0.9640558581559956, 0.964599785248757, 0.9653731319822793, 0.9657342505162403, 0.9664916627777391, 0.9673107699345813, 0.9674525119875887, 0.9685612926329144, 0.9691569444696598, 0.9692443855489489, 0.9693928009357761, 0.9694309715850925, 0.9703210159802008, 0.9705362626310733, 0.9708878423941749, 0.9714321648131184, 0.9718825995580432, 0.9721299167582467, 0.9723688143289152, 0.9729487984079745, 0.9726499705005474, 0.9723554847957768, 0.9729203815912083, 0.972850006897619, 0.971642696459103, 0.9717580748036873, 0.9718829137507364, 0.9724718522113955, 0.972385732391465, 0.9729336895832893, 0.9728387326023966, 0.9730252055838513, 0.9733907410136076, 0.9734965807123114, 0.9740660226030127, 0.9742228446344242, 0.9745894157507622, 0.9745845804245324, 0.9743639537428211, 0.9742257715006911, 0.9738947936780953, 0.9745546226055515, 0.9742534305438705, 0.9739778155371948, 0.9743898519513601, 0.9755115996431288, 0.9753261542240818, 0.9752406335552727, 0.9755147984724014, 0.9752988783335408, 0.9752790084669181, 0.9761584888321586, 0.9766757256989889, 0.9763785900696123, 0.9766761070662502, 0.9765277067751383, 0.9767242094976338, 0.9773754283538413, 0.9775709724220655, 0.9775050745096393, 0.9777364820872837, 0.977568038751202, 0.9762005671654413, 0.9768321066905915, 0.9767797496287214, 0.9765395688242102, 0.9768931396561211, 0.9773181300059852, 0.9775379330018337, 0.9780426753409546, 0.9781203774854766, 0.9775274978095573, 0.9774822594488674, 0.9778089905337887, 0.9782703150292286, 0.9779439288001613, 0.978342931343964, 0.9788485901060239, 0.9777855771121712, 0.9780494243474287, 0.9780149525949026, 0.9776512875425921, 0.9771681319276371, 0.9773426250575478, 0.9774623943958775, 0.9780909480360701, 0.9782800082539194, 0.978503604823784, 0.97886756610451, 0.9787370769416872, 0.9789452296261176, 0.978879125821867, 0.9794287132396803, 0.9796211807169304, 0.9792922414012359, 0.9791381021921293], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.06265913262424697, 0.12471567912274092, 0.18406498846950298, 0.23995651178934485, 0.29197507371282, 0.34003395517813434, 0.38380876231468836, 0.42338919420633697, 0.45871155467086894, 0.49066537096583024, 0.519647650228434, 0.546147770135937, 0.569542099861801, 0.5905796420405306, 0.6097383041071853, 0.6254836060853975, 0.6425527388880024, 0.6568335330997292, 0.6692650317758256, 0.6810424066102913, 0.6932036615253163, 0.7028091060373178, 0.7113053156331343, 0.7196335860747154, 0.7267241384235692, 0.7339051225480797, 0.7403822743087085, 0.7463898158913919, 0.7502116011979605, 0.7545830603835109, 0.7599172112822833, 0.763587693655561, 0.7670905583261495, 0.7702390184950406, 0.7736606291643618, 0.7753900987686334, 0.7778958283966646, 0.7801406899752964, 0.7814418800609746, 0.7845703412453892, 0.7862484963151274, 0.7878554626192321, 0.7891930455930016, 0.7901414050491382, 0.7916979359015286, 0.7927478982884089, 0.7933326834783934, 0.793994297001789, 0.7943048694194565, 0.7943493624756284, 0.7951788922954601, 0.7965215546472395, 0.7968733995590216, 0.7968441450699869, 0.797418019578425, 0.7979824523250252, 0.7990559361964384, 0.7989681489585718, 0.7985891122065399, 0.7985653619422112, 0.7987341516610172, 0.7990000437238312, 0.7987887159327733, 0.800368687524737, 0.8007458577406368, 0.8017333130998562, 0.8023768527894941, 0.8025755615240989, 0.8033444549198817, 0.804153381745288, 0.8047899366901117, 0.8050078027255433, 0.8050890184055491, 0.8055647974761387, 0.8053766170978773, 0.8059111445353034, 0.8055834666405081, 0.8057524237226922, 0.8063062876192483, 0.8058861492564198, 0.8053553633029163, 0.8048645664775192, 0.8052641049822523, 0.8052921878481084, 0.8045685684138247, 0.8051656637091139, 0.8049298884714856, 0.804382953370572, 0.8042233901494786, 0.8044704082504945, 0.8048992145639994, 0.804481535200973, 0.8040903282170203, 0.8038236911502129, 0.8043945293958844, 0.8045379548448501, 0.8050241301811483, 0.8050038507756088, 0.8047393996683039, 0.8049582013712777, 0.8047971480357312, 0.8042237773586641, 0.8036305305998309, 0.8030527808625135, 0.8039723533504339, 0.8030968671211887, 0.8032073964294012, 0.8039731119822442, 0.8055177776759626, 0.8054298965103994, 0.8056233291108655, 0.806297024296541, 0.806585967151149, 0.8066395256977058, 0.8072736658896069, 0.8075299507031313, 0.8064025380405742, 0.806162057156321, 0.8058357610792432, 0.8065492482243308, 0.8071690316097291, 0.807526523685955, 0.807398518342962, 0.8077492397390874, 0.8076111698220311, 0.8064125410740147, 0.8069706939696253, 0.8070141648588073, 0.8065130907580922, 0.8066623276160179, 0.8073297792011781, 0.8070586388657741, 0.8074464367355974, 0.8073467361155015, 0.807092284171873, 0.8075631962384959, 0.8083389619873873, 0.808569165939251, 0.8086105986017265, 0.8094779661229544, 0.8100266632983095, 0.8091562447094425, 0.8089385094516006, 0.8092450950181123, 0.8085250459361053, 0.8073744544637298, 0.8075219746611821, 0.8071563135749585, 0.8078689342882458, 0.8074919617931561, 0.8072401947836446, 0.8078498596520421, 0.8076864321864614, 0.807910705948237, 0.8082213861064254, 0.808473495660316, 0.8085783239463176, 0.8084163217474689, 0.808155508943957], "moving_var_accuracy_train": [0.03640219792509205, 0.06919955072664014, 0.0970042465960187, 0.1186523834399607, 0.13389665506017678, 0.1440496995020624, 0.15047111728742943, 0.15310554580615704, 0.15281647424279293, 0.15024984025765226, 0.14647656764819283, 0.14094962282507315, 0.13470632272189048, 0.12816618002183644, 0.12099472532256443, 0.11320843611060383, 0.10641632694136574, 0.09930982587166762, 0.09232050049351496, 0.08577735376650295, 0.07957052381313975, 0.07326244939306024, 0.06739275853584663, 0.06198599913337835, 0.05681111259893041, 0.052141393127358326, 0.04780828677771811, 0.04386774825620552, 0.03986578575003241, 0.03636106695031076, 0.033212423298712, 0.0302660612583885, 0.027502740886610987, 0.025014013610339326, 0.02275907868252202, 0.020642501553770162, 0.01871535530145338, 0.016975125578670744, 0.01534224188458485, 0.01394213703110739, 0.012626927002996497, 0.011417353354176771, 0.01035314806499515, 0.0093744890304427, 0.008482931302675554, 0.007661938186162191, 0.00691145270477082, 0.006240948782815805, 0.005629176180582376, 0.005070433259574193, 0.004578085962739142, 0.004149174716299624, 0.0037473441018312423, 0.0033841357371491273, 0.0030589814626848346, 0.002754819489345093, 0.0024911594167161934, 0.002248513488036444, 0.0020255277513132964, 0.0018281080475759097, 0.0016584694900741897, 0.0014995837837027996, 0.0013496307346449578, 0.0012260848347551735, 0.0011061390614198155, 0.0010009077418100796, 0.0009019906269892035, 0.0008169546242951032, 0.0007412975906751036, 0.0006673486488939099, 0.000611678334679558, 0.0005537037112071666, 0.0004984021535675752, 0.0004487601823542412, 0.00040389727710504026, 0.00037063716062191, 0.00033399042464612544, 0.0003017038571499161, 0.00027420005349680534, 0.00024860607128204386, 0.00022429595633148767, 0.00020238000914178073, 0.00018516944201526402, 0.00016745618087805352, 0.00015149105926307575, 0.0001392139288421677, 0.00012533710933543095, 0.00012592178485643622, 0.00011344941583238383, 0.00010224473711344849, 9.514189999609528e-05, 8.56944596069497e-05, 7.982732740290293e-05, 7.192574611659476e-05, 6.504612106024869e-05, 5.974405430788765e-05, 5.3870467253494384e-05, 5.1401797130114376e-05, 4.648295576292735e-05, 4.304402963663382e-05, 3.873983709638818e-05, 3.530393858089567e-05, 3.194539371116684e-05, 2.9736771211502955e-05, 3.0681462011925306e-05, 2.8429765732909763e-05, 2.6270461846762515e-05, 2.5171381721470442e-05, 3.397910450522026e-05, 3.0890704085708245e-05, 2.7867457740279358e-05, 2.575720958230907e-05, 2.3601082181368627e-05, 2.124452726762824e-05, 2.6081445956459914e-05, 2.5881107148489624e-05, 2.4087602673845597e-05, 2.247548967605691e-05, 2.042614452607011e-05, 1.8731049952996e-05, 2.067471894581776e-05, 1.895138439479509e-05, 1.709532876907474e-05, 1.586774109508858e-05, 1.4536325402812512e-05, 2.991249970329592e-05, 3.0510829279409002e-05, 2.748441770881717e-05, 2.52551573076359e-05, 2.385475257547542e-05, 2.3094828495226162e-05, 2.1220165858559363e-05, 2.139103273281372e-05, 1.9306268068902377e-05, 2.053919805307667e-05, 1.850369683127023e-05, 1.761410596482795e-05, 1.7768077979180936e-05, 1.6950021915985125e-05, 1.668784699403524e-05, 1.7320279347462873e-05, 2.5758221038613197e-05, 2.3808937206729598e-05, 2.1438738201556676e-05, 2.0485134813848878e-05, 2.053757546682717e-05, 1.8757848591618788e-05, 1.7011165982092444e-05, 1.8865766491277715e-05, 1.730088373590878e-05, 1.60207541968149e-05, 1.561088910194333e-05, 1.4203046986276565e-05, 1.3172690147969123e-05, 1.1894748549599997e-05, 1.3423690663008234e-05, 1.2414715164898701e-05, 1.2147053309095003e-05, 1.1146178040241587e-05], "duration": 91089.940754, "accuracy_train": [0.6359786677510152, 0.699885472902824, 0.7483783438884275, 0.779525965243171, 0.797192409849114, 0.8146978964793282, 0.8354338980020304, 0.8457332257636582, 0.8553578998823367, 0.8635409817391103, 0.8788386584648394, 0.8789577998108158, 0.8878158957987264, 0.8994856194052234, 0.9001846060008305, 0.8936978013104466, 0.9209928854051311, 0.9172951778216132, 0.9197144140480805, 0.9298520628576044, 0.9365946339170359, 0.9258775003460686, 0.9312707641196014, 0.9384551134528424, 0.9355959013935032, 0.9456172927625508, 0.9491507979766519, 0.9567300621193245, 0.9351555650724437, 0.9494766792981728, 0.957217982881137, 0.9555217057147471, 0.9515231707387413, 0.9567529531192323, 0.9605665576550388, 0.9555442362264673, 0.9567068106312293, 0.9597756465716132, 0.9521963824289406, 0.9666820595122739, 0.9615670926195091, 0.9591961618101699, 0.9666816990240864, 0.9653563642026578, 0.9653563642026578, 0.962449928190753, 0.9599863519172205, 0.9632404787859912, 0.961311686738649, 0.957591448643411, 0.9642402927740864, 0.9706584244647471, 0.9665901350244556, 0.9670540833217978, 0.9690068478336102, 0.962474982119786, 0.9699830498454227, 0.9681469032622739, 0.9559631235003692, 0.9676128400124585, 0.9729138188099853, 0.9708204639050388, 0.9631485542981728, 0.9741926506552234, 0.9694951290836102, 0.9723332525839794, 0.96898431732189, 0.9733083731312293, 0.9746827343461609, 0.9687281904646549, 0.9785403184408453, 0.9745178110003692, 0.9700313552625508, 0.9707285394172205, 0.9697745074289406, 0.9783314155361758, 0.9724734824889257, 0.9740520602620893, 0.9763310665836102, 0.9759365122623662, 0.9743557715600776, 0.9745188924649317, 0.9781686551195091, 0.9699605193337025, 0.9697051134528424, 0.9780044527500923, 0.9722166346553157, 0.9607769025124585, 0.9727964799049464, 0.9730064642741787, 0.9777722983573275, 0.9716106540120893, 0.9778653043097084, 0.9719841197743633, 0.9747034624169435, 0.9766805598814139, 0.974449138000646, 0.9791909996193245, 0.9756342429171282, 0.9778885557978036, 0.9745410624884644, 0.9723783136074198, 0.9729821313215209, 0.9709159932747323, 0.9804930829526578, 0.9715427019887413, 0.9714972804771133, 0.9780981796788483, 0.9856073288690477, 0.9736571454526578, 0.9744709475359912, 0.9779822827265596, 0.9733555970837948, 0.9751001796673128, 0.9840738121193245, 0.9813308575004615, 0.9737043694052234, 0.9793537600359912, 0.9751921041551311, 0.9784927340000923, 0.9832363980597084, 0.9793308690360835, 0.9769119932978036, 0.9798191502860835, 0.9760520487264673, 0.9638933228935955, 0.9825159624169435, 0.97630853607189, 0.9743779415836102, 0.9800752771433187, 0.9811430431547619, 0.9795161599644703, 0.9825853563930418, 0.9788196967861758, 0.9721915807262828, 0.9770751142026578, 0.9807495702980805, 0.9824222354881875, 0.9750064527385567, 0.9819339542381875, 0.9833995189645626, 0.9682184601674971, 0.9804240494647471, 0.9777047068221669, 0.9743783020717978, 0.9728197313930418, 0.9789130632267442, 0.9785403184408453, 0.9837479307978036, 0.9799815502145626, 0.9805159739525655, 0.9821432176310447, 0.9775626744762828, 0.9808186037859912, 0.9782841915836102, 0.984375, 0.9813533880121816, 0.9763317875599853, 0.9777508493101699], "end": "2016-02-01 17:03:20.351000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0], "moving_var_accuracy_valid": [0.03533550211100674, 0.06646108656978376, 0.0915160425922443, 0.11047919974413171, 0.1237846568309963, 0.13219309593717093, 0.13621989000203635, 0.13669739630039765, 0.1342566790094349, 0.1300204284908006, 0.12457813824302047, 0.11844063161472675, 0.11152222042315613, 0.10435320200913693, 0.09722137079788173, 0.0897304645275594, 0.08337961572650009, 0.07687712390370931, 0.07058029094734244, 0.06477062087453174, 0.059624623877052564, 0.054492542567805564, 0.04969295850848939, 0.045347903454573485, 0.04126559650262266, 0.03760313564932865, 0.03422040353876773, 0.031123178187699985, 0.028142314755295522, 0.025500070178464364, 0.023206141652915797, 0.0210067794552971, 0.019016532057871502, 0.017204094065000192, 0.01558905143465189, 0.014057065877195598, 0.01270786741819518, 0.011482435307940613, 0.010349429637898156, 0.00940257209854984, 0.008487660728637648, 0.0076621357220966356, 0.006912024303792437, 0.006228916344335586, 0.005627829804552017, 0.0050749686132215855, 0.004570549515365253, 0.004117434155917788, 0.0037065588373655515, 0.003335920770317424, 0.0030085217707835924, 0.0027238942734232026, 0.002452618999658405, 0.002207364802118723, 0.001989592309469766, 0.001793500337451702, 0.0016245216123061893, 0.0014621388104677608, 0.0013172179491555028, 0.0011855012309154537, 0.00106720751754648, 0.0009611230530934392, 0.0008654126827015557, 0.0008013382065141092, 0.0007224847022085553, 0.0006590118447657609, 0.0005968379502784377, 0.0005375095217014682, 0.0004890793430180282, 0.0004460606721959842, 0.0004051014247564003, 0.00036501847276531246, 0.00032857598936889064, 0.0002977556819481021, 0.00026829882044615555, 0.0002440404146337921, 0.00022060272839505085, 0.00019879937401612733, 0.0001816803235576892, 0.00016510093739720177, 0.00015112644721341042, 0.00013818173620644783, 0.00012580024173668164, 0.0001132273153892057, 0.0001066172096213431, 9.916419378411735e-05, 8.974808406981354e-05, 8.346551770433475e-05, 7.534810972763241e-05, 6.836246023493513e-05, 6.318108790195555e-05, 5.843308356444362e-05, 5.396716134663972e-05, 4.9210303140536895e-05, 4.722197955097482e-05, 4.268491933057667e-05, 4.054372551614034e-05, 3.649305425312763e-05, 3.347315832120773e-05, 3.055671015610522e-05, 2.773448273251041e-05, 2.7919819859142813e-05, 2.8295313325024264e-05, 2.846993482325502e-05, 3.3233463385789286e-05, 3.6808402285592164e-05, 3.3237512608798264e-05, 3.519064411870962e-05, 5.3145508654992163e-05, 4.790046568283954e-05, 4.3447164652863215e-05, 4.318723501639792e-05, 3.961990327381899e-05, 3.568372960762056e-05, 3.5734560693717774e-05, 3.275224177513437e-05, 4.0916551402867796e-05, 3.734537576380181e-05, 3.456906035666935e-05, 3.569372947685144e-05, 3.558153953250765e-05, 3.3173590840335515e-05, 3.0003700066814967e-05, 2.8110379539435372e-05, 2.5470911303455303e-05, 3.5854218053251215e-05, 3.507260814183233e-05, 3.1582354791505606e-05, 3.068379660202168e-05, 2.7815861699691386e-05, 2.9043700096518772e-05, 2.6800983820213594e-05, 2.5474370128746866e-05, 2.3016395038699697e-05, 2.1297467659376447e-05, 2.1163544463858595e-05, 2.4463502491849728e-05, 2.249409697774767e-05, 2.0260137269651188e-05, 2.500506129461625e-05, 2.521417247733925e-05, 2.9511411908213822e-05, 2.698694849995948e-05, 2.513420603630278e-05, 2.728702155716466e-05, 3.647306602817925e-05, 3.302161930326867e-05, 3.092282964274604e-05, 3.240100120752612e-05, 3.0439875445260492e-05, 2.796636754443917e-05, 2.8514952055819498e-05, 2.5903833678792795e-05, 2.376613879290207e-05, 2.225822435983929e-05, 2.0604434968321603e-05, 1.864289219740362e-05, 1.7014805389549343e-05, 1.5925534716876328e-05], "accuracy_test": 0.10798788265306122, "start": "2016-01-31 15:45:10.411000", "learning_rate_per_epoch": [0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352, 0.0007569505833089352], "accuracy_train_first": 0.6359786677510152, "accuracy_train_last": 0.9777508493101699, "batch_size_eval": 1024, "accuracy_train_std": [0.02346653152298484, 0.022295709223861558, 0.023297989822696662, 0.026461422404766226, 0.028337335242174362, 0.02832843309638732, 0.029397279676970912, 0.029829813103280346, 0.03041837359604297, 0.029860459220137994, 0.02911558985524938, 0.02821492527796776, 0.02786210109358178, 0.025839699596925596, 0.024243541443677952, 0.025273524703839718, 0.020959381215848834, 0.023523557165322496, 0.02162777731398759, 0.020052920763570948, 0.019685178852987616, 0.01815623376351457, 0.01794298732059115, 0.015203104534192758, 0.01658732699571135, 0.015355307118324007, 0.015122321694183125, 0.015112729690879454, 0.016544905380033614, 0.01366483182883098, 0.013555911277737772, 0.012330497159240877, 0.013394520431982996, 0.012507073994770223, 0.012414169916816363, 0.013653722583215316, 0.011991589664942246, 0.010975118821926496, 0.013888545949308018, 0.010244100603165615, 0.011386783577134116, 0.01066700593448894, 0.010777032103802162, 0.009309985595238185, 0.011714118428335687, 0.01160721829834124, 0.011042762376648498, 0.010870977494612683, 0.010305319964986672, 0.011621603061376946, 0.01047328474682044, 0.008509497929925752, 0.009453088637265458, 0.00909044679468793, 0.008895013106500668, 0.008783718727684069, 0.00933258898029753, 0.008450513544683678, 0.009274557756582077, 0.008909378923641883, 0.0069591804960273534, 0.009164225047955934, 0.009258252965109686, 0.007265605382780901, 0.008929984975296118, 0.007192762422331759, 0.008388534709748353, 0.007545365387682753, 0.006081195228584337, 0.00792784727213541, 0.006397919107476123, 0.0072358997524283415, 0.007592301181652073, 0.00806748971530177, 0.008163217118896365, 0.005891012470224962, 0.006101123137320649, 0.00798453630185816, 0.006700598997890183, 0.00693473810973938, 0.007917560066938882, 0.007280305902685011, 0.005685438706669605, 0.008596346883349091, 0.008436098284410248, 0.006923586459962844, 0.0069340098364583415, 0.009251096185153657, 0.007416830326088806, 0.007446558446871706, 0.007113925972648221, 0.0072460509291397916, 0.006846710882861451, 0.00831388726306221, 0.007368992556583816, 0.00743813737627425, 0.006297351136113987, 0.005545609213319578, 0.006158336074214707, 0.0057276867202532035, 0.007117511696039789, 0.007535317583490323, 0.007513413066450832, 0.005986137374963799, 0.005813229776368104, 0.007189581554301321, 0.007359552381851826, 0.0072889715889058035, 0.005491536832436355, 0.006283713783816113, 0.006524965241687636, 0.006922110568385354, 0.0072255212146258765, 0.0058690698675821626, 0.005870868037159337, 0.00554032886017682, 0.00753044875363955, 0.005409120340763501, 0.006859615554888109, 0.005531659854776879, 0.005699571689547851, 0.004720782033709938, 0.006514243825017754, 0.005656095561599988, 0.0075737224892053565, 0.007160709919678651, 0.005770989382615431, 0.006651312190421184, 0.007300936104253116, 0.006533171593223614, 0.006943012704115052, 0.005618074212483882, 0.006073806326856345, 0.006333859443769241, 0.008432135102205564, 0.0060876449062615585, 0.006031397566754354, 0.0057901736455963, 0.007154917630693846, 0.005991602073798601, 0.005299863704130174, 0.006586384064066293, 0.005725046762485481, 0.0061644437381772246, 0.006779041651961677, 0.007635100352392029, 0.006368035936436153, 0.006239807220389389, 0.0047993893096911055, 0.0051495479586246866, 0.0054641288808578905, 0.00498652105171334, 0.0057213936454448555, 0.005599622188762279, 0.006395111644311641, 0.005552969436598952, 0.005405521008983238, 0.00569794116626601, 0.005308349661554411], "accuracy_test_std": 0.007826617100027179, "error_valid": [0.3734086737575302, 0.31677540239081325, 0.2817912274096386, 0.25701977833207834, 0.23985786897590367, 0.2274361116340362, 0.22221797345632532, 0.22038691876882532, 0.22338720114834332, 0.2217502823795181, 0.21951183640813254, 0.2153511506965362, 0.21990893260542166, 0.22008247835090367, 0.21783373729292166, 0.23280867611069278, 0.2038250658885542, 0.21463931899472888, 0.21885148013930722, 0.2129612198795181, 0.19734504423945776, 0.21074189335466864, 0.2122287980045181, 0.2054119799510542, 0.20946089043674698, 0.20146602033132532, 0.20132335984563254, 0.19954230986445776, 0.21539233104292166, 0.2060738069465362, 0.1920754306287651, 0.20337796498493976, 0.2013836596385542, 0.20142483998493976, 0.19554487481174698, 0.20904467479292166, 0.1995526049510542, 0.1996555558170181, 0.20684740916792166, 0.18727350809487953, 0.19864810805722888, 0.19768184064382532, 0.1987687076430723, 0.20132335984563254, 0.19429328642695776, 0.19780244022966864, 0.20140424981174698, 0.20005118128765065, 0.2028999788215362, 0.20525020001882532, 0.1973553393260542, 0.19139448418674698, 0.19995999623493976, 0.20341914533132532, 0.19741710984563254, 0.1969376529555723, 0.19128270896084332, 0.20182193618222888, 0.20482221856174698, 0.20164839043674698, 0.19974674086972888, 0.19860692771084332, 0.20311323418674698, 0.1854115681475903, 0.1958596103162651, 0.18937958866716864, 0.1918312900037651, 0.19563605986445776, 0.1897355045180723, 0.1885662768260542, 0.18948106880647586, 0.1930314029555723, 0.19418004047439763, 0.1901531908885542, 0.19631700630647586, 0.18927810852786142, 0.19736563441265065, 0.19272696253765065, 0.18870893731174698, 0.1978950960090362, 0.19942171027861444, 0.1995526049510542, 0.19114004847515065, 0.19445506635918675, 0.20194400649472888, 0.1894604786332832, 0.19719208866716864, 0.20053946253765065, 0.19721267884036142, 0.19330642884036142, 0.19124152861445776, 0.1992775790662651, 0.1994305346385542, 0.1985760424510542, 0.1904679263930723, 0.19417121611445776, 0.19060029179216864, 0.19517866387424698, 0.19764066029743976, 0.19307258330195776, 0.19665233198418675, 0.20093655873493976, 0.20170869022966864, 0.20214696677334332, 0.1877514942582832, 0.2047825089420181, 0.19579783979668675, 0.18913544804216864, 0.1805802310805723, 0.19536103397966864, 0.19263577748493976, 0.18763971903237953, 0.19081354715737953, 0.1928784473832832, 0.1870190723832832, 0.19016348597515065, 0.20374417592243976, 0.19600227080195776, 0.19710090361445776, 0.18702936746987953, 0.18725291792168675, 0.18925604762801207, 0.19375352974397586, 0.1890942676957832, 0.19363145943147586, 0.20437511765813254, 0.18800592996987953, 0.1925945971385542, 0.19799657614834332, 0.19199454066265065, 0.18666315653237953, 0.19538162415286142, 0.18906338243599397, 0.19355056946536142, 0.1951977833207832, 0.18819859516189763, 0.1846791462725903, 0.18935899849397586, 0.19101650743599397, 0.18271572618599397, 0.18503506212349397, 0.19867752259036142, 0.19302110786897586, 0.1879956348832832, 0.19795539580195776, 0.20298086878765065, 0.19115034356174698, 0.1961346362010542, 0.18571747929216864, 0.19590079066265065, 0.19502570830195776, 0.18666315653237953, 0.1937844150037651, 0.1900708301957832, 0.18898249246987953, 0.18925751835466864, 0.19047822147966864, 0.19304169804216864, 0.19419180628765065], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.06938907373288002, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.0007569505800283788, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.3756707534253323e-06, "rotation_range": [0, 0], "momentum": 0.8022330341599173}, "accuracy_valid_max": 0.8194197689194277, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8058081937123494, "accuracy_valid_std": [0.016874090656167358, 0.014810893556721707, 0.01411411676707382, 0.01325620680830421, 0.0075220455398080375, 0.007262349347356856, 0.010937239123267481, 0.008956223369886862, 0.008691783883268162, 0.013279673696074261, 0.008061204545180887, 0.009477129661738831, 0.011589972558663925, 0.015886865045867553, 0.014219152574689576, 0.0142100321035247, 0.011784174185445242, 0.00941073834861083, 0.012845467645631907, 0.01208798604604295, 0.01271208698880681, 0.011651053038571087, 0.012919196138089771, 0.013426115269908107, 0.010971229709144043, 0.013293369678550365, 0.012962317534364767, 0.014059394839007346, 0.010689427405495043, 0.012612442719053775, 0.015039265557746307, 0.011917926721059907, 0.011898310635004681, 0.014937662614314855, 0.016033449198076134, 0.01627133217188918, 0.009697974584836549, 0.016258271605324413, 0.016189521684667668, 0.009280131370090022, 0.01660852796786882, 0.014310604098285865, 0.01174990799182746, 0.012664607428990853, 0.01328500338932582, 0.013091788390162052, 0.015299763777212233, 0.01309270805993065, 0.012279691368368513, 0.013882060364900075, 0.011884216827634676, 0.012070292289318944, 0.01284466751984377, 0.012576172547150374, 0.013071881321061454, 0.0108481092297825, 0.01067717084329968, 0.013353514817087896, 0.0130851326748993, 0.018367998422807258, 0.014927890851614493, 0.0076076241814445585, 0.014908442658782637, 0.01584186252989261, 0.010287109646267634, 0.011847731039929232, 0.013361306740963403, 0.014555992067054713, 0.007700531521110051, 0.013671834418895515, 0.01531940054859236, 0.009848826338832564, 0.011623158257187195, 0.01389448520100684, 0.014976261503352014, 0.009097352443145353, 0.013438706699911025, 0.01264005396043115, 0.015117500505908169, 0.012064131856264513, 0.01871710212450043, 0.013214998705428001, 0.01344899612144579, 0.010673725427193982, 0.012851028671069076, 0.013268852452358374, 0.014273926759672685, 0.0125686076584021, 0.01641536521368082, 0.013269036414779216, 0.010903046552617436, 0.007391841158028796, 0.01153684337791135, 0.011443006402290042, 0.011531397022531304, 0.013703681386434064, 0.007246530378313967, 0.015677458232269095, 0.011486119361823702, 0.013448328327946877, 0.013393552208693994, 0.012380861772968212, 0.010756516219718069, 0.013809431858932271, 0.011485666637712113, 0.016734945601172342, 0.013223734798744407, 0.016130646764010978, 0.014553229532766391, 0.012215586729250294, 0.014307069263522982, 0.01136605335853378, 0.006193292467972489, 0.010009748661711665, 0.012831767610820308, 0.012932229118402461, 0.012552785559368253, 0.01188661182748911, 0.014799838088817359, 0.007234198103279043, 0.01437725210201297, 0.01703256992521428, 0.012268130033547953, 0.013684246541951576, 0.015464826047538295, 0.013159097930531844, 0.010768329149017902, 0.017715939383545428, 0.014846916105008417, 0.01612062193508686, 0.011990211525667384, 0.012331775528338487, 0.015100063892284108, 0.010856308859285057, 0.01568244295823073, 0.01598892954556473, 0.017329104559303712, 0.014292570986003417, 0.013432056214399527, 0.013305985124741914, 0.016598824075050856, 0.011460216181280448, 0.010500247995268715, 0.010455945060181241, 0.012613529297741795, 0.016410359071612927, 0.014630541291454769, 0.012639314248353168, 0.011115667617515895, 0.013773645905101384, 0.015028201270991255, 0.013192586250472095, 0.017892841343717488, 0.013375335157944748, 0.018568906226352534, 0.015980389258198783, 0.01062911391431088, 0.015415760294344586, 0.013200754512556593], "accuracy_valid": [0.6265913262424698, 0.6832245976091867, 0.7182087725903614, 0.7429802216679217, 0.7601421310240963, 0.7725638883659638, 0.7777820265436747, 0.7796130812311747, 0.7766127988516567, 0.7782497176204819, 0.7804881635918675, 0.7846488493034638, 0.7800910673945783, 0.7799175216490963, 0.7821662627070783, 0.7671913238893072, 0.7961749341114458, 0.7853606810052711, 0.7811485198606928, 0.7870387801204819, 0.8026549557605422, 0.7892581066453314, 0.7877712019954819, 0.7945880200489458, 0.790539109563253, 0.7985339796686747, 0.7986766401543675, 0.8004576901355422, 0.7846076689570783, 0.7939261930534638, 0.8079245693712349, 0.7966220350150602, 0.7986163403614458, 0.7985751600150602, 0.804455125188253, 0.7909553252070783, 0.8004473950489458, 0.8003444441829819, 0.7931525908320783, 0.8127264919051205, 0.8013518919427711, 0.8023181593561747, 0.8012312923569277, 0.7986766401543675, 0.8057067135730422, 0.8021975597703314, 0.798595750188253, 0.7999488187123494, 0.7971000211784638, 0.7947497999811747, 0.8026446606739458, 0.808605515813253, 0.8000400037650602, 0.7965808546686747, 0.8025828901543675, 0.8030623470444277, 0.8087172910391567, 0.7981780638177711, 0.795177781438253, 0.798351609563253, 0.8002532591302711, 0.8013930722891567, 0.796886765813253, 0.8145884318524097, 0.8041403896837349, 0.8106204113328314, 0.8081687099962349, 0.8043639401355422, 0.8102644954819277, 0.8114337231739458, 0.8105189311935241, 0.8069685970444277, 0.8058199595256024, 0.8098468091114458, 0.8036829936935241, 0.8107218914721386, 0.8026343655873494, 0.8072730374623494, 0.811291062688253, 0.8021049039909638, 0.8005782897213856, 0.8004473950489458, 0.8088599515248494, 0.8055449336408133, 0.7980559935052711, 0.8105395213667168, 0.8028079113328314, 0.7994605374623494, 0.8027873211596386, 0.8066935711596386, 0.8087584713855422, 0.8007224209337349, 0.8005694653614458, 0.8014239575489458, 0.8095320736069277, 0.8058287838855422, 0.8093997082078314, 0.804821336125753, 0.8023593397025602, 0.8069274166980422, 0.8033476680158133, 0.7990634412650602, 0.7982913097703314, 0.7978530332266567, 0.8122485057417168, 0.7952174910579819, 0.8042021602033133, 0.8108645519578314, 0.8194197689194277, 0.8046389660203314, 0.8073642225150602, 0.8123602809676205, 0.8091864528426205, 0.8071215526167168, 0.8129809276167168, 0.8098365140248494, 0.7962558240775602, 0.8039977291980422, 0.8028990963855422, 0.8129706325301205, 0.8127470820783133, 0.8107439523719879, 0.8062464702560241, 0.8109057323042168, 0.8063685405685241, 0.7956248823418675, 0.8119940700301205, 0.8074054028614458, 0.8020034238516567, 0.8080054593373494, 0.8133368434676205, 0.8046183758471386, 0.810936617564006, 0.8064494305346386, 0.8048022166792168, 0.8118014048381024, 0.8153208537274097, 0.8106410015060241, 0.808983492564006, 0.817284273814006, 0.814964937876506, 0.8013224774096386, 0.8069788921310241, 0.8120043651167168, 0.8020446041980422, 0.7970191312123494, 0.808849656438253, 0.8038653637989458, 0.8142825207078314, 0.8040992093373494, 0.8049742916980422, 0.8133368434676205, 0.8062155849962349, 0.8099291698042168, 0.8110175075301205, 0.8107424816453314, 0.8095217785203314, 0.8069583019578314, 0.8058081937123494], "seed": 477909132, "model": "residualv3", "loss_std": [0.37256237864494324, 0.27679285407066345, 0.26513564586639404, 0.255953848361969, 0.2452140599489212, 0.235614612698555, 0.22351792454719543, 0.2080896645784378, 0.19190765917301178, 0.17906491458415985, 0.16709744930267334, 0.15557987987995148, 0.14875435829162598, 0.13559304177761078, 0.13079693913459778, 0.12941378355026245, 0.13078732788562775, 0.1250166893005371, 0.11649968475103378, 0.11528921872377396, 0.1135719045996666, 0.10762830823659897, 0.10658533871173859, 0.10375327616930008, 0.10259523242712021, 0.10180103778839111, 0.09456800669431686, 0.09875137358903885, 0.0928541049361229, 0.10082001239061356, 0.09147673845291138, 0.09314490854740143, 0.09024970233440399, 0.09686079621315002, 0.08535652607679367, 0.08856437355279922, 0.08924584835767746, 0.08854763954877853, 0.08410792052745819, 0.08641788363456726, 0.08609962463378906, 0.08199890702962875, 0.08350086957216263, 0.07935415953397751, 0.08516097068786621, 0.08350060135126114, 0.07877618074417114, 0.07612424343824387, 0.08224049210548401, 0.0805816650390625, 0.07740096002817154, 0.07622136175632477, 0.07600530236959457, 0.08282987028360367, 0.07827465236186981, 0.07710300385951996, 0.07546667754650116, 0.07658412307500839, 0.07786340266466141, 0.08189266175031662, 0.07191251218318939, 0.07490929216146469, 0.0726630687713623, 0.06899361312389374, 0.07659152150154114, 0.07181553542613983, 0.07010333240032196, 0.07473403215408325, 0.07654266059398651, 0.06990887969732285, 0.07339873909950256, 0.06878514587879181, 0.07247816771268845, 0.07544105499982834, 0.07083038240671158, 0.06732182204723358, 0.06894832849502563, 0.07199734449386597, 0.07125652581453323, 0.07241321355104446, 0.06906729936599731, 0.0754169300198555, 0.06756541877985, 0.06826575845479965, 0.06954003870487213, 0.07238033413887024, 0.06707940995693207, 0.07006291300058365, 0.07188612222671509, 0.0705050602555275, 0.0694005936384201, 0.06692507117986679, 0.07025421410799026, 0.06249900162220001, 0.0727556124329567, 0.06843273341655731, 0.06460517644882202, 0.06237281858921051, 0.07282794266939163, 0.06460423022508621, 0.0692196786403656, 0.06329361349344254, 0.0664229542016983, 0.06988629698753357, 0.06596323847770691, 0.06807591021060944, 0.06507272273302078, 0.0676354393362999, 0.06582679599523544, 0.06712089478969574, 0.06455329805612564, 0.0680222138762474, 0.06940861791372299, 0.06611640006303787, 0.06953016668558121, 0.06402778625488281, 0.06854771077632904, 0.06217080354690552, 0.07002357393503189, 0.06571286171674728, 0.0666065365076065, 0.06712404638528824, 0.06293945014476776, 0.0637437030673027, 0.0701923742890358, 0.057778093963861465, 0.06656217575073242, 0.06392802298069, 0.05897118151187897, 0.06654643267393112, 0.059370290488004684, 0.06678780913352966, 0.06198298931121826, 0.06652219593524933, 0.06298753619194031, 0.06603055447340012, 0.0682632252573967, 0.06453142315149307, 0.06838221848011017, 0.0601319819688797, 0.06372292339801788, 0.06931515783071518, 0.061217837035655975, 0.05800335481762886, 0.06250681728124619, 0.06086545065045357, 0.06419489532709122, 0.06743281334638596, 0.059043753892183304, 0.06421801447868347, 0.06774841248989105, 0.05876866728067398, 0.05971215292811394, 0.06667792797088623, 0.05952832102775574, 0.06617607176303864, 0.058330751955509186, 0.0603385865688324, 0.0634840801358223]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:28 2016", "state": "available"}], "summary": "992eaa2a02b2cccc0f78db02c47e51e8"}