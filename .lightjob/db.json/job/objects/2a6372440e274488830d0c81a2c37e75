{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 32, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.504284381866455, 1.0708543062210083, 0.8250324130058289, 0.7198334336280823, 0.653685450553894, 0.6110826134681702, 0.5714794993400574, 0.5425376892089844, 0.5200541019439697, 0.49841850996017456, 0.481150358915329, 0.4637160301208496, 0.4520028829574585, 0.43950098752975464, 0.4261700212955475, 0.4166578948497772, 0.40666887164115906, 0.39733991026878357, 0.39115288853645325, 0.3814385235309601, 0.3758839964866638, 0.3701377511024475, 0.36299124360084534, 0.3565492033958435, 0.351655513048172, 0.3438563346862793, 0.33920860290527344, 0.33532240986824036, 0.332419753074646, 0.3266528844833374, 0.32183799147605896, 0.3189108371734619, 0.3122456967830658, 0.310800701379776, 0.3077034652233124, 0.30438151955604553, 0.30034372210502625, 0.29878687858581543, 0.29409459233283997, 0.2919183075428009, 0.28722208738327026, 0.28766003251075745, 0.2831166982650757, 0.2812735140323639, 0.27908915281295776, 0.27579984068870544, 0.27407094836235046, 0.27123141288757324, 0.2697295844554901, 0.2660509943962097, 0.2647551894187927, 0.263203501701355, 0.2629079222679138, 0.25947943329811096, 0.25568562746047974, 0.2547885775566101, 0.2533470690250397, 0.2500089108943939, 0.24788625538349152, 0.24669376015663147, 0.24422548711299896, 0.24561764299869537, 0.24305306375026703, 0.24315506219863892, 0.2398328334093094, 0.2394704818725586, 0.2365419864654541, 0.23468132317066193, 0.23226134479045868, 0.23366330564022064, 0.23070038855075836, 0.23088274896144867, 0.22833430767059326, 0.22789667546749115, 0.2258967012166977, 0.2242894321680069, 0.22621461749076843, 0.22288693487644196, 0.22127075493335724, 0.21977482736110687, 0.2205025851726532, 0.21771138906478882, 0.21548543870449066, 0.21585378050804138, 0.21435128152370453, 0.21490435302257538, 0.21301981806755066, 0.21175499260425568, 0.2104436755180359, 0.2106175720691681, 0.20766538381576538, 0.2078717052936554, 0.20809485018253326, 0.20646128058433533, 0.2063559740781784, 0.20661500096321106, 0.2021128088235855, 0.2040465623140335, 0.20014898478984833, 0.2011924535036087, 0.19890597462654114, 0.19878242909908295, 0.1987515091896057, 0.19846142828464508, 0.19754132628440857, 0.19770580530166626, 0.19470228254795074, 0.1940910667181015, 0.19442369043827057, 0.19396398961544037, 0.19144649803638458, 0.19322237372398376, 0.18872803449630737, 0.19097967445850372, 0.19056200981140137, 0.1905059963464737, 0.18853572010993958, 0.18681062757968903, 0.18828411400318146, 0.18712036311626434, 0.18557725846767426, 0.18525096774101257, 0.1840783804655075, 0.18513378500938416, 0.1844346970319748, 0.18203502893447876, 0.18209540843963623, 0.1829788088798523, 0.1810794323682785, 0.18040905892848969, 0.17951396107673645, 0.17906416952610016, 0.1770351529121399, 0.17812824249267578, 0.17849142849445343, 0.17537042498588562, 0.17850017547607422, 0.17811241745948792, 0.17694087326526642, 0.17568421363830566, 0.17463482916355133, 0.17395979166030884, 0.17290104925632477, 0.17288514971733093, 0.17270229756832123, 0.1723846048116684, 0.17195679247379303, 0.17366236448287964, 0.17141477763652802, 0.16982950270175934, 0.17043226957321167, 0.17012281715869904, 0.16832537949085236, 0.16720883548259735, 0.1681671291589737, 0.16741472482681274, 0.16650688648223877, 0.16742227971553802, 0.16592057049274445, 0.16639170050621033, 0.16544082760810852, 0.1641981303691864, 0.1643630713224411, 0.16462339460849762, 0.16312019526958466, 0.16475646197795868], "moving_avg_accuracy_train": [0.05673110753506828, 0.12120029228382243, 0.1877916511656746, 0.25071587204255485, 0.3092029232888346, 0.36330364381640184, 0.41291744846623085, 0.4587018596331275, 0.5005241022643145, 0.5386917852168698, 0.5735472571658361, 0.6054169086698598, 0.6345230244485457, 0.6611531872814689, 0.685264384910834, 0.7073946513558431, 0.727555959683714, 0.745996322931151, 0.7627972989979013, 0.7780507830377568, 0.7920395155843871, 0.8048383859274804, 0.8165363335969601, 0.8273272283149681, 0.8370715495956897, 0.8461133369637786, 0.8542742691807912, 0.8618213961225312, 0.8686928293808023, 0.8749677640679989, 0.8808663213579044, 0.8862332597855138, 0.8912540584262869, 0.8958796619505833, 0.9001357471236499, 0.904068638473485, 0.9075476426240139, 0.9110437586737664, 0.9140717886757143, 0.9170387590560203, 0.9197601135744677, 0.9222930740470321, 0.9245471618354353, 0.9267130246247599, 0.9286903471161414, 0.9304930807000238, 0.9322294371683658, 0.9339060902815403, 0.9353522269679027, 0.9368954933641818, 0.9383264299946794, 0.9396467889966419, 0.9407236131020071, 0.9418414201253704, 0.9429171648618644, 0.9438947078175846, 0.9448697554812857, 0.9458496409750545, 0.9467293209170933, 0.9476046661244334, 0.9483506241324681, 0.9489173546432708, 0.9496691554815462, 0.9503923152610032, 0.9510523515112964, 0.951723041949637, 0.9523057730536766, 0.9528140992009205, 0.9533669959322681, 0.953915539971378, 0.9544371674411098, 0.95497881992342, 0.9554360081253379, 0.9558730901927875, 0.956247862863016, 0.9566758030209743, 0.957067816463109, 0.9574718899812961, 0.9578145577107412, 0.9582252652148608, 0.9586135031590448, 0.9589698927552389, 0.9593696624025186, 0.9597224796386419, 0.9601027941690099, 0.9604241509070554, 0.9607110828713055, 0.9610274143105499, 0.9612515866391849, 0.961518590096898, 0.9617426171671731, 0.9619418442839738, 0.9621467974236366, 0.9624102742600382, 0.9626381028175615, 0.9629197702835904, 0.9631826076470734, 0.9633981988861037, 0.9636365530238867, 0.9638998638240728, 0.9640903766168686, 0.9642757169256044, 0.9644541849963331, 0.9646612731873605, 0.9649336109676001, 0.9651276698424625, 0.9652929501369629, 0.9654068612186893, 0.9656000259469957, 0.9658133656834146, 0.9659937817509627, 0.9662003700879558, 0.9663606148078886, 0.9664839808141799, 0.9666136114103181, 0.9668371636944432, 0.9669571247370974, 0.9671115205540393, 0.9673178700559443, 0.9674826943171919, 0.9675821719784959, 0.9676926282129553, 0.9677385604013498, 0.9679821512685145, 0.9681526270216003, 0.9683059470529213, 0.9684625362715863, 0.9685617580850885, 0.9687114394886506, 0.9688717293887611, 0.9689950639595749, 0.9691106072244702, 0.9692193546069512, 0.9692916145654606, 0.9694310172412051, 0.9695681414422415, 0.9696590371886596, 0.9697919605854268, 0.9699348070817937, 0.9699890002154379, 0.970079626714289, 0.970142589372779, 0.9702086645071142, 0.9702655906862938, 0.9704286116345064, 0.970449700354546, 0.9705104968323344, 0.9706210172337724, 0.9706832832141141, 0.9707741277309271, 0.9708861507794013, 0.9709405045956565, 0.9709777972862385, 0.9710926327696393, 0.9711519150237754, 0.9712447244846223, 0.9713259638993937, 0.9713642742381827, 0.9714568462145121, 0.9715308243491516, 0.9716229452584132, 0.9717082513231957, 0.9717990137231759, 0.9718619905462256, 0.971983773853637, 0.9720817530862597], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05745584878576806, 0.12190672357398341, 0.1877995767248682, 0.24941748002753197, 0.30603531998599864, 0.35787645925057654, 0.4056403534704737, 0.44915584913811, 0.48866365113130206, 0.5244108378818767, 0.5569271618497131, 0.5864349645371063, 0.613117145794239, 0.6376580702867278, 0.6594254605001484, 0.6795685166244558, 0.6976341729627632, 0.7140051859658091, 0.7291164861286408, 0.742754306877599, 0.7549398078069325, 0.7659332317231519, 0.7761457255689089, 0.785286082887771, 0.7935022564608162, 0.8012630236140568, 0.8078682665745638, 0.81403271180152, 0.8194230506081903, 0.8244920230793743, 0.8290185067183495, 0.8330923419934272, 0.8368452724684068, 0.8397335991372288, 0.8427613687415781, 0.845521952970583, 0.8478345508305277, 0.8500023676318876, 0.8520012013694518, 0.85383574331835, 0.855524481674768, 0.8569711040080442, 0.8581377572555832, 0.8595438081019375, 0.8606627694886565, 0.8618549992227729, 0.8627672555599082, 0.8639066985844897, 0.8647135002527726, 0.8654640358167273, 0.8661751094093768, 0.8667285969153519, 0.8669703880144793, 0.8672968337762844, 0.8678358050955686, 0.8683473523627436, 0.8689165786757915, 0.8691704756839654, 0.8697693119634604, 0.870000000307777, 0.870195412786412, 0.8702471546873641, 0.8702825448756307, 0.87052397459364, 0.8707890599561886, 0.8709889566714131, 0.8710753254997537, 0.8711520279366006, 0.8712810657773532, 0.8713005730926902, 0.8713648987841741, 0.8715092706339194, 0.8715639040938709, 0.8716629318414868, 0.8718364765244314, 0.8719835482338105, 0.8721626818799325, 0.8724378834773309, 0.8726458552952604, 0.8727486102213067, 0.8727912320210887, 0.8728295916408925, 0.8729139729323755, 0.8730397737283698, 0.8731529944447647, 0.8731928284246104, 0.873218530992541, 0.8734003547099284, 0.8735395819930771, 0.8734553079993417, 0.8735880104448894, 0.8736474369982921, 0.8737975476376948, 0.8738095473919976, 0.8737582825059604, 0.8739573142421867, 0.8738170009749711, 0.8737904343017961, 0.8736932821084389, 0.8736180521656672, 0.8737833083195825, 0.8737001052643562, 0.8734766791223332, 0.8733864883844222, 0.873330760291462, 0.8733772317491381, 0.8735045052797965, 0.8734603600511391, 0.8732466423818686, 0.873423595943004, 0.8735319670057067, 0.8735328742207987, 0.8733617627682219, 0.8732963002056315, 0.8734072528281407, 0.8736922746744682, 0.8737890734212533, 0.8738995768472003, 0.8738993146632333, 0.8739367293000726, 0.8737974450184087, 0.8738826972221402, 0.8736521894069291, 0.8735332701179681, 0.8736571468429936, 0.8737208372791762, 0.8737293305467404, 0.8737349154702291, 0.8738020065662785, 0.8738278264762922, 0.8739344545967352, 0.8739836507974532, 0.8739282121107802, 0.8738030160879552, 0.8739619828808916, 0.8738853264320343, 0.8737420639319031, 0.8736731333293755, 0.8737697871933505, 0.8736746997108378, 0.8736247125616666, 0.8737160604884818, 0.8735144233778867, 0.8736178297317395, 0.8735888251377071, 0.8735362479232587, 0.8735601116004358, 0.8734483410748048, 0.8732978899680773, 0.8733089683470225, 0.8731236263880734, 0.8729700551649286, 0.8730292125814176, 0.8730203895913481, 0.8731854063551049, 0.8732352356838263, 0.8732291949373564, 0.873175959649193, 0.8731667280009153, 0.8732540167501461, 0.8731118205532942, 0.8730581156722871, 0.8731806797168806, 0.8731821535844244, 0.8732678997753043, 0.8732728586682558], "moving_var_accuracy_train": [0.028965767059399336, 0.06347567239298035, 0.09703778685326699, 0.12296912632460205, 0.14145883016350644, 0.15365493880157324, 0.16044321142789847, 0.1632648010382037, 0.16268022074270025, 0.1595231468661312, 0.15450496750258472, 0.14819554293521747, 0.14100048242319613, 0.13328292433344854, 0.1251867805602044, 0.11707584074052789, 0.10902656184789837, 0.10118432863338538, 0.09360635094120644, 0.08633973482527293, 0.07946692308709605, 0.07299453051692024, 0.06692665528232919, 0.061281980433432456, 0.056008348565086546, 0.051143298977865466, 0.046628376411935, 0.04247817089641414, 0.03865530316197859, 0.035144146093737984, 0.03194286828728487, 0.02900781771132813, 0.02633391171111602, 0.023893086411680264, 0.021666806119515804, 0.01963933421689069, 0.0177843320241162, 0.016115904268604615, 0.014586834532978418, 0.013207377298819096, 0.011953291502672854, 0.010815705351205728, 0.009779863021905606, 0.008844095374314676, 0.00799487407499752, 0.007224635302867884, 0.006529306176647475, 0.005901676049939987, 0.005330330246786777, 0.004818732262637059, 0.004355287253137853, 0.003935448658870638, 0.003552339744368637, 0.003208351202805097, 0.0028979311231674375, 0.002616738322923197, 0.0023636209521492786, 0.0021359004371624374, 0.0019292749246500197, 0.0017432434952731376, 0.0015739272258935835, 0.001419425154551097, 0.0012825694795998712, 0.001159019172239503, 0.0010470380856808625, 0.0009463827080895105, 0.0008548006171370967, 0.0007716461146711344, 0.0006972327563638346, 0.0006302175857930376, 0.0005696446841683441, 0.0005153207024558449, 0.0004656698216780165, 0.00042082220611338906, 0.00038000407649120164, 0.0003436518638512217, 0.00031066974831542874, 0.0002810722521567866, 0.00025402181749633576, 0.00023013776163216415, 0.00020848054378068507, 0.000188775611301095, 0.00017133639210896098, 0.0001553230729170153, 0.0001410925179033951, 0.00012791269749084074, 0.00011586239731073284, 0.00010517674779475003, 9.511135211160549e-05, 8.624183451832181e-05, 7.806934422043418e-05, 7.061963279500885e-05, 6.39357216206267e-05, 5.816692984844562e-05, 5.281738952820946e-05, 4.8249679628161364e-05, 4.404646298212934e-05, 4.006013292503558e-05, 3.656543388751713e-05, 3.353288369621712e-05, 3.0506251444564963e-05, 2.7764785570489048e-05, 2.5274964683866432e-05, 2.3133437885247027e-05, 2.1487604895634734e-05, 1.9677774028286898e-05, 1.7955854807209842e-05, 1.627705093734952e-05, 1.498515935396977e-05, 1.3896268006790474e-05, 1.279959082297735e-05, 1.1903740409513652e-05, 1.0944471700959158e-05, 9.98699707443751e-06, 9.139534190090327e-06, 8.675361384719129e-06, 7.93734111203947e-06, 7.358149615437641e-06, 7.005555706322026e-06, 6.5495034695521294e-06, 5.9836153684836664e-06, 5.495059049213805e-06, 4.964541037668705e-06, 5.002115528996588e-06, 4.763461817608452e-06, 4.498678923885943e-06, 4.269492682116528e-06, 3.9311481283767455e-06, 3.7396740186897554e-06, 3.5969422855179185e-06, 3.3741508041865794e-06, 3.1568879383317467e-06, 2.9476330832667914e-06, 2.6998632893741174e-06, 2.6047749144791177e-06, 2.513524841619983e-06, 2.336530687910217e-06, 2.2618952837921573e-06, 2.2193518491315743e-06, 2.023848725826007e-06, 1.8953823138896268e-06, 1.7415227497776857e-06, 1.6066637851967154e-06, 1.4751627155610243e-06, 1.5668289100102362e-06, 1.4141486260253804e-06, 1.3059996688260025e-06, 1.285332534149616e-06, 1.1916927515060412e-06, 1.1467980124703739e-06, 1.1450606817285909e-06, 1.0571436496292043e-06, 9.639459876038907e-07, 9.86236083074746e-07, 9.192419456663928e-07, 9.04840115303983e-07, 8.737546863852369e-07, 7.995883562698206e-07, 7.967556578565845e-07, 7.663349717137215e-07, 7.660778318510945e-07, 7.549641708641184e-07, 7.536080730291504e-07, 7.139419878991341e-07, 7.760283547857662e-07, 7.848248895350716e-07], "duration": 230043.761964, "accuracy_train": [0.5673110753506829, 0.7014229550226099, 0.787113881102344, 0.8170338599344776, 0.8355863845053525, 0.8502101285645073, 0.8594416903146919, 0.8707615601351975, 0.8769242859449982, 0.882200931789867, 0.8872465047065338, 0.8922437722060724, 0.8964780664567183, 0.9008246527777777, 0.9022651635751201, 0.9065670493609265, 0.9090077346345515, 0.9119595921580842, 0.9140060835986527, 0.9153321393964563, 0.9179381085040605, 0.9200282190153194, 0.9218178626222776, 0.9244452807770396, 0.9247704411221853, 0.927489423276578, 0.9277226591339055, 0.9297455385981912, 0.9305357287052418, 0.9314421762527685, 0.9339533369670543, 0.9345357056339978, 0.9364412461932448, 0.9375100936692506, 0.9384405136812477, 0.9394646606220007, 0.9388586799787744, 0.9425088031215393, 0.9413240586932448, 0.9437414924787744, 0.9442523042404946, 0.9450897183001107, 0.9448339519310631, 0.9462057897286821, 0.9464862495385751, 0.9467176829549648, 0.947856645383444, 0.9489959683001107, 0.9483674571451642, 0.9507848909306941, 0.9512048596691584, 0.9515300200143041, 0.9504150300502953, 0.9519016833356404, 0.9525988674903102, 0.9526925944190661, 0.9536451844545959, 0.9546686104189737, 0.9546464403954411, 0.9554827729904946, 0.9550642462047803, 0.9540179292404946, 0.9564353630260245, 0.9569007532761166, 0.956992677763935, 0.9577592558947029, 0.9575503529900333, 0.9573890345261166, 0.9583430665143964, 0.9588524363233666, 0.959131814668697, 0.9598536922642118, 0.9595507019425988, 0.9598068287998339, 0.9596208168950721, 0.9605272644425988, 0.960595937442322, 0.9611085516449798, 0.9608985672757475, 0.961921632751938, 0.9621076446567, 0.9621773991209857, 0.9629675892280363, 0.9628978347637505, 0.963525624942322, 0.9633163615494648, 0.963293470549557, 0.9638743972637505, 0.9632691375968992, 0.9639216212163161, 0.9637588607996493, 0.9637348883351791, 0.9639913756806018, 0.9647815657876523, 0.9646885598352714, 0.9654547774778516, 0.9655481439184201, 0.9653385200373754, 0.965781740263935, 0.9662696610257475, 0.9658049917520304, 0.9659437797042267, 0.9660603976328904, 0.9665250669066077, 0.9673846509897563, 0.9668741997162238, 0.9667804727874677, 0.9664320609542267, 0.9673385085017534, 0.967733423311185, 0.9676175263588963, 0.9680596651208934, 0.9678028172872831, 0.9675942748708011, 0.9677802867755629, 0.9688491342515688, 0.9680367741209857, 0.9685010829065154, 0.9691750155730897, 0.9689661126684201, 0.9684774709302326, 0.9686867343230897, 0.9681519500968992, 0.9701744690729974, 0.9696869087993725, 0.9696858273348099, 0.9698718392395718, 0.9694547544066077, 0.9700585721207088, 0.9703143384897563, 0.9701050750968992, 0.9701504966085271, 0.9701980810492802, 0.9699419541920451, 0.9706856413229051, 0.9708022592515688, 0.9704770989064231, 0.9709882711563308, 0.9712204255490956, 0.9704767384182356, 0.9708952652039498, 0.9707092532991879, 0.9708033407161315, 0.970777926298911, 0.9718958001684201, 0.9706394988349022, 0.971057665132429, 0.9716157008467147, 0.9712436770371908, 0.9715917283822444, 0.9718943582156699, 0.9714296889419527, 0.9713134315014765, 0.9721261521202473, 0.9716854553110004, 0.9720800096322444, 0.9720571186323367, 0.9717090672872831, 0.9722899940014765, 0.9721966275609081, 0.9724520334417681, 0.9724760059062385, 0.9726158753229974, 0.9724287819536729, 0.9730798236203396, 0.9729635661798633], "end": "2016-02-06 03:32:32.468000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0], "moving_var_accuracy_valid": [0.029710571037237413, 0.0641247512822096, 0.09678908902126525, 0.12128107418588654, 0.13800318498136083, 0.14839039996546893, 0.15408386628836776, 0.15571786492833167, 0.1541938762004979, 0.15027524082567223, 0.14476351866253664, 0.13812356057122613, 0.13071865368384988, 0.1230671010899792, 0.11502476447131114, 0.10717397241442303, 0.09939388662338518, 0.09186658856175976, 0.08473509223908457, 0.07793549440820255, 0.07147832286347137, 0.06541818890173959, 0.059815025286512266, 0.054585437945109325, 0.049734443724240046, 0.045303064913077416, 0.04116542153287562, 0.037390882844193354, 0.03391329633183028, 0.03075321703586984, 0.027862296819488052, 0.025225432342175454, 0.022829649492308184, 0.020621766421589822, 0.01864209627842404, 0.016846474078150507, 0.015209959650091845, 0.013731258552240985, 0.012394090723810714, 0.011184971548890046, 0.01009214092912898, 0.009101761281792284, 0.008203834871812995, 0.007401244195474499, 0.00667238844719176, 0.006017942308122783, 0.005423637981932297, 0.004892959157395473, 0.004409521602043423, 0.003973639174533927, 0.0035808258879680066, 0.00322550043494464, 0.0029034765578707313, 0.0026140880036022627, 0.002355293613989135, 0.0021221193780492104, 0.0019128236076034846, 0.001722121420059973, 0.001553136722060731, 0.0013983020038644896, 0.001258815477809297, 0.0011329580250471947, 0.0010196734947313052, 0.0009182307400368168, 0.0008270400982780731, 0.0007446957167210838, 0.0006702932812195559, 0.0006033169024719647, 0.000543135069103883, 0.0004888249870116596, 0.00043997972846175767, 0.0003961693446945717, 0.0003565792735596309, 0.00032100960465684876, 0.00028917970400396925, 0.00026045640439286967, 0.00023469956372213925, 0.0002119112306228206, 0.0001911093780540149, 0.00017209346742205433, 0.0001549004702401988, 0.00013942366636006233, 0.0001255453815452272, 0.00011313327595315986, 0.00010193531873343276, 9.175606757364269e-05, 8.258640641426244e-05, 7.462530455067762e-05, 6.733723222296666e-05, 6.066742795485106e-05, 5.475917461085516e-05, 4.931504078701349e-05, 4.458633554486918e-05, 4.012899793731221e-05, 3.613975094044457e-05, 3.2882298534627196e-05, 2.9771258997775056e-05, 2.6800485191109746e-05, 2.4205383610066038e-05, 2.1835781147664202e-05, 1.9897989400559385e-05, 1.7970495196094392e-05, 1.6622718844938345e-05, 1.5033656283288949e-05, 1.3558241238064975e-05, 1.2221853481665342e-05, 1.1145455097955283e-05, 1.0048448799078656e-05, 9.454681098596672e-06, 8.79102605392359e-06, 8.017622033612876e-06, 7.215867237604596e-06, 6.7577926766707874e-06, 6.120581532911655e-06, 5.619317739595369e-06, 5.788523041591288e-06, 5.2940007138446655e-06, 4.874499706774314e-06, 4.387050354760776e-06, 3.960944014733062e-06, 3.7394506133273836e-06, 3.4309169961642006e-06, 3.5660299724079075e-06, 3.3367031507499862e-06, 3.1411418227023214e-06, 2.8635358853822256e-06, 2.5778315171892746e-06, 2.3203290878037095e-06, 2.128807115545296e-06, 1.921926413768818e-06, 1.832059777014924e-06, 1.6706361947992507e-06, 1.5312336071396076e-06, 1.5191766436064327e-06, 1.5946929505541842e-06, 1.4881095558613699e-06, 1.5240158957696689e-06, 1.4143771578761402e-06, 1.3570171668803928e-06, 1.302690114167899e-06, 1.1949095384914164e-06, 1.1505185782433059e-06, 1.4013844397414755e-06, 1.3574818619217087e-06, 1.2293050740043836e-06, 1.1312538379163463e-06, 1.0232537299204497e-06, 1.0333622105267281e-06, 1.1337458091137261e-06, 1.021475802522852e-06, 1.228492997994848e-06, 1.3179007833987557e-06, 1.21760710438983e-06, 1.0965470003347415e-06, 1.2319670911880646e-06, 1.1311170400767222e-06, 1.0183337516302819e-06, 9.420063396197728e-07, 8.485727156271091e-07, 8.322893757448149e-07, 9.31038263762792e-07, 8.638923655824278e-07, 9.127006342684141e-07, 8.214501214114045e-07, 8.054767925238843e-07, 7.251504288452359e-07], "accuracy_test": 0.8120794802295919, "start": "2016-02-03 11:38:28.706000", "learning_rate_per_epoch": [0.0010348873911425471, 0.0005174436955712736, 0.0003449624637141824, 0.0002587218477856368, 0.00020697747822850943, 0.0001724812318570912, 0.00014784105587750673, 0.0001293609238928184, 0.00011498749518068507, 0.00010348873911425471, 9.408067853655666e-05, 8.62406159285456e-05, 7.960672519402578e-05, 7.392052793875337e-05, 6.899249274283648e-05, 6.46804619464092e-05, 6.087572910473682e-05, 5.7493747590342537e-05, 5.446776049211621e-05, 5.1744369557127357e-05, 4.928035195916891e-05, 4.704033926827833e-05, 4.499510396271944e-05, 4.31203079642728e-05, 4.139549855608493e-05, 3.980336259701289e-05, 3.832916263490915e-05, 3.696026396937668e-05, 3.568577449186705e-05, 3.449624637141824e-05, 3.338346505188383e-05, 3.23402309732046e-05, 3.136022496619262e-05, 3.043786455236841e-05, 2.956821299449075e-05, 2.8746873795171268e-05, 2.796993067022413e-05, 2.7233880246058106e-05, 2.6535575671005063e-05, 2.5872184778563678e-05, 2.524115734559018e-05, 2.4640175979584455e-05, 2.4067148842732422e-05, 2.3520169634139165e-05, 2.2997497580945492e-05, 2.249755198135972e-05, 2.201888128183782e-05, 2.15601539821364e-05, 2.1120151359355077e-05, 2.0697749278042465e-05, 2.0291910914238542e-05, 1.9901681298506446e-05, 1.9526178220985457e-05, 1.9164581317454576e-05, 1.8816135707311332e-05, 1.848013198468834e-05, 1.8155918951379135e-05, 1.7842887245933525e-05, 1.7540465705678798e-05, 1.724812318570912e-05, 1.6965366739896126e-05, 1.6691732525941916e-05, 1.6426783986389637e-05, 1.61701154866023e-05, 1.5921345038805157e-05, 1.568011248309631e-05, 1.54460813064361e-05, 1.5218932276184205e-05, 1.4998368897067849e-05, 1.4784106497245375e-05, 1.4575879504263867e-05, 1.4373436897585634e-05, 1.4176540389598813e-05, 1.3984965335112065e-05, 1.3798498912365176e-05, 1.3616940123029053e-05, 1.3440096154226921e-05, 1.3267787835502531e-05, 1.3099840543873142e-05, 1.2936092389281839e-05, 1.2776387848134618e-05, 1.262057867279509e-05, 1.246852298208978e-05, 1.2320087989792228e-05, 1.2175146366644185e-05, 1.2033574421366211e-05, 1.1895257557625882e-05, 1.1760084817069583e-05, 1.1627948879322503e-05, 1.1498748790472746e-05, 1.1372389053576626e-05, 1.124877599067986e-05, 1.1127821380796377e-05, 1.100944064091891e-05, 1.0893551916524302e-05, 1.07800769910682e-05, 1.0668942195479758e-05, 1.0560075679677539e-05, 1.0453408322064206e-05, 1.0348874639021233e-05, 1.0246410056424793e-05, 1.0145955457119271e-05, 1.004745081445435e-05, 9.950840649253223e-06, 9.856070391833782e-06, 9.763089110492729e-06, 9.671844964032061e-06, 9.582290658727288e-06, 9.494379810348619e-06, 9.408067853655666e-06, 9.323310223408043e-06, 9.24006599234417e-06, 9.158296052191872e-06, 9.077959475689568e-06, 8.999020792543888e-06, 8.921443622966763e-06, 8.84519158717012e-06, 8.770232852839399e-06, 8.696532859175932e-06, 8.62406159285456e-06, 8.552789040550124e-06, 8.482683369948063e-06, 8.413719115196727e-06, 8.345866262970958e-06, 8.279099347419105e-06, 8.213391993194818e-06, 8.148719643941149e-06, 8.08505774330115e-06, 8.022383553907275e-06, 7.960672519402578e-06, 7.899903721408918e-06, 7.840056241548155e-06, 7.781108251947444e-06, 7.72304065321805e-06, 7.66583252698183e-06, 7.6094661380921025e-06, 7.553922841907479e-06, 7.499184448533924e-06, 7.445233222824754e-06, 7.3920532486226875e-06, 7.339627245528391e-06, 7.287939752131933e-06, 7.236974852276035e-06, 7.186718448792817e-06, 7.1371546255249996e-06, 7.0882701947994065e-06, 7.040050604700809e-06, 6.9924826675560325e-06, 6.9455531956919e-06, 6.899249456182588e-06, 6.853559170849621e-06, 6.808470061514527e-06, 6.763970304746181e-06, 6.720048077113461e-06, 6.676692919427296e-06, 6.633893917751266e-06, 6.591639703401597e-06, 6.549920271936571e-06, 6.508726073661819e-06, 6.4680461946409196e-06, 6.427871994674206e-06, 6.388193924067309e-06, 6.349002433125861e-06, 6.310289336397545e-06, 6.272045084187994e-06, 6.23426149104489e-06], "accuracy_train_first": 0.5673110753506829, "accuracy_train_last": 0.9729635661798633, "batch_size_eval": 1024, "accuracy_train_std": [0.019684597644109066, 0.019375793052927366, 0.019131435171250873, 0.016764011713704258, 0.016496529214230067, 0.014709407663122785, 0.014169472878008184, 0.014095226372009849, 0.013537588493275141, 0.013515591645282717, 0.012508370933119175, 0.0134269273657203, 0.011520419925895142, 0.011433828480261352, 0.011733194619215178, 0.010862814900374322, 0.01106620676682296, 0.011504048073042481, 0.011494999051603134, 0.010378230653647088, 0.011496693503958424, 0.011194532497989547, 0.011322594796535187, 0.009793721835170743, 0.010157361750458492, 0.009799973445432526, 0.00974370048038778, 0.009366901805701825, 0.01008832169771827, 0.010368728344209264, 0.010252205067722509, 0.009787958412877312, 0.009198775907530892, 0.009219191496439814, 0.009181568506463912, 0.009670124301337436, 0.00966023335572859, 0.009350678151787662, 0.009083704732381931, 0.009246692210825089, 0.00922750733851137, 0.009407572832029046, 0.00856501289353451, 0.00857818256848919, 0.008642899844398883, 0.008293569290634649, 0.008392313862378759, 0.009006380128577043, 0.008659952921298565, 0.008531304993880856, 0.008465032953820804, 0.007612149720253082, 0.008325747946006632, 0.00797031467037844, 0.008839229666774136, 0.008066609046800757, 0.008251012741959489, 0.0072980743375169604, 0.0074132720412929815, 0.00731896844938089, 0.008357779836285576, 0.008291644462689434, 0.008024020901291175, 0.00801159577286326, 0.007996420830805814, 0.0073145021626918475, 0.007584716612041841, 0.007737827741499555, 0.007783404740731493, 0.007676662980622287, 0.007896041532305604, 0.00777121683674219, 0.007356158036891059, 0.007450826627699959, 0.008008882127820792, 0.007126128940900462, 0.007138102492871124, 0.00788707133904082, 0.007622338233800031, 0.007182802028897705, 0.006987878396373115, 0.007419088116289306, 0.007359931173103994, 0.007883175177929694, 0.006977465329017623, 0.007559726622858318, 0.007228815516364236, 0.007425412962418719, 0.0072625510937534225, 0.006666443207339854, 0.006885292223143806, 0.0072415423685608755, 0.007310506022508418, 0.007272149876279662, 0.0061581692968294705, 0.006745043151631402, 0.006995274820782453, 0.006889368138295296, 0.006888732539089622, 0.006957190744901757, 0.007622680124933659, 0.006992461604924182, 0.0071334107561413255, 0.006867621527818801, 0.007174256198342141, 0.007241806592749746, 0.0071583689227449, 0.007000883118136799, 0.006473943801266573, 0.006675161014313938, 0.007359122092072708, 0.007084121150020606, 0.006631085586786653, 0.006663916645344673, 0.006877562092809217, 0.007036580311205259, 0.007680879973441975, 0.006720065155193495, 0.006456523317826485, 0.006775037987125915, 0.006896413448906693, 0.0064084787936081805, 0.006321837056827915, 0.007199383683515112, 0.0062200893085810754, 0.00690450493893706, 0.006836993580566568, 0.006757866729518457, 0.0068333338585711886, 0.005923109757041357, 0.006847516012274614, 0.006429588508442132, 0.006363725470877708, 0.006768247602910097, 0.0062531523693534945, 0.005931644718056285, 0.0058097146982376766, 0.006173345160274592, 0.006560131757341271, 0.006718173826837079, 0.005917861067568859, 0.006321595580789668, 0.006455753638029118, 0.0065016954369948275, 0.006124132895257205, 0.00653151109499502, 0.006562293590624723, 0.006244680501057296, 0.006097143058163256, 0.005871260906639489, 0.0060623321137461324, 0.005631780589296982, 0.005952923581135202, 0.00574484926639171, 0.006471771400860964, 0.006304446416747726, 0.0058773967450706805, 0.006726098240419821, 0.005986213450182809, 0.006249092104140082, 0.006244348864264397, 0.006192196647121818, 0.006437054491652453, 0.005897554334700119, 0.005781636579591465, 0.005957160493284814], "accuracy_test_std": 0.008152113566694414, "error_valid": [0.4254415121423193, 0.29803540333207834, 0.21916474491716864, 0.19602139024849397, 0.18440412038780118, 0.17555328736822284, 0.16448459855045183, 0.15920468985316272, 0.15576613092996983, 0.15386448136295183, 0.15042592243975905, 0.1479948112763554, 0.14674322289156627, 0.1414736092808735, 0.14466802757906627, 0.13914397825677716, 0.13977491999246983, 0.13865569700677716, 0.1348818124058735, 0.13450530638177716, 0.13539068382906627, 0.1351259530308735, 0.13194182981927716, 0.13245070124246983, 0.13255218138177716, 0.12889007200677716, 0.1326845467808735, 0.1304872811558735, 0.13206390013177716, 0.12988722467996983, 0.1302431405308735, 0.1302431405308735, 0.12937835325677716, 0.1342714608433735, 0.12998870481927716, 0.1296327889683735, 0.13135206842996983, 0.1304872811558735, 0.13000929499246983, 0.12965337914156627, 0.12927687311746983, 0.13000929499246983, 0.13136236351656627, 0.1278017342808735, 0.1292665780308735, 0.1274149331701807, 0.1290224374058735, 0.12583831419427716, 0.1280252847326807, 0.1277811441076807, 0.12742522825677716, 0.1282900155308735, 0.1308534920933735, 0.12976515436746983, 0.1273134530308735, 0.1270487222326807, 0.12596038450677716, 0.12854445124246983, 0.12484116152108427, 0.1279238045933735, 0.1280458749058735, 0.12928716820406627, 0.12939894342996983, 0.12730315794427716, 0.1268251717808735, 0.12721197289156627, 0.1281473550451807, 0.12815765013177716, 0.1275575936558735, 0.12852386106927716, 0.12805616999246983, 0.1271913827183735, 0.12794439476656627, 0.12744581842996983, 0.12660162132906627, 0.12669280638177716, 0.12622511530496983, 0.12508530214608427, 0.1254823983433735, 0.12632659544427716, 0.1268251717808735, 0.1268251717808735, 0.12632659544427716, 0.1258280191076807, 0.1258280191076807, 0.12644866575677716, 0.12655014589608427, 0.12496323183358427, 0.12520737245858427, 0.12730315794427716, 0.1252176675451807, 0.12581772402108427, 0.12485145660768071, 0.12608245481927716, 0.1267031014683735, 0.12425140013177716, 0.12744581842996983, 0.12644866575677716, 0.12718108763177716, 0.12705901731927716, 0.12472938629518071, 0.1270487222326807, 0.1285341561558735, 0.12742522825677716, 0.1271707925451807, 0.12620452513177716, 0.12535003294427716, 0.12693694700677716, 0.12867681664156627, 0.12498382200677716, 0.12549269342996983, 0.1264589608433735, 0.12817824030496983, 0.1272928628576807, 0.12559417356927716, 0.12374252870858427, 0.1253397378576807, 0.12510589231927716, 0.12610304499246983, 0.1257265389683735, 0.12745611351656627, 0.12535003294427716, 0.12842238092996983, 0.1275370034826807, 0.12522796263177716, 0.1257059487951807, 0.1261942300451807, 0.1262148202183735, 0.12559417356927716, 0.12593979433358427, 0.12510589231927716, 0.12557358339608427, 0.12657073606927716, 0.12732374811746983, 0.12460731598268071, 0.1268045816076807, 0.12754729856927716, 0.1269472420933735, 0.1253603280308735, 0.12718108763177716, 0.1268251717808735, 0.1254618081701807, 0.12830031061746983, 0.12545151308358427, 0.12667221620858427, 0.12693694700677716, 0.12622511530496983, 0.1275575936558735, 0.12805616999246983, 0.12659132624246983, 0.12854445124246983, 0.1284120858433735, 0.1264383706701807, 0.12705901731927716, 0.12532944277108427, 0.1263163003576807, 0.1268251717808735, 0.12730315794427716, 0.12691635683358427, 0.12596038450677716, 0.1281679452183735, 0.12742522825677716, 0.12571624388177716, 0.1268045816076807, 0.12596038450677716, 0.1266825112951807], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.07877889220635677, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.0010348874243969158, "optimization": "rmsprop", "nb_data_augmentation": 2, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.908324318652433e-07, "rotation_range": [0, 0], "momentum": 0.8797071269882746}, "accuracy_valid_max": 0.8762574712914157, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8733174887048193, "accuracy_valid_std": [0.013570361547346234, 0.017805867391922574, 0.013130341514871205, 0.010364191861651734, 0.008820432102859764, 0.01238105875478959, 0.01666340590671085, 0.01826717954723144, 0.01877343023264419, 0.014064449746656438, 0.014715060528961835, 0.012004337202032263, 0.016116158011611172, 0.015906398754109145, 0.015139336379958195, 0.016113115262150614, 0.014027164798956314, 0.015447842880768397, 0.013055387206228364, 0.014883354323710544, 0.011718515859010821, 0.01337571931432691, 0.013014666190959076, 0.011451710147271726, 0.012618391002563626, 0.01161233008093958, 0.012417923300964374, 0.011018003987345351, 0.012814631591786193, 0.009254255177474104, 0.010821367978602739, 0.011390267319984704, 0.011530596693033213, 0.011944667869719526, 0.011453006946641406, 0.010906762525839492, 0.009924863518093593, 0.010996343697200218, 0.00925983098257132, 0.008659214910632441, 0.010440150594816285, 0.01001438980422587, 0.009128854736730552, 0.011477631393848758, 0.011462793273125778, 0.013515791691739738, 0.011033440291033576, 0.011622830721546955, 0.012551528973370377, 0.012943783588391615, 0.011392630217954116, 0.012148934231006395, 0.013049575874781299, 0.01179628695519436, 0.01099464560765553, 0.014131145135349455, 0.011243076389987715, 0.010256145003414811, 0.012526812227157413, 0.010963077065510909, 0.011519226069596432, 0.010262389479149276, 0.010151359708616485, 0.012518565087698642, 0.010791929544736551, 0.009377015461490622, 0.013479693061002385, 0.011588186651648982, 0.011388880189803476, 0.012671569207258393, 0.011109988316908436, 0.011334261635814963, 0.010471989804260822, 0.011812932811722437, 0.009342152852398034, 0.012620325836308646, 0.010631654932933741, 0.013284927597358076, 0.0113989251843094, 0.011104245725744505, 0.011744080015807892, 0.012143317445482549, 0.01243129546748929, 0.01335684759790812, 0.013140903007605925, 0.012753908795190729, 0.014195483860041736, 0.014342297645951038, 0.013016152515359423, 0.013186379698395607, 0.013473938151734621, 0.014028853841616997, 0.013175177453745763, 0.013351099438243724, 0.011958397682310945, 0.012664348502886903, 0.012403648510029713, 0.013113371577935417, 0.01273379150077953, 0.013199634010386757, 0.01387401929285204, 0.013242755293667221, 0.011981083407505084, 0.013214186170708169, 0.013757900069271611, 0.012177420529587575, 0.012506512623022409, 0.01315028468432974, 0.011033619963658241, 0.012909992600162474, 0.011423778301701085, 0.01286356691184917, 0.011647515466811599, 0.01446921357912825, 0.013505864745459378, 0.014714597708658598, 0.014718889464987965, 0.013840942850707316, 0.011228164580641242, 0.012851642421602779, 0.011544711878457747, 0.012863621729345052, 0.012042441255600993, 0.014551812728933482, 0.012917221925837203, 0.015117583206874986, 0.015173533032471724, 0.013748175001391963, 0.014016956826987613, 0.01512943874580068, 0.013492033050377673, 0.01619372368748378, 0.014060539435887798, 0.011222374910715696, 0.014147651614278937, 0.01464439443719343, 0.015004949299076257, 0.012789439006024101, 0.012438436001618175, 0.014543111491677028, 0.012746785048205655, 0.013387880880820349, 0.013067355278278433, 0.01538609188393112, 0.014954897625751171, 0.01469978741670437, 0.01322053321921166, 0.014705527402700157, 0.012145456635758896, 0.012480003372539976, 0.012325524404055871, 0.013897873541495902, 0.015246455419300102, 0.013370126813575422, 0.014581758097266275, 0.014704440661186424, 0.013215138160433244, 0.014756341962185185, 0.01648176735811864, 0.013355973081555676, 0.013027793023596978, 0.013718767261052431, 0.012963892702361583, 0.015421520975054797, 0.014543802785047556, 0.01465485413311644], "accuracy_valid": [0.5745584878576807, 0.7019645966679217, 0.7808352550828314, 0.803978609751506, 0.8155958796121988, 0.8244467126317772, 0.8355154014495482, 0.8407953101468373, 0.8442338690700302, 0.8461355186370482, 0.849574077560241, 0.8520051887236446, 0.8532567771084337, 0.8585263907191265, 0.8553319724209337, 0.8608560217432228, 0.8602250800075302, 0.8613443029932228, 0.8651181875941265, 0.8654946936182228, 0.8646093161709337, 0.8648740469691265, 0.8680581701807228, 0.8675492987575302, 0.8674478186182228, 0.8711099279932228, 0.8673154532191265, 0.8695127188441265, 0.8679360998682228, 0.8701127753200302, 0.8697568594691265, 0.8697568594691265, 0.8706216467432228, 0.8657285391566265, 0.8700112951807228, 0.8703672110316265, 0.8686479315700302, 0.8695127188441265, 0.8699907050075302, 0.8703466208584337, 0.8707231268825302, 0.8699907050075302, 0.8686376364834337, 0.8721982657191265, 0.8707334219691265, 0.8725850668298193, 0.8709775625941265, 0.8741616858057228, 0.8719747152673193, 0.8722188558923193, 0.8725747717432228, 0.8717099844691265, 0.8691465079066265, 0.8702348456325302, 0.8726865469691265, 0.8729512777673193, 0.8740396154932228, 0.8714555487575302, 0.8751588384789157, 0.8720761954066265, 0.8719541250941265, 0.8707128317959337, 0.8706010565700302, 0.8726968420557228, 0.8731748282191265, 0.8727880271084337, 0.8718526449548193, 0.8718423498682228, 0.8724424063441265, 0.8714761389307228, 0.8719438300075302, 0.8728086172816265, 0.8720556052334337, 0.8725541815700302, 0.8733983786709337, 0.8733071936182228, 0.8737748846950302, 0.8749146978539157, 0.8745176016566265, 0.8736734045557228, 0.8731748282191265, 0.8731748282191265, 0.8736734045557228, 0.8741719808923193, 0.8741719808923193, 0.8735513342432228, 0.8734498541039157, 0.8750367681664157, 0.8747926275414157, 0.8726968420557228, 0.8747823324548193, 0.8741822759789157, 0.8751485433923193, 0.8739175451807228, 0.8732968985316265, 0.8757485998682228, 0.8725541815700302, 0.8735513342432228, 0.8728189123682228, 0.8729409826807228, 0.8752706137048193, 0.8729512777673193, 0.8714658438441265, 0.8725747717432228, 0.8728292074548193, 0.8737954748682228, 0.8746499670557228, 0.8730630529932228, 0.8713231833584337, 0.8750161779932228, 0.8745073065700302, 0.8735410391566265, 0.8718217596950302, 0.8727071371423193, 0.8744058264307228, 0.8762574712914157, 0.8746602621423193, 0.8748941076807228, 0.8738969550075302, 0.8742734610316265, 0.8725438864834337, 0.8746499670557228, 0.8715776190700302, 0.8724629965173193, 0.8747720373682228, 0.8742940512048193, 0.8738057699548193, 0.8737851797816265, 0.8744058264307228, 0.8740602056664157, 0.8748941076807228, 0.8744264166039157, 0.8734292639307228, 0.8726762518825302, 0.8753926840173193, 0.8731954183923193, 0.8724527014307228, 0.8730527579066265, 0.8746396719691265, 0.8728189123682228, 0.8731748282191265, 0.8745381918298193, 0.8716996893825302, 0.8745484869164157, 0.8733277837914157, 0.8730630529932228, 0.8737748846950302, 0.8724424063441265, 0.8719438300075302, 0.8734086737575302, 0.8714555487575302, 0.8715879141566265, 0.8735616293298193, 0.8729409826807228, 0.8746705572289157, 0.8736836996423193, 0.8731748282191265, 0.8726968420557228, 0.8730836431664157, 0.8740396154932228, 0.8718320547816265, 0.8725747717432228, 0.8742837561182228, 0.8731954183923193, 0.8740396154932228, 0.8733174887048193], "seed": 734349020, "model": "residualv3", "loss_std": [0.35423535108566284, 0.26705339550971985, 0.24823540449142456, 0.24200263619422913, 0.23631635308265686, 0.2333243042230606, 0.22775062918663025, 0.22320592403411865, 0.2194642424583435, 0.21441879868507385, 0.21187421679496765, 0.20899325609207153, 0.20804962515830994, 0.20605714619159698, 0.20144161581993103, 0.2003091275691986, 0.19687604904174805, 0.1971004158258438, 0.19621172547340393, 0.189438596367836, 0.1891457736492157, 0.18942275643348694, 0.1884269118309021, 0.18651115894317627, 0.1835615038871765, 0.18215258419513702, 0.18077105283737183, 0.17786219716072083, 0.17675626277923584, 0.17606142163276672, 0.17594380676746368, 0.17605066299438477, 0.17292124032974243, 0.17192164063453674, 0.17201514542102814, 0.1704629808664322, 0.16708968579769135, 0.1677318960428238, 0.16589994728565216, 0.16681376099586487, 0.16448046267032623, 0.16346223652362823, 0.16387641429901123, 0.16262246668338776, 0.16283881664276123, 0.15817396342754364, 0.15967483818531036, 0.15877458453178406, 0.15889160335063934, 0.15793398022651672, 0.15649154782295227, 0.15959109365940094, 0.15646977722644806, 0.15532514452934265, 0.151285320520401, 0.15234637260437012, 0.1521705538034439, 0.15103772282600403, 0.15242469310760498, 0.14918296039104462, 0.15009717643260956, 0.1511230170726776, 0.14954163134098053, 0.14826956391334534, 0.14678668975830078, 0.1463748961687088, 0.14575763046741486, 0.14537595212459564, 0.14367908239364624, 0.14482414722442627, 0.14194846153259277, 0.14462651312351227, 0.14160394668579102, 0.14210335910320282, 0.1401320993900299, 0.13829410076141357, 0.14222295582294464, 0.14033618569374084, 0.13883425295352936, 0.1371208131313324, 0.14016422629356384, 0.13743522763252258, 0.1345582753419876, 0.13851884007453918, 0.13517621159553528, 0.13762541115283966, 0.1370258331298828, 0.1331111639738083, 0.13473449647426605, 0.1359594464302063, 0.1322115808725357, 0.1361079066991806, 0.13475851714611053, 0.13190695643424988, 0.1321793496608734, 0.1343803107738495, 0.1302369087934494, 0.13281431794166565, 0.13013187050819397, 0.12867823243141174, 0.12930621206760406, 0.12927846610546112, 0.1292799711227417, 0.1305784434080124, 0.12926334142684937, 0.12764352560043335, 0.12803736329078674, 0.12698116898536682, 0.12853413820266724, 0.1279912143945694, 0.12825559079647064, 0.12479399144649506, 0.12368907779455185, 0.1272750347852707, 0.12543979287147522, 0.12563762068748474, 0.12483219057321548, 0.12275292724370956, 0.12653999030590057, 0.1227022334933281, 0.12329965084791183, 0.12222984433174133, 0.12222281098365784, 0.12327445298433304, 0.12384936213493347, 0.12175316363573074, 0.12258895486593246, 0.12248888611793518, 0.1215672492980957, 0.12262152880430222, 0.11871588230133057, 0.11767268180847168, 0.11758960038423538, 0.12125971913337708, 0.12215683609247208, 0.11722462624311447, 0.1208697110414505, 0.12144488096237183, 0.11848536133766174, 0.11860780417919159, 0.11787009984254837, 0.11960163712501526, 0.11528487503528595, 0.11675158143043518, 0.11712794750928879, 0.1174488514661789, 0.11736931651830673, 0.11789437383413315, 0.1195543184876442, 0.115494504570961, 0.11762797832489014, 0.11550304293632507, 0.11443547904491425, 0.11198446899652481, 0.11763466149568558, 0.1162240281701088, 0.11385881155729294, 0.11238645017147064, 0.11477954685688019, 0.11560878902673721, 0.11392490565776825, 0.11353933811187744, 0.11255278438329697, 0.1140943393111229, 0.11268750578165054, 0.11585930734872818]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:41 2016", "state": "available"}], "summary": "350d653475fcb57e84a1d7e429f80107"}