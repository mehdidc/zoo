{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 16, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.04478651841945544, 0.042500027663683555, 0.047531507968683794, 0.04219755386487853, 0.03898532060375147, 0.04168971479223458, 0.03902369691339, 0.03518488107831606, 0.03192917434502976, 0.03298898001345234, 0.0350073018920747, 0.033748840508333856, 0.03484912793487158, 0.0354694092644865, 0.03494479320317242, 0.03339104783423534, 0.03235591655542878, 0.03387038969391425, 0.03531073617718311, 0.03253486217082849, 0.031258127287388256, 0.036223820080816975, 0.031965521400751366, 0.034801196685248156, 0.02935958965721163, 0.033407616446327414, 0.03206611210423817, 0.03484912793487158, 0.03355825818504537, 0.03482517055626845, 0.034221845947855815, 0.03458179852052041, 0.031017184643506075, 0.031882259583418344, 0.033835016468873554, 0.033518225427924625, 0.03297687800270166, 0.03290637802420518, 0.036056139552091485, 0.03603172584992891, 0.037920136515491636, 0.032008346503302004, 0.030665091284091308, 0.03336821857583192, 0.03449880292487668, 0.033694229174687354, 0.033053815179956585, 0.03330753642095865, 0.033139061023021076, 0.03226691505012467, 0.034574190139087586, 0.03402404859348364, 0.02817352177592151, 0.03167815941461116, 0.037813767117579646, 0.030867073183944726, 0.034482758620689655, 0.030411995240668288, 0.03317873100629296, 0.031627717366353836, 0.03436786320628522, 0.029355881545843687, 0.034362847560716635, 0.034826473011213575, 0.031522850164461366, 0.03399844253258957, 0.032808087173541486, 0.03505469254226965, 0.035795544106257626, 0.03269618569919939, 0.034808755448384146, 0.030676922408912885, 0.031441012910581316, 0.031290615327488044, 0.03331842924650403, 0.03384654362481218, 0.031972048132843454, 0.03198906812240818, 0.03242509539917134, 0.03205054829605818, 0.029311347619287227, 0.03284429020286243, 0.03295596406214711, 0.03273694659292467, 0.032698405286507366, 0.03247346083897372, 0.03250892030353249, 0.032498035293016383, 0.032454179217775096, 0.03226691505012467, 0.03189278586590891, 0.03220725629794747, 0.03228630850705693, 0.032816657909936896, 0.032454179217775096, 0.03247653366264759, 0.0318438233681077, 0.03167443632909282, 0.031854931927325324, 0.031898474302059075, 0.03131872505783546, 0.031385854462942064, 0.03134420485981531, 0.03115929538416575, 0.03080205294624097, 0.03115580147149886, 0.031141821900885637, 0.030596380543145344, 0.030856490993375636, 0.030998167809574408, 0.030409907088641178, 0.030764626200588965, 0.03060942374276021, 0.029729285729162516, 0.030046170721229844, 0.030540587760276697, 0.030298430269912482, 0.030261579781659827, 0.030228886114113757, 0.030676035232831196, 0.030390510253712016, 0.03042511748607259, 0.030691113738465593, 0.030631939824362887, 0.030676035232831196, 0.030056736381425333, 0.030776124295584285, 0.031116176375973668, 0.030734829088257892, 0.031201191818784807, 0.03154586458686304, 0.031616242040842735, 0.03160820683415283, 0.03174538605679207, 0.03224554064227216, 0.032299792739858675, 0.033152472051688774, 0.032608391153384784, 0.03238282143655506, 0.03231664011881557, 0.03202336425218339, 0.032479885502374], "moving_avg_accuracy_train": [0.053753294427710825, 0.11235575112951804, 0.1683876835466867, 0.2254641072100903, 0.27947078383847884, 0.33018757970161894, 0.37788465380977027, 0.4213956048444559, 0.462176789841938, 0.49947050016497313, 0.5335054719858252, 0.5641651845763993, 0.592575473347675, 0.6191260020671243, 0.6431956119507733, 0.6662607457858165, 0.688779531900006, 0.7101430131979571, 0.7296031094685229, 0.7465618497264899, 0.7630177694225156, 0.7783152018176134, 0.7923135009129605, 0.8057402833517849, 0.8190339131491365, 0.8312546746956686, 0.8427898811718849, 0.8536445526932506, 0.8631949129359737, 0.8712631287206896, 0.8795222638907892, 0.8870425525619512, 0.894568530739491, 0.9011089479968672, 0.9078095178056143, 0.9137929673804744, 0.9185709560339932, 0.9233135404004734, 0.9289302285291008, 0.9334534330858293, 0.9373125325483307, 0.9410045661910881, 0.9440144258370394, 0.9472080510244197, 0.9500823136930621, 0.9533398014502619, 0.9552196767269224, 0.9569327429397723, 0.959884046959048, 0.9627190609378421, 0.9651034989705639, 0.9669929984710979, 0.9676911007324218, 0.9682323257495411, 0.9698748311263943, 0.9717295919896585, 0.9734553526702107, 0.9751026637887318, 0.9757381052411839, 0.9763805974279088, 0.9773777033477686, 0.9787716159949195, 0.9798167059014516, 0.9807314020281739, 0.9807004305000553, 0.9808137458837847, 0.9815934405725146, 0.9824528276899619, 0.9829886066679537, 0.983579053230074, 0.9841245741118859, 0.9847920301043118, 0.9853150861300253, 0.9852210775170227, 0.985974195668935, 0.986675533632162, 0.9870808341846085, 0.9862007816396416, 0.9867100332949547, 0.9873542596341339, 0.988272918761082, 0.9892644333608774, 0.9902226850549102, 0.9910968773927926, 0.9918907099848387, 0.9926145719682826, 0.9932754604039845, 0.9938749663214175, 0.9944239342977095, 0.994920358639023, 0.9953694937088555, 0.9957737152717049, 0.9961398678409199, 0.9964670519905628, 0.9967662240505427, 0.9970354789045246, 0.9972778082731083, 0.9974982578674843, 0.9976966625024226, 0.997882286161819, 0.9980493474552756, 0.9981997026193866, 0.9983373754297371, 0.9984683404470044, 0.9985885621251955, 0.9986967616355675, 0.9987941411949024, 0.9988817827983038, 0.998960660241365, 0.9990340031027707, 0.9991023648406865, 0.9991638904048106, 0.9992192634125223, 0.9992690991194628, 0.9993139512557092, 0.9993566713409816, 0.9993951194177267, 0.9994297226867974, 0.9994632187916116, 0.9994933652859443, 0.9995228502934945, 0.9995517399629403, 0.9995777406654415, 0.9996011412976925, 0.9996222018667185, 0.9996435095414924, 0.999662686448789, 0.9996799456653558, 0.9996954789602659, 0.9997094589256852, 0.9997220408945624, 0.999733364666552, 0.9997435560613426, 0.9997527283166541, 0.999763336509085, 0.9997728838822728, 0.9997814765181419, 0.9997892098904241, 0.999796169925478, 0.9998024339570266, 0.9998080715854203, 0.9998131454509747], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234423, "moving_var_accuracy_train": [0.026004749956489517, 0.05431250634422528, 0.07713745276342217, 0.09874317073091901, 0.11511934374181708, 0.12675714981124617, 0.1345565327364279, 0.1381397052022704, 0.1392936801302331, 0.1378816995841359, 0.13451894338733808, 0.12952721083383376, 0.12383879032305586, 0.11779928646829102, 0.11123347290102141, 0.10489812920037533, 0.0989721778328473, 0.09318254504807377, 0.08727254866500357, 0.08113368363873792, 0.07545749091224256, 0.0700178447619616, 0.0647796316838306, 0.05992417489538325, 0.05552224274454624, 0.051314141585086404, 0.047380276322618244, 0.043702663734887154, 0.040153281788290515, 0.03672381856300012, 0.033665356530521905, 0.030807813552748187, 0.02823679532523262, 0.02579810931381462, 0.023622377104290188, 0.02158235441319523, 0.019629581552034096, 0.017869052355089326, 0.016366071789388762, 0.01491359902560797, 0.013556272961000482, 0.0123233256766737, 0.011172526404801273, 0.01014706694085838, 0.009206712719767703, 0.008381542486185693, 0.00757519361706932, 0.006844085618008861, 0.0062380688149357155, 0.00568659767178176, 0.0051691078071905965, 0.0046843289017342, 0.0042202821324661705, 0.003800890239891955, 0.0034450816311196826, 0.0031315347087467827, 0.0028451854872109652, 0.002585089643780698, 0.0023302147519580784, 0.0021009084426522944, 0.0018997655803258387, 0.0017272759545042388, 0.0015643782752684332, 0.001415470468779757, 0.0012739320550217675, 0.0011466544129052985, 0.001037460285883472, 0.0009403611732538348, 0.0008489085879477724, 0.0007671553734374721, 0.0006931181733861602, 0.000627815833563971, 0.0005674965386618897, 0.0005108264233695687, 0.0004648484635892685, 0.00042279049167831203, 0.0003819898593508014, 0.0003507613057528453, 0.00031801921041351154, 0.0002899525375569909, 0.00026855269512501633, 0.0002505453364269817, 0.00023375501956633313, 0.00021725742780221168, 0.00020120321667974124, 0.00018579868055144437, 0.00017114977421629993, 0.00015726946290000316, 0.00014425480916095, 0.0001320472623846909, 0.00012065803694480397, 0.00011006278889717516, 0.0001002631193429427, 9.120025261864662e-05, 8.288576265003528e-05, 7.524966997256694e-05, 6.825321468121374e-05, 6.186527542603691e-05, 5.6033027475918286e-05, 5.07398300146755e-05, 4.591703229515066e-05, 4.152878914400919e-05, 3.7546494453996514e-05, 3.394621153032715e-05, 3.068166964445825e-05, 2.771886688641511e-05, 2.503232520495977e-05, 2.2598222140284584e-05, 2.039439478547096e-05, 1.84033678847964e-05, 1.660509104121445e-05, 1.497865049246013e-05, 1.3508380973061487e-05, 1.2179895254931707e-05, 1.0980011156571403e-05, 9.898435092085435e-06, 8.921895874325546e-06, 8.040482762966406e-06, 7.246532388009302e-06, 6.530058449293401e-06, 5.88487689539615e-06, 5.3039007228626844e-06, 4.779594979351396e-06, 4.306563787723991e-06, 3.879899337060856e-06, 3.495995556393219e-06, 3.1497057847150397e-06, 2.837416131252062e-06, 2.555846067383765e-06, 2.3020204155434845e-06, 2.0732431274565937e-06, 1.8670728650195743e-06, 1.681300359267615e-06, 1.5139274957483527e-06, 1.363547549893393e-06, 1.228013165917154e-06, 1.105876349846051e-06, 9.958269602831422e-07, 8.966802430464021e-07, 8.073653615629372e-07, 7.269148710917957e-07, 6.544550809875897e-07], "duration": 30765.71366, "accuracy_train": [0.5375329442771084, 0.6397778614457831, 0.6726750753012049, 0.7391519201807228, 0.7655308734939759, 0.7866387424698795, 0.8071583207831325, 0.8129941641566265, 0.8292074548192772, 0.8351138930722891, 0.839820218373494, 0.8401025978915663, 0.8482680722891566, 0.8580807605421686, 0.8598221009036144, 0.8738469503012049, 0.8914486069277109, 0.9024143448795181, 0.9047439759036144, 0.8991905120481928, 0.911121046686747, 0.915992093373494, 0.9182981927710844, 0.9265813253012049, 0.9386765813253012, 0.9412415286144579, 0.9466067394578314, 0.9513365963855421, 0.9491481551204819, 0.9438770707831325, 0.9538544804216867, 0.9547251506024096, 0.9623023343373494, 0.959972703313253, 0.9681146460843374, 0.9676440135542169, 0.9615728539156626, 0.9659967996987951, 0.979480421686747, 0.9741622740963856, 0.9720444277108434, 0.9742328689759037, 0.9711031626506024, 0.9759506777108434, 0.9759506777108434, 0.9826571912650602, 0.9721385542168675, 0.9723503388554217, 0.9864457831325302, 0.9882341867469879, 0.9865634412650602, 0.9839984939759037, 0.9739740210843374, 0.9731033509036144, 0.9846573795180723, 0.9884224397590361, 0.9889871987951807, 0.9899284638554217, 0.981457078313253, 0.9821630271084337, 0.986351656626506, 0.9913168298192772, 0.989222515060241, 0.9889636671686747, 0.9804216867469879, 0.9818335843373494, 0.9886106927710844, 0.9901873117469879, 0.9878106174698795, 0.9888930722891566, 0.9890342620481928, 0.9907991340361446, 0.9900225903614458, 0.984375, 0.9927522590361446, 0.9929875753012049, 0.9907285391566265, 0.9782803087349398, 0.9912932981927711, 0.993152296686747, 0.9965408509036144, 0.9981880647590361, 0.9988469503012049, 0.9989646084337349, 0.999035203313253, 0.9991293298192772, 0.9992234563253012, 0.9992705195783133, 0.9993646460843374, 0.9993881777108434, 0.9994117093373494, 0.9994117093373494, 0.9994352409638554, 0.9994117093373494, 0.9994587725903614, 0.9994587725903614, 0.9994587725903614, 0.9994823042168675, 0.9994823042168675, 0.9995528990963856, 0.9995528990963856, 0.9995528990963856, 0.9995764307228916, 0.9996470256024096, 0.9996705572289156, 0.9996705572289156, 0.9996705572289156, 0.9996705572289156, 0.9996705572289156, 0.9996940888554217, 0.9997176204819277, 0.9997176204819277, 0.9997176204819277, 0.9997176204819277, 0.9997176204819277, 0.9997411521084337, 0.9997411521084337, 0.9997411521084337, 0.9997646837349398, 0.9997646837349398, 0.9997882153614458, 0.9998117469879518, 0.9998117469879518, 0.9998117469879518, 0.9998117469879518, 0.9998352786144579, 0.9998352786144579, 0.9998352786144579, 0.9998352786144579, 0.9998352786144579, 0.9998352786144579, 0.9998352786144579, 0.9998352786144579, 0.9998352786144579, 0.9998588102409639, 0.9998588102409639, 0.9998588102409639, 0.9998588102409639, 0.9998588102409639, 0.9998588102409639, 0.9998588102409639, 0.9998588102409639], "end": "2016-01-20 21:39:39.553000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0], "accuracy_valid": [0.5373114224137931, 0.6291756465517241, 0.6551724137931034, 0.7169989224137931, 0.7435344827586207, 0.7487877155172413, 0.763604525862069, 0.7602370689655172, 0.7681842672413793, 0.7646821120689655, 0.7584859913793104, 0.7535021551724138, 0.7545797413793104, 0.7594288793103449, 0.7524245689655172, 0.7594288793103449, 0.7660290948275862, 0.7646821120689655, 0.7660290948275862, 0.7547144396551724, 0.7615840517241379, 0.7641433189655172, 0.7654903017241379, 0.7645474137931034, 0.767645474137931, 0.7662984913793104, 0.7707435344827587, 0.7702047413793104, 0.7646821120689655, 0.7618534482758621, 0.7723599137931034, 0.7692618534482759, 0.7689924568965517, 0.7653556034482759, 0.7703394396551724, 0.7722252155172413, 0.7706088362068966, 0.7733028017241379, 0.7784213362068966, 0.7739762931034483, 0.7698006465517241, 0.7755926724137931, 0.7712823275862069, 0.7780172413793104, 0.7733028017241379, 0.7808459051724138, 0.7689924568965517, 0.7768049568965517, 0.7790948275862069, 0.7804418103448276, 0.7815193965517241, 0.7766702586206896, 0.7747844827586207, 0.7746497844827587, 0.7786907327586207, 0.7784213362068966, 0.7809806034482759, 0.7823275862068966, 0.7830010775862069, 0.7819234913793104, 0.7788254310344828, 0.7827316810344828, 0.7794989224137931, 0.7805765086206896, 0.7747844827586207, 0.7758620689655172, 0.7808459051724138, 0.7817887931034483, 0.7800377155172413, 0.7765355603448276, 0.7784213362068966, 0.7825969827586207, 0.7804418103448276, 0.7836745689655172, 0.7827316810344828, 0.779364224137931, 0.7820581896551724, 0.7788254310344828, 0.7863685344827587, 0.7821928879310345, 0.7848868534482759, 0.7863685344827587, 0.7873114224137931, 0.7866379310344828, 0.7867726293103449, 0.7873114224137931, 0.7877155172413793, 0.7878502155172413, 0.7879849137931034, 0.7882543103448276, 0.7883890086206896, 0.7885237068965517, 0.7883890086206896, 0.7882543103448276, 0.7879849137931034, 0.7877155172413793, 0.7878502155172413, 0.7882543103448276, 0.7877155172413793, 0.7875808189655172, 0.7873114224137931, 0.7873114224137931, 0.7878502155172413, 0.787176724137931, 0.7869073275862069, 0.7866379310344828, 0.7869073275862069, 0.7869073275862069, 0.7865032327586207, 0.7863685344827587, 0.7865032327586207, 0.7862338362068966, 0.7855603448275862, 0.7859644396551724, 0.7859644396551724, 0.7855603448275862, 0.7855603448275862, 0.7856950431034483, 0.7855603448275862, 0.7856950431034483, 0.7860991379310345, 0.7860991379310345, 0.7863685344827587, 0.7860991379310345, 0.7856950431034483, 0.7852909482758621, 0.7858297413793104, 0.7858297413793104, 0.7860991379310345, 0.7863685344827587, 0.7860991379310345, 0.7859644396551724, 0.7862338362068966, 0.7860991379310345, 0.7858297413793104, 0.7862338362068966, 0.7854256465517241, 0.7854256465517241, 0.78515625, 0.7848868534482759, 0.78515625, 0.7852909482758621], "accuracy_test": 0.6615584935897436, "start": "2016-01-20 13:06:53.839000", "learning_rate_per_epoch": [0.005703098606318235, 0.004032699856907129, 0.0032926856074482203, 0.0028515493031591177, 0.0025505032390356064, 0.002328280359506607, 0.0021555686835199594, 0.0020163499284535646, 0.0019010328687727451, 0.0018034782260656357, 0.0017195489490404725, 0.0016463428037241101, 0.0015817550010979176, 0.0015242171939462423, 0.0014725337969139218, 0.0014257746515795588, 0.0013832045951858163, 0.0013442332856357098, 0.001308380626142025, 0.0012752516195178032, 0.001244518207386136, 0.0012159047182649374, 0.0011891783215105534, 0.0011641401797533035, 0.0011406197445467114, 0.0011184696340933442, 0.001097561907954514, 0.0010777843417599797, 0.0010590387973934412, 0.0010412386618554592, 0.0010243067517876625, 0.0010081749642267823, 0.0009927820647135377, 0.00097807333804667, 0.0009639996569603682, 0.0009505164343863726, 0.0009375836816616356, 0.0009251647861674428, 0.0009132266859523952, 0.0009017391130328178, 0.0008906743605621159, 0.0008800072246231139, 0.0008697144221514463, 0.0008597744745202363, 0.0008501677657477558, 0.000840876018628478, 0.00083188246935606, 0.0008231714018620551, 0.0008147283806465566, 0.0008065399597398937, 0.0007985935662873089, 0.0007908775005489588, 0.0007833808776922524, 0.000776093453168869, 0.0007690056809224188, 0.0007621085969731212, 0.0007553938776254654, 0.0007488535484299064, 0.0007424802170135081, 0.0007362668984569609, 0.0007302069570869207, 0.0007242942810989916, 0.0007185228751040995, 0.0007128873257897794, 0.0007073823362588882, 0.0007020029006525874, 0.0006967444205656648, 0.0006916022975929081, 0.0006865723989903927, 0.0006816506502218544, 0.0006768332677893341, 0.0006721166428178549, 0.0006674972246401012, 0.0006629717536270618, 0.0006585371447727084, 0.0006541903130710125, 0.0006499284063465893, 0.0006457487470470369, 0.0006416487158276141, 0.0006376258097589016, 0.000633677642326802, 0.0006298018852248788, 0.0006259963847696781, 0.000622259103693068, 0.0006185878883115947, 0.0006149809341877699, 0.0006114363786764443, 0.0006079523591324687, 0.0006045272457413375, 0.0006011594086885452, 0.0005978472181595862, 0.0005945891607552767, 0.0005913837812840939, 0.0005882297409698367, 0.0005851255846209824, 0.0005820700898766518, 0.0005790619179606438, 0.0005760999629274011, 0.0005731830024160445, 0.0005703098722733557, 0.0005674795247614384, 0.000564690912142396, 0.0005619429866783321, 0.0005592348170466721, 0.0005565654719248414, 0.0005539339035749435, 0.0005513393552973866, 0.000548780953977257, 0.00054625776829198, 0.0005437690997496247, 0.0005413141916505992, 0.0005388921708799899, 0.0005365023971535265, 0.0005341441137716174, 0.0005318166804499924, 0.0005295193986967206, 0.0005272516864351928, 0.0005250127869658172, 0.0005228021764196455, 0.0005206193309277296, 0.0005184634937904775, 0.0005163343157619238, 0.0005142310983501375, 0.0005121533758938313, 0.0005101006827317178, 0.000508072436787188, 0.0005060681723989546, 0.0005040874821133912, 0.0005021298420615494, 0.000500194844789803, 0.0004982820246368647, 0.0004963910323567688, 0.0004945214022882283, 0.0004926726687699556, 0.0004908445989713073, 0.000489036669023335, 0.00048724861699156463, 0.0004854800063185394, 0.0004837305168621242, 0.0004819998284801841, 0.00048028756282292306, 0.00047859339974820614], "accuracy_train_last": 0.9998588102409639, "error_valid": [0.46268857758620685, 0.3708243534482759, 0.3448275862068966, 0.28300107758620685, 0.25646551724137934, 0.2512122844827587, 0.23639547413793105, 0.23976293103448276, 0.23181573275862066, 0.23531788793103448, 0.2415140086206896, 0.2464978448275862, 0.2454202586206896, 0.24057112068965514, 0.24757543103448276, 0.24057112068965514, 0.2339709051724138, 0.23531788793103448, 0.2339709051724138, 0.24528556034482762, 0.2384159482758621, 0.23585668103448276, 0.2345096982758621, 0.23545258620689657, 0.23235452586206895, 0.2337015086206896, 0.22925646551724133, 0.2297952586206896, 0.23531788793103448, 0.2381465517241379, 0.22764008620689657, 0.2307381465517241, 0.2310075431034483, 0.2346443965517241, 0.22966056034482762, 0.22777478448275867, 0.22939116379310343, 0.2266971982758621, 0.22157866379310343, 0.2260237068965517, 0.2301993534482759, 0.22440732758620685, 0.22871767241379315, 0.2219827586206896, 0.2266971982758621, 0.2191540948275862, 0.2310075431034483, 0.2231950431034483, 0.22090517241379315, 0.21955818965517238, 0.2184806034482759, 0.2233297413793104, 0.22521551724137934, 0.22535021551724133, 0.22130926724137934, 0.22157866379310343, 0.2190193965517241, 0.21767241379310343, 0.21699892241379315, 0.2180765086206896, 0.22117456896551724, 0.21726831896551724, 0.22050107758620685, 0.2194234913793104, 0.22521551724137934, 0.22413793103448276, 0.2191540948275862, 0.2182112068965517, 0.21996228448275867, 0.22346443965517238, 0.22157866379310343, 0.21740301724137934, 0.21955818965517238, 0.21632543103448276, 0.21726831896551724, 0.22063577586206895, 0.21794181034482762, 0.22117456896551724, 0.21363146551724133, 0.21780711206896552, 0.2151131465517241, 0.21363146551724133, 0.21268857758620685, 0.21336206896551724, 0.21322737068965514, 0.21268857758620685, 0.21228448275862066, 0.21214978448275867, 0.21201508620689657, 0.21174568965517238, 0.2116109913793104, 0.2114762931034483, 0.2116109913793104, 0.21174568965517238, 0.21201508620689657, 0.21228448275862066, 0.21214978448275867, 0.21174568965517238, 0.21228448275862066, 0.21241918103448276, 0.21268857758620685, 0.21268857758620685, 0.21214978448275867, 0.21282327586206895, 0.21309267241379315, 0.21336206896551724, 0.21309267241379315, 0.21309267241379315, 0.21349676724137934, 0.21363146551724133, 0.21349676724137934, 0.21376616379310343, 0.2144396551724138, 0.21403556034482762, 0.21403556034482762, 0.2144396551724138, 0.2144396551724138, 0.2143049568965517, 0.2144396551724138, 0.2143049568965517, 0.21390086206896552, 0.21390086206896552, 0.21363146551724133, 0.21390086206896552, 0.2143049568965517, 0.2147090517241379, 0.2141702586206896, 0.2141702586206896, 0.21390086206896552, 0.21363146551724133, 0.21390086206896552, 0.21403556034482762, 0.21376616379310343, 0.21390086206896552, 0.2141702586206896, 0.21376616379310343, 0.2145743534482759, 0.2145743534482759, 0.21484375, 0.2151131465517241, 0.21484375, 0.2147090517241379], "accuracy_train_std": [0.043864889588818234, 0.0461120459686625, 0.0445226582125224, 0.045430808032742895, 0.0421762377576595, 0.04041576547682233, 0.038368726528246376, 0.039329078348282094, 0.039548061088743446, 0.04014709964352982, 0.03965801009496724, 0.040049272872953824, 0.038800334135674075, 0.039752676693018775, 0.038707708832380124, 0.038731105741712214, 0.03565552395882686, 0.0350828930837708, 0.034885015261061296, 0.03627810651343529, 0.03442955208472218, 0.03399918584559001, 0.0320183039519793, 0.03020148358576491, 0.03004571298115751, 0.027134026596485634, 0.027319833270948946, 0.024915003443419178, 0.02561489695568744, 0.023910390625829996, 0.022842059063246985, 0.022213556288827263, 0.01988972363269297, 0.020419093482993998, 0.018663932700650535, 0.018343729242158078, 0.021139561723185946, 0.01825039588454344, 0.015576017130479396, 0.016330677534985447, 0.01677658097058353, 0.01691036777056295, 0.016627521220793626, 0.014815509058150982, 0.015291791705168568, 0.012912560596506325, 0.0172657366347118, 0.01630717947374536, 0.010682010722540138, 0.011432786915616711, 0.010989723069252904, 0.01233200925132083, 0.016222765436081907, 0.015399305666006164, 0.011863487648232337, 0.010118271043985914, 0.009892243499857881, 0.009704164313313019, 0.013129551088863316, 0.012829981414763655, 0.011187005528731348, 0.008837966417564413, 0.00945409269241747, 0.009912458506039367, 0.013026237870486592, 0.01368610402708442, 0.009572253792480567, 0.009789015209459873, 0.010234823893859105, 0.009972524520151115, 0.010692684083251608, 0.01038390180525417, 0.009553956280084945, 0.012352647110032733, 0.008705811375829063, 0.00823095796645412, 0.009220194556657868, 0.013873425591735144, 0.008635690144977236, 0.00799781101782773, 0.005522818097589803, 0.003909579865210953, 0.0032019445362220247, 0.003036947628934804, 0.0029060367838752506, 0.0027413082424134765, 0.002633335154608341, 0.0025039906352207703, 0.00237972117283443, 0.0023470364532740366, 0.0023136507711117135, 0.0023136507711117135, 0.0021974054084956235, 0.0022327781409063464, 0.0021611976673063475, 0.0021611976673063475, 0.0021611976673063475, 0.0020357237155386077, 0.0019433191662333, 0.0018146836064380717, 0.0018146836064380717, 0.0018146836064380717, 0.0017691026100538874, 0.0016226587921742575, 0.0015701079929981436, 0.0015701079929981436, 0.0015701079929981436, 0.0015701079929981436, 0.0015701079929981436, 0.001515370973933729, 0.0014582015610720698, 0.0014582015610720698, 0.0014582015610720698, 0.0014582015610720698, 0.0014582015610720698, 0.0013983014417117453, 0.0013983014417117453, 0.0013983014417117453, 0.001335303177627175, 0.001335303177627175, 0.0012687453470169965, 0.0011980348325826295, 0.0011980348325826295, 0.0011980348325826295, 0.0011980348325826295, 0.001122387049859923, 0.001122387049859923, 0.001122387049859923, 0.001122387049859923, 0.001122387049859923, 0.001122387049859923, 0.001122387049859923, 0.001122387049859923, 0.001122387049859923, 0.0010407259218512793, 0.0010407259218512793, 0.0010407259218512793, 0.0010407259218512793, 0.0010407259218512793, 0.0010407259218512793, 0.0010407259218512793, 0.0010407259218512793], "accuracy_test_std": 0.0399438550024693, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.9396691544390374, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.005703098722109731, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 3.8501181773989104e-10, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.08582251390318689}, "accuracy_valid_max": 0.7885237068965517, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234423\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7852909482758621, "loss_train": [1.4678454399108887, 1.066438913345337, 0.8695042729377747, 0.7521751523017883, 0.6647048592567444, 0.6103317737579346, 0.543408215045929, 0.4908474385738373, 0.44637566804885864, 0.40369880199432373, 0.3642132878303528, 0.32693931460380554, 0.2944481372833252, 0.29064875841140747, 0.2508193552494049, 0.23276636004447937, 0.20549161732196808, 0.16469056904315948, 0.14233921468257904, 0.1268126517534256, 0.11504831165075302, 0.10181697458028793, 0.09133391082286835, 0.07805410772562027, 0.06987979263067245, 0.0630735456943512, 0.057671885937452316, 0.0523085780441761, 0.0460163913667202, 0.04639038071036339, 0.04145506024360657, 0.036996547132730484, 0.03667587786912918, 0.03492968529462814, 0.025355787947773933, 0.02774954028427601, 0.023753900080919266, 0.029514184221625328, 0.023886876180768013, 0.021060429513454437, 0.019310185685753822, 0.018501829355955124, 0.01670129783451557, 0.023056330159306526, 0.01959223859012127, 0.012338481843471527, 0.01698044128715992, 0.02314033731818199, 0.019681788980960846, 0.013279169797897339, 0.008624971844255924, 0.009616720490157604, 0.01336970180273056, 0.022956307977437973, 0.017699379473924637, 0.01033249031752348, 0.009447130374610424, 0.0065073580481112, 0.0147024504840374, 0.01606704108417034, 0.014638889580965042, 0.008810482919216156, 0.004600222688168287, 0.0073381694965064526, 0.011173653416335583, 0.015125980600714684, 0.00767530407756567, 0.0083354152739048, 0.010204917751252651, 0.012679207138717175, 0.008062127977609634, 0.005593109875917435, 0.006235172972083092, 0.01179885771125555, 0.005442150868475437, 0.003105889307335019, 0.005156010389328003, 0.017071714624762535, 0.015354708768427372, 0.006113224197179079, 0.0021597181912511587, 0.0006969444220885634, 0.00023072653857525438, 0.00014787861437071115, 0.00012685336696449667, 0.00011282035848125815, 0.00010205611761193722, 9.329763997811824e-05, 8.58801940921694e-05, 7.950029248604551e-05, 7.392001134576276e-05, 6.89916851115413e-05, 6.461979501182213e-05, 6.071649841032922e-05, 5.722169589716941e-05, 5.408780270954594e-05, 5.128265911480412e-05, 4.8770092689665034e-05, 4.651890048990026e-05, 4.4512325985124335e-05, 4.271965008229017e-05, 4.1120496462099254e-05, 3.969903627876192e-05, 3.843779632006772e-05, 3.73211114492733e-05, 3.633560845628381e-05, 3.546822699718177e-05, 3.470541560091078e-05, 3.403511072974652e-05, 3.344834840390831e-05, 3.293504050816409e-05, 3.2485750125488266e-05, 3.209352871635929e-05, 3.174920129822567e-05, 3.144788934150711e-05, 3.118347376585007e-05, 3.094999919994734e-05, 3.074248161283322e-05, 3.0556464480469e-05, 3.0387969673029147e-05, 3.0233426514314488e-05, 3.008956991834566e-05, 2.9953009288874455e-05, 2.9820235795341432e-05, 2.9689101211261004e-05, 2.95568497676868e-05, 2.942106766568031e-05, 2.9279326554387808e-05, 2.912930904130917e-05, 2.896920523198787e-05, 2.879674320865888e-05, 2.8610498702619225e-05, 2.8408037906046957e-05, 2.8188809665152803e-05, 2.7950589355896227e-05, 2.769308957795147e-05, 2.7414847863838077e-05, 2.7115269404021092e-05, 2.6794601581059396e-05, 2.6452569727553055e-05, 2.608947761473246e-05, 2.5705865482450463e-05], "accuracy_train_first": 0.5375329442771084, "model": "residualv4", "loss_std": [0.25970059633255005, 0.13360366225242615, 0.11281591653823853, 0.10619194805622101, 0.09787055104970932, 0.10321888327598572, 0.09121761471033096, 0.08604403585195541, 0.0825350284576416, 0.0769236832857132, 0.0727343037724495, 0.06860694289207458, 0.06536497175693512, 0.11536706984043121, 0.07879998534917831, 0.08491093665361404, 0.07650219649076462, 0.05631434544920921, 0.047055598348379135, 0.04359520971775055, 0.039377350360155106, 0.039134103804826736, 0.03485189005732536, 0.029486065730452538, 0.028753452003002167, 0.02514021471142769, 0.023773234337568283, 0.023495888337492943, 0.022581668570637703, 0.02155790477991104, 0.021159017458558083, 0.020508859306573868, 0.019771043211221695, 0.022058958187699318, 0.014193706214427948, 0.018068592995405197, 0.01625978760421276, 0.020353930070996284, 0.01752544194459915, 0.016016822308301926, 0.01525952946394682, 0.014219996519386768, 0.01427388470619917, 0.017069319263100624, 0.015211863443255424, 0.01076450850814581, 0.014461402781307697, 0.019704420119524002, 0.016010725870728493, 0.011940479278564453, 0.007915893569588661, 0.009174617938697338, 0.01237673033028841, 0.020377786830067635, 0.016054688021540642, 0.009643551893532276, 0.009672407992184162, 0.007261394057422876, 0.014630123972892761, 0.0148156164214015, 0.013294386677443981, 0.009001251310110092, 0.005465367343276739, 0.010748600587248802, 0.012014048174023628, 0.016086265444755554, 0.008203435689210892, 0.009482653811573982, 0.010493379086256027, 0.01503104530274868, 0.009477454237639904, 0.007667069789022207, 0.007789756171405315, 0.014242374338209629, 0.007306984625756741, 0.004232233855873346, 0.007672550622373819, 0.01966826431453228, 0.01636381447315216, 0.008318137377500534, 0.00309102819301188, 0.0010118921054527164, 0.00019393497495912015, 5.180735752219334e-05, 4.003716821898706e-05, 3.30002949340269e-05, 2.7922011213377118e-05, 2.3951024559210055e-05, 2.0699581000371836e-05, 1.7970251064980403e-05, 1.563694968353957e-05, 1.361995964543894e-05, 1.187605994346086e-05, 1.0354578989790753e-05, 9.020403922477271e-06, 7.849357643863186e-06, 6.8234426180424634e-06, 5.927886377321556e-06, 5.140118446433917e-06, 4.45417526862002e-06, 3.853551788779441e-06, 3.325421630506753e-06, 2.8666866001003655e-06, 2.468211960149347e-06, 2.121485522366129e-06, 1.8222527842226555e-06, 1.5628151004420943e-06, 1.3383134955802234e-06, 1.14444640075817e-06, 9.779927268027677e-07, 8.355447675967298e-07, 7.131901043067046e-07, 6.08264485890686e-07, 5.186749376662192e-07, 4.4246942820791446e-07, 3.7722045931332104e-07, 3.2094075663735566e-07, 2.733790154252347e-07, 2.3230380463701295e-07, 1.9790562078014773e-07, 1.6923704038163123e-07, 1.4430246153551707e-07, 1.2376307267913944e-07, 1.0680974327215154e-07, 9.264405065323444e-08, 8.172251853011403e-08, 7.358482179142811e-08, 6.730007129363003e-08, 6.366312277350517e-08, 6.166710875277204e-08, 6.142906983086505e-08, 6.322252943391504e-08, 6.622024528724069e-08, 6.986435607814201e-08, 7.464218043651272e-08, 7.970611193286459e-08, 8.533971396218476e-08, 9.096628161842091e-08, 9.689888713637629e-08, 1.0295728714027064e-07, 1.0846245857010217e-07, 1.1412038958269477e-07]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:04 2016", "state": "available"}], "summary": "63b8fc245dec6901db69683a4a85e95c"}