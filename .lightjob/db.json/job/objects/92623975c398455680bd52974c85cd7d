{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 32, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.7215824127197266, 1.3483830690383911, 1.1751184463500977, 1.0753145217895508, 0.9955921769142151, 0.9290000796318054, 0.8675656318664551, 0.8166408538818359, 0.771891713142395, 0.7348841428756714, 0.7023316621780396, 0.6751841306686401, 0.648887574672699, 0.628676176071167, 0.6067625284194946, 0.5897455215454102, 0.5706987977027893, 0.5520381927490234, 0.5368346571922302, 0.5229143500328064, 0.5099663138389587, 0.49533921480178833, 0.48231077194213867, 0.4717981815338135, 0.4605158865451813, 0.449535071849823, 0.438414067029953, 0.42982277274131775, 0.4198412299156189, 0.41409745812416077, 0.40410926938056946, 0.3965499997138977, 0.3886621594429016, 0.37953925132751465, 0.37266427278518677, 0.3661242127418518, 0.36103537678718567, 0.35357600450515747, 0.3454839885234833, 0.33784928917884827, 0.33725669980049133, 0.3297209143638611, 0.3236456513404846, 0.3170416057109833, 0.31304672360420227, 0.30800771713256836, 0.30015429854393005, 0.2972029447555542, 0.2943100035190582, 0.2889631688594818, 0.2851966917514801, 0.2806231379508972, 0.2749277353286743, 0.2718236446380615, 0.26881641149520874, 0.2639686167240143, 0.2588326632976532, 0.255735844373703, 0.25347039103507996, 0.24925023317337036, 0.24862322211265564, 0.2428317666053772, 0.23806579411029816, 0.23602625727653503, 0.23312640190124512, 0.2294815480709076, 0.22621524333953857, 0.22284160554409027, 0.2224360704421997, 0.21787165105342865, 0.21699650585651398, 0.21300989389419556, 0.209473118185997, 0.20932453870773315, 0.20384913682937622, 0.20243032276630402, 0.20033502578735352, 0.19730545580387115, 0.19536523520946503, 0.19364194571971893, 0.1935673952102661, 0.190519317984581, 0.18797603249549866, 0.1865842193365097, 0.1849723607301712, 0.1817503422498703, 0.17891930043697357, 0.17836637794971466, 0.17457392811775208, 0.17324866354465485, 0.17220069468021393, 0.1692533791065216, 0.16848112642765045, 0.1683902144432068, 0.16656002402305603, 0.16418993473052979, 0.16195367276668549, 0.16122321784496307, 0.1588791012763977, 0.15723341703414917, 0.15667842328548431, 0.15521252155303955, 0.15194468200206757, 0.15284347534179688, 0.15038788318634033, 0.14886996150016785, 0.14800532162189484, 0.1484120786190033, 0.14519354701042175, 0.14399918913841248, 0.14293153584003448, 0.14141742885112762, 0.14081723988056183, 0.1387474536895752, 0.13913534581661224, 0.13805650174617767, 0.13681712746620178, 0.1350693553686142, 0.1348659247159958, 0.13324400782585144, 0.13239380717277527, 0.12992124259471893, 0.13053478300571442, 0.1278839111328125, 0.12933063507080078, 0.12799343466758728, 0.12588827311992645, 0.12573853135108948, 0.12533989548683167], "moving_avg_accuracy_train": [0.05070866570344222, 0.10472743668385473, 0.15851012629343159, 0.2100062738823424, 0.2583543993491746, 0.3040250898763741, 0.3473739399292036, 0.387978090469552, 0.4260046220176042, 0.46045890643590653, 0.4929647617087057, 0.5229987039636841, 0.551235787932395, 0.5775905243327233, 0.6015169776787976, 0.6242547801877728, 0.6449908448565647, 0.6643299574108675, 0.6820372838597033, 0.6991059727921622, 0.7149702052183262, 0.7296873954292363, 0.743302529230951, 0.7564298648725938, 0.7684911129679757, 0.7795367182121066, 0.7897894770675755, 0.7993632269660229, 0.8084607995317407, 0.8170741170730295, 0.8248722813970112, 0.8324323529123949, 0.839227152729821, 0.8452820186964568, 0.8511822966913829, 0.8568529089034738, 0.8622821609717827, 0.8677051646224357, 0.8725185467390035, 0.8768621442903247, 0.8808946510222372, 0.8854304627749414, 0.8888990703595736, 0.8924856306547347, 0.8957159682156455, 0.8992277386133039, 0.9023395398950151, 0.905256346391394, 0.9080512801988676, 0.9109222521005387, 0.9137783134180318, 0.9160393435680152, 0.9183347955161233, 0.9206820813241916, 0.9227967113585314, 0.9241908149954264, 0.9260709732983939, 0.92774909278057, 0.9297569100621291, 0.9315105032417319, 0.9331003628474221, 0.9343824630175526, 0.9356758620992414, 0.9371885854477336, 0.9383385200173475, 0.9398779823728478, 0.9410311278071208, 0.942278186042005, 0.9432445732390687, 0.9442726120795676, 0.9450000651919319, 0.9456084142121444, 0.9465905869624415, 0.9466864251377366, 0.9476189615645314, 0.9485488891033994, 0.9494741795431425, 0.9503930074936824, 0.9513664730729872, 0.9521588506883998, 0.9525859972386997, 0.953440181291131, 0.9542158863359289, 0.9548999978857524, 0.9559109375293939, 0.9567883032229754, 0.9573454535150651, 0.95794211173268, 0.9584419377963997, 0.958810437094233, 0.9595791423408359, 0.96009902419733, 0.9606389253836324, 0.9615408938929343, 0.9620574797989528, 0.9624177754179409, 0.9629303424797828, 0.9631243328199828, 0.9636175055618863, 0.9637846322724475, 0.9640791694893242, 0.9645186751940462, 0.9650025138854205, 0.9653821651362288, 0.9659145134643372, 0.966272647123902, 0.966625230400853, 0.9665450990989553, 0.9672424970367434, 0.9677981116164762, 0.9682097730346552, 0.9684407954312635, 0.9689160356036687, 0.9692368670112328, 0.9695812746541941, 0.9700216301102862, 0.9702806941433698, 0.970390654935059, 0.9707569036142183, 0.9711377888457274, 0.97152004098621, 0.9716711166102726, 0.9719419072540627, 0.9723018041763124, 0.9723630777373172, 0.9726622564207653, 0.9729572020192295, 0.9734620352387721, 0.9737398180221115], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04882797792733433, 0.10125351503670932, 0.15299398619870103, 0.20250848182770143, 0.2486259388971903, 0.291398240056417, 0.33178848946945, 0.3698081062604568, 0.4055089892055707, 0.43782288932492325, 0.46828994150688874, 0.4963429950783384, 0.5226426069974623, 0.5470732116968124, 0.5687169000339083, 0.5890151201397043, 0.6075429544171495, 0.6246992270288984, 0.6404053385409934, 0.6551644270042886, 0.6688413201472332, 0.6813824575696333, 0.6928169951334531, 0.7041497946317794, 0.7139078020379539, 0.7230216575645801, 0.7312251570472035, 0.7390620257551338, 0.7461630062086113, 0.7525671251566508, 0.7586014459147057, 0.7641523458921358, 0.7694696567189614, 0.7740822790082852, 0.7783689459210862, 0.78197456944833, 0.7858493957772169, 0.7893632125530343, 0.7921989992608334, 0.7948428335685602, 0.7973962714089934, 0.8000421923196753, 0.8022414451791987, 0.8043724046710681, 0.8062973277017023, 0.8084000874014116, 0.8099466562215113, 0.8113232726023722, 0.8129712364283549, 0.8142092337380796, 0.8153844664730819, 0.8165418912019033, 0.817579455423204, 0.8184298730209438, 0.8196520565584579, 0.8199025300253531, 0.8204697530205588, 0.8209477506571324, 0.8218560817661179, 0.8225951900334066, 0.8230650749739666, 0.8237504961286783, 0.8242432458380997, 0.8249521867380999, 0.8247692739283713, 0.8256463680905041, 0.8255303735066043, 0.8259041116171637, 0.8258955905156883, 0.82608426353302, 0.8262449507433475, 0.8262908834739826, 0.8270147871728946, 0.8268148968403943, 0.8273430033536441, 0.8276596078093188, 0.8277461507934472, 0.8279125772238916, 0.8286096184002223, 0.8290314949449892, 0.8289940857554601, 0.8289848315473839, 0.8292409394129768, 0.8293545137228087, 0.8293549563170187, 0.8295405191378771, 0.82948676960549, 0.8292898516340222, 0.8292632278693399, 0.8291252851652372, 0.8291649756810929, 0.8290724497809053, 0.8288172485245768, 0.8290199610309293, 0.8290447403890563, 0.8286722987767319, 0.8288050865477786, 0.8286753073734224, 0.8287182270314115, 0.8286480209510114, 0.8285238003224012, 0.8283733216455827, 0.8286071902999251, 0.8284138105489236, 0.8286558373441818, 0.8283080199877756, 0.8285993059502179, 0.8280607997245486, 0.8283381276022443, 0.8283781441436012, 0.8283897449683224, 0.8282059027192311, 0.8285206369484375, 0.8285535800777654, 0.8284357150105008, 0.8285280374759417, 0.8286121572034981, 0.8285790311857085, 0.8288177724571979, 0.8291170593116287, 0.8294871622565954, 0.8295475821849269, 0.8297138824189945, 0.8298757596609052, 0.8295758190016671, 0.8294544158006721, 0.8296534172270055, 0.8297470692919555, 0.8298445926903202], "moving_var_accuracy_train": [0.023142318996811122, 0.04709033566123832, 0.06841450140987528, 0.08543973021737772, 0.09793362832104888, 0.10691257324802511, 0.11313342113134689, 0.11665835238814165, 0.11800667106350256, 0.11688988339014775, 0.11471057069427845, 0.11135785281122902, 0.10739806372961043, 0.10290940653322646, 0.09777074240740063, 0.09264673713309499, 0.08725192282132038, 0.08189274200868032, 0.07652541249750304, 0.07149493252461017, 0.06661050410639133, 0.061898814885089165, 0.057377280212527575, 0.05319049466070993, 0.0491807085452041, 0.04536068624756622, 0.04177068920014517, 0.038418530464192836, 0.03532156985707005, 0.03245711602296671, 0.029758706722084426, 0.02729722818173544, 0.024983029104591936, 0.02281467881099807, 0.020846530453654945, 0.019051279994028687, 0.01741144299681695, 0.015934979414490222, 0.014549999299642052, 0.013264800926868435, 0.012084670829067869, 0.011061366040464817, 0.010063510583603842, 0.009172930258000896, 0.008349552959017688, 0.007625590445048749, 0.006950181165495617, 0.006331732890181918, 0.005768864496057164, 0.005266160363393108, 0.004812958103297326, 0.004377672609019802, 0.0039873272449324785, 0.0036381822764220634, 0.003314608990419044, 0.003000639815930777, 0.002732390791535658, 0.0024844964773502233, 0.002272328801740347, 0.0020727717229222604, 0.0018882434327222837, 0.0017142131170662918, 0.001557847736020284, 0.001422657949779918, 0.0012922933004314633, 0.0011843934694843424, 0.0010779218220691698, 0.0009841260280329823, 0.0008941185631615238, 0.0008142184815635386, 0.0007375593256833824, 0.0006671341898885858, 0.0006091027407025643, 0.000548275131234903, 0.0005012742357971057, 0.00045892969926530105, 0.0004207421909196909, 0.00038626617505196264, 0.0003561682746535856, 0.0003262022077568898, 0.0002952240745600989, 0.00027226834066293974, 0.00025045697144537046, 0.00022962335181425056, 0.00021585900730059857, 0.00020120104161300173, 0.0001838746854834821, 0.000168691226192961, 0.0001540705384194277, 0.00013988561017001737, 0.00013121521895840936, 0.00012052619136497429, 0.00011109701184721388, 0.00010730923538844403, 9.898006083427228e-05, 9.025037114840302e-05, 8.358985896953027e-05, 7.556956334139546e-05, 7.020158118746568e-05, 6.34328051051658e-05, 5.787029414377855e-05, 5.3821752109749844e-05, 5.054647581221178e-05, 4.6789043881153196e-05, 4.4660692174996135e-05, 4.134896042051605e-05, 3.833289908313327e-05, 3.455739840471446e-05, 3.547893351692318e-05, 3.470940821613608e-05, 3.276365350347593e-05, 2.9967630282740184e-05, 2.900354624767589e-05, 2.702958675162444e-05, 2.539417769723312e-05, 2.459997627690147e-05, 2.274400620834924e-05, 2.0578427968894625e-05, 1.9727828026878477e-05, 1.9060707260426667e-05, 1.8469686824515988e-05, 1.682813273973766e-05, 1.5805267620641795e-05, 1.5390473010380823e-05, 1.3885215752846612e-05, 1.3302265139229337e-05, 1.2754974779787433e-05, 1.3773186517791805e-05, 1.3090337338491122e-05], "duration": 90194.774505, "accuracy_train": [0.5070866570344224, 0.5908963755075674, 0.6425543327796235, 0.6734716021825398, 0.6934875285506644, 0.7150613046211701, 0.7375135904046696, 0.7534154453326873, 0.7682434059500739, 0.7705474662006275, 0.7855174591638981, 0.7933041842584901, 0.8053695436507937, 0.8147831519356773, 0.8168550577934662, 0.8288950027685493, 0.8316154268756922, 0.8383819703995938, 0.8414032218992249, 0.852724173184293, 0.8577482970538022, 0.8621421073274271, 0.8658387334463824, 0.8745758856473791, 0.877042345826412, 0.8789471654092839, 0.8820643067667959, 0.8855269760520488, 0.8903389526232004, 0.894593974944629, 0.8950557603128461, 0.900472996550849, 0.9003803510866556, 0.8997758123961794, 0.904284798645718, 0.9078884188122923, 0.9111454295865633, 0.9165121974783131, 0.9158389857881136, 0.915954522252215, 0.9171872116094499, 0.9262527685492802, 0.9201165386212625, 0.924764673311185, 0.9247890062638427, 0.9308336721922297, 0.9303457514304172, 0.931507604858804, 0.9332056844661315, 0.9367609992155776, 0.9394828652754706, 0.9363886149178663, 0.9389938630490956, 0.941807653596807, 0.9418283816675894, 0.9367377477274824, 0.9429923980251015, 0.942852168120155, 0.9478272655961609, 0.947292841858158, 0.9474090992986341, 0.9459213645487264, 0.9473164538344407, 0.9508030955841639, 0.9486879311438722, 0.9537331435723514, 0.9514094367155776, 0.9535017101559615, 0.9519420580126431, 0.9535249616440569, 0.9515471432032114, 0.9510835553940569, 0.9554301417151162, 0.9475489687153931, 0.9560117894056847, 0.9569182369532114, 0.9578017935008305, 0.9586624590485419, 0.9601276632867294, 0.9592902492271133, 0.956430316191399, 0.9611278377630121, 0.9611972317391103, 0.9610570018341639, 0.9650093943221669, 0.9646845944652085, 0.9623598061438722, 0.9633120356912146, 0.9629403723698781, 0.9621269307747323, 0.9664974895602622, 0.9647779609057769, 0.9654980360603543, 0.9696586104766519, 0.9667067529531194, 0.9656604359888336, 0.9675434460363603, 0.9648702458817828, 0.968056060239018, 0.9652887726674971, 0.9667300044412146, 0.9684742265365448, 0.9693570621077889, 0.9687990263935032, 0.9707056484173128, 0.9694958500599853, 0.969798479893411, 0.9658239173818751, 0.9735190784768365, 0.9727986428340717, 0.9719147257982651, 0.9705199970007383, 0.9731931971553157, 0.9721243496793098, 0.9726809434408453, 0.9739848292151162, 0.9726122704411223, 0.9713803020602622, 0.9740531417266519, 0.9745657559293098, 0.9749603102505537, 0.9730307972268365, 0.9743790230481728, 0.9755408764765596, 0.9729145397863603, 0.9753548645717978, 0.975611712405408, 0.9780055342146549, 0.9762398630721669], "end": "2016-02-02 10:51:02.640000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0], "moving_var_accuracy_valid": [0.02145754285625024, 0.04404772104148331, 0.06373663614191905, 0.07942814002427583, 0.0906267046408536, 0.09802926189486853, 0.1029086859342048, 0.10562723868919938, 0.106535492207826, 0.10527963625535484, 0.10310584404774684, 0.09987802397511579, 0.09611524786147296, 0.09187541308910885, 0.08690391498369802, 0.08192168314049826, 0.07681904061356071, 0.07178617576156246, 0.06682769563487952, 0.06210540230179794, 0.05757837872600988, 0.053236062004036626, 0.04908919364731862, 0.045336165382810156, 0.04165951722137975, 0.03824112676228372, 0.03502269071990796, 0.03207317024822519, 0.02931966853400871, 0.02675681633611358, 0.024408851945602043, 0.022245279166076732, 0.020275215399330748, 0.018439180418853368, 0.01676064199595976, 0.01520158248554571, 0.013816552748702454, 0.012546019648838355, 0.011363792860223683, 0.010290322312821738, 0.009319970484784168, 0.00845098151349601, 0.007649413780407507, 0.006925341297570647, 0.006266155125878374, 0.005679333998483031, 0.005132927474672472, 0.004636690381145716, 0.004197463405976871, 0.003791510801429156, 0.0034247902691190283, 0.003094367930233109, 0.002794619992829707, 0.0025216668843616465, 0.0022829437893198148, 0.0020552140430063997, 0.0018525883160423705, 0.0016693858201032633, 0.0015098728267248953, 0.0013638020733293778, 0.0012294089927127254, 0.0011106963128753905, 0.0010018119020730647, 0.0009061540866629969, 0.0008158397918603628, 0.0007411794601975527, 0.0006671826068692443, 0.0006017214677598808, 0.0005415499744664258, 0.00048771535458700494, 0.00043917620254437005, 0.0003952775706316255, 0.0003604661426561489, 0.00032477913369577773, 0.000294811288730231, 0.0002662323052893859, 0.00023967648195336386, 0.00021595811356878177, 0.0001987350998254083, 0.00018046340821408784, 0.00016242966241983005, 0.0001461874669411511, 0.00013215904139640312, 0.00011905922937144679, 0.00010715330819730883, 9.674787942194217e-05, 8.709909258983444e-05, 7.873817351823346e-05, 7.087073559002287e-05, 6.395491573755698e-05, 5.757360219724138e-05, 5.189329135736699e-05, 4.729011135271502e-05, 4.293093145952914e-05, 3.864336446287888e-05, 3.6027442807907494e-05, 3.258339185637268e-05, 2.9476636377604557e-05, 2.65455516132212e-05, 2.3935356495425496e-05, 2.1680697727033704e-05, 1.9716422443923595e-05, 1.8237031126886247e-05, 1.6749889567074393e-05, 1.5602093336973744e-05, 1.5130676224032498e-05, 1.43812362088728e-05, 1.555301318374766e-05, 1.4689908631097921e-05, 1.323532968022764e-05, 1.1913007924412785e-05, 1.1025888884929933e-05, 1.0814818711744295e-05, 9.74310408849912e-06, 8.893823246380567e-06, 8.081151860368262e-06, 7.336721831408952e-06, 6.612925645759414e-06, 6.464609633594786e-06, 6.624302261351126e-06, 7.194657744072857e-06, 6.5080470793217476e-06, 6.106144282047864e-06, 5.731368026880887e-06, 5.96791081577051e-06, 5.503768369100023e-06, 5.3098056413345424e-06, 4.857761460625775e-06, 4.457582633620573e-06], "accuracy_test": 0.8089425223214286, "start": "2016-02-01 09:47:47.865000", "learning_rate_per_epoch": [0.0002558329433668405, 0.000252842582995072, 0.00024988717632368207, 0.0002469663158990443, 0.000244079579715617, 0.0002412265894236043, 0.00023840695212129503, 0.000235620274906978, 0.00023286616487894207, 0.00023014424368739128, 0.00022745414753444493, 0.00022479548351839185, 0.00022216790239326656, 0.00021957102580927312, 0.00021700450452044606, 0.0002144679892808199, 0.0002119611162925139, 0.00020948355086147785, 0.00020703494374174625, 0.00020461496023926884, 0.00020222326565999538, 0.0001998595253098756, 0.0001975234190467745, 0.00019521461217664182, 0.00019293279910925776, 0.00019067764515057206, 0.00018844885926228017, 0.00018624612130224705, 0.00018406914023216814, 0.00018191759590990841, 0.0001797911972971633, 0.00017768965335562825, 0.00017561267304699868, 0.00017355997988488525, 0.0001715312828309834, 0.00016952629084698856, 0.00016754474199842662, 0.000165586345246993, 0.00016365083865821362, 0.00016173796029761434, 0.00015984744823072106, 0.00015797902597114444, 0.0001561324461363256, 0.00015430744679179043, 0.00015250378055498004, 0.00015072120004333556, 0.0001489594578742981, 0.00014721830666530877, 0.0001454974990338087, 0.00014379681670106947, 0.0001421160122845322, 0.0001404548529535532, 0.00013881310587748885, 0.00013719055277761072, 0.00013558696082327515, 0.00013400211173575372, 0.000132435787236318, 0.00013088776904623955, 0.0001293578534387052, 0.00012784582213498652, 0.00012635145685635507, 0.0001248745684279129, 0.00012341493857093155, 0.00012197237083455548, 0.00012054666149197146, 0.00011913762136828154, 0.00011774504673667252, 0.00011636875569820404, 0.00011500855180202052, 0.00011366424587322399, 0.00011233564873691648, 0.00011102258577011526, 0.00010972486779792234, 0.000108442320197355, 0.00010717476106947288, 0.00010592202306725085, 0.00010468392429174855, 0.00010346029739594087, 0.00010225097503280267, 0.00010105578985530883, 9.987457451643422e-05, 9.870716166915372e-05, 9.755339851835743e-05, 9.641311771702021e-05, 9.528616647003219e-05, 9.417239198228344e-05, 9.307163418270648e-05, 9.198374027619138e-05, 9.090856474358588e-05, 8.984595478978008e-05, 8.879576489562169e-05, 8.775784954195842e-05, 8.673207048559561e-05, 8.571828220738098e-05, 8.471633918816224e-05, 8.372611046070233e-05, 8.274745778180659e-05, 8.178024290828034e-05, 8.082433487288654e-05, 7.98795954324305e-05, 7.89459008956328e-05, 7.802312029525638e-05, 7.711112266406417e-05, 7.620979158673435e-05, 7.531899609602988e-05, 7.44386125006713e-05, 7.356851710937917e-05, 7.270859350683168e-05, 7.185871800174937e-05, 7.101878145476803e-05, 7.018866017460823e-05, 6.936823774594814e-05, 6.855740502942353e-05, 6.775605288567021e-05, 6.696406489936635e-05, 6.618133920710534e-05, 6.540775939356536e-05, 6.46432235953398e-05, 6.388762267306447e-05, 6.314085476333275e-05, 6.240281800273806e-05, 6.167340325191617e-05, 6.0952515923418105e-05, 6.024005779181607e-05, 5.953592699370347e-05, 5.884002530365251e-05, 5.815225813421421e-05, 5.747253089793958e-05, 5.6800749007379636e-05], "accuracy_train_first": 0.5070866570344224, "accuracy_train_last": 0.9762398630721669, "batch_size_eval": 1024, "accuracy_train_std": [0.017184357098629182, 0.01639845531195757, 0.016084489409602297, 0.016500163501477056, 0.015657193708941572, 0.01613117562902237, 0.01593054040955573, 0.015722231960896813, 0.015966304032695996, 0.015539025071557326, 0.015373121501417408, 0.016009694226309545, 0.016023872668730604, 0.016192761653522413, 0.015269954529624728, 0.016042721480818922, 0.015475433041376769, 0.015616220135747346, 0.01606329662200179, 0.015873215822884434, 0.01565426707595024, 0.015206702971105952, 0.014924652651819205, 0.014392951894658381, 0.014788432195825272, 0.013259594166301321, 0.012969605055367994, 0.013372786367784899, 0.014134541146036385, 0.01280521851037421, 0.013865141891139068, 0.013021755500970721, 0.013166613744171227, 0.014718144643430258, 0.01447365124781366, 0.013658151810861994, 0.01271771980553993, 0.013801464745658408, 0.013413839051955583, 0.013787027265704174, 0.013514526030083494, 0.012884045983146518, 0.012951859894457275, 0.013781787316519708, 0.01277899768972201, 0.012503615910084658, 0.012272294583519961, 0.013789515798751964, 0.012195240085277649, 0.012605227556533231, 0.01234671802949703, 0.01269974831337389, 0.012405525528252623, 0.011830290304072536, 0.013758444723111914, 0.013008986901682893, 0.01300728757067464, 0.012965513339412508, 0.011765812063660855, 0.01190703847088851, 0.011865131275399744, 0.012596209874415686, 0.012399508962585777, 0.010917219511814505, 0.012033129314348679, 0.01071511660651537, 0.011764304194702197, 0.011127037976194755, 0.01224123338081532, 0.011419304530897798, 0.011655361337518134, 0.011362000987360586, 0.011172571048674954, 0.012392711418298692, 0.010800890285676393, 0.011166814627743845, 0.010496426694254596, 0.010184784499326353, 0.009950224084422005, 0.010006476285444911, 0.009937981021599998, 0.010409826223229102, 0.009506654644553513, 0.010814373431154602, 0.00993935891169249, 0.00892858741859809, 0.009026227880419124, 0.008705543914063628, 0.008997774891856002, 0.008517367511371287, 0.00890906459822123, 0.008467493939882173, 0.008354889667123597, 0.008237016648781131, 0.007893637343424284, 0.008615023713085285, 0.007703925861181989, 0.008400724226932837, 0.007740286175031445, 0.008459052194186871, 0.008046288565227183, 0.00787989604463963, 0.007584938083276986, 0.008049856649088385, 0.008494094910392385, 0.008073944600711067, 0.007931220684714708, 0.007988784108002281, 0.006646066020948566, 0.006691751031826167, 0.006566273613673363, 0.007528330452804468, 0.006956591074653993, 0.006956989758483557, 0.00713412415912667, 0.005843996870274424, 0.007414291990379095, 0.007019958787449446, 0.007228040380301614, 0.006819995314809387, 0.006779551796577571, 0.00681076681699422, 0.006147352699604732, 0.007019522357024986, 0.006348751582233142, 0.006564416688164115, 0.005931342270209614, 0.006028004526874601, 0.005612788461475835], "accuracy_test_std": 0.011647825941070713, "error_valid": [0.5117202207266567, 0.42691665097891573, 0.3813417733433735, 0.35186105751129515, 0.3363169474774097, 0.32365104951054224, 0.304699265813253, 0.2880153426204819, 0.27318306428840367, 0.27135200960090367, 0.25750658885542166, 0.25117952277861444, 0.24066088573042166, 0.2330513460090362, 0.23648990493222888, 0.22830089890813254, 0.22570653708584332, 0.22089431946536142, 0.21823965785015065, 0.2120037768260542, 0.2080666415662651, 0.2057473056287651, 0.20427216679216864, 0.1938550098832832, 0.19827013130647586, 0.1949536426957832, 0.19494334760918675, 0.19040615587349397, 0.1899281697100903, 0.18979580431099397, 0.18708966726280118, 0.18588955431099397, 0.1826745458396084, 0.18440412038780118, 0.18305105186370485, 0.18557481880647586, 0.17927716726280118, 0.1790124364646084, 0.18227892036897586, 0.18136265766189763, 0.1796227880271084, 0.17614451948418675, 0.1779652790850903, 0.1764489599021084, 0.1763783650225903, 0.17267507530120485, 0.1761342243975903, 0.17628717996987953, 0.17219708913780118, 0.17464879047439763, 0.17403843891189763, 0.17304128623870485, 0.1730824665850903, 0.17391636859939763, 0.16934829160391573, 0.1778432087725903, 0.1744252400225903, 0.17475027061370485, 0.16996893825301207, 0.17075283556099397, 0.17270596056099397, 0.17008071347891573, 0.1713220067771084, 0.16866734516189763, 0.17687694135918675, 0.16645978445030118, 0.17551357774849397, 0.17073224538780118, 0.1741810993975903, 0.17221767931099397, 0.17230886436370485, 0.17329572195030118, 0.16647007953689763, 0.1749841161521084, 0.1679040380271084, 0.1694909520896084, 0.17147496234939763, 0.1705895849021084, 0.16511701101280118, 0.1671716161521084, 0.17134259695030118, 0.17109845632530118, 0.16845408979668675, 0.16962331748870485, 0.1706410603350903, 0.16878941547439763, 0.17099697618599397, 0.17248241010918675, 0.17097638601280118, 0.17211619917168675, 0.17047780967620485, 0.1717602833207832, 0.17347956278237953, 0.16915562641189763, 0.17073224538780118, 0.17467967573418675, 0.16999982351280118, 0.1724927051957832, 0.17089549604668675, 0.1719838337725903, 0.1725941853350903, 0.1729809864457832, 0.16928799181099397, 0.1733266072100903, 0.16916592149849397, 0.17482233621987953, 0.16877912038780118, 0.17678575630647586, 0.16916592149849397, 0.17126170698418675, 0.17150584760918675, 0.1734486775225903, 0.16864675498870485, 0.1711499317582832, 0.17262507059487953, 0.1706410603350903, 0.17063076524849397, 0.17171910297439763, 0.16903355609939763, 0.16818935899849397, 0.16718191123870485, 0.1699086384600903, 0.16878941547439763, 0.16866734516189763, 0.17312364693147586, 0.1716382130082832, 0.16855556993599397, 0.16941006212349397, 0.16927769672439763], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.011688769200917881, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.0002588586939904659, "optimization": "adam", "nb_data_augmentation": 4, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.4303154732407013e-08, "rotation_range": [0, 0], "momentum": 0.8792022696349224}, "accuracy_valid_max": 0.8348829889871988, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8307223032756024, "accuracy_valid_std": [0.015473980786606195, 0.014262582691724124, 0.014988549348513777, 0.015548815952977834, 0.018636162161032654, 0.017011368007789008, 0.01871755906187482, 0.016611953356081812, 0.017041009580534252, 0.016395918024484302, 0.01507273006794521, 0.01320410070516026, 0.014163634668723543, 0.01291699293084162, 0.01246662281150877, 0.01516339210979882, 0.014831085004860424, 0.019806910328218887, 0.015432405870280422, 0.015869942825459636, 0.016089860398136875, 0.012419235902452622, 0.013833843501680244, 0.01563406364519723, 0.012012869312687689, 0.011135147263011966, 0.015257226113886411, 0.014951925394254528, 0.014224713652816206, 0.014323345143971327, 0.012360470434093516, 0.013476251501411496, 0.014048756014356721, 0.015076089399371403, 0.01148473928852007, 0.011711136867301322, 0.009146895321892435, 0.011008891045013396, 0.009982479149070482, 0.011084271830773037, 0.010457946660536451, 0.005628987009852939, 0.0064057504172345524, 0.011777121751020936, 0.007753052154923523, 0.007158385471606396, 0.008694858377473039, 0.010254992994438916, 0.009609331248159386, 0.008951663062984686, 0.008993372046239703, 0.009623058696097465, 0.0077905787582358225, 0.006307929999276244, 0.009135492471477158, 0.006708875976609762, 0.007174611044181139, 0.007955184141272149, 0.011034156317559886, 0.009620960853670662, 0.008002785681980679, 0.010338645613384122, 0.00858624682398784, 0.009451923364807224, 0.008349704599163626, 0.009420638314533695, 0.005704520054766451, 0.007646813389178938, 0.007687520732478269, 0.009082898130446925, 0.006893623113235389, 0.007385292237595555, 0.008954881427762814, 0.01137765814543706, 0.007818050907104222, 0.009718672028604729, 0.009363020616615328, 0.010108385946372101, 0.008805030948414845, 0.009375970448478063, 0.011830573128639063, 0.010121738740989454, 0.010181235913541616, 0.011522257881255056, 0.010667512783465323, 0.01084640119469052, 0.013140891156158062, 0.009427852878251704, 0.008968266729629488, 0.00825197768076178, 0.009987990052008622, 0.009874741785937117, 0.011984760034672833, 0.009993112962144056, 0.010888307349745415, 0.011287246640672242, 0.010562976219136686, 0.00888944956309483, 0.008718679493846212, 0.00866793289737316, 0.009023447648299715, 0.011324688465329916, 0.011864809720509652, 0.011196902255705674, 0.011731317899792678, 0.010338545187971284, 0.00779009563611824, 0.011265571019185096, 0.01115844065339822, 0.010016053135258962, 0.010628214401168914, 0.009210200437150848, 0.009269633336717948, 0.009671654041750256, 0.011319130856652418, 0.00975700670367066, 0.011925716455666514, 0.008012393024272885, 0.011855341387732829, 0.008514081709923267, 0.009118337203432851, 0.011723458683275506, 0.009486193319788644, 0.009774340814133161, 0.010276209426423943, 0.009634233487075986, 0.010789835238757184, 0.007759437876210397, 0.008897912803056436], "accuracy_valid": [0.4882797792733434, 0.5730833490210843, 0.6186582266566265, 0.6481389424887049, 0.6636830525225903, 0.6763489504894578, 0.695300734186747, 0.7119846573795181, 0.7268169357115963, 0.7286479903990963, 0.7424934111445783, 0.7488204772213856, 0.7593391142695783, 0.7669486539909638, 0.7635100950677711, 0.7716991010918675, 0.7742934629141567, 0.7791056805346386, 0.7817603421498494, 0.7879962231739458, 0.7919333584337349, 0.7942526943712349, 0.7957278332078314, 0.8061449901167168, 0.8017298686935241, 0.8050463573042168, 0.8050566523908133, 0.809593844126506, 0.8100718302899097, 0.810204195689006, 0.8129103327371988, 0.814110445689006, 0.8173254541603916, 0.8155958796121988, 0.8169489481362951, 0.8144251811935241, 0.8207228327371988, 0.8209875635353916, 0.8177210796310241, 0.8186373423381024, 0.8203772119728916, 0.8238554805158133, 0.8220347209149097, 0.8235510400978916, 0.8236216349774097, 0.8273249246987951, 0.8238657756024097, 0.8237128200301205, 0.8278029108621988, 0.8253512095256024, 0.8259615610881024, 0.8269587137612951, 0.8269175334149097, 0.8260836314006024, 0.8306517083960843, 0.8221567912274097, 0.8255747599774097, 0.8252497293862951, 0.8300310617469879, 0.829247164439006, 0.827294039439006, 0.8299192865210843, 0.8286779932228916, 0.8313326548381024, 0.8231230586408133, 0.8335402155496988, 0.824486422251506, 0.8292677546121988, 0.8258189006024097, 0.827782320689006, 0.8276911356362951, 0.8267042780496988, 0.8335299204631024, 0.8250158838478916, 0.8320959619728916, 0.8305090479103916, 0.8285250376506024, 0.8294104150978916, 0.8348829889871988, 0.8328283838478916, 0.8286574030496988, 0.8289015436746988, 0.8315459102033133, 0.8303766825112951, 0.8293589396649097, 0.8312105845256024, 0.829003023814006, 0.8275175898908133, 0.8290236139871988, 0.8278838008283133, 0.8295221903237951, 0.8282397166792168, 0.8265204372176205, 0.8308443735881024, 0.8292677546121988, 0.8253203242658133, 0.8300001764871988, 0.8275072948042168, 0.8291045039533133, 0.8280161662274097, 0.8274058146649097, 0.8270190135542168, 0.830712008189006, 0.8266733927899097, 0.830834078501506, 0.8251776637801205, 0.8312208796121988, 0.8232142436935241, 0.830834078501506, 0.8287382930158133, 0.8284941523908133, 0.8265513224774097, 0.8313532450112951, 0.8288500682417168, 0.8273749294051205, 0.8293589396649097, 0.829369234751506, 0.8282808970256024, 0.8309664439006024, 0.831810641001506, 0.8328180887612951, 0.8300913615399097, 0.8312105845256024, 0.8313326548381024, 0.8268763530685241, 0.8283617869917168, 0.831444430064006, 0.830589937876506, 0.8307223032756024], "seed": 456930249, "model": "residualv4", "loss_std": [0.22939243912696838, 0.13884785771369934, 0.12593695521354675, 0.12549759447574615, 0.12422211468219757, 0.12207154929637909, 0.11995211243629456, 0.12067683786153793, 0.11729782074689865, 0.11562663316726685, 0.11480738967657089, 0.11233823001384735, 0.1089346706867218, 0.10900918394327164, 0.10716189444065094, 0.10685137659311295, 0.10520904511213303, 0.10241957753896713, 0.10140712559223175, 0.10067670047283173, 0.09819915890693665, 0.09666162729263306, 0.09427331387996674, 0.09466056525707245, 0.0936276912689209, 0.08918692916631699, 0.09073571115732193, 0.08900608122348785, 0.08616775274276733, 0.08635566383600235, 0.08568938076496124, 0.0864182710647583, 0.0830003023147583, 0.08144555240869522, 0.08034242689609528, 0.0807066559791565, 0.08114843815565109, 0.0790974423289299, 0.07884636521339417, 0.07657095044851303, 0.07798350602388382, 0.07522360980510712, 0.07501594722270966, 0.0744931697845459, 0.07446622103452682, 0.07269252836704254, 0.07121451944112778, 0.06948783993721008, 0.07045990228652954, 0.07149887830018997, 0.06949037313461304, 0.0691041573882103, 0.06705756485462189, 0.06609200686216354, 0.06608252972364426, 0.06480149924755096, 0.06566973775625229, 0.06546091288328171, 0.06253796070814133, 0.0648970827460289, 0.06367959082126617, 0.06209162250161171, 0.06099047511816025, 0.05909312516450882, 0.05955838784575462, 0.0600576177239418, 0.058555006980895996, 0.05950581282377243, 0.05810987576842308, 0.05749288201332092, 0.05649924278259277, 0.05660174414515495, 0.05358663201332092, 0.05562644451856613, 0.05389818921685219, 0.05324909836053848, 0.05297086015343666, 0.05255741998553276, 0.05089903995394707, 0.05245349183678627, 0.052279286086559296, 0.05287190526723862, 0.05014726147055626, 0.051142144948244095, 0.0482863187789917, 0.049873873591423035, 0.04808143153786659, 0.048344600945711136, 0.047890156507492065, 0.04900091886520386, 0.04837098717689514, 0.04675452411174774, 0.04832754656672478, 0.04678420349955559, 0.04626025632023811, 0.04611671715974808, 0.04667023941874504, 0.04397634416818619, 0.04495956003665924, 0.04477979242801666, 0.0453052781522274, 0.04401685297489166, 0.04384946823120117, 0.043598372489213943, 0.042691025882959366, 0.04199478402733803, 0.0428292378783226, 0.04144604504108429, 0.042155880481004715, 0.04134158790111542, 0.04073329269886017, 0.04175816476345062, 0.04043818637728691, 0.04070810228586197, 0.04160018637776375, 0.039609722793102264, 0.039520327001810074, 0.04039038345217705, 0.03974078968167305, 0.0386657677590847, 0.03875991329550743, 0.040142569690942764, 0.037877406924963, 0.03848753124475479, 0.03863368183374405, 0.0382353700697422, 0.03633987158536911, 0.037525199353694916, 0.03714380040764809]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:30 2016", "state": "available"}], "summary": "9f03401ee23c4b48fae68c3174e6860b"}