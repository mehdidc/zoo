{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 32, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [2.324162721633911, 2.0133907794952393, 1.907381296157837, 1.847583532333374, 1.8045011758804321, 1.7657727003097534, 1.7300119400024414, 1.6974221467971802, 1.666906714439392, 1.644331693649292, 1.619235873222351, 1.5999484062194824, 1.5797171592712402, 1.5622990131378174, 1.5460046529769897, 1.5261821746826172, 1.5123246908187866, 1.4970219135284424, 1.4809640645980835, 1.4652258157730103, 1.4507921934127808, 1.4354195594787598, 1.4215052127838135, 1.4093079566955566, 1.3941134214401245, 1.382699728012085, 1.3700177669525146, 1.358290433883667, 1.3449440002441406, 1.3351118564605713, 1.322855830192566, 1.3107712268829346, 1.3007020950317383, 1.292515754699707, 1.2812020778656006, 1.27004873752594, 1.2578444480895996, 1.248205542564392, 1.2417922019958496, 1.2309553623199463, 1.2206639051437378, 1.2121435403823853, 1.2022881507873535, 1.1934373378753662, 1.1849490404129028, 1.17653489112854, 1.168631672859192, 1.160986304283142, 1.1533132791519165, 1.1460844278335571, 1.1391712427139282, 1.1322662830352783, 1.1241426467895508, 1.1173352003097534, 1.1098076105117798, 1.1040623188018799, 1.0956733226776123, 1.08907151222229, 1.0823969841003418, 1.0766795873641968, 1.070244550704956, 1.0633604526519775, 1.0570895671844482, 1.049299716949463, 1.0428699254989624, 1.0366685390472412, 1.0343276262283325, 1.0261256694793701, 1.0195986032485962, 1.0123740434646606, 1.008586049079895, 1.0036243200302124, 0.9977620244026184, 0.9901858568191528, 0.9882485270500183, 0.9824416041374207, 0.9759871959686279, 0.9707095623016357, 0.9653756022453308, 0.9608842730522156, 0.9536882638931274, 0.9496216773986816, 0.9445140361785889, 0.9378983378410339, 0.9342623949050903, 0.9295908212661743, 0.9250501990318298, 0.9190838932991028, 0.9154983758926392, 0.9109888076782227, 0.9050262570381165, 0.902457594871521, 0.8980405330657959, 0.8919917345046997, 0.8891794681549072, 0.8832505941390991, 0.88155597448349, 0.8749316334724426, 0.868521511554718, 0.8671454191207886, 0.8612223863601685, 0.8568615317344666, 0.8518542647361755, 0.8470797538757324, 0.8443792462348938, 0.8396498560905457, 0.8378816246986389, 0.8299856781959534, 0.7999740242958069, 0.7936906218528748, 0.79364413022995, 0.7903919816017151, 0.7923340201377869, 0.7889817953109741, 0.7919617891311646, 0.7868290543556213, 0.7877016067504883, 0.7871667146682739, 0.7894410490989685, 0.7868813872337341, 0.7889962792396545, 0.7866590619087219, 0.7875628471374512, 0.784966766834259, 0.7854292392730713, 0.785397469997406, 0.7860814332962036, 0.7803256511688232, 0.7772198915481567, 0.7784997820854187, 0.778675377368927, 0.7801815271377563, 0.7773340940475464, 0.7779413461685181, 0.7767012119293213, 0.7786840200424194, 0.7774122953414917, 0.7773739099502563, 0.7781139612197876, 0.7777827978134155, 0.7792198657989502, 0.7787042260169983, 0.7796022891998291, 0.7759143710136414, 0.7787760496139526, 0.7780172824859619, 0.778997540473938, 0.778540313243866, 0.7783503532409668, 0.7797179222106934, 0.7782249450683594, 0.7793384194374084, 0.7780801057815552, 0.7794210910797119, 0.7791011333465576, 0.7768146991729736, 0.7796984314918518, 0.7811743021011353, 0.776451826095581, 0.7789241671562195, 0.778168261051178, 0.7789298892021179, 0.7791813611984253, 0.7787086367607117, 0.7790454626083374, 0.7800381183624268, 0.7777010202407837, 0.7782828211784363, 0.7778500318527222, 0.7780091762542725, 0.7805070281028748, 0.7791216373443604, 0.7807591557502747, 0.7779465913772583, 0.7761728763580322, 0.7798184156417847, 0.7804546356201172, 0.7785558104515076, 0.7784655094146729, 0.779231071472168, 0.7787815928459167], "moving_avg_accuracy_train": [0.02832188062361572, 0.059537116945251926, 0.09060182908101465, 0.1205639877888231, 0.14948491214001647, 0.17699918970600745, 0.2035262867295466, 0.22868390385185772, 0.2528926572178255, 0.2756732656424069, 0.29696411076859625, 0.3172461965713509, 0.33625098871285, 0.3539619852353911, 0.37084091778535694, 0.3867062141862694, 0.4014753350041688, 0.415858038531945, 0.42939051806284056, 0.44203924945607664, 0.45428813516476946, 0.465948862588215, 0.4770898725895449, 0.48748175785619946, 0.49740644120333144, 0.5072964012323615, 0.5164041593072612, 0.5248800512853575, 0.5330989058144445, 0.5412955097881865, 0.5491094731942885, 0.5562789716978109, 0.563577982664104, 0.5705281285479796, 0.5773017680279916, 0.5836790621265859, 0.5895445092904039, 0.5952465888211734, 0.600940966166677, 0.6060055961038576, 0.6111890117841695, 0.6159518502929066, 0.6208566782411906, 0.6255358740660195, 0.6304167931655085, 0.6346563047288949, 0.6389992413298795, 0.6431778538744017, 0.6471384237668157, 0.6512051327640268, 0.6547604670674506, 0.6581001013084723, 0.6615216493229278, 0.6652287606168715, 0.6686675394266776, 0.671669290207847, 0.674884615651348, 0.6773599899112409, 0.6801878232844579, 0.6826605413655267, 0.6852976831730493, 0.6881662593497919, 0.6899690891064887, 0.6924820957839258, 0.6950248924578404, 0.6977693549239425, 0.6997906895208339, 0.7021562285306368, 0.7042663961560707, 0.7063030552380919, 0.7083938875880368, 0.7102780339494342, 0.7124968520591973, 0.714914496097233, 0.7171135190731039, 0.7192253895264432, 0.7217605501688985, 0.7229214730209178, 0.7246336933937061, 0.7267024644601587, 0.7293011782508538, 0.7313239806684779, 0.7334630842800631, 0.7353020667804437, 0.7375871221628922, 0.7391741000940283, 0.7414622166569308, 0.7429497152004939, 0.7451974889277683, 0.746955490415667, 0.74853525845951, 0.750673051337027, 0.7524599171958493, 0.7539448635818845, 0.7556952639150489, 0.7576192162922317, 0.759367049473363, 0.7610609349768388, 0.7626945697287397, 0.7649833654840404, 0.7666665715078494, 0.7685442522412007, 0.7702969078702553, 0.772178748235177, 0.7737724231647969, 0.7752162474896055, 0.7768246457830499, 0.7781186723280837, 0.782066535392433, 0.7855683507300815, 0.7888384950256133, 0.7917141595272971, 0.7943185696692978, 0.7967950362304226, 0.7989891230985671, 0.8010032206632027, 0.8028368348106606, 0.8045033996338581, 0.8060010549235638, 0.8074231692021099, 0.8087287928849812, 0.8097759710150416, 0.8108532178654109, 0.8118342576283346, 0.8126312710554697, 0.8133904358184628, 0.8140898880491857, 0.8148777214687964, 0.8155471719190467, 0.8162241541838142, 0.8168775079030296, 0.817442310811047, 0.8178785538151674, 0.8183478663807714, 0.818721419564815, 0.8191017952578351, 0.8194650236720202, 0.8197501847126716, 0.8200206363468401, 0.820201335897372, 0.8203756272857171, 0.8206184479435427, 0.8207370411855948, 0.8208601232427459, 0.8210149307262878, 0.8211937849912374, 0.8213826916642251, 0.821522516784209, 0.8215717376279364, 0.8216508775706153, 0.8217242844725607, 0.8217997233771873, 0.8218444029520747, 0.8219077579111124, 0.8219392728349789, 0.8219025320997922, 0.8219577489952486, 0.8220144556964066, 0.8219841836167531, 0.8220568480462368, 0.8221175957351532, 0.8221792441016065, 0.8221417216790334, 0.8221544544749083, 0.8221776118328807, 0.8222123322502756, 0.8221808376568925, 0.8221758521573992, 0.8221736182590274, 0.8221947150433129, 0.8221858364122743, 0.8222035304277007, 0.8222380562320605, 0.8222389385702793, 0.8222396966258575, 0.8222288252294677, 0.8222352088679273, 0.8223060222603888, 0.8223233234350513, 0.8223132097088865, 0.8223878848101186], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.028162797675075294, 0.0592936835231551, 0.08999511571677332, 0.11957658823771647, 0.1480065541315653, 0.17482643359227923, 0.2006937525824489, 0.22484206840101123, 0.24793671015842517, 0.26963947410116695, 0.2899573296842731, 0.30925761281147834, 0.3274742708168516, 0.3444104904313261, 0.36044963364158206, 0.3755440422183124, 0.3894627178157583, 0.4029427037996192, 0.41581329011205187, 0.4278373004269009, 0.4392274926357168, 0.4501388748198108, 0.4605268192749532, 0.47020144109369283, 0.47931555079644705, 0.4884134808448144, 0.49693223724075464, 0.5048289925735315, 0.5124274421490097, 0.5199608180395304, 0.527183397991978, 0.5338159382756116, 0.5403763095741799, 0.5464006550380722, 0.5519955234103945, 0.5580004079575327, 0.5633162663052282, 0.5686194111826121, 0.5733972420428901, 0.5780076131416885, 0.5827968602989052, 0.5868812791259423, 0.5911096610025047, 0.595131842727932, 0.5995870319491147, 0.603560081154429, 0.6074542377603718, 0.6114635850216087, 0.6146650474908333, 0.6182564305429548, 0.6213024812951351, 0.6241304056995072, 0.6271027837571921, 0.6301939925802681, 0.6328846013229943, 0.63514128073324, 0.6377766137856691, 0.6398808883540148, 0.642327140397755, 0.6443680168135517, 0.6465455729541092, 0.6490821924753398, 0.6506327281694474, 0.6526517983965539, 0.6548016399706785, 0.6570019635489119, 0.6588367999029816, 0.6608055354341443, 0.6624085579920099, 0.6641107144763179, 0.6658634113833547, 0.667218023511209, 0.6688410359661874, 0.6705580948319181, 0.6723872980558045, 0.6741180006673928, 0.6759177146255029, 0.6763370500906635, 0.677327892097787, 0.6789216277374059, 0.6808351525577919, 0.682100517196591, 0.6835709942325795, 0.6846858745250595, 0.6859191413647222, 0.6866709595795301, 0.6882490043407036, 0.689101544136302, 0.6906399319384098, 0.6916450334828972, 0.6925628614128454, 0.6939148384022085, 0.6947979098142466, 0.6956893008264213, 0.6966268595897882, 0.697752453704228, 0.6984836971798143, 0.6997908479814112, 0.7004647364042792, 0.7017121786618181, 0.7026883923186032, 0.7035038904361405, 0.7043110809294241, 0.70504770038731, 0.7055306409566362, 0.7065146038752798, 0.707461205658309, 0.7080404745408968, 0.7108020273785993, 0.7131134379690527, 0.7151815004692106, 0.7172289507140817, 0.7189272305767849, 0.72046171243251, 0.7220309991147108, 0.723390410969053, 0.7244928408341206, 0.7255216488064314, 0.7262888845752612, 0.7268918885311386, 0.7276441706399976, 0.7282368048278804, 0.7289034234320654, 0.7293039496411932, 0.7296664822467274, 0.7301545415239372, 0.7306293864585164, 0.731031303328478, 0.7313421413691241, 0.7317114628590943, 0.7322401942087271, 0.7326550172671467, 0.7327790698514259, 0.7331400053455754, 0.7334017531167408, 0.7335376108434704, 0.7337107699398462, 0.7338289625241748, 0.7339729864524802, 0.7340527503542954, 0.7340370296298598, 0.7340859751514371, 0.734104582549697, 0.7340114659268809, 0.7340894408985754, 0.7340843171682812, 0.734251633757176, 0.7345263480170006, 0.7345009181287041, 0.7345278888628969, 0.7344636247789416, 0.7345197684192702, 0.7345448541244064, 0.7344687455003694, 0.734623062827215, 0.7345279558103068, 0.7344057384013394, 0.7343933989832687, 0.7344433286632551, 0.7345137089464026, 0.7346024947723948, 0.7347057865696281, 0.7345728455726803, 0.7344186365989966, 0.7343663272500909, 0.734406757072145, 0.7344309368807438, 0.7344862312762538, 0.7344403989995321, 0.7346931482091421, 0.7346764818727911, 0.734586180965256, 0.7344184314210648, 0.7345411590620908, 0.7346505844303546, 0.7346483224858131, 0.7347592385429548, 0.7348203828833129, 0.7349649800430238, 0.734859065858375, 0.73498655818067], "moving_var_accuracy_train": [0.007219160298525057, 0.015266763076212917, 0.022425233829291867, 0.028262289036249805, 0.032963838920571864, 0.03648077425832022, 0.03916587872095454, 0.040945442142314305, 0.04212547158389118, 0.04258352950724906, 0.042404877332210586, 0.041866656639582236, 0.040930630094698287, 0.03966068166562164, 0.038258698775296154, 0.03669819756676579, 0.03499152017769261, 0.033354127606834504, 0.03166686686643802, 0.029940093832518336, 0.028296401259208077, 0.026690514209682322, 0.025138561723361676, 0.023596627065583322, 0.022123458414892543, 0.020791414357785604, 0.019458834236365147, 0.01815951751614785, 0.01695151189246571, 0.0158610195535404, 0.01482443981519347, 0.013804611212802216, 0.012903630141296599, 0.012048007877431325, 0.01125614681593478, 0.010496561054520997, 0.009756536182952759, 0.009073505963434467, 0.008457988767267877, 0.007843044178146361, 0.007300549943365856, 0.006774656624972042, 0.006313706997295257, 0.005879390159669597, 0.005505861485004433, 0.005117036461168773, 0.004775082699933443, 0.0044547216551152425, 0.004150424514457986, 0.00388422516162416, 0.0036095662635436633, 0.0033489880489635344, 0.003119452161404198, 0.0029311910125749516, 0.0027444987086423945, 0.0025511434075484183, 0.002389073926162201, 0.0022053138330848433, 0.002056752224056477, 0.0019061060140268303, 0.0017780860648410012, 0.0016743360218928825, 0.0015361541758882739, 0.001439375581347039, 0.0013536303575361727, 0.0012860559898331526, 0.0011942225328231508, 0.0011251622528029297, 0.0010527212941895017, 0.0009847809867179667, 0.0009256471072863578, 0.0008650324641582291, 0.0008228376019803208, 0.0007931588660341304, 0.000757364297866395, 0.0007217678393849432, 0.0007074344107939405, 0.000648820646529613, 0.0006103238693215719, 0.0005878098059179368, 0.000589808645619683, 0.000567653347644428, 0.0005520698912298574, 0.0005272996118372242, 0.0005215629535612105, 0.0004920731487903044, 0.0004899851305601307, 0.0004609004847580388, 0.0004602828168434583, 0.0004420696582421987, 0.00042032369606910807, 0.00041942275194666154, 0.0004062164831288154, 0.00038544042674052554, 0.0003744714960035529, 0.00037033868115020384, 0.00036079910049675217, 0.0003505424233370468, 0.00033950704352690497, 0.00035270361325955403, 0.0003429318946008795, 0.0003403698695683787, 0.00033397909839804967, 0.0003324530969896819, 0.0003220659853224074, 0.00030862104491834825, 0.00030104144605970723, 0.00028600784374700734, 0.00039767766434598364, 0.00046827429384230045, 0.0005176914578804687, 0.0005403473290286195, 0.000547359165815568, 0.0005478192288893371, 0.0005363634606650815, 0.0005192364155974172, 0.0004975720416134942, 0.00047281178224141873, 0.00044571734631832904, 0.00041934729287770064, 0.00039275444240140093, 0.0003633482364859516, 0.00033745755982703, 0.0003123737549922628, 0.00028685345312034, 0.0002633550880446389, 0.00024142268004774474, 0.00022286654551647073, 0.00020461336611288648, 0.00018827677438288586, 0.00017329093668631146, 0.00015883286394182504, 0.00014466234917543907, 0.0001321784028159991, 0.00012021644036618093, 0.00010949696734012792, 9.973468453395794e-05, 9.049306745251081e-05, 8.210205748507904e-05, 7.418572268463315e-05, 6.704054780863124e-05, 6.086714987456925e-05, 5.4907014100656306e-05, 4.955265542572362e-05, 4.481307809579656e-05, 4.061966991903302e-05, 3.687887450702338e-05, 3.33669466339277e-05, 3.0052056193650142e-05, 2.710321874903004e-05, 2.4441394033406142e-05, 2.204847388504679e-05, 1.9861592876251177e-05, 1.7911558246137998e-05, 1.6129341135361063e-05, 1.45285559564235e-05, 1.310314051067569e-05, 1.1821767309214169e-05, 1.0647838167551735e-05, 9.630575424606397e-06, 8.700730417523868e-06, 7.864862065548718e-06, 7.0910472487555574e-06, 6.383401640697103e-06, 5.7498878456817654e-06, 5.185748627570229e-06, 4.676100949524474e-06, 4.208714551418801e-06, 3.7878880089943446e-06, 3.413104876859624e-06, 3.0725038599757323e-06, 2.768071177615319e-06, 2.5019923403540493e-06, 2.251800113005236e-06, 2.026625273539049e-06, 1.8250264315203105e-06, 1.6428905459281314e-06, 1.5237323203026091e-06, 1.374053064074662e-06, 1.2375683447796243e-06, 1.16399884699781e-06], "duration": 195625.586262, "accuracy_train": [0.28321880623615725, 0.3404742438399778, 0.3701842383028793, 0.3902234161590993, 0.40977323130075677, 0.42462768779992616, 0.44227015994139907, 0.45510245795265786, 0.4707714375115356, 0.4806987414636397, 0.4885817169043005, 0.4997849687961425, 0.5072941179863418, 0.5133609539382613, 0.5227513107350499, 0.5294938817944813, 0.5343974223652639, 0.5453023702819306, 0.5511828338409007, 0.5558778319952011, 0.5645281065430048, 0.5708954093992248, 0.5773589626015135, 0.5810087252560908, 0.5867285913275194, 0.5963060414936324, 0.5983739819813584, 0.6011630790882244, 0.6070685965762275, 0.6150649455518641, 0.6194351438492064, 0.6208044582295128, 0.629269081360742, 0.6330794415028608, 0.6382645233480989, 0.641074709013935, 0.6423335337647655, 0.6465653045980989, 0.652190362276209, 0.6515872655384828, 0.6578397529069767, 0.6588173968715394, 0.6650001297757475, 0.6676486364894795, 0.6743450650609081, 0.6728119087993725, 0.6780856707387413, 0.6807853667751015, 0.682783552798542, 0.6878055137389258, 0.686758475798265, 0.688156809477667, 0.692315581453027, 0.6985927622623661, 0.6996165487149317, 0.6986850472383721, 0.7038225446428571, 0.6996383582502769, 0.7056383236434108, 0.7049150040951458, 0.709031959440753, 0.7139834449404762, 0.7061945569167589, 0.71509915588086, 0.7179100625230712, 0.722469517118863, 0.7179827008928571, 0.723446079618863, 0.723257904784976, 0.7246329869762828, 0.7272113787375416, 0.7272353512020118, 0.7324662150470653, 0.7366732924395534, 0.7369047258559431, 0.7382322236064969, 0.7445769959509967, 0.7333697786890919, 0.7400436767488003, 0.7453214040582319, 0.7526896023671097, 0.7495292024270949, 0.7527150167843301, 0.7518529092838685, 0.758152620604928, 0.7534569014742525, 0.7620552657230528, 0.7563372020925618, 0.7654274524732374, 0.7627775038067552, 0.7627531708540974, 0.7699131872346806, 0.7685417099252492, 0.7673093810562015, 0.771448866913529, 0.774934787686877, 0.7750975481035437, 0.7763059045081212, 0.7773972824958472, 0.785582527281746, 0.78181542572213, 0.7854433788413622, 0.786070808531746, 0.7891153115194721, 0.7881154975313769, 0.7882106664128831, 0.7913002304240495, 0.7897649112333887, 0.8175973029715762, 0.8170846887689184, 0.8182697936854004, 0.817595140042451, 0.8177582609473052, 0.8190832352805463, 0.8187359049118678, 0.8191300987449243, 0.8193393621377814, 0.8195024830426356, 0.8194799525309154, 0.8202221977090256, 0.8204794060308231, 0.819200574185585, 0.8205484395187338, 0.8206636154946475, 0.8198043918996861, 0.8202229186854004, 0.8203849581256922, 0.8219682222452934, 0.8215722259712993, 0.822316994566722, 0.8227576913759689, 0.8225255369832041, 0.8218047408522517, 0.822571679471207, 0.822083398221207, 0.8225251764950166, 0.8227340793996861, 0.8223166340785345, 0.8224547010543558, 0.8218276318521595, 0.8219442497808231, 0.8228038338639718, 0.8218043803640642, 0.8219678617571059, 0.8224081980781653, 0.8228034733757843, 0.8230828517211147, 0.8227809428640642, 0.8220147252214839, 0.822363137054725, 0.8223849465900701, 0.8224786735188261, 0.8222465191260613, 0.822477952542451, 0.8222229071497784, 0.8215718654831118, 0.8224547010543558, 0.822524816006829, 0.8217117348998707, 0.8227108279115909, 0.8226643249354004, 0.8227340793996861, 0.8218040198758766, 0.8222690496377814, 0.8223860280546327, 0.822524816006829, 0.8218973863164452, 0.82213098266196, 0.8221535131736802, 0.8223845861018827, 0.8221059287329272, 0.8223627765665374, 0.8225487884712993, 0.8222468796142488, 0.8222465191260613, 0.82213098266196, 0.8222926616140642, 0.8229433427925433, 0.8224790340070136, 0.8222221861734034, 0.823059960721207], "end": "2016-02-05 17:59:50.674000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0], "moving_var_accuracy_valid": [0.00713828855598504, 0.01514664818356212, 0.022115184813859944, 0.027779237981039782, 0.03227568082946448, 0.03552186615510307, 0.03799174326524549, 0.0394408393505782, 0.040297017716649966, 0.040506405609774344, 0.040171102348260546, 0.039506500472547036, 0.038542470085254896, 0.03726974289019679, 0.03585805563544901, 0.03432282060443555, 0.03263410431747441, 0.03100608408485275, 0.029396343604599483, 0.0277579006606039, 0.026149738901527452, 0.02460628936188098, 0.023116844935720998, 0.021647545208169593, 0.020230393648417177, 0.01895230526406033, 0.01771019763245465, 0.016500406572280857, 0.015369993838612532, 0.014343760225522188, 0.013378875153495447, 0.012436902953672096, 0.011580558902480602, 0.010749137656647414, 0.009955946859915183, 0.009284879919743805, 0.008610717077524303, 0.008002755480086571, 0.007407928941642733, 0.006858435742496174, 0.00637902416324273, 0.00589126404131036, 0.005463050556825696, 0.005062347013634375, 0.004734750711239821, 0.004403341720006487, 0.004099487649050298, 0.0038342126732959594, 0.0035430356634430504, 0.0033048143871423344, 0.0030578387750918317, 0.0028240293055142346, 0.0026211416568230664, 0.0024450276390315245, 0.0022656792537862846, 0.0020849447460533003, 0.0019389550941229937, 0.0017849113278415743, 0.00166027753661094, 0.001531736371850846, 0.0014212384913732778, 0.0013370245895953459, 0.0012249595790841269, 0.0011391534224136043, 0.0010668344493167598, 0.0010037238190254505, 0.0009336510571388458, 0.0008751692277499227, 0.0008107794368641669, 0.0007557775234513975, 0.0007078472891376879, 0.0006535773263762885, 0.0006119271185997978, 0.0005772690270752762, 0.00054965598427624, 0.0005216483696164404, 0.0004986342656339438, 0.0004503534191616222, 0.0004141539881931831, 0.0003955985289747875, 0.00038899287122140514, 0.00036450391312137383, 0.00034751424622956174, 0.00032394944420564826, 0.0003052430236653887, 0.00027980579695190225, 0.00027423724467111625, 0.00025335493713171393, 0.00024931917668560784, 0.00023347932104962453, 0.00021771306192559965, 0.00021239233175094415, 0.0001981714346446818, 0.0001855054926094867, 0.00017486609126143263, 0.00016878214112944417, 0.00015671638020178655, 0.00015642253114464636, 0.00014486740848846025, 0.00014438567731265897, 0.00013852404751463578, 0.0001306569773805336, 0.00012345528807450608, 0.00011599323329868067, 0.0001064929943103229, 0.00010455734210668155, 0.00010216610231672093, 9.496946403005926e-05, 0.00015410808430585708, 0.00018678084613420924, 0.0002065947040618258, 0.0002236637062026481, 0.0002272547260109546, 0.00022572096449980598, 0.00022531281426822083, 0.00021941353814893298, 0.00020841034880057614, 0.00019709532651553138, 0.00018268365038872416, 0.00016768780928708702, 0.00015601238370016276, 0.0001435720828559737, 0.0001332142978413844, 0.00012133665925503051, 0.00011038586234020691, 0.00010149109282882114, 9.337128295299846e-05, 8.548798919093629e-05, 7.780877285945747e-05, 7.125548084009546e-05, 6.664594431684599e-05, 6.153005341332959e-05, 5.551554946499401e-05, 5.11364643969276e-05, 4.663942501862505e-05, 4.214159841397161e-05, 3.8197295226493765e-05, 3.4503291086756924e-05, 3.123964800540192e-05, 2.8172943725156843e-05, 2.535787362323217e-05, 2.284364723765119e-05, 2.0562398631316098e-05, 1.8584195117186538e-05, 1.6780496471364618e-05, 1.5102683097737313e-05, 1.3844368356238019e-05, 1.3139142841572862e-05, 1.1831048670384498e-05, 1.065449058787216e-05, 9.626210381464535e-06, 8.691958318462343e-06, 7.828426120035747e-06, 7.097716211907451e-06, 6.602269126999621e-06, 6.023450316286335e-06, 5.555539140150104e-06, 5.001355577280002e-06, 4.523656776043884e-06, 4.115871556742635e-06, 3.7752307071425514e-06, 3.4937303948096682e-06, 3.303417133353847e-06, 3.1870990880996878e-06, 2.893015591136148e-06, 2.6184251666245197e-06, 2.361844618256919e-06, 2.1531773880045245e-06, 1.9567650275095716e-06, 2.3360279913851507e-06, 2.1049250931529037e-06, 1.9678208689526325e-06, 2.0242979682447757e-06, 1.9574268362666795e-06, 1.869449353617041e-06, 1.6825504657933156e-06, 1.625016764800474e-06, 1.4961627615409225e-06, 1.534721532754905e-06, 1.4822097100677146e-06, 1.4802773692585632e-06], "accuracy_test": 0.7453583386479592, "start": "2016-02-03 11:39:25.087000", "learning_rate_per_epoch": [0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 0.00010395761637482792, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395761819381732e-05, 1.0395762046755408e-06, 1.0395761762538314e-07, 1.039576158490263e-08, 1.039576202899184e-09, 1.039576202899184e-10, 1.0395762375936535e-11, 1.0395761942255666e-12, 1.0395762077780937e-13, 1.0395762416594116e-14, 1.0395762416594116e-15, 1.0395762416594116e-16, 1.0395762747466361e-17, 1.0395762954261514e-18, 1.0395762954261514e-19, 1.0395762792702801e-20, 1.0395762994651193e-21, 1.0395762994651193e-22, 1.0395762836879012e-23, 1.0395762836879012e-24, 1.0395763083398045e-25, 1.0395763083398045e-26, 1.0395763468584033e-27, 1.0395762987101547e-28, 1.039576283663827e-29, 1.0395762648559174e-30, 1.0395762883658044e-31, 1.039576273672125e-32, 1.0395762553050258e-33, 1.0395762093872778e-34, 1.0395762380858703e-35, 1.0395762022126296e-36, 1.0395762470541805e-37, 1.0395762470541805e-38, 1.0395756865347947e-39, 1.039581291728652e-40, 1.0396233306825818e-41, 1.0397634605290143e-42, 1.0369608636003646e-43, 9.80908925027372e-45, 1.401298464324817e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.28321880623615725, "accuracy_train_last": 0.823059960721207, "batch_size_eval": 1024, "accuracy_train_std": [0.014687050632907142, 0.01453826865686734, 0.013491452380280104, 0.013983595917479644, 0.014973543352419154, 0.014287229313885015, 0.015650090128520016, 0.015400092214024112, 0.014822629008691484, 0.015466010059710164, 0.014829929728844256, 0.016899213972846237, 0.01635252991777141, 0.01643494235165373, 0.016879315803752654, 0.016459742649828794, 0.018501109846055557, 0.017983743878511014, 0.01885587038667311, 0.01911102437585192, 0.017162195473667815, 0.019332953846867534, 0.018886787270062863, 0.018598605918567913, 0.017443369885415637, 0.018161708466674892, 0.02000022324510311, 0.01836232204793672, 0.018669898535089238, 0.019432680490562394, 0.019126327313427637, 0.020603144876375903, 0.018710185433115763, 0.02060052293299352, 0.020101259380260257, 0.020336920246348984, 0.01927186381315914, 0.018529728673007406, 0.018723304755478544, 0.01915396789590454, 0.019432504741419117, 0.018728289626208407, 0.01914929582602139, 0.02003435989379977, 0.01888618718941227, 0.019584755442709014, 0.019754697508502533, 0.019332053488897858, 0.020381755722383887, 0.019724298243422533, 0.020409393405139493, 0.020124984618573823, 0.019418492033409944, 0.019693487742604928, 0.01953025641076458, 0.019681075777173915, 0.019225761164703422, 0.019267027310684994, 0.01891256037518272, 0.01901423448099244, 0.01860311667233568, 0.018874036517076954, 0.018674518086024005, 0.019028922716843828, 0.01828908915900509, 0.01676521287454033, 0.016424917679591266, 0.017518955549745998, 0.01751546592365351, 0.01627398601664071, 0.016912389261375733, 0.01691260975191923, 0.016562084232680106, 0.017138095167555677, 0.01820773443112558, 0.01728126907895572, 0.017436706226181396, 0.017196241820514584, 0.017245535650499955, 0.016928571046565595, 0.017792649287625056, 0.017685287422949697, 0.015728176727122796, 0.01677036357127387, 0.016747458020865295, 0.01629333355176699, 0.017620219130198605, 0.01750069964604103, 0.016697641084082077, 0.016479520876936524, 0.017539900476369417, 0.017478995120986454, 0.01653931049930488, 0.016446179935502216, 0.016894898308348002, 0.016603840219432942, 0.016616395172501065, 0.01695365604178584, 0.016585722673197596, 0.015754364098380977, 0.016785855540956865, 0.015233004874840755, 0.015535327486109139, 0.015757199817069155, 0.016325958697063058, 0.01404952181958875, 0.015180476748891676, 0.015962746957154404, 0.012199600407282548, 0.012971290260526739, 0.012783679559577944, 0.012176861593299467, 0.01335734872560499, 0.012753084112208019, 0.012177311726860438, 0.011844360986532841, 0.01224484004805044, 0.011789767556255446, 0.012110881800072413, 0.012676688060200976, 0.010942784674818821, 0.012179888780246695, 0.012338364664249033, 0.01252265643974334, 0.01215021271922191, 0.012502985555923002, 0.012115371260764392, 0.011699795218008624, 0.011801688529929147, 0.011443504135690374, 0.011608862956426977, 0.011447353916506895, 0.011493498070314039, 0.011420081694725337, 0.012172391571537572, 0.011829450468063074, 0.011518103370852475, 0.011413635608573171, 0.012243894190390786, 0.011984411734540531, 0.011962807576112728, 0.011635694623019303, 0.01166378759533744, 0.011684183284563037, 0.01192320149531536, 0.012230751989573376, 0.011637006324303874, 0.012276195155363538, 0.011243677706271925, 0.011684655484062648, 0.011747915904656495, 0.011664148159747971, 0.011726565683655535, 0.012251855798066666, 0.01157319301971642, 0.01166006164935667, 0.011815421266961267, 0.012219927294979002, 0.01162690983172046, 0.011932461524647922, 0.01196261167017863, 0.012053693902217465, 0.01159892284075285, 0.012072201116541507, 0.011577858960438444, 0.011994876221631109, 0.011539897698015786, 0.011161562095007732, 0.012035657641402058, 0.012458313999277866, 0.012284118716857483, 0.011525989346486038, 0.011259802974126964, 0.011488094628467371, 0.012005942273942739, 0.01166100969620137, 0.011954453997917586, 0.011606903680376706, 0.011320666345785461, 0.012361546453311523, 0.011789244466211834], "accuracy_test_std": 0.009169984203477515, "error_valid": [0.718372023249247, 0.6605283438441265, 0.6336919945406627, 0.6141901590737951, 0.5961237528237951, 0.5837946512612951, 0.5665003765060241, 0.5578230892319277, 0.5442115140248494, 0.5350356504141567, 0.5271819700677711, 0.5170398390436747, 0.5085758071347892, 0.5031635330384037, 0.49519807746611444, 0.48860628059111444, 0.4852692018072289, 0.47573742234563254, 0.4683514330760542, 0.46394660673945776, 0.45826077748493976, 0.4516586855233433, 0.4459816806287651, 0.44272696253765065, 0.4386574618787651, 0.4297051487198795, 0.4263989551957832, 0.42410020943147586, 0.41918651167168675, 0.4122387989457832, 0.40781338243599397, 0.40649119917168675, 0.40058034873870485, 0.39938023578689763, 0.39765066123870485, 0.38795563111822284, 0.38884100856551207, 0.38365228492093373, 0.3836022802146084, 0.3804990469691265, 0.3740999152861446, 0.37635895143072284, 0.37083490210843373, 0.36866852174322284, 0.36031626506024095, 0.36068247599774095, 0.3574983527861446, 0.35245228962725905, 0.3565217902861446, 0.34942112198795183, 0.35128306193524095, 0.3504182746611446, 0.3461458137236446, 0.34198512801204817, 0.3428999199924698, 0.34454860457454817, 0.3385053887424698, 0.3411806405308735, 0.33565659120858427, 0.33726409544427716, 0.3338564217808735, 0.32808823183358427, 0.33541245058358427, 0.32917656955948793, 0.3258497858621988, 0.32319512424698793, 0.3246496729103916, 0.3214758447853916, 0.3231642389871988, 0.3205698771649097, 0.31836231645331325, 0.32059046733810237, 0.31655185193900603, 0.31398837537650603, 0.3111498729292168, 0.31030567582831325, 0.30788485975150603, 0.3198889307228916, 0.31375452983810237, 0.30673475150602414, 0.3019431240587349, 0.3065112010542168, 0.30319471244352414, 0.3052802028426205, 0.30298145707831325, 0.3065626764871988, 0.2975485928087349, 0.30322559770331325, 0.2955145778426205, 0.2993090526167168, 0.2991766872176205, 0.29391736869352414, 0.2972544474774097, 0.29628818006400603, 0.2949351115399097, 0.29211719926581325, 0.2949351115399097, 0.2884447948042168, 0.2934702677899097, 0.28706084102033136, 0.28852568477033136, 0.28915662650602414, 0.28842420463102414, 0.2883227244917168, 0.2901228939194277, 0.2846297298569277, 0.2840193782944277, 0.28674610551581325, 0.26434399708207834, 0.26608386671686746, 0.26620593702936746, 0.26434399708207834, 0.26578825065888556, 0.2657279508659638, 0.2638454207454819, 0.26437488234186746, 0.2655852903802711, 0.2652190794427711, 0.2668059935052711, 0.2676810758659638, 0.2655852903802711, 0.2664294874811747, 0.2650970091302711, 0.2670913144766567, 0.2670707243034638, 0.2654529249811747, 0.2650970091302711, 0.26535144484186746, 0.26586031626506024, 0.2649646437311747, 0.26300122364457834, 0.26361157520707834, 0.26610445689006024, 0.26361157520707834, 0.2642425169427711, 0.2652396696159638, 0.2647307981927711, 0.26510730421686746, 0.2647307981927711, 0.26522937452936746, 0.26610445689006024, 0.26547351515436746, 0.2657279508659638, 0.2668265836784638, 0.2652087843561747, 0.26596179640436746, 0.2642425169427711, 0.26300122364457834, 0.2657279508659638, 0.26522937452936746, 0.2661147519766567, 0.2649749388177711, 0.26522937452936746, 0.2662162321159638, 0.2639880812311747, 0.26632800734186746, 0.26669421827936746, 0.26571765577936746, 0.26510730421686746, 0.2648528685052711, 0.2645984327936747, 0.2643645872552711, 0.26662362339984935, 0.2669692441641567, 0.26610445689006024, 0.26522937452936746, 0.26535144484186746, 0.2650161191641567, 0.2659720914909638, 0.26303210890436746, 0.26547351515436746, 0.26622652720256024, 0.2670913144766567, 0.2643542921686747, 0.2643645872552711, 0.26537203501506024, 0.2642425169427711, 0.2646293180534638, 0.26373364551957834, 0.2660941618034638, 0.2638660109186747], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.028536981463510292, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.00010395761833324801, "optimization": "nesterov_momentum", "nb_data_augmentation": 1, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.764175636121427e-05, "rotation_range": [0, 0], "momentum": 0.6505597158169639}, "accuracy_valid_max": 0.7369987763554217, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7361339890813253, "accuracy_valid_std": [0.015055196488883687, 0.014195045241818725, 0.013150866047023419, 0.016486982790653918, 0.012466435423356998, 0.014796498925838944, 0.019062838863971095, 0.019439061515961076, 0.015809742069435778, 0.017923654228324825, 0.01784345004333115, 0.01978732303391149, 0.020390295615721508, 0.020246193400149826, 0.020279202115258153, 0.01883597559657962, 0.016321098949533045, 0.01705290295149194, 0.019406214047155042, 0.01873198187199171, 0.022012684981788624, 0.020220088325423376, 0.01890939926807255, 0.021401696348155815, 0.018884888012283472, 0.02154857383856283, 0.018639318210254947, 0.021107600437461602, 0.018545301199664865, 0.018273570319929017, 0.016232562418028097, 0.017667592938138216, 0.016099612501159884, 0.017540727814452203, 0.015119507736645046, 0.01557953784337481, 0.01885672457242413, 0.013397305550981125, 0.019125303599855974, 0.015864492161700532, 0.017829315902406665, 0.016700440466429274, 0.017404727146801788, 0.018269271988421733, 0.021696319950357815, 0.01825304716211817, 0.016456996766961086, 0.017556733226753444, 0.017432265405845762, 0.017421065607160606, 0.018932857730351124, 0.018245307111541136, 0.017895194644264863, 0.015742616362958307, 0.014615945953897862, 0.015650640746910815, 0.014106803764071923, 0.013383126337771086, 0.014504510520663762, 0.014752837613045964, 0.01427228846820709, 0.012229635324985727, 0.01117274393267395, 0.012146015393849878, 0.010054584360607957, 0.014492732410865647, 0.01230762828898711, 0.011966043845893417, 0.011815152598802264, 0.01119080764539668, 0.010820654463012398, 0.014573068728397594, 0.011766898507313413, 0.011207213508612418, 0.01023569077752081, 0.011940902092870628, 0.012907318013798947, 0.011240976948506894, 0.01332482108880887, 0.010884254889719955, 0.011310076944698209, 0.0109253780769359, 0.011966570220775068, 0.012745001792141896, 0.01220923962097078, 0.013016705553890897, 0.01282942268750303, 0.011570000494730219, 0.013989545695799908, 0.012823930135025353, 0.01353515671012146, 0.011874821438586302, 0.01578475580636255, 0.01791561672909031, 0.015036895119014423, 0.015059196410091081, 0.01546677848392566, 0.015587066129154645, 0.016227763003125232, 0.014432983894122784, 0.013481481098981419, 0.014191826693619266, 0.013057251073553136, 0.01651400781452304, 0.015009715384148106, 0.015306988140140728, 0.016172236509897884, 0.016119304183688576, 0.011603820352389644, 0.013275992023438167, 0.012593699594407808, 0.011857871278103468, 0.012314911226269962, 0.01380306765395823, 0.011625650203404047, 0.011724675338452764, 0.012564289570864513, 0.012722833918193307, 0.01154736339785363, 0.012755563031199247, 0.012592721237200458, 0.012586025969865036, 0.012918300952558645, 0.013941562721322012, 0.012048742036909539, 0.011565174906788682, 0.012741763869180276, 0.0126371828311769, 0.013943564713941307, 0.011711597128511827, 0.012782020982808577, 0.01235879175535532, 0.01304485397686716, 0.013107754192857127, 0.011044321724258855, 0.01321556543682434, 0.012641406807078487, 0.013336545429761046, 0.013769803500767378, 0.012524970625133297, 0.014216761406079668, 0.013995381002622384, 0.013785783963980607, 0.013341851755883712, 0.012004168544927307, 0.012771721102189437, 0.013764065541644237, 0.012412966439171494, 0.013175619816294679, 0.013956483913851668, 0.013682744105350723, 0.013495061788328523, 0.012695133854523949, 0.014605215585483671, 0.013281675625096507, 0.012956159610705243, 0.013343954016743948, 0.014097732971041423, 0.012069670079226246, 0.01355474914963589, 0.012002246569494397, 0.011895055993271397, 0.013988325396575074, 0.014739091308803161, 0.01408196036110638, 0.013001316729263681, 0.012907847562724683, 0.013212480159911676, 0.014123551573287454, 0.013250686541625697, 0.012756586804997893, 0.013520046345842416, 0.013958653526424343, 0.012389779855740647, 0.012858235298563253, 0.01313315918260042, 0.0123295491935323, 0.011549732361551704, 0.013238961811459149, 0.013181239376293969, 0.013660087651542106], "accuracy_valid": [0.281627976750753, 0.3394716561558735, 0.36630800545933734, 0.38580984092620485, 0.40387624717620485, 0.41620534873870485, 0.4334996234939759, 0.4421769107680723, 0.4557884859751506, 0.4649643495858434, 0.4728180299322289, 0.4829601609563253, 0.49142419286521083, 0.4968364669615964, 0.5048019225338856, 0.5113937194088856, 0.5147307981927711, 0.5242625776543675, 0.5316485669239458, 0.5360533932605422, 0.5417392225150602, 0.5483413144766567, 0.5540183193712349, 0.5572730374623494, 0.5613425381212349, 0.5702948512801205, 0.5736010448042168, 0.5758997905685241, 0.5808134883283133, 0.5877612010542168, 0.592186617564006, 0.5935088008283133, 0.5994196512612951, 0.6006197642131024, 0.6023493387612951, 0.6120443688817772, 0.6111589914344879, 0.6163477150790663, 0.6163977197853916, 0.6195009530308735, 0.6259000847138554, 0.6236410485692772, 0.6291650978915663, 0.6313314782567772, 0.639683734939759, 0.639317524002259, 0.6425016472138554, 0.647547710372741, 0.6434782097138554, 0.6505788780120482, 0.648716938064759, 0.6495817253388554, 0.6538541862763554, 0.6580148719879518, 0.6571000800075302, 0.6554513954254518, 0.6614946112575302, 0.6588193594691265, 0.6643434087914157, 0.6627359045557228, 0.6661435782191265, 0.6719117681664157, 0.6645875494164157, 0.6708234304405121, 0.6741502141378012, 0.6768048757530121, 0.6753503270896084, 0.6785241552146084, 0.6768357610128012, 0.6794301228350903, 0.6816376835466867, 0.6794095326618976, 0.683448148060994, 0.686011624623494, 0.6888501270707832, 0.6896943241716867, 0.692115140248494, 0.6801110692771084, 0.6862454701618976, 0.6932652484939759, 0.6980568759412651, 0.6934887989457832, 0.6968052875564759, 0.6947197971573795, 0.6970185429216867, 0.6934373235128012, 0.7024514071912651, 0.6967744022966867, 0.7044854221573795, 0.7006909473832832, 0.7008233127823795, 0.7060826313064759, 0.7027455525225903, 0.703711819935994, 0.7050648884600903, 0.7078828007341867, 0.7050648884600903, 0.7115552051957832, 0.7065297322100903, 0.7129391589796686, 0.7114743152296686, 0.7108433734939759, 0.7115757953689759, 0.7116772755082832, 0.7098771060805723, 0.7153702701430723, 0.7159806217055723, 0.7132538944841867, 0.7356560029179217, 0.7339161332831325, 0.7337940629706325, 0.7356560029179217, 0.7342117493411144, 0.7342720491340362, 0.7361545792545181, 0.7356251176581325, 0.7344147096197289, 0.7347809205572289, 0.7331940064947289, 0.7323189241340362, 0.7344147096197289, 0.7335705125188253, 0.7349029908697289, 0.7329086855233433, 0.7329292756965362, 0.7345470750188253, 0.7349029908697289, 0.7346485551581325, 0.7341396837349398, 0.7350353562688253, 0.7369987763554217, 0.7363884247929217, 0.7338955431099398, 0.7363884247929217, 0.7357574830572289, 0.7347603303840362, 0.7352692018072289, 0.7348926957831325, 0.7352692018072289, 0.7347706254706325, 0.7338955431099398, 0.7345264848456325, 0.7342720491340362, 0.7331734163215362, 0.7347912156438253, 0.7340382035956325, 0.7357574830572289, 0.7369987763554217, 0.7342720491340362, 0.7347706254706325, 0.7338852480233433, 0.7350250611822289, 0.7347706254706325, 0.7337837678840362, 0.7360119187688253, 0.7336719926581325, 0.7333057817206325, 0.7342823442206325, 0.7348926957831325, 0.7351471314947289, 0.7354015672063253, 0.7356354127447289, 0.7333763766001506, 0.7330307558358433, 0.7338955431099398, 0.7347706254706325, 0.7346485551581325, 0.7349838808358433, 0.7340279085090362, 0.7369678910956325, 0.7345264848456325, 0.7337734727974398, 0.7329086855233433, 0.7356457078313253, 0.7356354127447289, 0.7346279649849398, 0.7357574830572289, 0.7353706819465362, 0.7362663544804217, 0.7339058381965362, 0.7361339890813253], "seed": 316388437, "model": "residualv3", "loss_std": [0.3110572099685669, 0.1528759002685547, 0.16452623903751373, 0.17752425372600555, 0.18631596863269806, 0.19490677118301392, 0.2001928985118866, 0.20684263110160828, 0.21204307675361633, 0.21643994748592377, 0.21878795325756073, 0.22425150871276855, 0.22515007853507996, 0.22703611850738525, 0.2304111123085022, 0.23021970689296722, 0.23252642154693604, 0.2325267642736435, 0.23359522223472595, 0.23479223251342773, 0.23534099757671356, 0.2368394136428833, 0.2399207204580307, 0.23924991488456726, 0.2393799126148224, 0.24029983580112457, 0.24185439944267273, 0.24218203127384186, 0.24045433104038239, 0.24312494695186615, 0.24110114574432373, 0.24405701458454132, 0.24425914883613586, 0.2444462925195694, 0.24414803087711334, 0.24417807161808014, 0.2444555014371872, 0.24444888532161713, 0.24557456374168396, 0.24463696777820587, 0.24538081884384155, 0.2447795569896698, 0.2457531839609146, 0.24586732685565948, 0.2457038164138794, 0.2477581799030304, 0.24531304836273193, 0.2457323521375656, 0.24592998623847961, 0.24764665961265564, 0.24728699028491974, 0.24563951790332794, 0.24732692539691925, 0.2470056712627411, 0.2431715428829193, 0.24451497197151184, 0.243532195687294, 0.24547474086284637, 0.24465131759643555, 0.2427426129579544, 0.24408914148807526, 0.24510447680950165, 0.24523821473121643, 0.24401654303073883, 0.2436019480228424, 0.24336116015911102, 0.24232007563114166, 0.24019606411457062, 0.2435920685529709, 0.2415771335363388, 0.24078652262687683, 0.24169234931468964, 0.24063745141029358, 0.2401394248008728, 0.24072708189487457, 0.24012263119220734, 0.23895259201526642, 0.23857828974723816, 0.2361847162246704, 0.2360333353281021, 0.23682309687137604, 0.23742300271987915, 0.23592212796211243, 0.2363988608121872, 0.23434005677700043, 0.23303073644638062, 0.23382820188999176, 0.2339792251586914, 0.23463499546051025, 0.23263336718082428, 0.23013633489608765, 0.231944739818573, 0.23328281939029694, 0.2317979782819748, 0.22955025732517242, 0.23183497786521912, 0.22996391355991364, 0.228183776140213, 0.22888101637363434, 0.2294100522994995, 0.22668738663196564, 0.22657723724842072, 0.22589461505413055, 0.22636452317237854, 0.22803160548210144, 0.22592492401599884, 0.22543534636497498, 0.22089235484600067, 0.21241191029548645, 0.21184881031513214, 0.2095191329717636, 0.21120984852313995, 0.21200381219387054, 0.2118314653635025, 0.21261659264564514, 0.21077843010425568, 0.2122427523136139, 0.2125309556722641, 0.21433021128177643, 0.21290209889411926, 0.21459929645061493, 0.2122296690940857, 0.21316015720367432, 0.21257168054580688, 0.20894399285316467, 0.21191047132015228, 0.2115703523159027, 0.2115795910358429, 0.2120732218027115, 0.20932462811470032, 0.2127278447151184, 0.21015805006027222, 0.21148765087127686, 0.21103957295417786, 0.20814669132232666, 0.21085822582244873, 0.20898689329624176, 0.209830179810524, 0.20718041062355042, 0.2119661569595337, 0.20955613255500793, 0.21195876598358154, 0.21025872230529785, 0.21082334220409393, 0.21129843592643738, 0.21020862460136414, 0.21034881472587585, 0.20985092222690582, 0.2087377905845642, 0.2110915184020996, 0.20996417105197906, 0.20937955379486084, 0.20888374745845795, 0.2117239087820053, 0.21113097667694092, 0.21097689867019653, 0.21002279222011566, 0.21116675436496735, 0.21102091670036316, 0.21118181943893433, 0.20830769836902618, 0.2097170054912567, 0.21102426946163177, 0.208394855260849, 0.2127121388912201, 0.20954029262065887, 0.20825603604316711, 0.2102646827697754, 0.21078036725521088, 0.21166680753231049, 0.21305716037750244, 0.21102885901927948, 0.21281807124614716, 0.20808647572994232, 0.209382563829422, 0.2121002972126007, 0.2108096331357956, 0.21048927307128906, 0.20776501297950745, 0.21147899329662323, 0.20923739671707153]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:40 2016", "state": "available"}], "summary": "98ca0a9f43c0f501b3d23e80832b96c3"}