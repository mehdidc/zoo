{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 16, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [2.1324188709259033, 1.8442901372909546, 1.7425146102905273, 1.6623319387435913, 1.5860223770141602, 1.515052318572998, 1.4519492387771606, 1.399639368057251, 1.3513727188110352, 1.3100543022155762, 1.2764025926589966, 1.239369511604309, 1.206230878829956, 1.17562997341156, 1.1496737003326416, 1.1222403049468994, 1.097491979598999, 1.0732154846191406, 1.0510096549987793, 1.0300179719924927, 1.0102512836456299, 0.989732563495636, 0.9732924699783325, 0.9573422074317932, 0.9390655755996704, 0.9217842221260071, 0.9068451523780823, 0.8931335806846619, 0.8785912394523621, 0.861041784286499, 0.8457579612731934, 0.8346084356307983, 0.8208043575286865, 0.8073053359985352, 0.7968573570251465, 0.7846700549125671, 0.7710196375846863, 0.7591996192932129, 0.7470465302467346, 0.7369337677955627, 0.7262831926345825, 0.7168110013008118, 0.7061059474945068, 0.6940562129020691, 0.683036208152771, 0.674160361289978, 0.6650006771087646, 0.6548796892166138, 0.6449860334396362, 0.6357848048210144, 0.6273090243339539, 0.6204832196235657, 0.609032154083252, 0.5995146632194519, 0.5916552543640137, 0.5856577754020691, 0.5765020847320557, 0.5710369348526001, 0.5603424310684204, 0.5537483096122742, 0.5484652519226074, 0.5382956266403198, 0.5356501340866089, 0.5250852108001709, 0.5183916091918945, 0.5123001337051392, 0.5087162852287292, 0.49764764308929443, 0.48925161361694336, 0.48931798338890076, 0.4813615083694458, 0.4754985570907593, 0.47115620970726013, 0.46415597200393677, 0.4589780867099762, 0.44832727313041687, 0.4504321813583374, 0.4442192018032074, 0.4374881982803345, 0.433855265378952, 0.43047767877578735, 0.42070016264915466, 0.4171085059642792, 0.4139147400856018, 0.40728405117988586, 0.40451300144195557, 0.40169256925582886, 0.39681684970855713, 0.3942590057849884, 0.36679527163505554, 0.35561463236808777, 0.34974977374076843, 0.3526325225830078, 0.3503073751926422, 0.3475474417209625, 0.3465651571750641, 0.3454182744026184, 0.3439449071884155, 0.3408297896385193, 0.339697927236557, 0.34149616956710815, 0.3416775166988373, 0.3431961238384247, 0.34333422780036926, 0.34156015515327454, 0.3416171371936798, 0.34179291129112244, 0.3403971493244171, 0.3430033326148987, 0.3423711657524109, 0.34363287687301636, 0.34306439757347107, 0.34138843417167664, 0.34346362948417664, 0.34292072057724, 0.3408879041671753, 0.3423785865306854, 0.34064579010009766, 0.34360623359680176, 0.3449774980545044, 0.3408951163291931, 0.34741976857185364, 0.3432327210903168, 0.3447910249233246, 0.34341153502464294, 0.3403380215167999, 0.34316951036453247, 0.34144628047943115, 0.3412330746650696, 0.34330493211746216, 0.3395761251449585, 0.3421437442302704, 0.34130609035491943, 0.3415072560310364, 0.34152287244796753, 0.34167492389678955, 0.3409068286418915, 0.3424443006515503, 0.3404955267906189, 0.3409639894962311, 0.3436625003814697, 0.34199750423431396, 0.3398771286010742, 0.34162068367004395, 0.34141606092453003, 0.3443223237991333, 0.34187111258506775, 0.3427012860774994, 0.3431898057460785, 0.342776894569397, 0.3436805009841919, 0.3407931625843048, 0.33948129415512085, 0.34010136127471924, 0.34310245513916016, 0.3435695469379425, 0.3426935076713562, 0.340463250875473, 0.3407767117023468, 0.3436974287033081, 0.34071001410484314, 0.3463324010372162, 0.3424515426158905, 0.34095287322998047, 0.34299349784851074, 0.3440089225769043, 0.34322142601013184, 0.3432643413543701, 0.3403496742248535, 0.3423589766025543, 0.3426959216594696, 0.3418917953968048, 0.34296056628227234, 0.34071454405784607, 0.34101974964141846, 0.3378761410713196, 0.3426513671875, 0.3429139256477356, 0.3403300940990448, 0.3428773581981659, 0.3429262936115265, 0.3423800468444824, 0.34410351514816284, 0.34216514229774475, 0.3407098054885864, 0.34542596340179443, 0.34311020374298096, 0.3396545946598053, 0.341520220041275, 0.34279754757881165, 0.3421699106693268, 0.34104123711586, 0.34222060441970825, 0.3415413796901703, 0.34283682703971863], "moving_avg_accuracy_train": [0.028558324825812102, 0.061432351147217595, 0.09566869567443012, 0.13014987774043119, 0.16533235890672582, 0.200439524513377, 0.23476239979482813, 0.26708971321950736, 0.2983184935183115, 0.3280191955288373, 0.3564494750692537, 0.3833453528495653, 0.4087118200101606, 0.4328480856950988, 0.4557726465019732, 0.47737397779354407, 0.49759181170715755, 0.5168178310543912, 0.5347326184085313, 0.5519599761747749, 0.5681968397953003, 0.5830400986394599, 0.5973360305206228, 0.6104188243458677, 0.6232694861503913, 0.6353278691268067, 0.6467478582615606, 0.6573370579351276, 0.6675301492473273, 0.6775151920699183, 0.6870479603363945, 0.6958970969404903, 0.7044775564163379, 0.7126902519040416, 0.7204259801108781, 0.7277950365386976, 0.7351593568570038, 0.7419198867720787, 0.7482156998955813, 0.7545885606519165, 0.7606450779659699, 0.7663191217855135, 0.7714562404993579, 0.777174431942916, 0.7822603503730706, 0.7871142254732681, 0.7920916857586286, 0.7968549600237588, 0.8013208712230717, 0.8058818428286162, 0.8102568310725172, 0.8145473465740696, 0.817881109892161, 0.8218092312534432, 0.8258905899606478, 0.8295173819185789, 0.8326932111235926, 0.8366185565164013, 0.8400770347056619, 0.8429778962902359, 0.8463581878258487, 0.8489845369615455, 0.8515620567276925, 0.8544957719493953, 0.85778439958096, 0.860553538295806, 0.8627971524606421, 0.865353190144626, 0.8673678470495525, 0.8699227246854057, 0.8721803700255583, 0.8738682009055236, 0.8760100280902574, 0.877733023412461, 0.8801068218810155, 0.8821177545646379, 0.8843088462870224, 0.8863737266430932, 0.888157750250471, 0.8894077679245287, 0.8905583244192667, 0.8921401991859503, 0.8941263922437764, 0.8954979085541903, 0.8969672214097808, 0.8981010135845097, 0.8998165018405383, 0.9009119038436384, 0.9022675003559615, 0.9052963948444056, 0.908364232807824, 0.9110369673689575, 0.9135469520239499, 0.9158641750801374, 0.9179844809652115, 0.9198368805927121, 0.9216018767515565, 0.9232321538754502, 0.9248319728179163, 0.9263136625447072, 0.9274890731797714, 0.9285259082655871, 0.9295893763226108, 0.9305138733929614, 0.9313692443420097, 0.9320948643199535, 0.9326850711846082, 0.9333115163663505, 0.9338823645739844, 0.9344007782584741, 0.9348046076054765, 0.9351309597832824, 0.9355687638718607, 0.9357860401932387, 0.9360467651467831, 0.9363743154108977, 0.9366808445391047, 0.9368962308366247, 0.9371273169341637, 0.9372306266767017, 0.9374676204759012, 0.9376136297749795, 0.9377218227048736, 0.9378053175465586, 0.9378803187088001, 0.9380314890631415, 0.9381466881404006, 0.9382967981884869, 0.9382390180270301, 0.9383125739174335, 0.9384391559902066, 0.9384881198843107, 0.9384460847854145, 0.9385383894321037, 0.9385634069915234, 0.9385301192235727, 0.9385930940871603, 0.9385568376096458, 0.938668257859617, 0.9386105341119997, 0.9387051031629629, 0.9388180449969066, 0.9387755334212654, 0.9387349478543787, 0.9387147689834849, 0.9387082337437281, 0.9387347959648241, 0.9387982294935724, 0.9387368091777976, 0.9386907954400197, 0.938700392154554, 0.9387347860786336, 0.9386936609972099, 0.938654323275119, 0.9385584654561896, 0.9385140100489058, 0.9385414655466453, 0.9385894269827061, 0.9386185332358474, 0.9386377894660648, 0.938655192170898, 0.9386684934076196, 0.938466658976649, 0.9385918555339952, 0.9385556508141597, 0.9385695695424983, 0.9385239676777649, 0.9385759680007045, 0.9385878550103885, 0.9386055287655327, 0.9386399642380012, 0.938626922531117, 0.9385988729044357, 0.9386387324070892, 0.9386722447618491, 0.9387210431204279, 0.9387626725431581, 0.9386164162188442, 0.9386499071412565, 0.938621848153552, 0.9386175214039038, 0.9385578598066104, 0.9385158261619129, 0.9385221376602472, 0.9385139392135284, 0.9385971693174155, 0.9385836847073332, 0.9385716567047155, 0.9385026667333026, 0.938508041123326, 0.9385128420255282, 0.9385356558815302, 0.9386026552793036, 0.9386607016861278, 0.9385386293891927], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.028078230892319273, 0.0605134306758283, 0.09413949842338101, 0.12777930658179593, 0.16135515039650788, 0.19490313498035108, 0.2274411006144696, 0.2582757097265919, 0.2874480210958002, 0.31596963817861473, 0.34292700888635563, 0.36823344674018993, 0.3915738527721197, 0.4136554664595161, 0.43436105592049223, 0.4539767669700093, 0.47246510307421324, 0.489631566929066, 0.5058433678792317, 0.5212243572396519, 0.5351669629313494, 0.5479950402639674, 0.5600784487469833, 0.5707226122966073, 0.5812718625033171, 0.5911913747657866, 0.6000121610467382, 0.6084014993471848, 0.6162347245536561, 0.6238990968366188, 0.6305783348374449, 0.6372285326978269, 0.6431729716437822, 0.6486715100874612, 0.6534950358482934, 0.6585514234061599, 0.6633808749096705, 0.6676673756152396, 0.6716880356911403, 0.6755853324608817, 0.6791793782810586, 0.6822431210817178, 0.6850625542672206, 0.6882215732192636, 0.6911369029549427, 0.6939539531997345, 0.6967152020345051, 0.69865909857616, 0.7006903966910591, 0.7026182802617875, 0.704795917126422, 0.7067415242560238, 0.708307406186596, 0.7099374559952708, 0.7113536136807588, 0.7130381941895654, 0.713851309305624, 0.7153531853874863, 0.7168259146650027, 0.7179835611032463, 0.7192054598404368, 0.7196337819851583, 0.7210752536548652, 0.7226014232253727, 0.7240666021095372, 0.7250139046244872, 0.7255724786292824, 0.7261575559263692, 0.7270951935942744, 0.7280671677871814, 0.7294038998036441, 0.7302030970785508, 0.731241816455786, 0.7313811478467285, 0.7321717557653689, 0.7331387681124013, 0.7335430059472455, 0.7342487639462709, 0.7346152443852282, 0.7348525680735879, 0.7350814549503406, 0.735385109389418, 0.7363430216432473, 0.7371116044563322, 0.7376456670905183, 0.7379655730377164, 0.7382371633243061, 0.7388711900735774, 0.7393441578979215, 0.7406700607263523, 0.7420353012180997, 0.7431419473481723, 0.7441989640214876, 0.7454951644284502, 0.7465498224961473, 0.7470920646911862, 0.747769365187429, 0.7484114386931591, 0.7489618022598372, 0.749745097749215, 0.7503493189136761, 0.7507079534756218, 0.7512667762097615, 0.7515957297070082, 0.7520159171843496, 0.7522709860927971, 0.7525982043603999, 0.7529191738810618, 0.7531673073212689, 0.7534048934660246, 0.7535576858400547, 0.7536209272805221, 0.7536279869432831, 0.7537075828272681, 0.7540173297685624, 0.754107849003679, 0.754225937409034, 0.754458405320992, 0.754506876018185, 0.754784492256728, 0.7548858034790973, 0.7551408225287779, 0.7552319442951019, 0.7550789317650646, 0.7551783016250793, 0.7552259658620443, 0.7552454791214723, 0.7552874551174575, 0.7551533055676847, 0.7551434637627987, 0.7552312328797417, 0.7552135983436501, 0.7551743427073272, 0.7553221181033867, 0.7555192396420691, 0.7555104550321544, 0.7555126968971618, 0.7554780934819185, 0.7554846010106092, 0.7554538366926808, 0.7555492486277049, 0.7556361488778862, 0.7555291946169801, 0.7555305920321647, 0.7557556943029693, 0.7556744361019646, 0.7556867529398104, 0.7558077013751214, 0.7557090354356515, 0.75555508289924, 0.7556037491198582, 0.7557096133833242, 0.7556319337656243, 0.7556261457919233, 0.7554957777771135, 0.7553794760724444, 0.7555210041805613, 0.7556117583841166, 0.7556059289312472, 0.7556118599462551, 0.7555418966549429, 0.7554698111874907, 0.7555168565653531, 0.7556680311780196, 0.7555833322582598, 0.7555813749266357, 0.7556029978820142, 0.7556723161755146, 0.7556471944035956, 0.7555370765727993, 0.7556628446308808, 0.7557882429144042, 0.7557657945171656, 0.7559063413832201, 0.7559585618665096, 0.7558173072894219, 0.7560137380345008, 0.7559443260627525, 0.7560426057117484, 0.7559295678438868, 0.7558309222887902, 0.7557767033656341, 0.7557045217809533, 0.7556517653859904, 0.7556653197867739, 0.755640897653729, 0.7556820119075579, 0.7556315064999346, 0.755697973931643, 0.7556947004466112, 0.7556439556937423, 0.7555606348137506, 0.7555477106866677, 0.7554496002448835, 0.7553490938160277], "moving_var_accuracy_train": [0.0073402012517093645, 0.016332495585762583, 0.02524839160646028, 0.03342411969583267, 0.041221970555399494, 0.04819239119225501, 0.05397568998130396, 0.05798361772249058, 0.06096238642080033, 0.06280533307798276, 0.06379932692290045, 0.06392988840477155, 0.06332801847018045, 0.06223825051408772, 0.060744244856371685, 0.058869377992848304, 0.056661287466989604, 0.05432191709975248, 0.05177818184327374, 0.049271400359401744, 0.04671698198554546, 0.0440281847850234, 0.04146472932167871, 0.03885869183797562, 0.036459078233506234, 0.03412181181020873, 0.03188337599572891, 0.029704218743696078, 0.02766888886381603, 0.025799309698955216, 0.02403724176646072, 0.022338282557556193, 0.02076707286515054, 0.01929740088319921, 0.01790623421288971, 0.016604337725328235, 0.015432002876550985, 0.01430014547148948, 0.013226866290315154, 0.012269699849260374, 0.0113728624821132, 0.010525329193296795, 0.009710306172088286, 0.00903355497534609, 0.008362998574315165, 0.0077387396482785096, 0.007187841681481729, 0.006673256548857215, 0.006185430159532833, 0.005754109301458793, 0.005350963070521363, 0.00498154347289077, 0.004583414926351158, 0.0042639446705767, 0.003987467603590902, 0.0037071034223868384, 0.003427166100402913, 0.0032231245184382216, 0.003008461709064719, 0.002783350519553966, 0.0026078528053901898, 0.002409146912894348, 0.0022280246949088213, 0.0020826823904364, 0.001971749796684579, 0.0018435879800146543, 0.0017045334226990702, 0.0015928800382066692, 0.0014701216163871137, 0.0013818560523560465, 0.0012895431094576601, 0.001186227756226177, 0.0011088917938069382, 0.0010247210303492604, 0.0009729631998381424, 0.0009120615321768728, 0.0008640633253823018, 0.0008160305708080527, 0.0007630721758123737, 0.000700827855900247, 0.0006426590925384759, 0.0006009141332818583, 0.0005763273857202837, 0.0005356241600558346, 0.0005014916664586846, 0.00046291186207210287, 0.0004431067754740394, 0.00040959524786219687, 0.0003851745002139807, 0.00042922486659172467, 0.00047100704786067377, 0.0004881979333831007, 0.0004960783467594671, 0.0004947962163126632, 0.0004857778680979159, 0.0004680825407078051, 0.0004493111896036418, 0.0004283003021495009, 0.00040850505777260943, 0.0003874131920136469, 0.0003611061842614797, 0.00033467080879193926, 0.0003113824066875349, 0.0002879364195165625, 0.0002657277127091877, 0.00024389366060978843, 0.00022263939183657958, 0.00020390735474447397, 0.00018644942835545786, 0.00017022326025430793, 0.00015466863750238015, 0.00014016032544777055, 0.00012786934468277691, 0.0001155072912129831, 0.00010456835960429103, 9.507712622355587e-05, 8.641505455915441e-05, 7.819107041767309e-05, 7.085257043618835e-05, 6.386336951869865e-05, 5.7982527114560064e-05, 5.237614284186032e-05, 4.724387994838585e-05, 4.258223445083934e-05, 3.8374637574793595e-05, 3.4742846101599835e-05, 3.138799893805203e-05, 2.8451996283074766e-05, 2.563684357828891e-05, 2.3121853441577216e-05, 2.0953875287747247e-05, 1.888006492530507e-05, 1.7007960978627475e-05, 1.538384621096859e-05, 1.3851094494385618e-05, 1.2475957724403374e-05, 1.1264054452958079e-05, 1.0149479797118096e-05, 9.246262066339073e-06, 8.351624139056034e-06, 7.596951473751137e-06, 6.952059047067004e-06, 6.273118248931784e-06, 5.660631118194169e-06, 5.098232687849676e-06, 4.588793803292814e-06, 4.136264387269499e-06, 3.7588522616676933e-06, 3.416919132209738e-06, 3.0942825955674187e-06, 2.78568320837937e-06, 2.5177613656636946e-06, 2.2812066799962933e-06, 2.0670131194103516e-06, 1.9430103005184332e-06, 1.766495819597492e-06, 1.5966304768428745e-06, 1.4576701232997414e-06, 1.3195276767171305e-06, 1.1909121306650842e-06, 1.0745466048181653e-06, 9.686842504212557e-07, 1.2384500631058921e-06, 1.2556726585372904e-06, 1.1419024283288495e-06, 1.0294557644830249e-06, 9.452259586391965e-07, 8.750396650477179e-07, 7.888074075360079e-07, 7.127379213704926e-07, 6.52136345110615e-07, 5.884534856656554e-07, 5.366891711117271e-07, 4.973192735666176e-07, 4.576950475039231e-07, 4.333570609534306e-07, 4.0561843438972e-07, 5.575748025672619e-07, 5.119120992668131e-07, 4.6780665045912725e-07, 4.2119447227588625e-07, 4.111105807725918e-07, 3.8590096827440437e-07, 3.476693865479867e-07, 3.135073786505819e-07, 3.445018925230219e-07, 3.1168821565235967e-07, 2.81821449709885e-07, 2.964758501388303e-07, 2.6708822173805723e-07, 2.4058683752185485e-07, 2.2121240200079008e-07, 2.3949143551885363e-07, 2.458667600737229e-07, 3.5539489517733956e-07], "duration": 225153.184318, "accuracy_train": [0.2855832482581211, 0.35729858803986714, 0.4037957964193429, 0.4404805163344408, 0.4819746894033776, 0.5164040149732374, 0.5436682773278885, 0.5580355340416205, 0.579377516207549, 0.5953255136235696, 0.6123219909330011, 0.6254082528723699, 0.6370100244555187, 0.6500744768595423, 0.6620936937638427, 0.6717859594176818, 0.6795523169296788, 0.6898520051794943, 0.6959657045957918, 0.7070061960709672, 0.7143286123800295, 0.7166294282368956, 0.725999417451089, 0.7281639687730712, 0.7389254423911037, 0.7438533159145442, 0.7495277604743448, 0.7526398549972315, 0.7592679710571244, 0.7673805774732374, 0.7728428747346806, 0.7755393263773532, 0.7817016916989663, 0.7866045112933739, 0.7900475339724069, 0.7941165443890735, 0.8014382397217608, 0.8027646560077519, 0.8048780180071059, 0.8119443074589332, 0.815153733792451, 0.8173855161614065, 0.8176903089239571, 0.8286381549349391, 0.8280336162444629, 0.8307991013750462, 0.8368888283268733, 0.8397244284099299, 0.8415140720168882, 0.8469305872785161, 0.8496317252676264, 0.85316198608804, 0.8478849797549834, 0.8571623235049834, 0.862622818325489, 0.8621585095399593, 0.8612756739687154, 0.8719466650516795, 0.871203338409007, 0.8690856505514026, 0.876780811646364, 0.8726216791828165, 0.8747597346230158, 0.8808992089447213, 0.8873820482650425, 0.8854757867294205, 0.8829896799441677, 0.8883575293004798, 0.8854997591938908, 0.8929166234080842, 0.8924991780869325, 0.8890586788252122, 0.8952864727528608, 0.8932399813122923, 0.9014710080980066, 0.9002161487172389, 0.9040286717884828, 0.9049576498477298, 0.9042139627168696, 0.9006579269910484, 0.9009133328719084, 0.9063770720861019, 0.9120021297642118, 0.9078415553479143, 0.910191037110096, 0.908305143157069, 0.9152558961447952, 0.9107705218715393, 0.9144678689668696, 0.9325564452404023, 0.9359747744785898, 0.9350915784191584, 0.9361368139188816, 0.936719182585825, 0.9370672339308784, 0.9365084772402179, 0.9374868421811554, 0.9379046479904946, 0.9392303433001107, 0.939648870085825, 0.9380677688953488, 0.9378574240379292, 0.939160588835825, 0.9388343470261166, 0.939067582883444, 0.938625444121447, 0.9379969329665007, 0.9389495230020304, 0.9390199984426911, 0.9390665014188816, 0.9384390717284975, 0.9380681293835363, 0.9395090006690661, 0.9377415270856404, 0.9383932897286821, 0.9393222677879292, 0.9394396066929678, 0.9388347075143041, 0.9392070918120154, 0.9381604143595422, 0.939600564668697, 0.938927713466685, 0.9386955590739202, 0.9385567711217239, 0.9385553291689737, 0.939392022252215, 0.9391834798357327, 0.9396477886212625, 0.9377189965739202, 0.9389745769310631, 0.9395783946451642, 0.9389287949312477, 0.9380677688953488, 0.939369131252307, 0.9387885650263011, 0.9382305293120154, 0.9391598678594499, 0.9382305293120154, 0.9396710401093578, 0.938091020383444, 0.9395562246216316, 0.9398345215023993, 0.9383929292404946, 0.9383696777523993, 0.9385331591454411, 0.9386494165859173, 0.938973855954688, 0.939369131252307, 0.938184026335825, 0.9382766718000184, 0.9387867625853636, 0.9390443313953488, 0.9383235352643964, 0.9383002837763011, 0.937695745085825, 0.9381139113833518, 0.9387885650263011, 0.9390210799072536, 0.9388804895141197, 0.9388110955380213, 0.9388118165143964, 0.9387882045381136, 0.9366501490979143, 0.9397186245501107, 0.9382298083356404, 0.9386948380975452, 0.9381135508951642, 0.9390439709071613, 0.9386948380975452, 0.9387645925618309, 0.9389498834902179, 0.9385095471691584, 0.9383464262643041, 0.9389974679309707, 0.938973855954688, 0.9391602283476375, 0.9391373373477298, 0.9373001093000184, 0.9389513254429678, 0.9383693172642118, 0.938578580657069, 0.9380209054309707, 0.9381375233596345, 0.9385789411452565, 0.9384401531930602, 0.9393462402523993, 0.9384623232165927, 0.9384634046811554, 0.9378817569905868, 0.9385564106335363, 0.9385560501453488, 0.9387409805855482, 0.9392056498592655, 0.9391831193475452, 0.9374399787167773], "end": "2016-02-04 00:21:06.701000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0], "moving_var_accuracy_valid": [0.007095483450381533, 0.01585431477030868, 0.024445295182744853, 0.03218549590088499, 0.03911298190162588, 0.045330889138203316, 0.05032627309284668, 0.053850603855438404, 0.05612475722549291, 0.057833625272111916, 0.05859056126417185, 0.058495247309404474, 0.05754869356208213, 0.05618220317322865, 0.054422475770244144, 0.05244321327302304, 0.050275259092838816, 0.047899920515074634, 0.045475330873997306, 0.04305697128994581, 0.04050084044221862, 0.03793179251046143, 0.03545269210452207, 0.03292710685311015, 0.030635976287113027, 0.028457949170129268, 0.026312410688644334, 0.024314598593853973, 0.02243537348868625, 0.020720519562244247, 0.019049977588464936, 0.017543006013858508, 0.016106732601912366, 0.014768164666870674, 0.013500745807072313, 0.0123807747225832, 0.011352609666747721, 0.010382715494762548, 0.009489935312299773, 0.008677642080072726, 0.007926132360283233, 0.00721799780379223, 0.006567740854800644, 0.006000781375974882, 0.005477195565587005, 0.005000897957763442, 0.004569428618134793, 0.004146494360203235, 0.0037689804724672436, 0.00342553304078108, 0.003125658657530909, 0.002847161275702634, 0.0025845130241168067, 0.002349975283113971, 0.002133027278114074, 0.0019452648537185283, 0.0017566887740743413, 0.0016013205825543375, 0.0014607089080225905, 0.0013266993247041366, 0.0012074667209492512, 0.0010883711875912547, 0.000998234634003242, 0.000919373912624404, 0.0008467572638253785, 0.0007701579759363141, 0.0006959502226121799, 0.0006294360393430598, 0.0005744049149752269, 0.0005254670279627972, 0.0004890019975210424, 0.0004458502443269024, 0.00041097566139600876, 0.00037005281438492554, 0.0003386730808755853, 0.0003132217887018446, 0.00028337028387573587, 0.00025951610466685797, 0.0002347732654094168, 0.00021180284166598504, 0.0001910940603205328, 0.0001728145084538221, 0.00016379142058276644, 0.00015272875438961537, 0.0001400228850257576, 0.00012694165485865674, 0.00011491134092672049, 0.00010703811610317104, 9.834759155863679e-05, 0.00010433499719673864, 0.00011067643187982237, 0.00011063077960668132, 0.00010962325987501219, 0.00011378215334260317, 0.00011241467076617099, 0.00010381944307227924, 9.756612242494835e-05, 9.151983566329705e-05, 8.509395259670675e-05, 8.210652375015135e-05, 7.718162031537997e-05, 7.062102702503947e-05, 6.636946995625715e-05, 6.0706416590789213e-05, 5.622479257674066e-05, 5.118785465157635e-05, 4.703271533829566e-05, 4.3256636703210975e-05, 3.948510487023086e-05, 3.6044618968826405e-05, 3.2650266657999475e-05, 2.9421235310331077e-05, 2.6479560328842666e-05, 2.3888623838684492e-05, 2.2363249963586416e-05, 2.0200668554562706e-05, 1.8306105542419992e-05, 1.6961866958989213e-05, 1.5286824939467643e-05, 1.4451779428645474e-05, 1.3098977159782716e-05, 1.2374391885104593e-05, 1.1211681283276221e-05, 1.030122866408434e-05, 9.359975119389826e-06, 8.44442452281991e-06, 7.603408976179497e-06, 6.858925936712172e-06, 6.33499825837932e-06, 5.702370182652103e-06, 5.201463925387506e-06, 4.684116324617262e-06, 4.2295737370035356e-06, 4.0031544724279135e-06, 3.952551134297801e-06, 3.55799054521021e-06, 3.2022367243175936e-06, 2.892789619004333e-06, 2.60389178847084e-06, 2.3520205989421476e-06, 2.1987494751532916e-06, 2.046839408972148e-06, 1.94510839340864e-06, 1.7506151289905576e-06, 2.03159290698403e-06, 1.8878596733604084e-06, 1.7004390464750235e-06, 1.6620518578654234e-06, 1.5834613805823236e-06, 1.6384276937319835e-06, 1.4959005336220886e-06, 1.4471756607726504e-06, 1.3567652017493929e-06, 1.2213901873305236e-06, 1.2522135421665058e-06, 1.2487269665304876e-06, 1.3041261183620558e-06, 1.247840435692478e-06, 1.1233622348100501e-06, 1.0113426037802503e-06, 9.542621025834205e-07, 9.056027238851907e-07, 8.349618597005951e-07, 9.57149545364337e-07, 9.259997539041884e-07, 8.334342588375523e-07, 7.542988027475414e-07, 7.221141547970436e-07, 6.555826701364567e-07, 6.991578330563723e-07, 7.71600489653082e-07, 8.359630062832222e-07, 7.569020805021419e-07, 8.589926664716433e-07, 7.97636209699316e-07, 8.974482886634147e-07, 1.1549687983073013e-06, 1.0828341148744128e-06, 1.061480708047648e-06, 1.0703306733791187e-06, 1.0508761159038926e-06, 9.722457289674002e-07, 9.219127865740458e-07, 8.547706428018724e-07, 7.709470745470809e-07, 6.992203323345175e-07, 6.44511735912239e-07, 6.030177281137359e-07, 5.824772306035002e-07, 5.243259488814289e-07, 4.950686234868729e-07, 5.080430825215027e-07, 4.587420718170453e-07, 4.994987937192297e-07, 5.404627945194505e-07], "accuracy_test": 0.7503308354591837, "start": "2016-02-01 09:48:33.517000", "learning_rate_per_epoch": [0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 0.0003240853257011622, 3.2408534025307745e-05, 3.2408534025307745e-05, 3.2408534025307745e-05, 3.2408534025307745e-05, 3.2408534025307745e-05, 3.2408534025307745e-05, 3.2408534025307745e-05, 3.2408534025307745e-05, 3.2408534025307745e-06, 3.2408533456873556e-07, 3.240853274633082e-08, 3.240853363450924e-09, 3.240853418962075e-10, 3.240853418962075e-11, 3.2408533322259014e-12, 3.240853223805684e-13, 3.2408530882804126e-14, 3.2408530882804126e-15, 3.240853141219972e-16, 3.24085327356887e-17, 3.2408533149279005e-18, 3.2408533149279005e-19, 3.240853250304415e-20, 3.240853250304415e-21, 3.2408531493302192e-22, 3.240853275547964e-23, 3.2408531966618736e-24, 3.2408530980542604e-25, 3.240853221313777e-26, 3.240853221313777e-27, 3.240853317610274e-28, 3.2408531972396525e-29, 3.2408533477029294e-30, 3.2408534417424775e-31, 3.240853500517195e-32, 3.240853353580401e-33, 3.240853307662653e-34, 3.2408534224570233e-35, 3.240853494203505e-36, 3.2408535838866064e-37, 3.2408535838866064e-38, 3.240853023367221e-39, 3.2408530233672207e-40, 3.240923088290437e-41, 3.241203347983302e-42, 3.2369994525903274e-43, 3.2229864679470793e-44, 2.802596928649634e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.2855832482581211, "accuracy_train_last": 0.9374399787167773, "batch_size_eval": 1024, "accuracy_train_std": [0.015727041434845032, 0.016135430253095173, 0.015034829486796247, 0.014816196097958808, 0.016046503808421014, 0.01612271423201332, 0.019096792212765976, 0.018561543676050486, 0.016984084608803306, 0.0184186326339859, 0.016408628517816215, 0.017348294802650165, 0.018032846505638436, 0.017998138320184837, 0.017431774784699813, 0.018010366748241233, 0.017609802617154637, 0.01705429195688051, 0.017394645697888653, 0.018345843504877127, 0.017415238259413667, 0.017706584956550794, 0.016955794692600997, 0.014912780199899854, 0.015367366443098112, 0.01718980287941323, 0.01520628973449339, 0.0163309343880247, 0.01556559217539155, 0.01440686539047365, 0.016622334304167992, 0.01488486354040228, 0.016238673879702008, 0.014892299927184565, 0.013572928039235864, 0.01683656788694647, 0.014711125883639991, 0.015766926143355755, 0.01628300823773914, 0.014836872291135908, 0.015098989756887388, 0.015937403800266026, 0.015202486140719177, 0.015301841023518956, 0.017281805444056785, 0.01712853383596208, 0.016042633567429163, 0.016218570417391703, 0.017930215652059753, 0.017570908831448195, 0.01656087478515702, 0.017083547742787458, 0.018262209230736046, 0.017454562445142117, 0.016613401662445203, 0.016061027967960682, 0.01784599359300783, 0.01681928604616128, 0.0162679253253496, 0.01750393796687413, 0.01640893075132378, 0.017037446302368135, 0.01772278830971069, 0.015751087938652385, 0.016788915153010286, 0.0166153549564261, 0.015374206869128557, 0.016248952348864035, 0.01631046914108498, 0.014897196410466567, 0.015516203015755783, 0.015581704646391034, 0.015564814757553877, 0.016520281474587853, 0.015697406790500815, 0.015650128382016, 0.015093075966248353, 0.014263435910731698, 0.014572558008158423, 0.014989976772112697, 0.015961891829456565, 0.015313814345461318, 0.014682230562231422, 0.013765240397385387, 0.014630581712365273, 0.015316636368318126, 0.014725998855431592, 0.015158358293422408, 0.01489875909700412, 0.011270044243785207, 0.010469631483262127, 0.010798154343766573, 0.010439029396946641, 0.010984335634959427, 0.010863838297824256, 0.011319249563849101, 0.010081533022135759, 0.009563127665061916, 0.010315929811627798, 0.009836500448507667, 0.010023056040240489, 0.009905778148896557, 0.009938600211145406, 0.009426773466851755, 0.00978574778778303, 0.00994082805680319, 0.009823778256669509, 0.009792589907638575, 0.00944086002487915, 0.009370462472575738, 0.009568516094268776, 0.009891441296024215, 0.009712101052555454, 0.009497378249575906, 0.009669125058704032, 0.01013956109322021, 0.009561036896655023, 0.009490140773411846, 0.009300208559319014, 0.009835858060547173, 0.009701985304949394, 0.009845592980956698, 0.00960455663186662, 0.009520041416817638, 0.009866198209470238, 0.009940072091267599, 0.010198976855955806, 0.00981998472868524, 0.009942404754558562, 0.009472665043354386, 0.01021314782542874, 0.009729158373613255, 0.009437318138730536, 0.010127616853188928, 0.010018606355236525, 0.009625157288517741, 0.00951198839589884, 0.010313020787329083, 0.009657447858802558, 0.01001831269309616, 0.009630918055133178, 0.009728139404853323, 0.009630674081813229, 0.009652817077851006, 0.009308649723323958, 0.009534978696390686, 0.00987468926502467, 0.010098428259391572, 0.009382531024177783, 0.009430001631282122, 0.010033799059579688, 0.009908177891632591, 0.010166259762305521, 0.009482912035728444, 0.009196665017792163, 0.009937611655846187, 0.009471029699561337, 0.010275089981622411, 0.009916885405087447, 0.009650195685584985, 0.009231043668927696, 0.010038226373306095, 0.009803671263699649, 0.009829091236950246, 0.009759282548332925, 0.009616997578012274, 0.009545063485392904, 0.010118319337712357, 0.009481460744125416, 0.009729021475756487, 0.009644412295011975, 0.010007539130595615, 0.009925089486422125, 0.009447814978791807, 0.0095592219671991, 0.009930991946712943, 0.010093187034678154, 0.009567224329964194, 0.00958939701364957, 0.009948439074186067, 0.00937869213417637, 0.009904021813149535, 0.009734940401898966, 0.00958322154348316, 0.009499498363933618, 0.00984288792163826, 0.00974376601818539, 0.009980952816263287, 0.00966997542203501, 0.00977374241948807, 0.010149218300059252, 0.009177339899212914, 0.009613114375219887, 0.009819237476625402, 0.00999019671989186], "accuracy_test_std": 0.012910127200349423, "error_valid": [0.7192176910768072, 0.6475697712725903, 0.6032258918486446, 0.5694624199924698, 0.5364622552710843, 0.5031650037650602, 0.4797172086784638, 0.4642128082643072, 0.4500011765813253, 0.4273358080760542, 0.41445665474397586, 0.4040086125753012, 0.39836249294051207, 0.38761001035391573, 0.37928863893072284, 0.3694818335843373, 0.36113987198795183, 0.35587025837725905, 0.34825042356927716, 0.34034673851656627, 0.3393495858433735, 0.3365522637424698, 0.3311708749058735, 0.33347991575677716, 0.32378488563629515, 0.31953301487198793, 0.3206007624246988, 0.31609445594879515, 0.31326624858810237, 0.3071215526167168, 0.3093085231551205, 0.3029196865587349, 0.3033270778426205, 0.3018416439194277, 0.3030932323042168, 0.29594108857304224, 0.2931540615587349, 0.2937541180346386, 0.292126023625753, 0.2893389966114458, 0.28847420933734935, 0.29018319371234935, 0.289562547063253, 0.28334725621234935, 0.2826251294239458, 0.2806925945971386, 0.27843355845256024, 0.2838458325489458, 0.28102792027484935, 0.2800307676016567, 0.27560535109186746, 0.27574801157756024, 0.277599656438253, 0.2753920957266567, 0.27590096714984935, 0.2718005812311747, 0.27883065464984935, 0.271129929875753, 0.26991952183734935, 0.27159762095256024, 0.26979745152484935, 0.27651131871234935, 0.2659515013177711, 0.26366305064006024, 0.2627467879329819, 0.2664603727409638, 0.26940035532756024, 0.26857674839984935, 0.26446606739457834, 0.2631850644766567, 0.2585655120481928, 0.2626041274472892, 0.25940970914909633, 0.2673648696347892, 0.26071277296686746, 0.2581581207643072, 0.2628188535391567, 0.2593994140625, 0.2620864316641567, 0.2630115187311747, 0.26285856315888556, 0.26188200065888556, 0.2550357680722892, 0.25597115022590367, 0.2575477692018072, 0.2591552734375, 0.25931852409638556, 0.2554225691829819, 0.2563991316829819, 0.24739681381777112, 0.24567753435617468, 0.24689823748117468, 0.24628788591867468, 0.24283903190888556, 0.24395825489457834, 0.2480277555534638, 0.24613493034638556, 0.24580989975527112, 0.24608492564006024, 0.24320524284638556, 0.24421269060617468, 0.24606433546686746, 0.2437038191829819, 0.24544368881777112, 0.24420239551957834, 0.24543339373117468, 0.24445683123117468, 0.2441921004329819, 0.24459949171686746, 0.24445683123117468, 0.24506718279367468, 0.24580989975527112, 0.24630847609186746, 0.24557605421686746, 0.24319494775978923, 0.24507747788027112, 0.24471126694277112, 0.24344938347138556, 0.24505688770707834, 0.24271696159638556, 0.24420239551957834, 0.24256400602409633, 0.2439479598079819, 0.24629818100527112, 0.24392736963478923, 0.24434505600527112, 0.24457890154367468, 0.24433476091867468, 0.24605404038027112, 0.24494511248117468, 0.24397884506777112, 0.24494511248117468, 0.24517895801957834, 0.24334790333207834, 0.24270666650978923, 0.24456860645707834, 0.24446712631777112, 0.24483333725527112, 0.24445683123117468, 0.24482304216867468, 0.24359204395707834, 0.2435817488704819, 0.24543339373117468, 0.24445683123117468, 0.24221838525978923, 0.24505688770707834, 0.24420239551957834, 0.24310376270707834, 0.24517895801957834, 0.2458304899284638, 0.24395825489457834, 0.2433376082454819, 0.24506718279367468, 0.24442594597138556, 0.24567753435617468, 0.24566723926957834, 0.24320524284638556, 0.24357145378388556, 0.24444653614457834, 0.24433476091867468, 0.24508777296686746, 0.24517895801957834, 0.24405973503388556, 0.2429713973079819, 0.24517895801957834, 0.2444362410579819, 0.24420239551957834, 0.2437038191829819, 0.24457890154367468, 0.24545398390436746, 0.24320524284638556, 0.24308317253388556, 0.2444362410579819, 0.24282873682228923, 0.24357145378388556, 0.24545398390436746, 0.24221838525978923, 0.2446803816829819, 0.24307287744728923, 0.24508777296686746, 0.24505688770707834, 0.24471126694277112, 0.24494511248117468, 0.24482304216867468, 0.24421269060617468, 0.24457890154367468, 0.2439479598079819, 0.24482304216867468, 0.2437038191829819, 0.24433476091867468, 0.24481274708207834, 0.24518925310617468, 0.24456860645707834, 0.24543339373117468, 0.24555546404367468], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.026692111391718332, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.00032408533205885686, "optimization": "nesterov_momentum", "nb_data_augmentation": 1, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 6.283637116178995e-08, "rotation_range": [0, 0], "momentum": 0.653859079507616}, "accuracy_valid_max": 0.7577816147402108, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7544445359563253, "accuracy_valid_std": [0.015622796800240157, 0.014197318336545865, 0.01910941756624902, 0.0210294483009576, 0.025968188067034537, 0.016446667937010037, 0.02277535176158899, 0.019116596225122285, 0.01885932421641445, 0.01857682140624649, 0.016894387607975382, 0.016019639757763732, 0.012174126171595608, 0.018913157562269772, 0.02063768839161431, 0.015328848795879174, 0.019734357372653614, 0.013499325539715362, 0.015726639067210044, 0.014080770806184143, 0.013507008847033776, 0.015103185563530354, 0.01624766201609939, 0.013411507835936034, 0.016143622397408457, 0.015178188393542536, 0.015189997556796868, 0.015606320129743221, 0.014141967238818885, 0.016986634327048805, 0.015330094736776878, 0.014970485242401885, 0.014850598091338008, 0.014920170953544426, 0.017766578653094885, 0.015008213691981052, 0.016047963022209694, 0.016637261160386243, 0.015920816999413622, 0.011495937145881393, 0.015652494346067258, 0.014466568294232802, 0.012371507861684799, 0.017201474870242837, 0.015504868257775637, 0.01634805868517407, 0.012696132942003474, 0.018000190744769714, 0.01771860519978508, 0.014124967766715376, 0.012518999119431464, 0.012595276897946277, 0.013938583285490035, 0.01167934244854416, 0.01435244753891695, 0.012668659376962743, 0.012385935567003584, 0.014308495713405929, 0.014842791432043667, 0.009450246230940441, 0.017644972008769, 0.012326764020629216, 0.01436226051187958, 0.01610259842903477, 0.013592483370455148, 0.013489611366301613, 0.013675525303677665, 0.016846422358412143, 0.007880965330898186, 0.013419148498264215, 0.00959691416047924, 0.012746713522514488, 0.013908577001278719, 0.009662099593368466, 0.015175487597155335, 0.013321889710558574, 0.0150714097489985, 0.011778994518884688, 0.016085397350948934, 0.019114684805033543, 0.014093557787404818, 0.011298675360354754, 0.011974802028830151, 0.01025634957588, 0.009188017285483022, 0.008285508410475253, 0.012078356613697055, 0.010055320076306261, 0.009476457706556364, 0.016543998827424376, 0.015388272466963619, 0.014674608339283579, 0.015028240492370793, 0.012104288660727618, 0.015694525424149945, 0.016849892326083128, 0.012453061474655748, 0.014860775761341115, 0.017436873464380477, 0.012941382411390126, 0.015239477932037874, 0.017064116792214975, 0.014795357463682508, 0.0165824995997094, 0.015232939860757602, 0.01470613869628142, 0.015415269119654216, 0.014623458665428347, 0.017117396272246805, 0.014975950106810498, 0.014664811593715969, 0.015974195805094182, 0.016530500488066494, 0.016986251864323208, 0.013828588785062768, 0.016284653763536234, 0.016268654698060332, 0.014494019185209926, 0.014780099494530838, 0.014021649708584417, 0.014429157941172506, 0.010356129872279992, 0.014292457102241526, 0.015281283318058171, 0.012776151506104498, 0.015631051109358217, 0.016405148088762263, 0.01522667279707194, 0.016125206891976718, 0.014981559451947882, 0.01627785566851783, 0.015482438847763905, 0.014342097738972307, 0.014835531074027315, 0.012920577932295335, 0.014664739835547162, 0.017235853996242098, 0.016208848456929492, 0.01503949567753154, 0.015507529333770989, 0.01398659774584252, 0.01412757458497762, 0.015815370801428835, 0.016014543071233355, 0.013525736219919223, 0.014437373748225995, 0.015162344462171535, 0.014900687644124194, 0.013971590821154397, 0.017620389513569472, 0.01451052519463216, 0.013099592172981291, 0.015270118240894697, 0.013199749047310133, 0.015426957652532089, 0.014406345595397236, 0.01452141729013811, 0.01325816602847791, 0.015261270100701838, 0.014306356854612307, 0.016830294665512453, 0.013937419982097101, 0.014080813434662295, 0.013924239673820194, 0.014457995285019856, 0.014055189106769396, 0.014577109909550991, 0.013863611420505505, 0.016339618006621395, 0.01687293610160667, 0.013883506333149538, 0.01457451314839685, 0.013703035396724517, 0.012854958675273297, 0.01332095704115874, 0.017236407363548326, 0.01271716504378219, 0.013842772356198187, 0.012756625210813454, 0.016709449502699078, 0.014972420878624415, 0.016922328929050004, 0.015358750450773133, 0.015706124572739396, 0.015168912961354721, 0.01571483235538244, 0.013529784790578717, 0.01598448737123484, 0.013699261608416747, 0.01544432856047599, 0.015214312834163057, 0.014974415561004635, 0.014344206839673041, 0.015898067742867523, 0.014868288276655997], "accuracy_valid": [0.2807823089231928, 0.35243022872740964, 0.3967741081513554, 0.4305375800075301, 0.4635377447289157, 0.49683499623493976, 0.5202827913215362, 0.5357871917356928, 0.5499988234186747, 0.5726641919239458, 0.5855433452560241, 0.5959913874246988, 0.6016375070594879, 0.6123899896460843, 0.6207113610692772, 0.6305181664156627, 0.6388601280120482, 0.644129741622741, 0.6517495764307228, 0.6596532614834337, 0.6606504141566265, 0.6634477362575302, 0.6688291250941265, 0.6665200842432228, 0.6762151143637049, 0.6804669851280121, 0.6793992375753012, 0.6839055440512049, 0.6867337514118976, 0.6928784473832832, 0.6906914768448795, 0.6970803134412651, 0.6966729221573795, 0.6981583560805723, 0.6969067676957832, 0.7040589114269578, 0.7068459384412651, 0.7062458819653614, 0.707873976374247, 0.7106610033885542, 0.7115257906626506, 0.7098168062876506, 0.710437452936747, 0.7166527437876506, 0.7173748705760542, 0.7193074054028614, 0.7215664415474398, 0.7161541674510542, 0.7189720797251506, 0.7199692323983433, 0.7243946489081325, 0.7242519884224398, 0.722400343561747, 0.7246079042733433, 0.7240990328501506, 0.7281994187688253, 0.7211693453501506, 0.728870070124247, 0.7300804781626506, 0.7284023790474398, 0.7302025484751506, 0.7234886812876506, 0.7340484986822289, 0.7363369493599398, 0.7372532120670181, 0.7335396272590362, 0.7305996446724398, 0.7314232516001506, 0.7355339326054217, 0.7368149355233433, 0.7414344879518072, 0.7373958725527108, 0.7405902908509037, 0.7326351303652108, 0.7392872270331325, 0.7418418792356928, 0.7371811464608433, 0.7406005859375, 0.7379135683358433, 0.7369884812688253, 0.7371414368411144, 0.7381179993411144, 0.7449642319277108, 0.7440288497740963, 0.7424522307981928, 0.7408447265625, 0.7406814759036144, 0.7445774308170181, 0.7436008683170181, 0.7526031861822289, 0.7543224656438253, 0.7531017625188253, 0.7537121140813253, 0.7571609680911144, 0.7560417451054217, 0.7519722444465362, 0.7538650696536144, 0.7541901002447289, 0.7539150743599398, 0.7567947571536144, 0.7557873093938253, 0.7539356645331325, 0.7562961808170181, 0.7545563111822289, 0.7557976044804217, 0.7545666062688253, 0.7555431687688253, 0.7558078995670181, 0.7554005082831325, 0.7555431687688253, 0.7549328172063253, 0.7541901002447289, 0.7536915239081325, 0.7544239457831325, 0.7568050522402108, 0.7549225221197289, 0.7552887330572289, 0.7565506165286144, 0.7549431122929217, 0.7572830384036144, 0.7557976044804217, 0.7574359939759037, 0.7560520401920181, 0.7537018189947289, 0.7560726303652108, 0.7556549439947289, 0.7554210984563253, 0.7556652390813253, 0.7539459596197289, 0.7550548875188253, 0.7560211549322289, 0.7550548875188253, 0.7548210419804217, 0.7566520966679217, 0.7572933334902108, 0.7554313935429217, 0.7555328736822289, 0.7551666627447289, 0.7555431687688253, 0.7551769578313253, 0.7564079560429217, 0.7564182511295181, 0.7545666062688253, 0.7555431687688253, 0.7577816147402108, 0.7549431122929217, 0.7557976044804217, 0.7568962372929217, 0.7548210419804217, 0.7541695100715362, 0.7560417451054217, 0.7566623917545181, 0.7549328172063253, 0.7555740540286144, 0.7543224656438253, 0.7543327607304217, 0.7567947571536144, 0.7564285462161144, 0.7555534638554217, 0.7556652390813253, 0.7549122270331325, 0.7548210419804217, 0.7559402649661144, 0.7570286026920181, 0.7548210419804217, 0.7555637589420181, 0.7557976044804217, 0.7562961808170181, 0.7554210984563253, 0.7545460160956325, 0.7567947571536144, 0.7569168274661144, 0.7555637589420181, 0.7571712631777108, 0.7564285462161144, 0.7545460160956325, 0.7577816147402108, 0.7553196183170181, 0.7569271225527108, 0.7549122270331325, 0.7549431122929217, 0.7552887330572289, 0.7550548875188253, 0.7551769578313253, 0.7557873093938253, 0.7554210984563253, 0.7560520401920181, 0.7551769578313253, 0.7562961808170181, 0.7556652390813253, 0.7551872529179217, 0.7548107468938253, 0.7554313935429217, 0.7545666062688253, 0.7544445359563253], "seed": 563403589, "model": "residualv4", "loss_std": [0.1905648410320282, 0.1574450135231018, 0.17445434629917145, 0.18707256019115448, 0.1978549212217331, 0.20841772854328156, 0.2156248837709427, 0.2191746085882187, 0.223294198513031, 0.22749318182468414, 0.23039010167121887, 0.2301516830921173, 0.23133248090744019, 0.23154696822166443, 0.22969427704811096, 0.2310967743396759, 0.23244932293891907, 0.23050861060619354, 0.23278175294399261, 0.23367831110954285, 0.23176464438438416, 0.2291560024023056, 0.23117265105247498, 0.2286948412656784, 0.23053349554538727, 0.22889262437820435, 0.22935181856155396, 0.22676526010036469, 0.226721853017807, 0.22335049510002136, 0.22364050149917603, 0.22182273864746094, 0.22586283087730408, 0.2213650494813919, 0.22017471492290497, 0.2178569883108139, 0.22013647854328156, 0.2197922021150589, 0.2149781882762909, 0.21292655169963837, 0.2154306173324585, 0.21578842401504517, 0.21364668011665344, 0.2098492532968521, 0.20980551838874817, 0.2074584662914276, 0.2071666419506073, 0.2061949372291565, 0.2033580094575882, 0.2030210942029953, 0.20163117349147797, 0.20285047590732574, 0.1982361078262329, 0.1979135423898697, 0.19508835673332214, 0.19505783915519714, 0.19334270060062408, 0.19321899116039276, 0.19169576466083527, 0.18743406236171722, 0.18927890062332153, 0.18230167031288147, 0.18292532861232758, 0.18055641651153564, 0.18206250667572021, 0.18009689450263977, 0.18113596737384796, 0.17618103325366974, 0.17356696724891663, 0.17859308421611786, 0.17445290088653564, 0.17074814438819885, 0.17082953453063965, 0.17000757157802582, 0.16960661113262177, 0.16454531252384186, 0.16817782819271088, 0.1645277440547943, 0.16540874540805817, 0.1593814641237259, 0.16103322803974152, 0.15932713449001312, 0.15989825129508972, 0.15829001367092133, 0.1575985550880432, 0.15422159433364868, 0.15634217858314514, 0.15425214171409607, 0.15315653383731842, 0.14413011074066162, 0.14180389046669006, 0.13936077058315277, 0.14236684143543243, 0.14181756973266602, 0.14172206819057465, 0.1403767168521881, 0.13918937742710114, 0.13957713544368744, 0.13688266277313232, 0.13876669108867645, 0.1385713517665863, 0.13863855600357056, 0.13998886942863464, 0.14252020418643951, 0.13676489889621735, 0.1396903544664383, 0.14236882328987122, 0.13642039895057678, 0.1393609344959259, 0.13893666863441467, 0.13981471955776215, 0.1396569013595581, 0.13900096714496613, 0.14117668569087982, 0.13829852640628815, 0.13600529730319977, 0.13807009160518646, 0.13578423857688904, 0.1407065987586975, 0.1402623951435089, 0.1374070644378662, 0.13839952647686005, 0.1386789083480835, 0.13929420709609985, 0.14020904898643494, 0.1392480731010437, 0.14002443850040436, 0.13846351206302643, 0.1393377184867859, 0.13959838449954987, 0.13810011744499207, 0.13688553869724274, 0.13909441232681274, 0.14051726460456848, 0.13729920983314514, 0.1366465985774994, 0.14047111570835114, 0.13828526437282562, 0.13787151873111725, 0.13749700784683228, 0.139521986246109, 0.13791228830814362, 0.13864029943943024, 0.1398392766714096, 0.13821062445640564, 0.13862256705760956, 0.13868263363838196, 0.1384342908859253, 0.13744981586933136, 0.13979902863502502, 0.14082102477550507, 0.1376722753047943, 0.14035166800022125, 0.13738399744033813, 0.1366845667362213, 0.13809186220169067, 0.13688786327838898, 0.1414853036403656, 0.13800810277462006, 0.13999788463115692, 0.13657833635807037, 0.13995333015918732, 0.13966545462608337, 0.13957761228084564, 0.14058469235897064, 0.13640549778938293, 0.14122554659843445, 0.13784541189670563, 0.13801980018615723, 0.1395917534828186, 0.1400333195924759, 0.13809598982334137, 0.13752010464668274, 0.1396002322435379, 0.13906334340572357, 0.13562364876270294, 0.13985580205917358, 0.13996319472789764, 0.13829605281352997, 0.139128640294075, 0.13964888453483582, 0.14065605401992798, 0.14126743376255035, 0.14067436754703522, 0.14104890823364258, 0.14029891788959503, 0.13924452662467957, 0.1377750188112259, 0.1374763548374176, 0.13669316470623016, 0.136318176984787, 0.13689066469669342, 0.13811147212982178, 0.13918465375900269, 0.13825124502182007]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:36 2016", "state": "available"}], "summary": "ceef36869cc16e879984e9c5d281840d"}