{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 64, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.699275255203247, 1.246711254119873, 1.041013479232788, 0.9160429239273071, 0.8366658687591553, 0.7742199301719666, 0.7269138693809509, 0.688583254814148, 0.6491950750350952, 0.6240901350975037, 0.5940958857536316, 0.5750353932380676, 0.5541582703590393, 0.5327754020690918, 0.515621542930603, 0.4980785846710205, 0.4828340411186218, 0.4764636158943176, 0.46166345477104187, 0.45580780506134033, 0.44358041882514954, 0.43426862359046936, 0.42810603976249695, 0.42028290033340454, 0.4118031859397888, 0.4054240882396698, 0.40219005942344666, 0.3959049582481384, 0.3923695385456085, 0.3874276280403137, 0.38389045000076294, 0.3784717917442322, 0.3762603998184204, 0.37247326970100403, 0.37132149934768677, 0.36155790090560913, 0.35950618982315063, 0.35290026664733887, 0.35309964418411255, 0.35218310356140137, 0.346161812543869, 0.34845125675201416, 0.34386929869651794, 0.339489221572876, 0.3413570821285248, 0.3328447639942169, 0.33507075905799866, 0.32764333486557007, 0.32673972845077515, 0.328619122505188, 0.3284526467323303, 0.32420992851257324, 0.32398083806037903, 0.31674084067344666, 0.3154725730419159, 0.31690311431884766, 0.3145299553871155, 0.31710508465766907, 0.3056495189666748, 0.3078407049179077, 0.31568440794944763, 0.3027051091194153, 0.3007057011127472, 0.3067278563976288, 0.2992043197154999, 0.3027942478656769, 0.2940353751182556, 0.2996668517589569, 0.29646655917167664, 0.29273104667663574, 0.29188641905784607, 0.28901365399360657, 0.2937462627887726, 0.2896559536457062, 0.29042068123817444, 0.289352148771286, 0.28406304121017456, 0.2816421091556549, 0.2876332998275757, 0.2784266769886017, 0.2807985842227936, 0.28294843435287476, 0.2777133584022522, 0.28020361065864563, 0.27960920333862305, 0.27370986342430115, 0.27614936232566833, 0.27169886231422424, 0.29693087935447693, 0.26458853483200073, 0.2698076069355011, 0.27001598477363586, 0.2704671621322632, 0.2713462710380554, 0.2630705237388611, 0.26907312870025635, 0.2692858874797821, 0.26446640491485596, 0.2659528851509094, 0.26230454444885254, 0.26485970616340637, 0.2621917128562927, 0.261245459318161, 0.25965628027915955, 0.25834792852401733, 0.27642616629600525, 0.25244754552841187, 0.25401273369789124, 0.25371554493904114, 0.25629642605781555, 0.2596224248409271, 0.2507510781288147, 0.2559320330619812, 0.24970069527626038, 0.25689536333084106, 0.25493791699409485, 0.2515920102596283, 0.24867691099643707, 0.25108543038368225, 0.25051626563072205, 0.2504870295524597, 0.2483513206243515, 0.2539934813976288, 0.24659694731235504, 0.24374009668827057, 0.24699777364730835, 0.24675239622592926, 0.2410404533147812, 0.24496494233608246, 0.24620771408081055, 0.2393369972705841, 0.24248501658439636, 0.24243108928203583, 0.24101953208446503, 0.24089212715625763, 0.24007833003997803, 0.23995839059352875, 0.24072961509227753, 0.23724882304668427, 0.23701047897338867, 0.23991966247558594, 0.23658160865306854, 0.23668663203716278, 0.2360461801290512, 0.23739150166511536, 0.23581236600875854, 0.23569010198116302, 0.23274768888950348, 0.2372538298368454, 0.23264414072036743, 0.22973453998565674, 0.2345733344554901, 0.22610510885715485, 0.23547406494617462, 0.23204420506954193, 0.23024335503578186, 0.23311904072761536, 0.23090960085391998, 0.2268264889717102, 0.22500364482402802, 0.2287721335887909, 0.22534912824630737, 0.236159086227417, 0.262352854013443, 0.22068753838539124, 0.21648216247558594, 0.22459781169891357, 0.2284824252128601, 0.22333796322345734, 0.21712413430213928, 0.23264656960964203, 0.22238324582576752, 0.22474920749664307, 0.2233743965625763, 0.22197987139225006, 0.22336842119693756, 0.21943433582782745, 0.22331558167934418, 0.22536391019821167, 0.21710464358329773, 0.22431229054927826, 0.21575593948364258, 0.22186163067817688, 0.22043392062187195, 0.22033104300498962, 0.2186642438173294, 0.22378773987293243, 0.22060364484786987, 0.2188022881746292, 0.21558016538619995, 0.2228684276342392, 0.21497361361980438, 0.21196593344211578, 0.213151216506958, 0.2226409912109375, 0.21933798491954803, 0.21153539419174194, 0.21401043236255646, 0.21325205266475677, 0.21092112362384796, 0.22013112902641296, 0.21070852875709534, 0.2099650651216507, 0.21304115653038025, 0.21873946487903595, 0.20945799350738525, 0.21400409936904907, 0.21146568655967712, 0.2166532278060913, 0.2100742608308792, 0.21045057475566864, 0.21079927682876587, 0.21215783059597015, 0.20947755873203278], "moving_avg_accuracy_train": [0.053051947068798434, 0.11519331719788203, 0.176080975920831, 0.2349902632320794, 0.2922343724928046, 0.34657204303814393, 0.39580364831581716, 0.44145584883353406, 0.4834143635660444, 0.5235437038740302, 0.5586022395405216, 0.5908966441106019, 0.622479095505651, 0.6510220645945747, 0.6768012012852649, 0.7012625468151917, 0.7231896184302765, 0.7428121954969018, 0.7598772407128078, 0.7772841654106761, 0.7925459298923568, 0.805381937678315, 0.8171502590855942, 0.8291691734282807, 0.8391588038486956, 0.8494628198162624, 0.8594107273418343, 0.8683404844802977, 0.8763589170561696, 0.8817527339030629, 0.887734685993792, 0.8930485442158875, 0.8976497992039056, 0.9038880687240374, 0.9083564292219732, 0.9107995381166364, 0.9140209329633798, 0.9167643673063921, 0.9206261502543704, 0.9227346034563697, 0.9250158248429217, 0.9287360197384282, 0.9311704477110602, 0.934644770834011, 0.9346493472816637, 0.9374805586463822, 0.9399867240984199, 0.9422260690612246, 0.9443367745801206, 0.9450345599543344, 0.9466155533636629, 0.9492127557273243, 0.950206265793896, 0.9499405360860457, 0.9502754748120202, 0.9517626013629795, 0.9531381815909764, 0.9543529883568972, 0.955016161916464, 0.9559502367950925, 0.9568001687322776, 0.9579696473197826, 0.9582526258902222, 0.9588212016929035, 0.9581217697272861, 0.9593496783200337, 0.9605617528987446, 0.9601392365351344, 0.9605725215494874, 0.9618228191707475, 0.9621179728560629, 0.9619699509776641, 0.961815877045457, 0.9614655498671664, 0.9622221129590764, 0.96344924946794, 0.964005153499782, 0.9641495030676701, 0.9653838994121121, 0.9657089918733095, 0.9665502020609785, 0.9674793522417854, 0.9668601884450246, 0.9671933288267126, 0.9667678889857542, 0.9672801033229207, 0.9675853473049513, 0.9663628512995391, 0.9679944024041275, 0.9688884505934858, 0.9695047929591557, 0.9695619192430205, 0.9689972405616125, 0.9687145691828691, 0.9692715697788864, 0.9688848076653388, 0.9687388655142996, 0.9691748899367069, 0.969723168984749, 0.9705513334101021, 0.9712222405821963, 0.9715354134358907, 0.9723915447113493, 0.9728691662068993, 0.9734106326957517, 0.9739351909654899, 0.9743863670689686, 0.974671409677548, 0.9750117615288685, 0.9740671841843519, 0.9741865074325834, 0.9737289953417337, 0.9734055540659121, 0.973521285861702, 0.974188202587455, 0.9742676303561181, 0.9748110484586107, 0.9754140930913395, 0.9754080620929291, 0.9761490790598544, 0.9766484754693452, 0.9757703443652863, 0.9759589140204428, 0.9756799090386643, 0.9764610624800361, 0.9769781607701461, 0.9776876177586077, 0.9771658797922707, 0.9761988779726044, 0.9762469740658479, 0.9766413580200052, 0.9768893467335086, 0.9771659068518244, 0.9775752462261657, 0.9780413079130729, 0.9783027614586981, 0.9781474446497607, 0.9783609739943084, 0.9785369104115536, 0.9785255533727976, 0.9788827055498219, 0.9793389290424588, 0.9795728188763082, 0.9796671343839339, 0.9793777054312826, 0.9799076978714969, 0.980335826893871, 0.9796982217330922, 0.9802450267169442, 0.9802511590524019, 0.980675204940028, 0.981275374178168, 0.9812971264544266, 0.9813050417101928, 0.9813353448308402, 0.9811301388072892, 0.9809849809158552, 0.9809497420123925, 0.9810272008468675, 0.9809970405455418, 0.980267629236235, 0.9798437460364487, 0.9792508068102495, 0.980195703807796, 0.980927528516302, 0.9813071889456334, 0.9812047799094126, 0.9818031449244237, 0.981839513392714, 0.9818907741070141, 0.982336870393941, 0.9821803213378895, 0.9822324145386336, 0.9819631142300269, 0.9817928596141947, 0.9817558158027845, 0.9820433469082296, 0.9823090643007399, 0.9826645755409318, 0.9820869921678095, 0.982425110993895, 0.9818111283528849, 0.9821558711723583, 0.9816407479313222, 0.9815002966500948, 0.981169421597027, 0.9811272562230386, 0.9816055264709821, 0.9812315042548547, 0.9817504669246073, 0.9820455444131174, 0.9826853910134723, 0.9826637257585629, 0.9822954907565346, 0.9822290741213573, 0.9825645744473168, 0.9827990954252042, 0.9827171955553028, 0.9832178695259814, 0.9832476241650685, 0.9832744393890656, 0.9825266236859009, 0.9825580735935198, 0.9823003491579866, 0.982154427671959, 0.9829135942797632, 0.9833062366744152, 0.9833573454843638, 0.9835614174835465, 0.9836986153554392, 0.9835988431056095, 0.9842158933188581, 0.9842201782429246, 0.9837731000008119], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05222888624811746, 0.11288087408226655, 0.1720820680181664, 0.2289328903014401, 0.28354616071218464, 0.3347987429655143, 0.3808538543146707, 0.4232547205304175, 0.4617033213312914, 0.4985959539477857, 0.5304311062939709, 0.558901696954107, 0.5866055714040276, 0.6112632971608387, 0.6330859508784897, 0.65453724495668, 0.6731139292053644, 0.6901351793571171, 0.704754385677806, 0.7198099382621338, 0.7322642442288421, 0.7427173131700392, 0.7530579571354148, 0.7629728292494335, 0.7710437809818698, 0.7794856895930201, 0.7873753465843958, 0.7950071172723417, 0.8010709292649871, 0.8053229524905969, 0.8100367686402571, 0.8144970177927977, 0.8179211864954457, 0.8229957729475578, 0.8265201555549104, 0.8288305482260759, 0.8308151868334381, 0.831988951000245, 0.8351542137033079, 0.836433243575447, 0.8382182536493782, 0.8412743109086874, 0.8434611799872463, 0.8467305140310669, 0.8464226087593608, 0.8486673490129428, 0.8509104303296455, 0.852453129295928, 0.8540451069348592, 0.8548990087978493, 0.8557223785788324, 0.8577014690812653, 0.8583637216930935, 0.8575994739590251, 0.8579555727792371, 0.8591885005351989, 0.8607396476578839, 0.8620227282610715, 0.8630045433491209, 0.8632167902096155, 0.8640609620922683, 0.8649630831270174, 0.8647037147616048, 0.8657898191890588, 0.8646228466357403, 0.8656885057786422, 0.866814526482781, 0.8662805936010691, 0.8668020600786881, 0.8684392839465723, 0.8686422246690084, 0.8684810154268816, 0.8682432713296, 0.8680150355934774, 0.8687811854603344, 0.8701807871703251, 0.8704231270809282, 0.8708996396740402, 0.8720904844886391, 0.8724257049121397, 0.8729866924028534, 0.8739769209411826, 0.8733006261550462, 0.8734201176152193, 0.8729489289899925, 0.8733234638018366, 0.8736686341291078, 0.8732499540746307, 0.8746309545256014, 0.8752380597978153, 0.8760153586278983, 0.8759814761913132, 0.875376222020977, 0.8747816356340148, 0.8754520625261856, 0.8746167819136272, 0.8745437697143579, 0.8749806060335847, 0.875715555595889, 0.8767838131951856, 0.8775041929355315, 0.8775676267105025, 0.8786674623075245, 0.8786613382529769, 0.8792377931419112, 0.8796262961601748, 0.8797816658852718, 0.8805318502003591, 0.8810300876671454, 0.8797795179535183, 0.8799358905651694, 0.8791365374367398, 0.8790671808033821, 0.8789234286492487, 0.87907069539464, 0.8794138435227211, 0.8800766807442442, 0.8809316409171842, 0.8811223741334477, 0.8820641065054946, 0.8826013423157885, 0.8819140090537126, 0.8824530180692751, 0.8819371496207814, 0.8828106409840044, 0.883213217698857, 0.8840851435287453, 0.8836379961280545, 0.8824593139380653, 0.8823731505224214, 0.8830544984031612, 0.8837734567424385, 0.8836992748953784, 0.8839896036652531, 0.8846506430634116, 0.8846900850635463, 0.8845598319694657, 0.8843703915059529, 0.8846922943734298, 0.8850552491416591, 0.885463239617177, 0.8855486398177334, 0.885894054685734, 0.8862202236241636, 0.885645017432681, 0.886002120075708, 0.8869176860237096, 0.8861771363595916, 0.8870029880003644, 0.8873748959962617, 0.8877737368747982, 0.8881784332645021, 0.8882273362200549, 0.8886803579632603, 0.8889598301676873, 0.8888827948165813, 0.8886252099885376, 0.8885297200043676, 0.8882921471003163, 0.8878169833597878, 0.887599944050541, 0.8875480045212398, 0.8874768448823689, 0.8890191288617525, 0.8896807925474899, 0.8901349530329518, 0.8891142982322922, 0.8900889753367738, 0.8903955427880361, 0.890507614544624, 0.8910621982991225, 0.8911533441036229, 0.8914449538762426, 0.8903909552412538, 0.8902694931414507, 0.8906902271386761, 0.8909294628491307, 0.8912821408582688, 0.8917063258217642, 0.8910565246119523, 0.8909243931880312, 0.8901442362016829, 0.8903770358495868, 0.8900341506004715, 0.8899036588743852, 0.8894972185328804, 0.8896583835865954, 0.8899499165099388, 0.8891482254048786, 0.8896525540786226, 0.8899415814267845, 0.8905629165069976, 0.8909431306450779, 0.8905456949337328, 0.8905664207622721, 0.8913379389834847, 0.8916456513445489, 0.891454607247368, 0.8918768411291824, 0.8917145947045172, 0.8909093932348185, 0.8898317375144993, 0.8901965318071006, 0.8900558319396435, 0.889817279760363, 0.8905364942259681, 0.8910962790089436, 0.8909632606713023, 0.8913237364208135, 0.8911201737256448, 0.8907975424129447, 0.891818474118563, 0.8919032635666464, 0.8916346886689427], "moving_var_accuracy_train": [0.02533058179011532, 0.05755147254478167, 0.08516208815316423, 0.1078786165215207, 0.12658274727485327, 0.14049771451001247, 0.14826170168296082, 0.15219264222365325, 0.15281803062831253, 0.1520295031474685, 0.1478884611404295, 0.1424859721252108, 0.13721443603777544, 0.13082530219369917, 0.12372384697098485, 0.11673667910009661, 0.10939017941660444, 0.10191657125156486, 0.09434585604039661, 0.08763827968329228, 0.08097074481081165, 0.0743565381926611, 0.06816732487210023, 0.06265068110268171, 0.05728374743584184, 0.052510927397796496, 0.048150482435252835, 0.04405309925469498, 0.04022644667798973, 0.036465641351791214, 0.0331411309739541, 0.03008115167939948, 0.027263580438642385, 0.024887466454230397, 0.02257841601866298, 0.020374293446437326, 0.01843026056462123, 0.016654972396108882, 0.015123695464333661, 0.013651336092045486, 0.012333038221971099, 0.011224293050318974, 0.010155201701272472, 0.009248319821609263, 0.008323488027943195, 0.007563281045274275, 0.006863480728203732, 0.00622226464814531, 0.005640133883418262, 0.005080502634932637, 0.0045949482328824336, 0.004196162550654465, 0.0037854298558604336, 0.003407522380773098, 0.003067779798247206, 0.0027809057268295984, 0.002519845142819545, 0.0022811424278443336, 0.002056986377590877, 0.001859140202741755, 0.0016797276411482046, 0.0015240639985330783, 0.001372378290521723, 0.0012380499674601037, 0.0011186478163848403, 0.0010203528703556463, 0.0009315397063792971, 0.0008399924164390325, 0.0007576827979280946, 0.0006959837154108454, 0.0006271693851513584, 0.0005646496409245851, 0.0005083983258213986, 0.00045866305542590036, 0.00041794823929167644, 0.00038970619146497986, 0.00035351683595204593, 0.00031835268353658686, 0.0003002310241994724, 0.0002711590877544718, 0.0002504118901975684, 0.00023314058170425332, 0.00021327679779879974, 0.0001929479606441209, 0.00017528215610418152, 0.0001601152122385542, 0.0001449422560117917, 0.00014389849875984882, 0.0001534662799458154, 0.000145313551435289, 0.00013420109749723595, 0.00012081035845828617, 0.0001115990807315876, 0.00010115830063367499, 9.38347175459798e-05, 8.579751018366367e-05, 7.740945116834694e-05, 7.137956172393245e-05, 6.694709478223711e-05, 6.642509214279696e-05, 6.383363083062497e-05, 5.833296287418213e-05, 5.909631343412843e-05, 5.523978272782009e-05, 5.235447808198879e-05, 4.959548267894692e-05, 4.6467973298204206e-05, 4.255241956673513e-05, 3.9339732054337285e-05, 4.343579608686977e-05, 3.922035881629938e-05, 3.718217875413243e-05, 3.4405489208864994e-05, 3.1085484924988956e-05, 3.197993770429235e-05, 2.8838722867776366e-05, 2.8612579688049332e-05, 2.902428718081173e-05, 2.6122185819206998e-05, 2.8451922544726222e-05, 2.7851301254563834e-05, 3.200619925234915e-05, 2.9125605960726776e-05, 2.6913639383369083e-05, 2.9714081735733227e-05, 2.9149189336872998e-05, 3.076423336947842e-05, 3.0137704582187326e-05, 3.553976679711071e-05, 3.2006609225067175e-05, 3.0205796632230342e-05, 2.7738702587232795e-05, 2.565320181989519e-05, 2.459591014838142e-05, 2.4091240597568764e-05, 2.2297338146491615e-05, 2.0284714132088792e-05, 1.8666595747726823e-05, 1.7078518779171343e-05, 1.5371827742217944e-05, 1.498266406597573e-05, 1.535765653648208e-05, 1.4314230972236565e-05, 1.2962866609821202e-05, 1.2420502016535153e-05, 1.3706479695040499e-05, 1.3985481863727668e-05, 1.6245796746820163e-05, 1.7312178285427626e-05, 1.5581298906728354e-05, 1.5641503249369607e-05, 1.7319180954119244e-05, 1.5591521312409155e-05, 1.403293304263285e-05, 1.2637904250458294e-05, 1.1753099434326886e-05, 1.0767426811904223e-05, 9.701860153569006e-06, 8.7856729775562e-06, 7.915292473785106e-06, 1.1912130949709356e-05, 1.233801055828801e-05, 1.4268401836151838e-05, 2.087703467628788e-05, 2.3609437844479627e-05, 2.2545772434432662e-05, 2.03855836872864e-05, 2.1569391539261167e-05, 1.9424356374707148e-05, 1.7505569684711374e-05, 1.7546029791129647e-05, 1.601199527457232e-05, 1.4435219061188968e-05, 1.3644401061011616e-05, 1.2540840662819803e-05, 1.1299106792212002e-05, 1.0913263342376975e-05, 1.0457388602281915e-05, 1.054914391917802e-05, 1.2496652503426625e-05, 1.2275906318064389e-05, 1.4441087837412582e-05, 1.4066607557877684e-05, 1.5048114383189937e-05, 1.3720842006456836e-05, 1.33340625124947e-05, 1.2016657530117476e-05, 1.28736736477168e-05, 1.2845339846356567e-05, 1.398470613509183e-05, 1.3369872039611554e-05, 1.5717517883522174e-05, 1.4149990544602572e-05, 1.3955364640611384e-05, 1.259952870140468e-05, 1.2352620049734604e-05, 1.1612358846384362e-05, 1.0511491259954677e-05, 1.171641195819539e-05, 1.0552738809300615e-05, 9.503936434512653e-06, 1.3586597724157147e-05, 1.2236839821944536e-05, 1.1610952801788364e-05, 1.064149524236985e-05, 1.4764351163775807e-05, 1.4675428498100406e-05, 1.3231394642379686e-05, 1.2283063605795394e-05, 1.1224166549682745e-05, 1.0191340411239063e-05, 1.259896506114614e-05, 1.1339233800199833e-05, 1.2004221011314942e-05], "duration": 129169.992429, "accuracy_train": [0.5305194706879844, 0.6744656483596346, 0.7240699044273717, 0.765173849033315, 0.8074313558393319, 0.8356110779461978, 0.8388880958148762, 0.8523256534929864, 0.8610409961586378, 0.8847077666459026, 0.8741290605389442, 0.8815462852413253, 0.9067211580610927, 0.9079087863948875, 0.9088134315014765, 0.921414656584533, 0.9205332629660392, 0.9194153890965301, 0.9134626476559615, 0.9339464876914912, 0.9299018102274824, 0.920906007751938, 0.9230651517511074, 0.9373394025124585, 0.929065477632429, 0.9421989635243633, 0.9489418950719823, 0.9487082987264673, 0.948524810239018, 0.9302970855251015, 0.9415722548103543, 0.9408732682147471, 0.9390610940960686, 0.9600324944052234, 0.948571673703396, 0.9327875181686047, 0.9430134865840717, 0.9414552763935032, 0.9553821967861758, 0.9417106822743633, 0.94554681732189, 0.9622177737979882, 0.9530802994647471, 0.9659136789405685, 0.9346905353105389, 0.9629614609288483, 0.9625422131667589, 0.9623801737264673, 0.9633331242501846, 0.9513146283222591, 0.9608444940476191, 0.9725875770002769, 0.9591478563930418, 0.9475489687153931, 0.9532899233457919, 0.9651467403216132, 0.9655184036429494, 0.9652862492501846, 0.9609847239525655, 0.9643569107027501, 0.9644495561669435, 0.9684949546073275, 0.9607994330241787, 0.9639383839170359, 0.9518268820367294, 0.9704008556547619, 0.9714704241071429, 0.9563365892626431, 0.9644720866786637, 0.9730754977620893, 0.9647743560239018, 0.9606377540720746, 0.9604292116555924, 0.9583126052625508, 0.969031180786268, 0.9744934780477114, 0.9690082897863603, 0.9654486491786637, 0.9764934665120893, 0.9686348240240864, 0.97412109375, 0.9758417038690477, 0.9612877142741787, 0.9701915922619048, 0.9629389304171282, 0.9718900323574198, 0.9703325431432264, 0.9553603872508305, 0.9826783623454227, 0.9769348842977114, 0.9750518742501846, 0.9700760557978036, 0.9639151324289406, 0.9661705267741787, 0.9742845751430418, 0.965403948643411, 0.9674253861549464, 0.9730991097383721, 0.9746576804171282, 0.9780048132382798, 0.9772604051310447, 0.97435396911914, 0.9800967261904762, 0.9771677596668512, 0.9782838310954227, 0.9786562153931341, 0.9784469520002769, 0.9772367931547619, 0.978074928190753, 0.9655659880837025, 0.9752604166666666, 0.9696113865240864, 0.970494582583518, 0.9745628720238095, 0.9801904531192323, 0.9749824802740864, 0.9797018113810447, 0.9808414947858989, 0.9753537831072352, 0.9828182317621816, 0.9811430431547619, 0.967867164428756, 0.9776560409168512, 0.9731688642026578, 0.9834914434523809, 0.981632045381137, 0.9840727306547619, 0.9724702380952381, 0.9674958615956073, 0.9766798389050388, 0.9801908136074198, 0.9791212451550388, 0.9796549479166666, 0.9812593005952381, 0.9822358630952381, 0.9806558433693245, 0.9767495933693245, 0.9802827380952381, 0.9801203381667589, 0.9784233400239941, 0.9820970751430418, 0.9834449404761905, 0.9816778273809523, 0.9805159739525655, 0.9767728448574198, 0.9846776298334257, 0.9841889880952381, 0.9739597752860835, 0.9851662715716132, 0.9803063500715209, 0.9844916179286637, 0.9866768973214286, 0.981492896940753, 0.9813762790120893, 0.9816080729166666, 0.9792832845953304, 0.9796785598929494, 0.9806325918812293, 0.9817243303571429, 0.9807255978336102, 0.9737029274524732, 0.9760287972383721, 0.9739143537744556, 0.9886997767857143, 0.9875139508928571, 0.9847241328096161, 0.9802830985834257, 0.9871884300595238, 0.9821668296073275, 0.9823521205357143, 0.9863517369762828, 0.9807713798334257, 0.9827012533453304, 0.9795394114525655, 0.9802605680717055, 0.9814224215000923, 0.9846311268572352, 0.9847005208333334, 0.9858641767026578, 0.9768887418097084, 0.9854681804286637, 0.9762852845837948, 0.9852585565476191, 0.9770046387619971, 0.9802362351190477, 0.9781915461194168, 0.9807477678571429, 0.9859099587024732, 0.9778653043097084, 0.9864211309523809, 0.9847012418097084, 0.9884440104166666, 0.982468738464378, 0.9789813757382798, 0.9816313244047619, 0.9855840773809523, 0.9849097842261905, 0.9819800967261905, 0.9877239352620893, 0.9835154159168512, 0.9835157764050388, 0.9757962823574198, 0.9828411227620893, 0.9799808292381875, 0.9808411342977114, 0.98974609375, 0.9868400182262828, 0.9838173247739018, 0.9853980654761905, 0.9849333962024732, 0.9827008928571429, 0.9897693452380952, 0.9842587425595238, 0.9797493958217978], "end": "2016-02-02 05:39:26.337000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0], "moving_var_accuracy_valid": [0.024550709028469144, 0.055203610779726145, 0.08122628197267776, 0.10219179772396933, 0.11881610169618634, 0.13057593621527666, 0.13660800212619775, 0.1391277030161889, 0.13851958684647467, 0.13691722523420782, 0.1323467950349305, 0.12640728632427067, 0.12067409962767629, 0.11407872062039161, 0.10695690249587703, 0.1004026344049503, 0.09346820974353315, 0.08672889537973671, 0.07997949658278487, 0.07402157389708153, 0.06801540414138485, 0.06219726357985115, 0.05693989748043396, 0.05213064993372669, 0.04750384729715985, 0.043394854956435, 0.03961558964776558, 0.036178225997489544, 0.03289133174067994, 0.029764915880212062, 0.02698840485642603, 0.024468608773308077, 0.022127272277715014, 0.020146307898883147, 0.01824346856386192, 0.01646716293613051, 0.014855895756133957, 0.013382705681394077, 0.012134605105069276, 0.010935867851286765, 0.009870957414834411, 0.008967917047100557, 0.008114166909691318, 0.0073989471245329425, 0.006659905662986748, 0.006039264825942535, 0.005480621067488348, 0.004953978241644633, 0.004481389952705885, 0.004039813292959857, 0.0036419334038299968, 0.003312991256398383, 0.002985639337455405, 0.0026923320751011228, 0.0024242401249188177, 0.0021954971100897258, 0.001997601915646677, 0.0018126583865904935, 0.0016400681957355388, 0.001476466814730093, 0.0013352337687642395, 0.0012090347931398462, 0.0010887367613666528, 0.0009904796906760036, 0.0009036881460701923, 0.0008235399961428264, 0.0007525973001638869, 0.0006799033290470561, 0.0006143603417278737, 0.0005770488254972139, 0.0005197146073788987, 0.0004679770424187326, 0.00042168803847898923, 0.00037998805859228136, 0.0003472721232994193, 0.00033017487548895615, 0.0002976859456305003, 0.0002699609293299992, 0.0002557278387491153, 0.00023116640946519072, 0.00021088213120130743, 0.00019861889110426944, 0.00018287337373363915, 0.00016471454024176392, 0.0001502412547024752, 0.0001364796161597757, 0.00012390393753725442, 0.00011309118067568111, 0.00011894652281834346, 0.00011036906184045898, 0.00010476989689764769, 9.430323938346335e-05, 8.816990894150052e-05, 8.253471479139688e-05, 7.832649327196946e-05, 7.677308726021474e-05, 6.91437555653726e-05, 6.39468137369964e-05, 6.241349009547663e-05, 6.644270977202417e-05, 6.44689615275292e-05, 5.805827996903986e-05, 6.31391970364273e-05, 5.6825614869181495e-05, 5.4133755533049e-05, 5.007879133654414e-05, 4.528816996618012e-05, 4.582434152898885e-05, 4.347607253587734e-05, 5.320378676006405e-05, 4.810347962712918e-05, 4.904382047978783e-05, 4.4182731515125483e-05, 3.9950440499974843e-05, 3.615058389866097e-05, 3.3595281249045476e-05, 3.418993176426877e-05, 3.734955066366491e-05, 3.3942008035374417e-05, 3.8529545976884935e-05, 3.7274192221955884e-05, 3.7798616118162504e-05, 3.663353097606479e-05, 3.5365260183820795e-05, 3.869561862006639e-05, 3.628466886013319e-05, 3.949849384955903e-05, 3.7348111646104006e-05, 4.6116925826472755e-05, 4.157205045158426e-05, 4.159295981772326e-05, 4.208577367849768e-05, 3.792672282854717e-05, 3.489266769724567e-05, 3.533615870077984e-05, 3.1816543873073525e-05, 2.8787582302424145e-05, 2.6231813275125382e-05, 2.4541225052422093e-05, 2.327272802120338e-05, 2.2443561272102307e-05, 2.0264843893187732e-05, 1.9312162383192834e-05, 1.8338421732440273e-05, 1.948233902367586e-05, 1.8681805800220663e-05, 2.4357974266457228e-05, 2.685790108503823e-05, 3.031038936963807e-05, 2.8524190449385293e-05, 2.7103437821972505e-05, 2.5867106550329664e-05, 2.33019193868529e-05, 2.2818785746518927e-05, 2.123984958929255e-05, 1.9169274638243634e-05, 1.7849496667163825e-05, 1.6146612034138577e-05, 1.5039918793378684e-05, 1.556795213685915e-05, 1.443511147899839e-05, 1.3015879763434708e-05, 1.1759865034929455e-05, 3.19916373890062e-05, 3.273266314731801e-05, 3.1315752551580704e-05, 3.755980329540874e-05, 4.235378208787402e-05, 3.896425629864827e-05, 3.518087137640585e-05, 3.443085250554751e-05, 3.106253527409499e-05, 2.872160808207098e-05, 3.584766537688653e-05, 3.239567621439506e-05, 3.0749262460747065e-05, 2.8189439741083168e-05, 2.648993177014162e-05, 2.5460334542427808e-05, 2.671447559864284e-05, 2.4200156457465397e-05, 2.7257945121851468e-05, 2.5019911694244346e-05, 2.357605317136773e-05, 2.1371700669423817e-05, 2.072127436330422e-05, 1.8882914497824346e-05, 1.7759546056580663e-05, 2.176796910231567e-05, 2.188029889252818e-05, 2.04441002751445e-05, 2.187420578476112e-05, 2.098785032345074e-05, 2.031066159297744e-05, 1.8283461473397487e-05, 2.1812278617024553e-05, 2.048323282968734e-05, 1.8763390170327216e-05, 1.8491584211863042e-05, 1.6879340911526573e-05, 2.1026551481618508e-05, 2.9375972997286914e-05, 2.7636049580788423e-05, 2.5050612697031442e-05, 2.3057715707484133e-05, 2.540736916455875e-05, 2.568686327736092e-05, 2.3277421852964627e-05, 2.2119164561540097e-05, 2.0280188043165192e-05, 1.918898791425949e-05, 2.6650803050661455e-05, 2.4050426000152002e-05, 2.2294575681226285e-05], "accuracy_test": 0.09968909438775511, "start": "2016-01-31 17:46:36.345000", "learning_rate_per_epoch": [0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602, 0.0008273633429780602], "accuracy_train_first": 0.5305194706879844, "accuracy_train_last": 0.9797493958217978, "batch_size_eval": 1024, "accuracy_train_std": [0.020158981842465045, 0.017693595458633124, 0.01847451843813438, 0.01816709707654699, 0.023316830683092923, 0.01967131498713563, 0.019158525841128357, 0.01744757588235709, 0.018866168891486672, 0.01924096244186932, 0.01995325924346903, 0.020616556169072202, 0.0191822515977117, 0.018550654418314265, 0.01829491048449606, 0.019133360639833424, 0.016742895667989307, 0.018058669660409095, 0.016418083372228432, 0.015667984723253117, 0.016107379344016015, 0.01491685715183227, 0.016226317226718558, 0.015756361646827862, 0.013762807145310458, 0.015042492655453269, 0.013140097597833881, 0.012799344546111976, 0.012961469661048641, 0.014464983730811454, 0.012768012741473923, 0.01338846434685945, 0.013748498215847556, 0.010008587905064668, 0.011764988988558595, 0.012379647913666162, 0.013807526403603604, 0.012819804432004363, 0.011754073382708787, 0.013107035346229007, 0.012316491916973716, 0.011520192194916918, 0.011235835006970413, 0.010366379747581313, 0.014140291467295571, 0.010253084582319045, 0.010555365313603213, 0.01096137291912816, 0.01052705620086417, 0.010308625156866763, 0.011070736474253546, 0.0069884288776095385, 0.011773052553085844, 0.012743211151759085, 0.009957321975209694, 0.01077484048790588, 0.01113045577196586, 0.011050970480053086, 0.011803827711363981, 0.009352349227327786, 0.010207797307273241, 0.008286233124144591, 0.01022578134545945, 0.010542027143836894, 0.00997802490942064, 0.00991443561440459, 0.008847091919633109, 0.009443762804433658, 0.010368932879910883, 0.008200672773589752, 0.009534850474381316, 0.008479665901754724, 0.011061669072078546, 0.009976352567947763, 0.008289486541115875, 0.007790894012760542, 0.008794203030392389, 0.009433214479365603, 0.007434272668299287, 0.009418636134445567, 0.00880777294969505, 0.007916610996202203, 0.010272790446136909, 0.008779338244545262, 0.00913396650166858, 0.00842121624011737, 0.007628215698862922, 0.0098100790824354, 0.006017996412932207, 0.00853820999499928, 0.00856630904877292, 0.008936755192122772, 0.010263865793282518, 0.008238246962413348, 0.007652043535296156, 0.009348238293443277, 0.00998800965962627, 0.007577034413860556, 0.007443967539527901, 0.007700711991954143, 0.007460493890671894, 0.008199458735008772, 0.0068722210868581496, 0.006618409373711189, 0.007158420776671671, 0.006723100222611274, 0.005718726039165548, 0.0077555825632846, 0.007549302656669755, 0.009327388084075369, 0.0077208946126191644, 0.009394384693568273, 0.008352862670427522, 0.009190545907027916, 0.006989633159372609, 0.008579538858532946, 0.007291834043261365, 0.006247373344337512, 0.007525809314175753, 0.005732441653074278, 0.006883893540806306, 0.008806371083041659, 0.007350641531836596, 0.008047971029091409, 0.006392425698452226, 0.0061272243193034, 0.006141254589172659, 0.008333509793651477, 0.009185364296782194, 0.006691701892546623, 0.006124457087889445, 0.006316885518746786, 0.006299780342653537, 0.006272043121667705, 0.006188216583013308, 0.005461776146479512, 0.007379249798723859, 0.0066001469491109264, 0.006012787924087588, 0.007532447639088431, 0.005726543326234323, 0.006519544780681591, 0.005685547513235835, 0.006535181800590299, 0.006604019314868373, 0.0049369233331284955, 0.006180873591403474, 0.006674003310388788, 0.004553564370695094, 0.006829950735102165, 0.005611614900214895, 0.005369281640046266, 0.005575641557807006, 0.005382119101850901, 0.005568961967248083, 0.006339006983289323, 0.006583929157091101, 0.005422889851871963, 0.0062758346464466186, 0.005645026673602265, 0.007843767253169678, 0.007363844065050878, 0.006356311735959783, 0.004230748666450467, 0.005443679389688439, 0.005207024094275356, 0.006628578814678065, 0.004645470429863114, 0.0064935452292234725, 0.00653846563297479, 0.005179219090802582, 0.006049143920728851, 0.005865299057394805, 0.006229324677102017, 0.006173374023014197, 0.006868254246449438, 0.005821557003727364, 0.005958009510977581, 0.005452422149193923, 0.0059221163254459, 0.0052908139149850705, 0.006330269181359872, 0.004433419226270495, 0.006841039529822895, 0.00692441651615862, 0.006521651738763288, 0.006301110374238959, 0.00441559140891951, 0.005332892879382878, 0.005519376319398388, 0.005512045995779927, 0.004403134490909468, 0.005990441891170366, 0.006284859719591411, 0.007170662423691297, 0.0050059325300587385, 0.005574201791836126, 0.005793914399077685, 0.004264247164959567, 0.005183287615156126, 0.00531781410124248, 0.007139474735704458, 0.005477064995701745, 0.005397606620300745, 0.00631888677653714, 0.004456103865425345, 0.004557704219053405, 0.004847195657956975, 0.005140840393988551, 0.004851794940851282, 0.00602478317011417, 0.004263336452636318, 0.005512760622031297, 0.005854922044535701], "accuracy_test_std": 0.007448442678108315, "error_valid": [0.4777111375188253, 0.3412512354103916, 0.2951071865587349, 0.25940970914909633, 0.22493440559111444, 0.2039280167545181, 0.20465014354292166, 0.19513748352786142, 0.19225927146084332, 0.1693703525037651, 0.18305252259036142, 0.18486298710466864, 0.16405955854668675, 0.16681717102786142, 0.17051016566265065, 0.1524011083396084, 0.15969591255647586, 0.1566735692771084, 0.16367275743599397, 0.14469008847891573, 0.1556470020707832, 0.16320506635918675, 0.15387624717620485, 0.14779332172439763, 0.15631765342620485, 0.1445371329066265, 0.14161774049322284, 0.1363069465361446, 0.14435476280120485, 0.15640883847891573, 0.14753888601280118, 0.14536073983433728, 0.15126129518072284, 0.13133294898343373, 0.14176040097891573, 0.15037591773343373, 0.15132306570030118, 0.15744717149849397, 0.1363584219691265, 0.15205548757530118, 0.14571665568524095, 0.13122117375753017, 0.13685699830572284, 0.12384547957454817, 0.15634853868599397, 0.1311299887048193, 0.12890183782003017, 0.13366258000753017, 0.13162709431475905, 0.13741587443524095, 0.1368672933923193, 0.12448671639683728, 0.13567600480045183, 0.1492787556475903, 0.1388395378388554, 0.1297151496611446, 0.12530002823795183, 0.12642954631024095, 0.12815912085843373, 0.13487298804593373, 0.1283414909638554, 0.12691782756024095, 0.1376306005271084, 0.12443524096385539, 0.1458799063441265, 0.12472056193524095, 0.12305128717996983, 0.13852480233433728, 0.12850474162274095, 0.11682570124246983, 0.12953130882906627, 0.13296986775225905, 0.13389642554593373, 0.1340390860316265, 0.12432346573795183, 0.11722279743975905, 0.1273958137236446, 0.12481174698795183, 0.11719191217996983, 0.12455731127635539, 0.12196442018072284, 0.11711102221385539, 0.1327860269201807, 0.12550445924322284, 0.13129176863704817, 0.12330572289156627, 0.12322483292545183, 0.13051816641566272, 0.11294004141566272, 0.11929799275225905, 0.11698895190135539, 0.12432346573795183, 0.13007106551204817, 0.1305696418486446, 0.11851409544427716, 0.13290074359939763, 0.12611334007906627, 0.12108786709337349, 0.11766989834337349, 0.11360186841114461, 0.11601238940135539, 0.12186146931475905, 0.11143401731927716, 0.12139377823795183, 0.11557411285768071, 0.11687717667545183, 0.11882000658885539, 0.11271649096385539, 0.11448577513177716, 0.1314756094691265, 0.11865675592996983, 0.1280576407191265, 0.12155702889683728, 0.12237034073795183, 0.11960390389683728, 0.11749782332454817, 0.11395778426204817, 0.11137371752635539, 0.11716102692018071, 0.10946030214608427, 0.11256353539156627, 0.12427199030496983, 0.11269590079066272, 0.12270566641566272, 0.10932793674698793, 0.11316359186746983, 0.10806752400225905, 0.12038633047816272, 0.12814882577183728, 0.11840232021837349, 0.11081337067018071, 0.10975591820406627, 0.11696836172816272, 0.11339743740587349, 0.10940000235316272, 0.11495493693524095, 0.11661244587725905, 0.11733457266566272, 0.11241057981927716, 0.11167815794427716, 0.11086484610316272, 0.11368275837725905, 0.11099721150225905, 0.11084425592996983, 0.11953183829066272, 0.11078395613704817, 0.10484222044427716, 0.12048781061746983, 0.10556434723268071, 0.10927793204066272, 0.10863669521837349, 0.10817929922816272, 0.11133253717996983, 0.1072424463478916, 0.10852491999246983, 0.11181052334337349, 0.11369305346385539, 0.11232968985316272, 0.11384600903614461, 0.11645949030496983, 0.11435340973268071, 0.11291945124246983, 0.11316359186746983, 0.09710031532379515, 0.10436423428087349, 0.1057776025978916, 0.12007159497364461, 0.1011389307228916, 0.10684535015060237, 0.10848373964608427, 0.1039465479103916, 0.10802634365587349, 0.10593055817018071, 0.11909503247364461, 0.11082366575677716, 0.10552316688629515, 0.10691741575677716, 0.10554375705948793, 0.10447600950677716, 0.11479168627635539, 0.11026478962725905, 0.11687717667545183, 0.10752776731927716, 0.11305181664156627, 0.1112707666603916, 0.11416074454066272, 0.10889113092996983, 0.10742628717996983, 0.11806699454066272, 0.10580848785768071, 0.10745717243975905, 0.10384506777108427, 0.10563494211219882, 0.11303122646837349, 0.10924704678087349, 0.10171839702560237, 0.10558493740587349, 0.11026478962725905, 0.10432305393448793, 0.10974562311746983, 0.11633741999246983, 0.11986716396837349, 0.10652031955948793, 0.11121046686746983, 0.11232968985316272, 0.10299057558358427, 0.10386565794427716, 0.11023390436746983, 0.10543198183358427, 0.11071189053087349, 0.11210613940135539, 0.09899314053087349, 0.10733363140060237, 0.1107824854103916], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.09777410236311096, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.0008273633462521503, "optimization": "adam", "nb_data_augmentation": 1, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.248341266503006e-05, "rotation_range": [0, 0], "momentum": 0.8288908557329194}, "accuracy_valid_max": 0.9028996846762049, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8892175145896084, "accuracy_valid_std": [0.013272534801675194, 0.01864014253186812, 0.0139920723138462, 0.013411139220502865, 0.018570042114968547, 0.018967502517637614, 0.01811050290854433, 0.00859334283727843, 0.01118975079918152, 0.014251376742777645, 0.009861899445601998, 0.008623390883402401, 0.007253284509961275, 0.014236997990759543, 0.017318617666807846, 0.006222200573779418, 0.01402751475927085, 0.009841835537505068, 0.0050114714628172385, 0.008360173334111416, 0.013287018027179763, 0.007844163743816595, 0.009945190240395599, 0.01193024708917212, 0.0070329398282366435, 0.0014649264391247419, 0.005711072115474136, 0.009964681815978657, 0.00918694182702983, 0.007728675157558344, 0.011237603533330046, 0.009783655765285615, 0.01010083418383856, 0.0076755379790555784, 0.009443591635580416, 0.010154044675829893, 0.0124261147763583, 0.0071749583463808515, 0.007601241110967205, 0.010331003799857484, 0.012635242361586469, 0.009747958084631886, 0.008881974843062508, 0.004174689933389954, 0.015124061593496354, 0.010495817677692434, 0.011467134076496157, 0.009996589995535047, 0.010248342052144897, 0.010843363136965748, 0.01145607204877191, 0.011530706996051087, 0.008324223359181383, 0.01406393944680385, 0.009782157124346859, 0.006028876505048929, 0.005186171756524101, 0.012799648836493932, 0.011372458897031541, 0.008499137711742401, 0.00937793073262225, 0.008218538467181612, 0.012945717993855244, 0.013916996385747296, 0.00597765247757566, 0.008533069876019903, 0.010802591214427911, 0.01092970116589281, 0.00807820002174879, 0.008047407181934382, 0.013315850233008878, 0.011428404202937548, 0.008182995049058459, 0.010543586369719653, 0.005377069984203969, 0.010242699652409138, 0.009791558526074953, 0.007059684045971287, 0.006706988137046825, 0.009745833305888754, 0.01410032089346314, 0.0074158605831571986, 0.016661727993966962, 0.012429511759540815, 0.00798085128053978, 0.008517400533230877, 0.006999166835016264, 0.012511350561890285, 0.007861744821676682, 0.009727214772656643, 0.006366874559584027, 0.006277204719896908, 0.009126153637496446, 0.006714375881786383, 0.009544810233524716, 0.016100349345254693, 0.008715807476949606, 0.013591175935761424, 0.009272246566326652, 0.006987528060082926, 0.007539138076691691, 0.009585908425922114, 0.010273123288313868, 0.005516560612616756, 0.010142969469986161, 0.010171589683186013, 0.010945694001007153, 0.012954152376704626, 0.008955418615196937, 0.008971403552495611, 0.009387466451570634, 0.007937413389750039, 0.00930362839541735, 0.005549785820807069, 0.013270464257898512, 0.007867581970872074, 0.009896287006823835, 0.009734110233738543, 0.013688642329957493, 0.011434859936814463, 0.01188647216274723, 0.010044400139221954, 0.008531500129525122, 0.007408198350153022, 0.010942661766658505, 0.010151810679327784, 0.012888080909326288, 0.009880769336606969, 0.015687291939081607, 0.014230609771251854, 0.009844827165368133, 0.004118374730050188, 0.009828958455060103, 0.008553010713550375, 0.012168321925052707, 0.010084036007340431, 0.009162982469826586, 0.009093257317763245, 0.00826893955716611, 0.007769468499704726, 0.009286786171915586, 0.00982674080210314, 0.00956604473696235, 0.007515570215950732, 0.012466528423754761, 0.010107981935751778, 0.00786208543075696, 0.00788072599513954, 0.009210107787446397, 0.008365236203771562, 0.01021789204928578, 0.007243179171364772, 0.009454165791723748, 0.010351830948432972, 0.011884745097621196, 0.00990277555076892, 0.007364358237042818, 0.006595014985742766, 0.00931430966005287, 0.012328036761648863, 0.009871716167757187, 0.00767038252177326, 0.009793196755700702, 0.008392181779700001, 0.006790170089665326, 0.009877997170471928, 0.007847913062938848, 0.007596290510151447, 0.011766082392911955, 0.008831404359901577, 0.011672270779132287, 0.01222718085177983, 0.007834292950791983, 0.007307151281515604, 0.007753288038784547, 0.00807684288873932, 0.006829029536516754, 0.00929245826251471, 0.012150972300278982, 0.01216903507595373, 0.009562515770132686, 0.00885591812729951, 0.00634999208110659, 0.007316817668999528, 0.013300301421554486, 0.007925835876791871, 0.011688014088245192, 0.006000829880496881, 0.009130496144437821, 0.00924310868770027, 0.013191983607172367, 0.011659585526060749, 0.012453542861574903, 0.010776805349492777, 0.006562348556598454, 0.011363608509979443, 0.007555661920061248, 0.006147969384779397, 0.010537096956329645, 0.008284801553160505, 0.00917584793455778, 0.008686218214995711, 0.009760286286957716, 0.009963021523679248, 0.005703241887163806, 0.009264453462967432, 0.007597872980826989, 0.011276763240184767, 0.009477569693528544, 0.009445403575943671, 0.010595729626789995, 0.010626364665234622, 0.012069773305239678, 0.009798882874351073], "accuracy_valid": [0.5222888624811747, 0.6587487645896084, 0.7048928134412651, 0.7405902908509037, 0.7750655944088856, 0.7960719832454819, 0.7953498564570783, 0.8048625164721386, 0.8077407285391567, 0.8306296474962349, 0.8169474774096386, 0.8151370128953314, 0.8359404414533133, 0.8331828289721386, 0.8294898343373494, 0.8475988916603916, 0.8403040874435241, 0.8433264307228916, 0.836327242564006, 0.8553099115210843, 0.8443529979292168, 0.8367949336408133, 0.8461237528237951, 0.8522066782756024, 0.8436823465737951, 0.8554628670933735, 0.8583822595067772, 0.8636930534638554, 0.8556452371987951, 0.8435911615210843, 0.8524611139871988, 0.8546392601656627, 0.8487387048192772, 0.8686670510165663, 0.8582395990210843, 0.8496240822665663, 0.8486769342996988, 0.842552828501506, 0.8636415780308735, 0.8479445124246988, 0.854283344314759, 0.8687788262424698, 0.8631430016942772, 0.8761545204254518, 0.843651461314006, 0.8688700112951807, 0.8710981621799698, 0.8663374199924698, 0.868372905685241, 0.862584125564759, 0.8631327066076807, 0.8755132836031627, 0.8643239951995482, 0.8507212443524097, 0.8611604621611446, 0.8702848503388554, 0.8746999717620482, 0.873570453689759, 0.8718408791415663, 0.8651270119540663, 0.8716585090361446, 0.873082172439759, 0.8623693994728916, 0.8755647590361446, 0.8541200936558735, 0.875279438064759, 0.8769487128200302, 0.8614751976656627, 0.871495258377259, 0.8831742987575302, 0.8704686911709337, 0.867030132247741, 0.8661035744540663, 0.8659609139683735, 0.8756765342620482, 0.882777202560241, 0.8726041862763554, 0.8751882530120482, 0.8828080878200302, 0.8754426887236446, 0.8780355798192772, 0.8828889777861446, 0.8672139730798193, 0.8744955407567772, 0.8687082313629518, 0.8766942771084337, 0.8767751670745482, 0.8694818335843373, 0.8870599585843373, 0.880702007247741, 0.8830110480986446, 0.8756765342620482, 0.8699289344879518, 0.8694303581513554, 0.8814859045557228, 0.8670992564006024, 0.8738866599209337, 0.8789121329066265, 0.8823301016566265, 0.8863981315888554, 0.8839876105986446, 0.878138530685241, 0.8885659826807228, 0.8786062217620482, 0.8844258871423193, 0.8831228233245482, 0.8811799934111446, 0.8872835090361446, 0.8855142248682228, 0.8685243905308735, 0.8813432440700302, 0.8719423592808735, 0.8784429711031627, 0.8776296592620482, 0.8803960961031627, 0.8825021766754518, 0.8860422157379518, 0.8886262824736446, 0.8828389730798193, 0.8905396978539157, 0.8874364646084337, 0.8757280096950302, 0.8873040992093373, 0.8772943335843373, 0.8906720632530121, 0.8868364081325302, 0.891932475997741, 0.8796136695218373, 0.8718511742281627, 0.8815976797816265, 0.8891866293298193, 0.8902440817959337, 0.8830316382718373, 0.8866025625941265, 0.8905999976468373, 0.885045063064759, 0.883387554122741, 0.8826654273343373, 0.8875894201807228, 0.8883218420557228, 0.8891351538968373, 0.886317241622741, 0.889002788497741, 0.8891557440700302, 0.8804681617093373, 0.8892160438629518, 0.8951577795557228, 0.8795121893825302, 0.8944356527673193, 0.8907220679593373, 0.8913633047816265, 0.8918207007718373, 0.8886674628200302, 0.8927575536521084, 0.8914750800075302, 0.8881894766566265, 0.8863069465361446, 0.8876703101468373, 0.8861539909638554, 0.8835405096950302, 0.8856465902673193, 0.8870805487575302, 0.8868364081325302, 0.9028996846762049, 0.8956357657191265, 0.8942223974021084, 0.8799284050263554, 0.8988610692771084, 0.8931546498493976, 0.8915162603539157, 0.8960534520896084, 0.8919736563441265, 0.8940694418298193, 0.8809049675263554, 0.8891763342432228, 0.8944768331137049, 0.8930825842432228, 0.8944562429405121, 0.8955239904932228, 0.8852083137236446, 0.889735210372741, 0.8831228233245482, 0.8924722326807228, 0.8869481833584337, 0.8887292333396084, 0.8858392554593373, 0.8911088690700302, 0.8925737128200302, 0.8819330054593373, 0.8941915121423193, 0.892542827560241, 0.8961549322289157, 0.8943650578878012, 0.8869687735316265, 0.8907529532191265, 0.8982816029743976, 0.8944150625941265, 0.889735210372741, 0.8956769460655121, 0.8902543768825302, 0.8836625800075302, 0.8801328360316265, 0.8934796804405121, 0.8887895331325302, 0.8876703101468373, 0.8970094244164157, 0.8961343420557228, 0.8897660956325302, 0.8945680181664157, 0.8892881094691265, 0.8878938605986446, 0.9010068594691265, 0.8926663685993976, 0.8892175145896084], "seed": 847641764, "model": "residualv3", "loss_std": [0.34723716974258423, 0.128672793507576, 0.1099533662199974, 0.09844466298818588, 0.08899121731519699, 0.08665252476930618, 0.07987292855978012, 0.0769663006067276, 0.07562664151191711, 0.07173430919647217, 0.0688038244843483, 0.07071296125650406, 0.06751812249422073, 0.06411381810903549, 0.06160244345664978, 0.05954328551888466, 0.05731644853949547, 0.05650937184691429, 0.055362850427627563, 0.05640121176838875, 0.05253351479768753, 0.049573179334402084, 0.05141690373420715, 0.05010230094194412, 0.04770125076174736, 0.04471971094608307, 0.04820473864674568, 0.042940642684698105, 0.04287535324692726, 0.043067097663879395, 0.04503346234560013, 0.040858760476112366, 0.04158303514122963, 0.04108310863375664, 0.04271558299660683, 0.03895840421319008, 0.039152126759290695, 0.03528732806444168, 0.03759485110640526, 0.037909992039203644, 0.038347531110048294, 0.03699282929301262, 0.037786900997161865, 0.03695648908615112, 0.03876512870192528, 0.03537629917263985, 0.03736724331974983, 0.038489457219839096, 0.03375937044620514, 0.036671992391347885, 0.03686706721782684, 0.036393020302057266, 0.036551982164382935, 0.037158966064453125, 0.03499998897314072, 0.0340680368244648, 0.032808344811201096, 0.035239219665527344, 0.032388102263212204, 0.03532089293003082, 0.04382682964205742, 0.03478830307722092, 0.03425337001681328, 0.03599769249558449, 0.03248041495680809, 0.034767188131809235, 0.03167879953980446, 0.032564740628004074, 0.0333140529692173, 0.03440668061375618, 0.03397529572248459, 0.03181147947907448, 0.034774020314216614, 0.032133977860212326, 0.03090875782072544, 0.030573586001992226, 0.03164636343717575, 0.028031112626194954, 0.03219373896718025, 0.029581338167190552, 0.032405462116003036, 0.030313022434711456, 0.029401438310742378, 0.030327698215842247, 0.030203089118003845, 0.030179811641573906, 0.029687458649277687, 0.029728827998042107, 0.06449763476848602, 0.02704646997153759, 0.02990599535405636, 0.02956428937613964, 0.028459349647164345, 0.02992679364979267, 0.027971414849162102, 0.030274828895926476, 0.030702659860253334, 0.028467193245887756, 0.028554877266287804, 0.029262535274028778, 0.029644818976521492, 0.027930838987231255, 0.02696334756910801, 0.029888825491070747, 0.027809981256723404, 0.04639215022325516, 0.027124110609292984, 0.028868326917290688, 0.028335561975836754, 0.027565639466047287, 0.029384104534983635, 0.026245011016726494, 0.029695779085159302, 0.025469277054071426, 0.02968902513384819, 0.027941908687353134, 0.028205927461385727, 0.02588154748082161, 0.02804083749651909, 0.02678718976676464, 0.02872958406805992, 0.026674699038267136, 0.029727837070822716, 0.02905372343957424, 0.028204740956425667, 0.027323033660650253, 0.027359722182154655, 0.024339521303772926, 0.02940640226006508, 0.029145991429686546, 0.026758546009659767, 0.025773372501134872, 0.02644730731844902, 0.027468468993902206, 0.028087226673960686, 0.025510676205158234, 0.02556419000029564, 0.028364699333906174, 0.026406822726130486, 0.02590608038008213, 0.026888234540820122, 0.026472976431250572, 0.026969239115715027, 0.02725738286972046, 0.027082227170467377, 0.025731772184371948, 0.027093827724456787, 0.026700537651777267, 0.0273942518979311, 0.02642086334526539, 0.02700386568903923, 0.02627016045153141, 0.026020456105470657, 0.025848453864455223, 0.02622966468334198, 0.027180902659893036, 0.02732248231768608, 0.027362236753106117, 0.024689605459570885, 0.02451619878411293, 0.025938162580132484, 0.02598257176578045, 0.030406909063458443, 0.07440882921218872, 0.023417221382260323, 0.02252303995192051, 0.02583538368344307, 0.026780225336551666, 0.0242754053324461, 0.022500241175293922, 0.02852727100253105, 0.02500521019101143, 0.028328850865364075, 0.024322763085365295, 0.024419177323579788, 0.026936817914247513, 0.02308567240834236, 0.02488408237695694, 0.028134256601333618, 0.0240255668759346, 0.02408215031027794, 0.024403182789683342, 0.025136487558484077, 0.026480162516236305, 0.02715473249554634, 0.024604711681604385, 0.026480527594685555, 0.025060666725039482, 0.0256347618997097, 0.02552756853401661, 0.027226507663726807, 0.02395051345229149, 0.0230601467192173, 0.02551090531051159, 0.027191871777176857, 0.03136264160275459, 0.02387479692697525, 0.025841938331723213, 0.024510087445378304, 0.024670029059052467, 0.027104835957288742, 0.024646751582622528, 0.0233767069876194, 0.02700483612716198, 0.02779787965118885, 0.02560664340853691, 0.026456762105226517, 0.02457444556057453, 0.025770775973796844, 0.02421141043305397, 0.024470258504152298, 0.0229648407548666, 0.025429563596844673, 0.02517283894121647]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:30 2016", "state": "available"}], "summary": "0b807cc4cf7b67f9f4426c6f7efa74ef"}