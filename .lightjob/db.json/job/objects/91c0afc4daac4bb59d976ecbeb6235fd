{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 16, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6656309366226196, 1.3667242527008057, 1.2325317859649658, 1.137892484664917, 1.0625752210617065, 0.9993244409561157, 0.9409879446029663, 0.888623833656311, 0.8443880677223206, 0.8054000735282898, 0.770961344242096, 0.739521861076355, 0.7118585109710693, 0.6886505484580994, 0.6676806211471558, 0.6461868286132812, 0.6280280947685242, 0.6113647222518921, 0.59663325548172, 0.5811857581138611, 0.568091869354248, 0.550962507724762, 0.5418547987937927, 0.5266591906547546, 0.5144653916358948, 0.5043804049491882, 0.49715369939804077, 0.484758198261261, 0.4764591157436371, 0.4635058343410492, 0.457849383354187, 0.44912654161453247, 0.4390462338924408, 0.4310542345046997, 0.42327091097831726, 0.41617071628570557, 0.4120500385761261, 0.4026843011379242, 0.39595112204551697, 0.39071008563041687, 0.3836846649646759, 0.3773609399795532, 0.37238797545433044, 0.36740976572036743, 0.3626958131790161, 0.3561781048774719, 0.35138943791389465, 0.34510883688926697, 0.3396818935871124, 0.3372901380062103, 0.3304557800292969, 0.3274896442890167, 0.3237866461277008, 0.3201795816421509, 0.31541603803634644, 0.31136971712112427, 0.30748313665390015, 0.3039759397506714, 0.2986450493335724, 0.29647472500801086, 0.2919943034648895, 0.2899596095085144, 0.28396546840667725, 0.28140610456466675, 0.2769305109977722, 0.27651911973953247, 0.27320021390914917, 0.2673344016075134, 0.2659284770488739, 0.2632242441177368, 0.26129865646362305, 0.25795209407806396, 0.25628426671028137, 0.2509670555591583, 0.2502276599407196, 0.24798402190208435, 0.24506023526191711, 0.24213643372058868, 0.23949727416038513, 0.23572804033756256, 0.23480550944805145, 0.23225702345371246, 0.23047448694705963, 0.22662201523780823, 0.22519329190254211, 0.2252236306667328, 0.220235675573349, 0.2180674523115158, 0.21893897652626038, 0.21658608317375183, 0.1834055781364441, 0.1713947057723999, 0.1659359484910965, 0.16418157517910004, 0.16279388964176178, 0.16014975309371948, 0.15901640057563782, 0.15755215287208557, 0.15608534216880798, 0.1549874097108841, 0.1562049239873886, 0.15475541353225708, 0.15351229906082153, 0.15130387246608734, 0.15316179394721985, 0.15085619688034058, 0.1532547026872635, 0.1496475785970688, 0.14758354425430298, 0.15065997838974, 0.15050041675567627, 0.14738203585147858, 0.14448925852775574, 0.14386631548404694, 0.14510171115398407, 0.14455652236938477, 0.1435822695493698, 0.142339289188385, 0.14462903141975403, 0.14282430708408356, 0.14219307899475098, 0.1439211517572403, 0.14397625625133514, 0.14380162954330444, 0.1446250081062317, 0.14302204549312592, 0.14394018054008484, 0.14360100030899048, 0.14504605531692505, 0.1439395248889923, 0.1427495777606964, 0.1434781700372696, 0.1427316963672638, 0.14311711490154266, 0.14415614306926727, 0.14441190659999847, 0.1451694816350937, 0.14475290477275848, 0.1425628811120987, 0.14458401501178741, 0.14342260360717773, 0.14198538661003113, 0.1425752341747284, 0.1430968940258026, 0.14436660706996918, 0.14511306583881378, 0.14429739117622375, 0.14344201982021332, 0.1421072632074356, 0.14450713992118835, 0.1448449045419693, 0.14489783346652985, 0.14422035217285156, 0.14475543797016144, 0.14275820553302765, 0.14414678514003754, 0.14509835839271545, 0.14384658634662628, 0.14608114957809448], "moving_avg_accuracy_train": [0.04684026696313214, 0.09554486527835454, 0.1433333969594984, 0.19047897447238946, 0.23696411648851334, 0.281920336980212, 0.3250287031356884, 0.36571127948140336, 0.4045364902710353, 0.4411971405125733, 0.47378042673240306, 0.5048090334800746, 0.5336114688006256, 0.5606870064962829, 0.5859659801210639, 0.6087265372226988, 0.6301992629070364, 0.6493197784883336, 0.6676394833983595, 0.684759442000661, 0.700346224908153, 0.7145584128178545, 0.7284000968567593, 0.7411015728726799, 0.7520633293739408, 0.7630144302797546, 0.7729194294640807, 0.7815594169751754, 0.789712151939941, 0.7979563132974881, 0.8047390758919272, 0.8113013642006672, 0.8177422800023613, 0.8237807394559826, 0.8295571858880606, 0.8345977333626082, 0.8396831414528425, 0.8432929270733851, 0.8477319219317997, 0.8512409530567261, 0.8545086154049387, 0.8563006717135128, 0.8588389316174199, 0.860844275576156, 0.8644160540389815, 0.8672496186413126, 0.8706390973571924, 0.8727759168169789, 0.8736200410878926, 0.8755819269638763, 0.8790075642093491, 0.8809443393671794, 0.8834477246211223, 0.8855078200472991, 0.8875245221522499, 0.8897931022598378, 0.8924763130863642, 0.8948749988862087, 0.8973615539417554, 0.8999249743250808, 0.901216070786768, 0.902059620361838, 0.9034488582091443, 0.9054569184418937, 0.9069549278597013, 0.9086125253226699, 0.9105927163869791, 0.9126446056067622, 0.9137030444093197, 0.9140603651875645, 0.9149584465974717, 0.9161969805426065, 0.9171605985182462, 0.9177675101272926, 0.9180809633527509, 0.9198582500866433, 0.9210462207590421, 0.9209296766178001, 0.9207014458573213, 0.921249386387176, 0.9225330834592834, 0.9235116995146563, 0.9239508198859573, 0.9245737486081866, 0.9249787076344111, 0.925856740254368, 0.926414418682558, 0.927518795151327, 0.9281802016446383, 0.9277128023850286, 0.9324957539894567, 0.9369818080894037, 0.9411006009388705, 0.9448168150986287, 0.9483009528198013, 0.9514459413152759, 0.9542160491897931, 0.9568068385756772, 0.9591105751396211, 0.961230441023361, 0.9631919068878023, 0.9649966815979238, 0.9666442303251285, 0.9680874606010319, 0.9694328708255356, 0.9706995796478363, 0.9718791811664875, 0.9729198240963504, 0.9738889908653791, 0.9748379348194004, 0.9756524207994389, 0.9763273655100543, 0.9771626442841226, 0.977919081527222, 0.9785928995995827, 0.97918077072305, 0.9797144329341522, 0.9802435930979628, 0.9806943327061252, 0.9810604708237094, 0.9814109214688209, 0.981723965851793, 0.9819824543083726, 0.9822592356978566, 0.9824874847067439, 0.982720774551638, 0.9829168205680043, 0.9830443978089153, 0.9832010700043065, 0.9833629652706255, 0.9835016955638841, 0.9836079876861594, 0.9837129151426265, 0.9838213367951228, 0.9839072544895031, 0.9839660152727879, 0.9840584275075062, 0.984139273369943, 0.9841655316699457, 0.9842031150328052, 0.9842508909522361, 0.9842729989892568, 0.9842997995713666, 0.9843355458393129, 0.9843723677780837, 0.9843706663396533, 0.9843970368307802, 0.9844300348192138, 0.9844458181647656, 0.9845064540543152, 0.984519245773976, 0.9845400589169088, 0.9845448398526911, 0.9845561541901425, 0.9845267735152682, 0.9845444366376247, 0.9845673809918116, 0.984601981803437, 0.9846307973850904], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04656776520143071, 0.09365762365869726, 0.14051965332031247, 0.18648083628459144, 0.23152860050063534, 0.27487008697730375, 0.3161704347366517, 0.3556266981624594, 0.39288397022309596, 0.42780814614882856, 0.4584392408901204, 0.48764208590878905, 0.5144464602433619, 0.5399519977657125, 0.5633077545497587, 0.5845536921971924, 0.604335392348708, 0.6214816547422257, 0.6383700456497803, 0.6535279759021968, 0.6671680541120525, 0.6796617920461032, 0.6914097329939778, 0.7025596988417939, 0.7114176455615302, 0.7203826851751814, 0.7284257772563079, 0.7355923474504813, 0.742583488034876, 0.7492162819271715, 0.7548003189735357, 0.7599896441921461, 0.7653640737394224, 0.7701878237920615, 0.7742982947543464, 0.7779122694016527, 0.7817354885269995, 0.784430727324902, 0.7872429492083757, 0.7898869007107309, 0.7921576232902602, 0.793318249327198, 0.7951268552585595, 0.7968787299266042, 0.7998501072163233, 0.8016311744785012, 0.804248348116871, 0.8057929927856056, 0.8062533795951474, 0.8073808830795333, 0.8098157698751192, 0.8111414982810561, 0.8129358869036282, 0.8138865094331448, 0.8153616868501465, 0.8167065540273306, 0.8182251987940253, 0.819709931361912, 0.8214827023446967, 0.8231880595104528, 0.8240839972999949, 0.8242138070485346, 0.8253489669592685, 0.826895513222679, 0.8276098410890406, 0.8281703754759949, 0.8296372999483502, 0.8308669352114217, 0.8317559394030054, 0.8317096399845423, 0.831998589860335, 0.8329279724499793, 0.8336973516451168, 0.8338851866048521, 0.8340176169748639, 0.8348936402453745, 0.8356394101157918, 0.8352566802770289, 0.8349732585783923, 0.8354495714159598, 0.8364530129471801, 0.8369095977123265, 0.8373317015235486, 0.8377552755353505, 0.8379086765868606, 0.838305144206789, 0.8385144511810649, 0.8395471716314825, 0.8396891929849005, 0.8393196124477056, 0.842569827172438, 0.8454025117179954, 0.8479753123628374, 0.8501433190595356, 0.8522176249077236, 0.8539878734297525, 0.855716403951988, 0.85708897595325, 0.8583456162909069, 0.8596007219246175, 0.860766938088707, 0.8619040408724568, 0.8628165405879219, 0.8634892469395212, 0.8641323332583704, 0.8647365545164941, 0.8652935901887151, 0.8657684492138948, 0.8664145193903969, 0.8667009547819294, 0.8669109480179684, 0.8672851064164728, 0.8676940616539671, 0.8680010862114619, 0.8683007928670476, 0.8684861091469844, 0.8687495205402679, 0.8690354189192231, 0.8691584501165327, 0.869282414734021, 0.8693817758585105, 0.8696054782143011, 0.8696837105133529, 0.8697174984887496, 0.8697611442065162, 0.8699224956650061, 0.8699700557276471, 0.8701725806989336, 0.8702561674144318, 0.8703680165521301, 0.8704076456198087, 0.8704066906869694, 0.8703803876762544, 0.8703577444752705, 0.8704584063982252, 0.8705500316375443, 0.8705592521654315, 0.8706407928280299, 0.8705422514782087, 0.8705034218970293, 0.870578338555218, 0.8706457635475878, 0.870595553250811, 0.8705747780462119, 0.8707035942457323, 0.8707198135579813, 0.8707242629250748, 0.8706916462617089, 0.87053919144352, 0.8706227381783095, 0.8707101372708701, 0.8707033472354246, 0.8707572418511141, 0.8707457413576443, 0.8707221543736118, 0.8707629907528921, 0.8707875364629944, 0.8707485924458366, 0.8708356131428945], "moving_var_accuracy_train": [0.0197460954825974, 0.039120727007762146, 0.05576234814914315, 0.070190462645445, 0.08261923223523338, 0.0925468648597941, 0.1000171594671663, 0.10491109168357406, 0.10798655545095079, 0.10928392939104717, 0.1079105713198926, 0.10578448411821811, 0.10267225822994705, 0.09900279508053296, 0.09485375414018107, 0.09003076536235399, 0.08517739036095195, 0.07994999836970838, 0.07497550282465142, 0.07011578938508689, 0.06529074065922577, 0.06057954315992919, 0.056245918797232126, 0.05207327435435597, 0.04794738786925681, 0.04423198858177497, 0.04069177080317098, 0.03729443818058074, 0.03416319814917407, 0.03135857410266011, 0.028636769508106796, 0.026160665207919362, 0.02391796725440804, 0.021854337462124503, 0.019969209716356053, 0.01820095281430896, 0.016613609911876038, 0.015069523890724922, 0.013739913579829706, 0.012476741916768052, 0.011325166280088582, 0.010221552844397623, 0.009257382430015905, 0.008367836826549867, 0.00764587155638242, 0.006953546195944431, 0.006361588670038608, 0.0057665237796682475, 0.005196284313764134, 0.004711296848101181, 0.0043457820781292, 0.003944963752424177, 0.0036068698167486843, 0.0032843787735584076, 0.0029925446826235833, 0.0027396083157021156, 0.002530444067188199, 0.0023291829025667635, 0.0021519112167084713, 0.0019958602115924566, 0.0018112765610936415, 0.001636553087954684, 0.001490267615326713, 0.0013775316068791832, 0.001259974736133829, 0.0011587059266636051, 0.0010781257438577758, 0.0010082054138043563, 0.0009174675067127538, 0.000826869859288568, 0.0007514418253291004, 0.0006901033397954487, 0.0006294500422426871, 0.0005698201133291778, 0.0005137223783172116, 0.0004907788736957227, 0.0004544024551924672, 0.00040908445250494095, 0.00036864481077470546, 0.00033448247911555294, 0.0003158651347604323, 0.00029289782573889113, 0.0002653434834694264, 0.00024230149685928779, 0.00021954727348964549, 0.00020453101767605513, 0.00018687696297186584, 0.00017916609313761405, 0.00016518661076840273, 0.00015063410830251525, 0.00034146033192497755, 0.0004884364312213331, 0.0005922728789305694, 0.0006573378201681994, 0.0007008569790922573, 0.000719789854913043, 0.0007168723481498917, 0.0007055948201129904, 0.0006828001575061599, 0.0006549646240409467, 0.0006240942966731682, 0.000590999772794501, 0.0005563295467916739, 0.0005194428147760683, 0.0004837896913482526, 0.0004498516833778754, 0.00041738965272532615, 0.0003853971268200557, 0.0003553109721737554, 0.00032788432660724263, 0.0003010663806516327, 0.0002750596958479581, 0.00025383294193684537, 0.00023359942346789071, 0.00021432575827286257, 0.00019600351456583716, 0.00017896632130927955, 0.00016358978348902906, 0.00014905930088942343, 0.00013535988489081415, 0.00012292923729366448, 0.00011151828463569145, 0.00010096780271178665, 9.156049387868962e-05, 8.28733229813429e-05, 7.507580804878493e-05, 6.79141336087042e-05, 6.12692038194197e-05, 5.536319902875611e-05, 5.00627698211892e-05, 4.522970768747881e-05, 4.0808419056051015e-05, 3.682666509053183e-05, 3.324979587404889e-05, 2.999125293851262e-05, 2.702320311153158e-05, 2.4397742990509e-05, 2.2016793172716594e-05, 1.9821319340316237e-05, 1.7851899988759155e-05, 1.6087252836180364e-05, 1.4482926440270532e-05, 1.3041098237056272e-05, 1.1748488574399503e-05, 1.058584241353312e-05, 9.5272842262144e-06, 8.580814428813451e-06, 7.732532791098083e-06, 6.961521537959559e-06, 6.298459784076904e-06, 5.670086458496128e-06, 5.106976494915176e-06, 4.596484561546253e-06, 4.137988233479314e-06, 3.7319584266360372e-06, 3.3615704569948717e-06, 3.0301514017968903e-06, 2.7379112071034326e-06, 2.471593126107266e-06], "duration": 97624.314976, "accuracy_train": [0.46840266963132154, 0.5338862501153562, 0.5734301820897932, 0.614789172088409, 0.6553303946336286, 0.6865263214055002, 0.713003998534976, 0.7318544665928387, 0.7539633873777224, 0.7711429926864157, 0.7670300027108711, 0.7840664942091177, 0.792833386685585, 0.8043668457571982, 0.8134767427440938, 0.8135715511374124, 0.8234537940660761, 0.8214044187200074, 0.8325168275885935, 0.8388390694213732, 0.8406272710755813, 0.842468104005168, 0.8529752532069029, 0.8554148570159652, 0.8507191378852897, 0.8615743384320783, 0.8620644221230158, 0.8593193045750278, 0.8630867666228312, 0.8721537655154117, 0.8657839392418788, 0.8703619589793282, 0.875710522217608, 0.8781268745385751, 0.8815452037767626, 0.8799626606335363, 0.8854518142649501, 0.8757809976582688, 0.8876828756575305, 0.8828222331810631, 0.883917576538852, 0.8724291784906791, 0.8816832707525839, 0.8788923712047803, 0.8965620602044113, 0.8927517000622923, 0.9011444058001107, 0.8920072919550572, 0.8812171595261166, 0.8932388998477298, 0.9098382994186047, 0.8983753157876523, 0.9059781919066077, 0.9040486788828904, 0.905674841096807, 0.9102103232281286, 0.9166252105251015, 0.9164631710848099, 0.9197405494416758, 0.9229957577750092, 0.9128359389419527, 0.9096515665374677, 0.9159519988349022, 0.9235294605366371, 0.9204370126199704, 0.9235309024893872, 0.9284144359657622, 0.9311116085848099, 0.9232289936323367, 0.9172762521917681, 0.9230411792866371, 0.9273437860488187, 0.9258331602990033, 0.9232297146087117, 0.9209020423818751, 0.9358538306916758, 0.9317379568106312, 0.9198807793466224, 0.9186473690130121, 0.9261808511558692, 0.9340863571082503, 0.9323192440130121, 0.927902903227667, 0.9301801071082503, 0.9286233388704319, 0.9337590338339794, 0.931433524536268, 0.9374581833702473, 0.9341328600844407, 0.9235062090485419, 0.9755423184293098, 0.9773562949889257, 0.9781697365840717, 0.9782627425364526, 0.9796581923103543, 0.9797508377745479, 0.9791470200604466, 0.9801239430486341, 0.9798442042151162, 0.980309233977021, 0.980845099667774, 0.981239653989018, 0.9814721688699704, 0.9810765330841639, 0.9815415628460686, 0.9820999590485419, 0.9824955948343485, 0.9822856104651162, 0.9826114917866371, 0.9833784304055924, 0.982982794619786, 0.9824018679055924, 0.9846801532507383, 0.9847270167151162, 0.9846572622508305, 0.9844716108342562, 0.9845173928340717, 0.9850060345722591, 0.9847509891795865, 0.9843557138819674, 0.9845649772748246, 0.9845413652985419, 0.9843088504175894, 0.9847502682032114, 0.9845417257867294, 0.9848203831556847, 0.9846812347153008, 0.9841925929771133, 0.9846111197628276, 0.9848200226674971, 0.9847502682032114, 0.9845646167866371, 0.9846572622508305, 0.9847971316675894, 0.9846805137389257, 0.9844948623223514, 0.9848901376199704, 0.9848668861318751, 0.9844018563699704, 0.9845413652985419, 0.9846808742271133, 0.9844719713224437, 0.9845410048103543, 0.9846572622508305, 0.984703765227021, 0.9843553533937799, 0.9846343712509228, 0.9847270167151162, 0.9845878682747323, 0.9850521770602622, 0.9846343712509228, 0.9847273772033037, 0.9845878682747323, 0.9846579832272055, 0.984262347441399, 0.9847034047388336, 0.9847738801794942, 0.9849133891080657, 0.9848901376199704], "end": "2016-02-02 12:53:56.700000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0], "moving_var_accuracy_valid": [0.019517010802700234, 0.0375224026481588, 0.05353461079939785, 0.06719302277474143, 0.07873743004504583, 0.0877700470906063, 0.09434451090693335, 0.0989212303279806, 0.10152204618778517, 0.10234712414583032, 0.10055678741669725, 0.09817636408968697, 0.09482499803192947, 0.0911972902274728, 0.08698698357932522, 0.08235079402006049, 0.07763755556601472, 0.0725197488360195, 0.06783473367943489, 0.06311912595732552, 0.05848167896373178, 0.05403835245544125, 0.04987664425852987, 0.04600787547834406, 0.042113256911316836, 0.03862527863765417, 0.035344972745918066, 0.0322727130264584, 0.029485326143849487, 0.026932739122823596, 0.024520098438157743, 0.02231045046036251, 0.020339365850953993, 0.018514846346991614, 0.01681542545607853, 0.015251430225233033, 0.013857840243033487, 0.012537435028329607, 0.01135486885279364, 0.010282296283435534, 0.00930047228439064, 0.008382548531130138, 0.00757373317675173, 0.006843981442749386, 0.006239045045455172, 0.0056436903462412724, 0.005140967692297545, 0.004648344267441642, 0.004185417444827079, 0.0037783170773100917, 0.0034538434329449467, 0.0031242770919072235, 0.0028408278574758475, 0.002564878220470885, 0.0023279757341284854, 0.0021114561702340424, 0.0019210670905573197, 0.0017488002586848767, 0.0016022046854330203, 0.0014681584044548839, 0.001328566904713961, 0.0011958618693799084, 0.001087872974648354, 0.0010006119252873396, 0.0009051431114645523, 0.0008174565895087218, 0.0007550777372262042, 0.0006931779894252864, 0.0006309731465566388, 0.0005678951246263252, 0.0005118570404401786, 0.0004684451043775647, 0.0004269280930530037, 0.00038455282149659224, 0.00034625537957304613, 0.0003185365925500262, 0.00029168848759162464, 0.00026383797799777755, 0.0002381771309313224, 0.00021640128311127367, 0.00020382320895934584, 0.00018531711489328583, 0.00016838894805099177, 0.00015316478773715715, 0.00013806009590688117, 0.00012566876547905826, 0.00011349617361647729, 0.00011174516001322617, 0.00010075217459534374, 9.190626509706858e-05, 0.00017779070039917108, 0.0002322285459710119, 0.000268579419796807, 0.00028402375514948006, 0.00029434608240097517, 0.00029311549262858554, 0.0002906943032624276, 0.00027858045802402133, 0.00026493471666565795, 0.000252618856365039, 0.0002395975120009869, 0.00022727478546819186, 0.00021204120849788877, 0.00019490989216743932, 0.00017914094307211316, 0.0001645125987238203, 0.00015085393751257937, 0.00013779796360547292, 0.0001277748273016146, 0.00011573575167315502, 0.00010455905093847865, 9.536309640917294e-05, 8.733198624472175e-05, 7.944716433039384e-05, 7.231086461197597e-05, 6.538885726326495e-05, 5.947444159594239e-05, 5.426263838415073e-05, 4.8972604625338604e-05, 4.421364920030593e-05, 3.988113797781365e-05, 3.634340887590833e-05, 3.27641506218519e-05, 2.949801020519934e-05, 2.6565353722793674e-05, 2.4143126988925833e-05, 2.1749171926058962e-05, 1.994340200940445e-05, 1.8011942459534023e-05, 1.6323340280015495e-05, 1.4705140419059593e-05, 1.3234634584224182e-05, 1.191739776115583e-05, 1.0730272415997408e-05, 9.748440578994449e-06, 8.84915318141746e-06, 7.965003026486372e-06, 7.228342640750649e-06, 6.592901955296581e-06, 5.947181387137959e-06, 5.40297579949154e-06, 4.9035933859069554e-06, 4.435923712438036e-06, 3.996215823329469e-06, 3.745936760326403e-06, 3.373710679102254e-06, 3.036517782999818e-06, 2.7424406252619603e-06, 2.677378807037037e-06, 2.4724614383793354e-06, 2.293962706965189e-06, 2.064981377500823e-06, 1.884624906153575e-06, 1.6973527676886657e-06, 1.532624603261543e-06, 1.3943706317899828e-06, 1.2603559955708485e-06, 1.147970124265297e-06, 1.1013265272867782e-06], "accuracy_test": 0.8601662149234693, "start": "2016-02-01 09:46:52.385000", "learning_rate_per_epoch": [0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 0.00016753317322582006, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753318050177768e-05, 1.6753317595430417e-06, 1.6753317311213323e-07, 1.6753316955941955e-08, 1.6753316511852745e-09, 1.67533167894085e-10, 1.675331748329789e-11, 1.6753317483297891e-12, 1.6753316941196805e-13, 1.6753317280009984e-14, 1.675331749176822e-15, 1.6753317756466017e-16, 1.6753317425593772e-17, 1.6753317012003466e-18, 1.6753316753509524e-19, 1.675331707662695e-20, 1.6753317480523735e-21, 1.6753317985394714e-22, 1.6753318300939076e-23, 1.675331908979998e-24, 1.6753318843280948e-25, 1.675331945957853e-26, 1.675331984476452e-27, 1.6753320326247005e-28, 1.6753320025320452e-29, 1.6753320401478644e-30, 1.6753320636577514e-31, 1.6753320342703926e-32, 1.6753319975361942e-33, 1.6753319975361942e-34, 1.6753319975361942e-35, 1.6753319975361942e-36, 1.6753320199569696e-37, 1.675332047982939e-38, 1.6753321881127853e-39, 1.6753363920081783e-40, 1.6753924439467513e-41, 1.6759529633324812e-42, 1.6815581571897805e-43, 1.6815581571897805e-44, 1.401298464324817e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.46840266963132154, "accuracy_train_last": 0.9848901376199704, "batch_size_eval": 1024, "accuracy_train_std": [0.016803219060367098, 0.014797096102389723, 0.014459355680240616, 0.014603221108082905, 0.015963656582350272, 0.016702583796106986, 0.017118090204923494, 0.017138303914977854, 0.01657248825229029, 0.01691139448878926, 0.016542542185116554, 0.01865506075010686, 0.016405486536585127, 0.013969536219164367, 0.015434161511185719, 0.01568915728196959, 0.015007610031963412, 0.01747562572560114, 0.016255270796724286, 0.01644505229283915, 0.017762975120286642, 0.015172104607631406, 0.014899513893219038, 0.01630846698647765, 0.016467485117939204, 0.016871172740758486, 0.017159943027379165, 0.018235627947609623, 0.016479022658968615, 0.01691042791150446, 0.01689393772465723, 0.016633104238827058, 0.016187858548738783, 0.017298876232210585, 0.015736558921823244, 0.016959522643040535, 0.016817447059235983, 0.01659865975879539, 0.016108963160316087, 0.016448647778064516, 0.016778641038285004, 0.018348335276673358, 0.017400644369470086, 0.01662834884317071, 0.01537533717436964, 0.0151920470072348, 0.015403513466905784, 0.01593552701232689, 0.017649881576874134, 0.01504895620563418, 0.01583446661626093, 0.016748157233313382, 0.014880334543070974, 0.015355914427041734, 0.01625406542781532, 0.015696433401778318, 0.015434087226737567, 0.014962098733331994, 0.01563892018389537, 0.01428149761392561, 0.015502879796890525, 0.01500578167351648, 0.014984293110617745, 0.01533388246591701, 0.01444517492056882, 0.015305559440521973, 0.01437739927404781, 0.01206397690587341, 0.014007048146464677, 0.0150530352674547, 0.014951247025596524, 0.01404032150129336, 0.013275274012937365, 0.013045943421831388, 0.014400602325664334, 0.012313076496852392, 0.013293565594063916, 0.01310903478010376, 0.014663216936307094, 0.013805212386218029, 0.014113753501903767, 0.013016649722028104, 0.013755200154052603, 0.012727651876035553, 0.012092295744519983, 0.013490918195784513, 0.014323902528054796, 0.01163052869742821, 0.012827521437351518, 0.013721149656636228, 0.007258297320404391, 0.00666769150714134, 0.0068013628599451455, 0.006274235205790823, 0.006387137969496326, 0.006156640834054619, 0.005828460308410509, 0.005903985404264073, 0.0057899835376563295, 0.006232057868387905, 0.005774096584778099, 0.005783021988559439, 0.005550274477917157, 0.005690916747577515, 0.006109586909194564, 0.005587293405666108, 0.005791995010652464, 0.005588028382202971, 0.00532293846295801, 0.005611387001294614, 0.00527329778685975, 0.005595743787377636, 0.005579311014852665, 0.0054850930833281126, 0.005224191181836235, 0.005504450461089652, 0.00546875505953102, 0.0053380687941627855, 0.00549078767076052, 0.005587549815371349, 0.005634949583823311, 0.0055582108151443835, 0.0056614782237543, 0.005435723084858575, 0.005465922482779121, 0.005546929942361381, 0.005501314115892269, 0.00546916188354297, 0.005294083104549211, 0.0052410862629317725, 0.005628632107493523, 0.005684711235144816, 0.005646139640113648, 0.005312510212158783, 0.0053707781094190065, 0.005406175596708361, 0.005604046185119934, 0.00540612753322222, 0.005531977239173788, 0.005529540308812832, 0.00532686778982641, 0.00558470902777268, 0.005498001215543734, 0.005532388912617617, 0.00537574452190742, 0.005411665975678334, 0.005426445106059332, 0.005243825088584784, 0.0056296986690326165, 0.00532933221672984, 0.005295139426474897, 0.005594350621321911, 0.00523685776125555, 0.005730968089027447, 0.00577982808800731, 0.005414809715798574, 0.005550467776975157, 0.0056843474001723, 0.005472852605269477], "accuracy_test_std": 0.00807910396835154, "error_valid": [0.5343223479856928, 0.48253365022590367, 0.43772207972515065, 0.39986851703689763, 0.3630415215549698, 0.3350565347326807, 0.3121264354292168, 0.2892669310052711, 0.2718005812311747, 0.25787427051957834, 0.265880906438253, 0.24953230892319278, 0.2443141707454819, 0.23049816453313254, 0.22649043439382532, 0.22423286897590367, 0.21762930628765065, 0.22420198371611444, 0.20963443618222888, 0.2100506518260542, 0.21007124199924698, 0.20789456654743976, 0.20285879847515065, 0.19709060852786142, 0.20886083396084332, 0.19893195830195776, 0.1991863940135542, 0.19990852080195776, 0.1944962467055723, 0.19108857304216864, 0.19494334760918675, 0.19330642884036142, 0.1862660603350903, 0.18639842573418675, 0.1887074665850903, 0.1895619587725903, 0.18385553934487953, 0.19131212349397586, 0.18744705384036142, 0.1863175357680723, 0.18740587349397586, 0.19623611634036142, 0.18859569135918675, 0.18735439806099397, 0.17340749717620485, 0.18233922016189763, 0.17219708913780118, 0.1803052051957832, 0.18960313911897586, 0.18247158556099397, 0.1682702489646084, 0.17692694606551207, 0.17091461549322284, 0.17755788780120485, 0.17136171639683728, 0.17118964137801207, 0.16810699830572284, 0.1669274755271084, 0.16256235881024095, 0.16146372599774095, 0.1678525625941265, 0.1746179052146084, 0.1644345938441265, 0.1591855704066265, 0.16596120811370485, 0.16678481504141573, 0.15716037980045183, 0.15806634742093373, 0.16024302287274095, 0.1687070547816265, 0.16540086125753017, 0.15870758424322284, 0.1593782355986446, 0.16442429875753017, 0.16479050969503017, 0.15722215032003017, 0.15764866105045183, 0.16818788827183728, 0.16757753670933728, 0.16026361304593373, 0.15451601327183728, 0.1589811394013554, 0.15886936417545183, 0.15843255835843373, 0.16071071394954817, 0.1581266472138554, 0.15960178605045183, 0.15115834431475905, 0.15903261483433728, 0.16400661238704817, 0.12817824030496983, 0.12910332737198793, 0.12886948183358427, 0.1303446206701807, 0.12911362245858427, 0.13007988987198793, 0.1287268213478916, 0.1305578760353916, 0.1303446206701807, 0.12910332737198793, 0.12873711643448793, 0.12786203407379515, 0.1289709619728916, 0.13045639589608427, 0.13007988987198793, 0.1298254541603916, 0.12969308876129515, 0.12995781955948793, 0.12777084902108427, 0.13072112669427716, 0.1311991128576807, 0.12934746799698793, 0.12862534120858427, 0.12923569277108427, 0.1290018472326807, 0.12984604433358427, 0.1288797769201807, 0.1283914956701807, 0.1297342691076807, 0.12960190370858427, 0.12972397402108427, 0.12838120058358427, 0.1296121987951807, 0.1299784097326807, 0.12984604433358427, 0.12862534120858427, 0.12960190370858427, 0.12800469455948793, 0.12899155214608427, 0.12862534120858427, 0.12923569277108427, 0.12960190370858427, 0.1298563394201807, 0.12984604433358427, 0.1286356362951807, 0.12862534120858427, 0.12935776308358427, 0.12862534120858427, 0.1303446206701807, 0.12984604433358427, 0.12874741152108427, 0.12874741152108427, 0.1298563394201807, 0.1296121987951807, 0.12813705995858427, 0.12913421263177716, 0.12923569277108427, 0.12960190370858427, 0.1308329019201807, 0.12862534120858427, 0.12850327089608427, 0.12935776308358427, 0.1287577066076807, 0.12935776308358427, 0.1294901284826807, 0.12886948183358427, 0.12899155214608427, 0.12960190370858427, 0.12838120058358427], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.07171534240907061, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.00016753317735982738, "optimization": "rmsprop", "nb_data_augmentation": 2, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.2870233256738096e-07, "rotation_range": [0, 0], "momentum": 0.7889922424846797}, "accuracy_valid_max": 0.8722291509789157, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8716187994164157, "accuracy_valid_std": [0.01845507606300423, 0.009394639028746142, 0.013201794881998338, 0.014030164891332702, 0.014685653284862606, 0.01368261447501889, 0.009722742892654497, 0.010227798095917312, 0.009779830105573463, 0.00963254374582004, 0.016061731142553576, 0.01027839376228731, 0.014346813323313817, 0.011389600458864797, 0.010867756372105972, 0.01222624685156977, 0.013621527934948809, 0.012649574798188625, 0.012773110891115222, 0.010504593152715501, 0.011255462455217405, 0.011316116016500977, 0.010795494222093728, 0.010902983762538874, 0.012003933664338023, 0.012739068480479504, 0.012385803977673733, 0.013956793879154137, 0.012196040062250706, 0.011642752289566308, 0.013213962894963655, 0.010669586600765234, 0.010973074041225326, 0.008381552246154719, 0.013104693051101068, 0.009406009307408042, 0.014968213738214838, 0.013132355419931048, 0.011889197103124716, 0.011645390123311426, 0.00861453403815552, 0.013127240492050964, 0.010660934563161481, 0.014231652065474045, 0.010446946933339826, 0.012271255232993813, 0.010163980601886462, 0.010874119668576196, 0.010003750391018765, 0.01160169138564589, 0.012113904621832733, 0.013464154735371194, 0.013938073265878911, 0.013319928252504038, 0.018467712587173002, 0.012881147316462655, 0.012841383644624721, 0.010193668311864015, 0.014929400695597849, 0.0146603797020111, 0.014371469426538375, 0.012152671854154591, 0.010596452265983953, 0.011993027693514381, 0.010702433214712563, 0.01081571213340843, 0.01646139658034687, 0.01379200872240594, 0.01325219609554942, 0.013188640572533553, 0.013352676368886221, 0.010130907430703006, 0.015464794717036783, 0.015016398614025876, 0.014628966594807306, 0.014012726626034254, 0.015539572230579238, 0.015398896561285019, 0.01514174313702843, 0.013020818705427595, 0.010882332496096181, 0.01765907902779898, 0.017748611260721615, 0.011556866379017865, 0.014565861111925439, 0.016179572973595766, 0.016127462970669623, 0.01636826531976283, 0.015404835555010675, 0.01732187141782827, 0.015440952824951861, 0.01994669806003535, 0.01896128471403194, 0.017792823374398557, 0.01819661659910923, 0.019199927048943834, 0.01965779719289266, 0.02033850653894132, 0.017283031279261354, 0.01900406015346661, 0.018537550271818, 0.02050116889495737, 0.019494093870783488, 0.01709965126484811, 0.018709370633851988, 0.019899433585419783, 0.020596753800558074, 0.019524020782594288, 0.01802728883490394, 0.016958704379836962, 0.017400765125799165, 0.019354284473858757, 0.01855246536428645, 0.01818498552607301, 0.017152446782560598, 0.017969019718670556, 0.01748651393851674, 0.017176247583373935, 0.01720215508614548, 0.018145120926427588, 0.018410731815302835, 0.017899546492059788, 0.017519972514196167, 0.01780860240042282, 0.019163285870718045, 0.017663732983555242, 0.01864413162433426, 0.019043248569933523, 0.018467456607994107, 0.018896258210675802, 0.018263480542675965, 0.01895484514316794, 0.01814372882590333, 0.019094735426099643, 0.017112306551051934, 0.01785837623380924, 0.018290223443507534, 0.018208728302675767, 0.01807862169895456, 0.018459877114504, 0.018602037268079546, 0.018029280505945953, 0.01809767846034967, 0.017863616219354044, 0.018076324527019284, 0.01693124743078216, 0.019464178676789845, 0.018438390642739153, 0.018580114255346822, 0.018410561163441322, 0.018745192859678662, 0.017941462842283786, 0.01741417268565829, 0.01808045572108686, 0.018129637832173046, 0.017786977123423715, 0.01835089695748731, 0.01802647716584836, 0.0183792905438655], "accuracy_valid": [0.4656776520143072, 0.5174663497740963, 0.5622779202748494, 0.6001314829631024, 0.6369584784450302, 0.6649434652673193, 0.6878735645707832, 0.7107330689947289, 0.7281994187688253, 0.7421257294804217, 0.734119093561747, 0.7504676910768072, 0.7556858292545181, 0.7695018354668675, 0.7735095656061747, 0.7757671310240963, 0.7823706937123494, 0.7757980162838856, 0.7903655638177711, 0.7899493481739458, 0.789928758000753, 0.7921054334525602, 0.7971412015248494, 0.8029093914721386, 0.7911391660391567, 0.8010680416980422, 0.8008136059864458, 0.8000914791980422, 0.8055037532944277, 0.8089114269578314, 0.8050566523908133, 0.8066935711596386, 0.8137339396649097, 0.8136015742658133, 0.8112925334149097, 0.8104380412274097, 0.8161444606551205, 0.8086878765060241, 0.8125529461596386, 0.8136824642319277, 0.8125941265060241, 0.8037638836596386, 0.8114043086408133, 0.812645601939006, 0.8265925028237951, 0.8176607798381024, 0.8278029108621988, 0.8196947948042168, 0.8103968608810241, 0.817528414439006, 0.8317297510353916, 0.8230730539344879, 0.8290853845067772, 0.8224421121987951, 0.8286382836031627, 0.8288103586219879, 0.8318930016942772, 0.8330725244728916, 0.837437641189759, 0.838536274002259, 0.8321474374058735, 0.8253820947853916, 0.8355654061558735, 0.8408144295933735, 0.8340387918862951, 0.8332151849585843, 0.8428396201995482, 0.8419336525790663, 0.839756977127259, 0.8312929452183735, 0.8345991387424698, 0.8412924157567772, 0.8406217644013554, 0.8355757012424698, 0.8352094903049698, 0.8427778496799698, 0.8423513389495482, 0.8318121117281627, 0.8324224632906627, 0.8397363869540663, 0.8454839867281627, 0.8410188605986446, 0.8411306358245482, 0.8415674416415663, 0.8392892860504518, 0.8418733527861446, 0.8403982139495482, 0.848841655685241, 0.8409673851656627, 0.8359933876129518, 0.8718217596950302, 0.8708966726280121, 0.8711305181664157, 0.8696553793298193, 0.8708863775414157, 0.8699201101280121, 0.8712731786521084, 0.8694421239646084, 0.8696553793298193, 0.8708966726280121, 0.8712628835655121, 0.8721379659262049, 0.8710290380271084, 0.8695436041039157, 0.8699201101280121, 0.8701745458396084, 0.8703069112387049, 0.8700421804405121, 0.8722291509789157, 0.8692788733057228, 0.8688008871423193, 0.8706525320030121, 0.8713746587914157, 0.8707643072289157, 0.8709981527673193, 0.8701539556664157, 0.8711202230798193, 0.8716085043298193, 0.8702657308923193, 0.8703980962914157, 0.8702760259789157, 0.8716187994164157, 0.8703878012048193, 0.8700215902673193, 0.8701539556664157, 0.8713746587914157, 0.8703980962914157, 0.8719953054405121, 0.8710084478539157, 0.8713746587914157, 0.8707643072289157, 0.8703980962914157, 0.8701436605798193, 0.8701539556664157, 0.8713643637048193, 0.8713746587914157, 0.8706422369164157, 0.8713746587914157, 0.8696553793298193, 0.8701539556664157, 0.8712525884789157, 0.8712525884789157, 0.8701436605798193, 0.8703878012048193, 0.8718629400414157, 0.8708657873682228, 0.8707643072289157, 0.8703980962914157, 0.8691670980798193, 0.8713746587914157, 0.8714967291039157, 0.8706422369164157, 0.8712422933923193, 0.8706422369164157, 0.8705098715173193, 0.8711305181664157, 0.8710084478539157, 0.8703980962914157, 0.8716187994164157], "seed": 395133561, "model": "residualv3", "loss_std": [0.30645155906677246, 0.18087968230247498, 0.17923280596733093, 0.17792657017707825, 0.17609131336212158, 0.1741088628768921, 0.1725967526435852, 0.17261725664138794, 0.17234990000724792, 0.16861861944198608, 0.1685955673456192, 0.16839498281478882, 0.16809241473674774, 0.16394859552383423, 0.16423682868480682, 0.16382142901420593, 0.16152216494083405, 0.16313937306404114, 0.15933696925640106, 0.1576818972826004, 0.15418946743011475, 0.1534150093793869, 0.15381835401058197, 0.1479906588792801, 0.14922718703746796, 0.1484728753566742, 0.14682692289352417, 0.14619864523410797, 0.14310689270496368, 0.14512477815151215, 0.14332719147205353, 0.142296701669693, 0.14170144498348236, 0.1375385820865631, 0.13896891474723816, 0.13679851591587067, 0.1355391889810562, 0.1338268667459488, 0.1302827149629593, 0.13302896916866302, 0.13333609700202942, 0.13187679648399353, 0.12923114001750946, 0.127838134765625, 0.1273491531610489, 0.12709499895572662, 0.12726913392543793, 0.12320362031459808, 0.12214689701795578, 0.12203159928321838, 0.12269908934831619, 0.12076503038406372, 0.12087453901767731, 0.11969294399023056, 0.11598324775695801, 0.11486990749835968, 0.11418908089399338, 0.11663611233234406, 0.11511297523975372, 0.11310166865587234, 0.11300579458475113, 0.11196091771125793, 0.11147189885377884, 0.11204077303409576, 0.10930374264717102, 0.11081265658140182, 0.10968886315822601, 0.10449825972318649, 0.10578140616416931, 0.10412848740816116, 0.10421878844499588, 0.10334522277116776, 0.10491842776536942, 0.10305988043546677, 0.10135776549577713, 0.10037818551063538, 0.1014959067106247, 0.0983482226729393, 0.09954500198364258, 0.09731390327215195, 0.09699337184429169, 0.09639320522546768, 0.09677675366401672, 0.09465906023979187, 0.09504133462905884, 0.09403157979249954, 0.09310490638017654, 0.09051202237606049, 0.09322512149810791, 0.09374666213989258, 0.08427319675683975, 0.07832624763250351, 0.07859007269144058, 0.07517275214195251, 0.07491657137870789, 0.07403424382209778, 0.0724698081612587, 0.07298226654529572, 0.07264416664838791, 0.07310233265161514, 0.07196933776140213, 0.07050049304962158, 0.07274466007947922, 0.07008472830057144, 0.07136701047420502, 0.07279926538467407, 0.07251374423503876, 0.07020290940999985, 0.06902377307415009, 0.07253390550613403, 0.07163678854703903, 0.07107848674058914, 0.06726985424757004, 0.06874042749404907, 0.07045762985944748, 0.069762222468853, 0.06823980063199997, 0.06745819747447968, 0.07002802938222885, 0.06800520420074463, 0.06917063146829605, 0.06749559938907623, 0.06874728202819824, 0.06628383696079254, 0.06957711279392242, 0.06891094893217087, 0.06682246178388596, 0.06913740932941437, 0.06936126202344894, 0.06752815842628479, 0.0684165433049202, 0.0683903694152832, 0.06555101275444031, 0.06774336099624634, 0.06805083900690079, 0.0681791752576828, 0.070785291492939, 0.06935905665159225, 0.0680156871676445, 0.06922300904989243, 0.06944043189287186, 0.06726545095443726, 0.06782117486000061, 0.0686536654829979, 0.06760670989751816, 0.06765797734260559, 0.06939498335123062, 0.06894268840551376, 0.0686424970626831, 0.06963270902633667, 0.06861709803342819, 0.0698583796620369, 0.06880135834217072, 0.06864120811223984, 0.06723666936159134, 0.0681983008980751, 0.06913977116346359, 0.06933479011058807, 0.07014095783233643]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:30 2016", "state": "available"}], "summary": "ff4dc7a17666f492ff173d47cc5ea67f"}