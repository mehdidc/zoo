{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 16, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5138301849365234, 1.1253596544265747, 0.9746291041374207, 0.8729380965232849, 0.7998354434967041, 0.7417914867401123, 0.6936002373695374, 0.6518222689628601, 0.614989697933197, 0.5802363753318787, 0.5490210056304932, 0.5206444263458252, 0.49270200729370117, 0.4695046544075012, 0.4442586302757263, 0.4199402630329132, 0.3989676833152771, 0.38180115818977356, 0.3613119125366211, 0.34657686948776245, 0.33044150471687317, 0.32230931520462036, 0.30669519305229187, 0.2949385643005371, 0.2818523049354553, 0.2788824141025543, 0.26543980836868286, 0.25424227118492126, 0.24928559362888336, 0.24003015458583832, 0.2376478761434555, 0.2286873608827591, 0.2270059585571289, 0.21695606410503387, 0.2146269977092743, 0.21123924851417542, 0.2045575976371765, 0.20384322106838226, 0.19945894181728363, 0.1946163922548294, 0.19514970481395721, 0.19073103368282318, 0.19041232764720917, 0.18758922815322876, 0.18605872988700867, 0.1847737729549408, 0.18143229186534882, 0.1778697967529297, 0.17851026356220245, 0.17719916999340057, 0.17472927272319794, 0.176930233836174, 0.1726575642824173, 0.17110125720500946, 0.17275138199329376, 0.17545360326766968, 0.17036856710910797, 0.17048560082912445, 0.16674481332302094, 0.16652455925941467, 0.16910569369792938, 0.16650338470935822, 0.1637870967388153, 0.16573333740234375, 0.16856226325035095, 0.1654578596353531, 0.16463755071163177, 0.16381147503852844, 0.16230373084545135, 0.1648041158914566, 0.16413164138793945, 0.1584477722644806, 0.16362492740154266, 0.16145576536655426, 0.16245616972446442, 0.15851262211799622, 0.16010817885398865, 0.15979914367198944, 0.1588822305202484, 0.16153880953788757, 0.16179701685905457, 0.16185534000396729, 0.15903906524181366, 0.15922324359416962, 0.16298401355743408, 0.16005457937717438, 0.16083480417728424, 0.15918031334877014, 0.16059258580207825, 0.16032297909259796, 0.15915314853191376, 0.1580258011817932, 0.1574852168560028, 0.15562313795089722, 0.1571006029844284, 0.16209453344345093, 0.15793254971504211, 0.15803416073322296, 0.1562928706407547, 0.16070926189422607, 0.15681494772434235, 0.1570964753627777, 0.15726745128631592, 0.15998157858848572, 0.16010044515132904, 0.1556500494480133, 0.15828849375247955, 0.15958009660243988, 0.15906354784965515, 0.1574769914150238, 0.15933942794799805, 0.1584392786026001, 0.15759283304214478, 0.1606447696685791, 0.1573207974433899, 0.1572052240371704, 0.1579841524362564, 0.157811239361763, 0.1559402495622635, 0.15840160846710205, 0.1588318943977356, 0.15640002489089966, 0.15799649059772491, 0.1628408432006836, 0.1557421088218689, 0.15904316306114197, 0.15686668455600739, 0.1570640206336975, 0.1561727523803711, 0.15694233775138855, 0.15515297651290894, 0.15494762361049652, 0.15539968013763428, 0.15703845024108887, 0.15730491280555725, 0.15878556668758392, 0.15549275279045105, 0.1563614308834076, 0.15491032600402832, 0.15600010752677917, 0.15398098528385162, 0.15556015074253082, 0.15595868229866028, 0.15503814816474915, 0.15520688891410828, 0.15695422887802124, 0.15623295307159424, 0.15431858599185944, 0.15668423473834991, 0.15459176898002625, 0.15350784361362457, 0.1538202464580536, 0.1553564816713333, 0.15736709535121918, 0.15590956807136536, 0.1571587324142456, 0.1554354578256607, 0.15431897342205048, 0.15444402396678925, 0.15888506174087524, 0.1539231687784195, 0.15420906245708466, 0.152847558259964, 0.15635830163955688, 0.15784180164337158, 0.1537124514579773, 0.1585790067911148, 0.1533399522304535, 0.15507279336452484, 0.1526852399110794, 0.15532799065113068, 0.15394581854343414, 0.15214960277080536, 0.157194122672081], "moving_avg_accuracy_train": [0.0566892188076781, 0.11672893018641563, 0.17536352638196748, 0.23151227505537092, 0.2837685433970099, 0.33171961939368766, 0.37528717517880206, 0.4164716301876845, 0.4548950219169781, 0.4905434619721888, 0.5220224472337647, 0.5509811078548419, 0.5773785435982891, 0.6009505483019983, 0.6232856056268834, 0.6441939478073659, 0.6628997404804868, 0.6804581833125137, 0.6962441453314838, 0.7109283108497843, 0.725315898767436, 0.7377646046063624, 0.7503125200685759, 0.7613312403762255, 0.7716826751875786, 0.7804136418477152, 0.7890175241215077, 0.7967631630726367, 0.8047109087751091, 0.8122893460906583, 0.8195958236782056, 0.8257766305511104, 0.8329432407806523, 0.8388746457538975, 0.8447384659584081, 0.8494766138139331, 0.853843217382706, 0.8583239324719085, 0.8628937935736471, 0.8669300828497727, 0.8710463741506665, 0.8754996621405001, 0.8792218082718728, 0.8834780791911787, 0.8869298679578765, 0.888987907832447, 0.8919748884362454, 0.8948818431142155, 0.8972468060088662, 0.8992940366009936, 0.9005205238946594, 0.9038190145445992, 0.9070759024842885, 0.9087608939657416, 0.9112283361133258, 0.9134232411163343, 0.9151895724702785, 0.9167309111984722, 0.9194103580596603, 0.921243078425252, 0.9221461539864274, 0.9242422238902487, 0.9254408392930751, 0.9271006280162685, 0.9278504623457324, 0.9295436202743652, 0.9312256446268199, 0.9322815203750091, 0.9339989634602472, 0.9343355488071904, 0.9350870851444023, 0.9365284778550451, 0.9368912377661076, 0.9373805902491867, 0.9378092014958166, 0.9387740218178112, 0.9395839069479994, 0.9396178003640338, 0.9406269397455429, 0.9412447378806232, 0.9402665184760972, 0.9409947635000268, 0.9421779928013252, 0.9421430677367704, 0.9424420586513547, 0.9433037389815403, 0.9435842108751914, 0.9450546891162991, 0.9459387745547522, 0.9454767622363202, 0.9468698087770846, 0.9478863133875637, 0.9491266523215094, 0.9494245049811082, 0.9506203427985659, 0.9503158549784269, 0.9508063212639913, 0.9508389473163481, 0.9508542517241558, 0.9519143787042873, 0.9523241918720645, 0.9534137477563789, 0.9532479775915291, 0.9534477009598679, 0.9538435826842022, 0.9551439947992261, 0.9557144052122529, 0.9564580364137575, 0.9568248549057796, 0.9574619111914567, 0.9585167118474126, 0.9581592627580017, 0.9580539416120926, 0.9579846571200417, 0.9591451491069116, 0.9591830187534833, 0.9587055326484838, 0.9587153203766217, 0.9594146985283744, 0.959627793032772, 0.9594776009676361, 0.9601517961876407, 0.9599821163785388, 0.9607338153396144, 0.9606872231248206, 0.9610104466410575, 0.9612941921151484, 0.9607403031096597, 0.9609163124523944, 0.961530486076341, 0.9616811357891203, 0.9627928864936155, 0.9628960628336412, 0.9634910815848838, 0.9638196962657734, 0.9639851329987844, 0.9642294292573224, 0.9643235936590173, 0.9639294691122371, 0.9646282296749429, 0.9641361761622751, 0.9646792992425685, 0.9639197213970307, 0.9646146261895443, 0.9646636198909572, 0.9643450991543993, 0.964276922381955, 0.9650547617628994, 0.9652479347652732, 0.9650336708603128, 0.9654405775434306, 0.9663320608962765, 0.9662346714710084, 0.9662561587870397, 0.9660640530250763, 0.966051917546535, 0.9663102081942901, 0.9668730572011347, 0.9670867967525698, 0.9672604169631102, 0.9674817072216256, 0.9675950542412497, 0.9679922162624459, 0.9684239226481522, 0.9682570362203617, 0.967736995579361, 0.9680014529750978, 0.9674814300705373, 0.9679108447992717, 0.9685367723336948, 0.9685631779837695, 0.9685773901318673, 0.9689134848961353, 0.9691788038518429], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.056678716820406615, 0.11691645978445027, 0.17528802563770704, 0.23021104206866524, 0.28034770565170836, 0.32588265341296824, 0.3672404653495328, 0.4058470381669892, 0.4410324068276999, 0.47345004458054735, 0.5016462675321312, 0.5273677535895355, 0.5502963349700397, 0.5704375999105358, 0.5889838954541207, 0.6067050701029857, 0.6215360808825968, 0.6357711329035239, 0.6479235000348582, 0.6586347268386012, 0.6690804950244701, 0.6777473525720984, 0.6867814640298735, 0.6945916930033922, 0.7013095462463511, 0.7068043858141105, 0.7122572891942959, 0.7166807390211012, 0.7216150218113857, 0.7256692222846146, 0.7291348972417706, 0.7320657516911628, 0.7357025853134622, 0.7385708445249624, 0.7415612868985204, 0.7437857293212438, 0.7457388993766947, 0.746787715105441, 0.7490286830997914, 0.7503863746072068, 0.7523560143961096, 0.754717716232101, 0.7568625144036952, 0.7586728214629491, 0.7606581607398469, 0.7613401562245972, 0.7627627047493513, 0.7640938855639492, 0.7645808519586085, 0.7654056286791633, 0.7655193391546204, 0.766766050994053, 0.7683784319168616, 0.7688561007733682, 0.7696511841730644, 0.7709843173559086, 0.7712858173786008, 0.7716109959946564, 0.7724998893357029, 0.7738655348147833, 0.7738789130915881, 0.7746467599695528, 0.7741190309793746, 0.7743309042368589, 0.7744359938771791, 0.7749925297963136, 0.7759674273249654, 0.7766077539637037, 0.7772930287838243, 0.7771802957002462, 0.7773219470413661, 0.7775888581354222, 0.777444483099365, 0.7777113476188713, 0.7774001502628576, 0.7772492024428972, 0.777701345922252, 0.7778061877984004, 0.7778058306902471, 0.7781596602718248, 0.7765401303798081, 0.7769522882755622, 0.7772642542428102, 0.776999825241722, 0.7767974307258328, 0.7773547570244845, 0.7777882560490692, 0.7789718622024454, 0.7794857323169146, 0.7782717341040485, 0.779482293656746, 0.7805117916065835, 0.7813487725080487, 0.7816737797169577, 0.7825441347083644, 0.7819265870601937, 0.7823810363172767, 0.7819862885308051, 0.7822018045384174, 0.7827843349279491, 0.7823655823462988, 0.7829867401320003, 0.7827827691496738, 0.78286774995308, 0.7829554101987358, 0.783354775758305, 0.7834090289806673, 0.7839604041793626, 0.7836234752072095, 0.7843069496462025, 0.7842527489398654, 0.7833300625247945, 0.7833257519142579, 0.7830278741061152, 0.7838228253061965, 0.7837580608949293, 0.7833559170324695, 0.7830916438062557, 0.7836310769406905, 0.783685202633293, 0.7833737347984275, 0.7835084528095486, 0.7829990514291962, 0.7844500346051773, 0.7846062524360752, 0.7845190329247719, 0.7846114338020989, 0.7838736349719644, 0.7835900930109125, 0.7836014009161466, 0.7830632911332669, 0.7838892627070637, 0.7841362668768694, 0.7845172620359445, 0.7856597917623199, 0.7854072126708018, 0.785725089860047, 0.7850397643736657, 0.7847291767258323, 0.7855615171951918, 0.7851305125483383, 0.7858128561541973, 0.7854096637710818, 0.7859298149109164, 0.7858476050218579, 0.785048253734657, 0.7857133796507244, 0.7859112199612543, 0.7863487124229602, 0.7860109161994896, 0.7863153392162123, 0.7865781424086724, 0.7861788701482268, 0.7857715794248199, 0.78583035192285, 0.7855576282893, 0.7854077742517857, 0.7855863174685499, 0.7861854299799781, 0.7864051894104442, 0.7866294459776829, 0.7864507999021285, 0.7862279537692198, 0.786469933900581, 0.7852522868019385, 0.7850822268435067, 0.7848243100703006, 0.7846848407537826, 0.7849112632578472, 0.7851882856990052, 0.7852840620330956, 0.786114889640027, 0.786308170536717, 0.7862675443245363], "moving_var_accuracy_train": [0.028923007761223245, 0.058473609467079884, 0.08356839135951072, 0.1035856900218609, 0.11780357924861523, 0.1267169725269062, 0.1311284625280171, 0.13328105028462334, 0.1332401585440057, 0.13135344419493464, 0.12713643839332778, 0.12197023077869554, 0.11604462922529078, 0.10944092095452694, 0.10298652193042737, 0.09662229869200996, 0.09010922893857702, 0.08387299627688936, 0.07772846602097974, 0.07189624187160137, 0.06656964185923464, 0.06130741016688826, 0.05659372079222114, 0.05202705848796292, 0.04778872246304986, 0.043695918226128616, 0.039992567515147286, 0.03653326506848379, 0.03344843851739593, 0.030620489074967724, 0.028038901700106932, 0.025578832892497565, 0.02348319232288739, 0.021451507175208395, 0.019615815944204994, 0.01785628475569184, 0.01624226132066403, 0.014798726457993102, 0.013506806486596447, 0.012302750518021894, 0.01122497015288404, 0.010280959102879208, 0.00937755253900092, 0.008602839864347588, 0.007849789489121947, 0.007102930293337648, 0.006472935742151099, 0.005901695637433942, 0.0053618635191282245, 0.0048633975450914834, 0.004390596230316049, 0.0040494569723941035, 0.00373997714661993, 0.0033915321985910603, 0.0031071734154970287, 0.002839814545697412, 0.00258391242919501, 0.0023469027119507753, 0.002176827360093081, 0.0019893743995298653, 0.0017977768687996096, 0.001657540763294996, 0.0015047167969205303, 0.0013790392046792376, 0.001246195547906098, 0.0011473770470571151, 0.0010581021956516557, 0.0009623258384470167, 0.0008926397513616044, 0.0008043953834874356, 0.0007290391069340415, 0.0006748337127572856, 0.0006085346942592237, 0.0005498364175075631, 0.0004965061441634453, 0.00045523343403070394, 0.00041561331594453394, 0.0003740623232229348, 0.0003458213515224553, 0.00031467428719158754, 0.00029181907730294884, 0.00026741023690655784, 0.00025326949743096505, 0.000227953525529076, 0.0002059627330792041, 0.0001920488966941438, 0.00017355198737288336, 0.0001756575449537348, 0.00016512625402072533, 0.00015053472706009924, 0.000152946462336711, 0.0001469513507111664, 0.00014610218167960442, 0.00013229040937311453, 0.00013193162120675981, 0.00011957287457960069, 0.00010978060171711942, 9.881212167903903e-05, 8.893301753522025e-05, 9.015453870772248e-05, 8.265060632930288e-05, 8.506973392176977e-05, 7.681007825758167e-05, 6.94880752465688e-05, 6.394976877886943e-05, 7.277443692108977e-05, 6.842530558258552e-05, 6.655966129898603e-05, 6.111469742389236e-05, 5.8655794081589654e-05, 6.280365448767437e-05, 5.767321770259298e-05, 5.2005728826314066e-05, 4.6848359011231425e-05, 5.428419797441278e-05, 4.886868516815467e-05, 4.603375347554755e-05, 4.1431240324591726e-05, 4.169028448447442e-05, 3.792993944626751e-05, 3.433996440950859e-05, 3.499682072065069e-05, 3.1756259787137114e-05, 3.366609576116273e-05, 3.0319023695361034e-05, 2.8227382298862603e-05, 2.612924751558045e-05, 2.6277460037633e-05, 2.3928527632438655e-05, 2.4930558032360125e-05, 2.264176025276862e-05, 3.150149088800065e-05, 2.8447150013470413e-05, 2.878886084109773e-05, 2.688186323345358e-05, 2.4440000723772645e-05, 2.2533126608816133e-05, 2.0359616358853475e-05, 1.972166214834094e-05, 2.2143892849442927e-05, 2.2108553498456923e-05, 2.2552542271737148e-05, 2.5489914575449573e-05, 2.7286957153830206e-05, 2.4579864883450376e-05, 2.3034977531661706e-05, 2.0773312429203913e-05, 2.41412881092152e-05, 2.2063001577908555e-05, 2.02698826088373e-05, 1.973305178684729e-05, 2.4912429723775594e-05, 2.250654905278449e-05, 2.026004949025811e-05, 1.8566186155248376e-05, 1.671089296827836e-05, 1.564023019991046e-05, 1.6927398220472076e-05, 1.5645819761053828e-05, 1.435253358252112e-05, 1.3358004630893701e-05, 1.2137832089523256e-05, 1.234368792029658e-05, 1.278665275940345e-05, 1.1758647201489141e-05, 1.3016762895972373e-05, 1.2344526033814353e-05, 1.3543887821839394e-05, 1.3849072122941729e-05, 1.5990232415789e-05, 1.4397484499412857e-05, 1.2959553915853548e-05, 1.2680235739383827e-05, 1.2045759499765472e-05], "duration": 99223.121569, "accuracy_train": [0.5668921880767811, 0.6570863325950536, 0.7030748921419343, 0.7368510131160022, 0.7540749584717608, 0.7632793033637875, 0.767395177244832, 0.7871317252676264, 0.8007055474806202, 0.8113794224690846, 0.8053333145879475, 0.8116090534445367, 0.8149554652893135, 0.813098590635382, 0.824301121550849, 0.8323690274317092, 0.8312518745385751, 0.8384841688007567, 0.838317803502215, 0.8430858005144887, 0.8548041900263011, 0.8498029571567, 0.8632437592284975, 0.8604997231450721, 0.8648455884897563, 0.8589923417889442, 0.8664524645856404, 0.8664739136327981, 0.8762406200973607, 0.8804952819306018, 0.8853541219661315, 0.8814038924072536, 0.8974427328465301, 0.8922572905131044, 0.8975128477990033, 0.8921199445136582, 0.8931426495016611, 0.8986503682747323, 0.9040225434892949, 0.9032566863349022, 0.9080929958587117, 0.9155792540490033, 0.9127211234542267, 0.9217845174649317, 0.917995966858158, 0.9075102667035806, 0.9188577138704319, 0.9210444352159468, 0.9185314720607235, 0.9177191119301403, 0.9115589095376523, 0.9335054303940569, 0.9363878939414912, 0.9239258172988187, 0.9334353154415835, 0.933177386143411, 0.9310865546557769, 0.930602959752215, 0.9435253798103543, 0.9377375617155776, 0.9302738340370063, 0.94310685302464, 0.9362283779185124, 0.9420387265250092, 0.9345989713109081, 0.9447820416320598, 0.946363863798911, 0.9417844021087117, 0.9494559512273901, 0.9373648169296788, 0.9418509121793098, 0.9495010122508305, 0.9401560769656699, 0.9417847625968992, 0.9416667027154854, 0.9474574047157622, 0.9468728731196937, 0.9399228411083426, 0.9497091941791252, 0.9468049210963455, 0.9314625438353636, 0.9475489687153931, 0.9528270565130121, 0.9418287421557769, 0.9451329768826136, 0.9510588619532114, 0.946108457918051, 0.958288993286268, 0.9538955435008305, 0.9413186513704319, 0.9594072276439645, 0.9570348548818751, 0.960289702727021, 0.9521051789174971, 0.9613828831556847, 0.9475754645971761, 0.9552205178340717, 0.95113258178756, 0.950991991394426, 0.9614555215254706, 0.9560125103820598, 0.9632197507152085, 0.9517560461078812, 0.9552452112749169, 0.9574065182032114, 0.9668477038344407, 0.9608480989294942, 0.9631507172272978, 0.9601262213339794, 0.9631954177625508, 0.9680099177510151, 0.9549422209533037, 0.957106051298911, 0.9573610966915835, 0.9695895769887413, 0.9595238455726283, 0.9544081577034883, 0.9588034099298633, 0.9657091018941492, 0.9615456435723514, 0.9581258723814139, 0.9662195531676817, 0.9584549980966224, 0.9674991059892949, 0.9602678931916758, 0.9639194582871908, 0.9638479013819674, 0.9557553020602622, 0.9625003965370063, 0.9670580486918604, 0.9630369832041344, 0.9727986428340717, 0.9638246498938722, 0.9688462503460686, 0.9667772283937799, 0.9654740635958842, 0.9664280955841639, 0.965171073274271, 0.9603823481912146, 0.9709170747392949, 0.9597076945482651, 0.9695674069652085, 0.9570835207871908, 0.9708687693221669, 0.9651045632036729, 0.9614784125253784, 0.9636633314299556, 0.972055316191399, 0.9669864917866371, 0.9631052957156699, 0.9691027376914912, 0.97435541107189, 0.9653581666435955, 0.9664495446313216, 0.9643351011674051, 0.965942698239664, 0.9686348240240864, 0.9719386982627353, 0.9690104527154854, 0.9688229988579733, 0.9694733195482651, 0.9686151774178663, 0.9715666744532114, 0.9723092801195091, 0.9667550583702473, 0.9630566298103543, 0.9703815695367294, 0.9628012239294942, 0.9717755773578812, 0.9741701201435032, 0.9688008288344407, 0.9687052994647471, 0.9719383377745479, 0.9715666744532114], "end": "2016-02-02 13:21:00.932000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0], "moving_var_accuracy_valid": [0.0289122924636706, 0.05867833431392327, 0.08347565818398077, 0.10227693197046045, 0.11467240409056757, 0.12186604689009682, 0.12507365967470935, 0.12598050088962445, 0.12452454231077312, 0.12153021721696938, 0.11653243839389124, 0.11083354815951334, 0.10448167194066367, 0.09768453972722659, 0.09101177146001392, 0.08473695459243272, 0.0782428890598939, 0.072242330508251, 0.06634721769947852, 0.06074506934630169, 0.0556525890686077, 0.050763359939504864, 0.04642156047403791, 0.04232840151620323, 0.038501727334328315, 0.03492329395777422, 0.03169857195945953, 0.028704816938845953, 0.026053459564851836, 0.023596042481660816, 0.021344536359672657, 0.01928739189393709, 0.017477691733709964, 0.01580396475847817, 0.014304052992936493, 0.012918180990470941, 0.011660696750813439, 0.010504527205627884, 0.009499271923030425, 0.008565934666791154, 0.0077442565281943005, 0.0070200295954339995, 0.006359428068662461, 0.005752980166635277, 0.0052131562983712895, 0.004696026729105138, 0.004244636854942146, 0.003836121550698316, 0.0034546436220542318, 0.0031153015695977314, 0.002803887783288016, 0.002537487618654446, 0.002307136806951134, 0.0020784766340843077, 0.0018763183891881308, 0.0017046817470181197, 0.0015350316926894588, 0.0013824801936115714, 0.001251343356596224, 0.001142993909107394, 0.001028696129001267, 0.0009311328155531414, 0.0008405260149814975, 0.0007568774259784804, 0.0006812890778731559, 0.0006159477601494223, 0.0005629068108568222, 0.0005103062936096421, 0.00046350207846049973, 0.00041726624934764725, 0.0003757202103348523, 0.0003387893630905394, 0.00030509802414081436, 0.00027522917177267474, 0.0002485778487449162, 0.00022392513106958193, 0.0002033725214959318, 0.00018313419571728724, 0.0001648207772932946, 0.0001494654579191602, 0.00015812480576746823, 0.00014384119237001366, 0.00013033297801550114, 0.00011792898448349981, 0.00010650475789570752, 9.864979553465548e-05, 9.047610862003277e-05, 9.403680949481988e-05, 8.700969099624031e-05, 9.157284684419642e-05, 9.560465203542275e-05, 9.558298109035599e-05, 9.232951624607699e-05, 8.404723179405458e-05, 8.24601689142472e-05, 7.764643790267377e-05, 7.174051125777638e-05, 6.596889246631715e-05, 5.979002756551958e-05, 5.6865099701519805e-05, 5.275677325311719e-05, 5.095362888044312e-05, 4.62327034470801e-05, 4.167442873490002e-05, 3.7576144729426036e-05, 3.525396590801351e-05, 3.1755060026442364e-05, 3.131568551142481e-05, 2.9205807150767806e-05, 3.048946221450096e-05, 2.7466955442157843e-05, 3.23824118829491e-05, 2.9144337926922983e-05, 2.7028484831485515e-05, 3.001316304293249e-05, 2.704959659934037e-05, 2.5800114114433076e-05, 2.3848665745830913e-05, 2.4082692129981845e-05, 2.1700789232381098e-05, 2.0403820218544876e-05, 1.8526778679374227e-05, 1.9009508708181652e-05, 3.6056727430182085e-05, 3.2670690783378006e-05, 2.9472086893408073e-05, 2.6601719503244526e-05, 2.88406715766518e-05, 2.668016881208083e-05, 2.4013302749359784e-05, 2.42180317203016e-05, 2.7936289914754718e-05, 2.5691760462391756e-05, 2.442900021730056e-05, 3.373446777643416e-05, 3.093518677603989e-05, 2.8751081265417287e-05, 3.0103012339428263e-05, 2.796089328836581e-05, 3.1399919871932134e-05, 2.9931812935222836e-05, 3.112896680980973e-05, 2.9479147009049994e-05, 2.8966247182585994e-05, 2.6130448657058498e-05, 2.9268066114499612e-05, 3.0322791861070563e-05, 2.7642779771198882e-05, 2.660109868052432e-05, 2.496794540979165e-05, 2.3305211226807544e-05, 2.1596279765831742e-05, 2.0871416830900333e-05, 2.0277246748171025e-05, 1.8280609932076157e-05, 1.7121952561539305e-05, 1.561186339841949e-05, 1.433757618085012e-05, 1.6134240774912742e-05, 1.4955464562930305e-05, 1.3912537178184501e-05, 1.2808513243165061e-05, 1.197460550941951e-05, 1.1304134414239613e-05, 2.351770108430648e-05, 2.1426214481032267e-05, 1.988228259003842e-05, 1.8069119543284603e-05, 1.6723611942077698e-05, 1.5741923644016414e-05, 1.425028923516102e-05, 1.903773092359929e-05, 1.747017537646724e-05, 1.57380122408659e-05], "accuracy_test": 0.5231485172193878, "start": "2016-02-01 09:47:17.811000", "learning_rate_per_epoch": [0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886, 0.0008088909671641886], "accuracy_train_first": 0.5668921880767811, "accuracy_train_last": 0.9715666744532114, "batch_size_eval": 1024, "accuracy_train_std": [0.016825656171720545, 0.021833999201479153, 0.019703036952595964, 0.020981413241884028, 0.0215752704701577, 0.022158271492368226, 0.022075011153703028, 0.023150728443742486, 0.024224077384859086, 0.022981718350530804, 0.0229906242349958, 0.02532095306646358, 0.0264160969008586, 0.027140648580953695, 0.027909377941765116, 0.027339139412113505, 0.029329879826055477, 0.027605356877795365, 0.028700969219887768, 0.02863607696344985, 0.02804169725914242, 0.030161994961511198, 0.02846819702371291, 0.028675623097628687, 0.026821987389376904, 0.026191421440744975, 0.026657960040542224, 0.02821033812698559, 0.02618911005752334, 0.026176823068344568, 0.02740024915483732, 0.02516833695860853, 0.026710477985797308, 0.026106414732652388, 0.02452116533863217, 0.024463890766177068, 0.025711147070156863, 0.02536426333676385, 0.02219845615078783, 0.02182263951929954, 0.021764819397220293, 0.021118060964810833, 0.02011013319919243, 0.02146533035357131, 0.021946621764590935, 0.023053765605451436, 0.019333992322871602, 0.01988967040786902, 0.01980914815984231, 0.019015632510934823, 0.020380094691194253, 0.01898452817100118, 0.017163681530784028, 0.016971235918461305, 0.018587883710288248, 0.018520876259069678, 0.01697411195111892, 0.016498725656269447, 0.016747927308246553, 0.017846815402038512, 0.017455632686234988, 0.015477069809600105, 0.016707066579791218, 0.013873939269576037, 0.014828864041858465, 0.013630748768443553, 0.013577314380826351, 0.012883045947934088, 0.012670286533834993, 0.015645414203535383, 0.014481017380538109, 0.01389591024326013, 0.014532633553294564, 0.014577864714188777, 0.013990168612474721, 0.01373761539060606, 0.014273872917216305, 0.01267278822679684, 0.011859216439743915, 0.014331778521322423, 0.013228260947016105, 0.012256375061920465, 0.011909082766688063, 0.01464621229338913, 0.01426505789237999, 0.011321976944556045, 0.01331363052713851, 0.011494054060371855, 0.01229298538409781, 0.012545683467868869, 0.01065608428651925, 0.012339281120691837, 0.009622365692649643, 0.013008552110332394, 0.01002094788595195, 0.01157319473770503, 0.010904320765315317, 0.012319726914157687, 0.012764056771217893, 0.009211014175681587, 0.00923228580739907, 0.01031168928630263, 0.012316216053125998, 0.010011315022270373, 0.01163032548630315, 0.008860646198521071, 0.009862652281688892, 0.00948641100825597, 0.012028237800719545, 0.009897420051785191, 0.00925138590177045, 0.010419506558691182, 0.01107291793716645, 0.010464532814193126, 0.007737847976557147, 0.010307106362448942, 0.010650324357774767, 0.010088913129982146, 0.008811692190210095, 0.009723609654683064, 0.010098840398820327, 0.00905682402021779, 0.009076671399709428, 0.00844592548167852, 0.007886176018347119, 0.008344327613421831, 0.009178461746788203, 0.010776230582759769, 0.009583355093246797, 0.00861938154055695, 0.008717913542852295, 0.007348359245246995, 0.010320747540326566, 0.007924350536539835, 0.007706197774097706, 0.008401100202990432, 0.009530864051894933, 0.009261198709734086, 0.009343066818932712, 0.008076362085522187, 0.01104448156608132, 0.00870250153193335, 0.010459334438610763, 0.007448383117703553, 0.007901402260890327, 0.008517936367129676, 0.007859558366078925, 0.0070301532184899, 0.008567686881427828, 0.008084944317711326, 0.008006224841567978, 0.008204683448119863, 0.009346622340802966, 0.008892971091599524, 0.009256617455124865, 0.008090828034330214, 0.008631130749349243, 0.007053585574992654, 0.008503429145361974, 0.007743445635112081, 0.008563837369724478, 0.008181143240865763, 0.007650273990252053, 0.007848015546099936, 0.0070697492050211435, 0.008684306713371703, 0.006651086943112518, 0.007834635214081674, 0.007337127832383968, 0.0064902480718621765, 0.00874879371548314, 0.008216689420577728, 0.007770671808143684, 0.006861676204037325], "accuracy_test_std": 0.011315556508813489, "error_valid": [0.43321283179593373, 0.3409438535391567, 0.2993678816829819, 0.2754818100527108, 0.26842232210090367, 0.2643028167356928, 0.26053922722138556, 0.24669380647590367, 0.24229927522590367, 0.23479121564382532, 0.24458772590361444, 0.24113887189382532, 0.24334643260542166, 0.248291015625, 0.24409944465361444, 0.23380435805722888, 0.24498482210090367, 0.23611339890813254, 0.24270519578313254, 0.24496423192771077, 0.23690759130271077, 0.24425092949924698, 0.23191153285015065, 0.23511624623493976, 0.2382297745670181, 0.2437420580760542, 0.2386665803840362, 0.24350821253765065, 0.2339764330760542, 0.23784297345632532, 0.23967402814382532, 0.24155655826430722, 0.23156591208584332, 0.2356148225715362, 0.23152473173945776, 0.23619428887424698, 0.23668257012424698, 0.24377294333584332, 0.2308026049510542, 0.2373944018260542, 0.2299172275037651, 0.22402696724397586, 0.22383430205195776, 0.2250344150037651, 0.2214737857680723, 0.23252188441265065, 0.22443435852786142, 0.22392548710466864, 0.23103645048945776, 0.22717138083584332, 0.2334572665662651, 0.2220135424510542, 0.21711013977786142, 0.2268448795180723, 0.22319306522966864, 0.21701748399849397, 0.22600068241716864, 0.22546239646084332, 0.21950007059487953, 0.21384365587349397, 0.22600068241716864, 0.2184426181287651, 0.23063052993222888, 0.2237622364457832, 0.22461819935993976, 0.21999864693147586, 0.21525849491716864, 0.21762930628765065, 0.2165394978350903, 0.22383430205195776, 0.2214031908885542, 0.2200089420180723, 0.22385489222515065, 0.2198868717055723, 0.2254006259412651, 0.22410932793674698, 0.2182293627635542, 0.2212502353162651, 0.22219738328313254, 0.21865587349397586, 0.23803563864834332, 0.21933829066265065, 0.21992805205195776, 0.2253800357680723, 0.22502411991716864, 0.21762930628765065, 0.21831025272966864, 0.21037568241716864, 0.21588943665286142, 0.23265424981174698, 0.20962267036897586, 0.21022272684487953, 0.2111183993787651, 0.21540115540286142, 0.20962267036897586, 0.22363134177334332, 0.21352892036897586, 0.22156644154743976, 0.2158585513930723, 0.2119728915662651, 0.2214031908885542, 0.21142283979668675, 0.2190529696912651, 0.2163674228162651, 0.21625564759036142, 0.2130509342055723, 0.2161026920180723, 0.21107721903237953, 0.21940888554216864, 0.20954178040286142, 0.21623505741716864, 0.22497411521084332, 0.2167130435805723, 0.21965302616716864, 0.2090226138930723, 0.21682481880647586, 0.22026337772966864, 0.21928681522966864, 0.21151402484939763, 0.2158276661332832, 0.21942947571536142, 0.21527908509036142, 0.22158556099397586, 0.20249111681099397, 0.21398778708584332, 0.21626594267695776, 0.21455695830195776, 0.22276655449924698, 0.2189617846385542, 0.21629682793674698, 0.22177969691265065, 0.2086769931287651, 0.21364069559487953, 0.21205378153237953, 0.20405744070030118, 0.21686599915286142, 0.21141401543674698, 0.2211281650037651, 0.21806611210466864, 0.2069474185805723, 0.21874852927334332, 0.2080460513930723, 0.21821906767695776, 0.2093888248305723, 0.21489228397966864, 0.22214590785015065, 0.20830048710466864, 0.21230821724397586, 0.20971385542168675, 0.21702924981174698, 0.2109448536332832, 0.21105662885918675, 0.2174145801957832, 0.21789403708584332, 0.21364069559487953, 0.21689688441265065, 0.21594091208584332, 0.2128067935805723, 0.20842255741716864, 0.21161697571536142, 0.21135224491716864, 0.21515701477786142, 0.21577766142695776, 0.21135224491716864, 0.22570653708584332, 0.21644831278237953, 0.2174969408885542, 0.21657038309487953, 0.2130509342055723, 0.2123185123305723, 0.2138539509600903, 0.2064076618975903, 0.2119523013930723, 0.2140980915850903], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.09271090217598543, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.0008088909802347991, "optimization": "rmsprop", "nb_data_augmentation": 0, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.327858759432941e-06, "rotation_range": [0, 0], "momentum": 0.54873704405241}, "accuracy_valid_max": 0.797508883189006, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7859019084149097, "accuracy_valid_std": [0.022362219579055938, 0.02612538029283392, 0.019918690844252494, 0.02135276045049126, 0.01854366893334692, 0.015750345075723305, 0.01895022913274981, 0.017420825227586756, 0.018309111058722738, 0.01741733829535575, 0.01933028050512116, 0.02236113195095764, 0.017397857652949704, 0.01382577649879665, 0.016985408340260535, 0.01716820343893133, 0.016189019113329168, 0.0166979490976987, 0.014059495764198562, 0.012337640416888172, 0.015016998981517716, 0.01912249720208144, 0.01764204145563085, 0.01690325642146389, 0.013013797850341038, 0.022706239729272604, 0.01550178463825871, 0.025114586414616447, 0.021748096465232385, 0.0172369633483436, 0.019926130703921803, 0.015109185962583585, 0.025356121091870003, 0.018887180465321163, 0.019918143797858164, 0.02196145298347305, 0.022018635340953094, 0.01920073456880602, 0.020297230784286004, 0.018317045447642347, 0.02295955820989636, 0.023791755248195857, 0.02132049483200034, 0.01899533654407761, 0.021353772819752273, 0.021339580277908107, 0.022203339291479388, 0.0167089021147471, 0.0178896402348534, 0.01780789044706824, 0.020051104286286047, 0.016648698444687665, 0.01563254887106219, 0.02015490507058747, 0.023024648845022578, 0.02401371913663141, 0.019045093847346074, 0.01651936103043009, 0.021353281531878793, 0.02222782293371505, 0.01890688739359244, 0.01777808877030099, 0.013774522484030109, 0.020926602816670754, 0.012491070161229705, 0.020921769365357275, 0.017805287988958703, 0.015680112365518497, 0.02085590168607464, 0.016519338443350646, 0.020526524297143987, 0.02012052800345465, 0.01799832132294861, 0.02161013017417052, 0.017612219635089698, 0.018147419640401195, 0.01546864712351853, 0.016567998922808542, 0.018905666655967737, 0.02045504991600273, 0.018224395350236993, 0.016098868233565516, 0.019793044754372727, 0.019997372946857245, 0.019970267850672296, 0.015279664841939591, 0.01719091911701235, 0.015546225807592724, 0.015199180569391102, 0.013989906647084993, 0.013457019676448415, 0.014638826485539387, 0.022417204510278163, 0.014674705033348428, 0.017990711183137095, 0.016666924879070847, 0.017184702079261566, 0.012487857419941637, 0.016147167105522456, 0.01667971065746547, 0.01766080609467261, 0.01535434206061795, 0.014144698461711042, 0.014944439074163696, 0.015857287239022, 0.017441439069820118, 0.014060946791435155, 0.021425369411483947, 0.016297356671976015, 0.011725139034510898, 0.01636518439082783, 0.01598757347480625, 0.01588039341904316, 0.017348927784100177, 0.014507606472166814, 0.0172559349554895, 0.017426496047632026, 0.0139663411585448, 0.020037743898823956, 0.021737198235484183, 0.015820857324513368, 0.01557333171437789, 0.013708865248917733, 0.015525516046966448, 0.013331411169280034, 0.014559623945728045, 0.012911860954612029, 0.016213659874341344, 0.012587411860118554, 0.013106820230922123, 0.014648474120457318, 0.01311535477416986, 0.021723376333720942, 0.015107044704118172, 0.02139626787810133, 0.01526518421381847, 0.012648767753013884, 0.01567320648874085, 0.015620287468200062, 0.01660070822400526, 0.015822575193782617, 0.01570180041694659, 0.01311849587056481, 0.014232354839517916, 0.01033409609553554, 0.010434089000626291, 0.018114894258207993, 0.016807962531647656, 0.01877528336494412, 0.01109544847503798, 0.014444661051348269, 0.01782233280156959, 0.020453521196687703, 0.01045523226017772, 0.018232668772345233, 0.011891436850840943, 0.011994866842028095, 0.014435310850893635, 0.011445963630752816, 0.013761931579711558, 0.015589126191234422, 0.015868180670805008, 0.015693096360920134, 0.014010382361032291, 0.01262548071677268, 0.017353471655112352, 0.011740704689433189, 0.01752296866142388, 0.017138068133420958, 0.01491983403015886, 0.019281499465168474, 0.014838072534190451, 0.01669349339531152, 0.022788246545064336], "accuracy_valid": [0.5667871682040663, 0.6590561464608433, 0.7006321183170181, 0.7245181899472892, 0.7315776778990963, 0.7356971832643072, 0.7394607727786144, 0.7533061935240963, 0.7577007247740963, 0.7652087843561747, 0.7554122740963856, 0.7588611281061747, 0.7566535673945783, 0.751708984375, 0.7559005553463856, 0.7661956419427711, 0.7550151778990963, 0.7638866010918675, 0.7572948042168675, 0.7550357680722892, 0.7630924086972892, 0.755749070500753, 0.7680884671498494, 0.7648837537650602, 0.7617702254329819, 0.7562579419239458, 0.7613334196159638, 0.7564917874623494, 0.7660235669239458, 0.7621570265436747, 0.7603259718561747, 0.7584434417356928, 0.7684340879141567, 0.7643851774284638, 0.7684752682605422, 0.763805711125753, 0.763317429875753, 0.7562270566641567, 0.7691973950489458, 0.7626055981739458, 0.7700827724962349, 0.7759730327560241, 0.7761656979480422, 0.7749655849962349, 0.7785262142319277, 0.7674781155873494, 0.7755656414721386, 0.7760745128953314, 0.7689635495105422, 0.7728286191641567, 0.7665427334337349, 0.7779864575489458, 0.7828898602221386, 0.7731551204819277, 0.7768069347703314, 0.782982516001506, 0.7739993175828314, 0.7745376035391567, 0.7804999294051205, 0.786156344126506, 0.7739993175828314, 0.7815573818712349, 0.7693694700677711, 0.7762377635542168, 0.7753818006400602, 0.7800013530685241, 0.7847415050828314, 0.7823706937123494, 0.7834605021649097, 0.7761656979480422, 0.7785968091114458, 0.7799910579819277, 0.7761451077748494, 0.7801131282944277, 0.7745993740587349, 0.775890672063253, 0.7817706372364458, 0.7787497646837349, 0.7778026167168675, 0.7813441265060241, 0.7619643613516567, 0.7806617093373494, 0.7800719479480422, 0.7746199642319277, 0.7749758800828314, 0.7823706937123494, 0.7816897472703314, 0.7896243175828314, 0.7841105633471386, 0.767345750188253, 0.7903773296310241, 0.7897772731551205, 0.7888816006212349, 0.7845988445971386, 0.7903773296310241, 0.7763686582266567, 0.7864710796310241, 0.7784335584525602, 0.7841414486069277, 0.7880271084337349, 0.7785968091114458, 0.7885771602033133, 0.7809470303087349, 0.7836325771837349, 0.7837443524096386, 0.7869490657944277, 0.7838973079819277, 0.7889227809676205, 0.7805911144578314, 0.7904582195971386, 0.7837649425828314, 0.7750258847891567, 0.7832869564194277, 0.7803469738328314, 0.7909773861069277, 0.7831751811935241, 0.7797366222703314, 0.7807131847703314, 0.7884859751506024, 0.7841723338667168, 0.7805705242846386, 0.7847209149096386, 0.7784144390060241, 0.797508883189006, 0.7860122129141567, 0.7837340573230422, 0.7854430416980422, 0.777233445500753, 0.7810382153614458, 0.783703172063253, 0.7782203030873494, 0.7913230068712349, 0.7863593044051205, 0.7879462184676205, 0.7959425592996988, 0.7831340008471386, 0.788585984563253, 0.7788718349962349, 0.7819338878953314, 0.7930525814194277, 0.7812514707266567, 0.7919539486069277, 0.7817809323230422, 0.7906111751694277, 0.7851077160203314, 0.7778540921498494, 0.7916995128953314, 0.7876917827560241, 0.7902861445783133, 0.782970750188253, 0.7890551463667168, 0.7889433711408133, 0.7825854198042168, 0.7821059629141567, 0.7863593044051205, 0.7831031155873494, 0.7840590879141567, 0.7871932064194277, 0.7915774425828314, 0.7883830242846386, 0.7886477550828314, 0.7848429852221386, 0.7842223385730422, 0.7886477550828314, 0.7742934629141567, 0.7835516872176205, 0.7825030591114458, 0.7834296169051205, 0.7869490657944277, 0.7876814876694277, 0.7861460490399097, 0.7935923381024097, 0.7880476986069277, 0.7859019084149097], "seed": 174033913, "model": "residualv3", "loss_std": [0.35746726393699646, 0.27924075722694397, 0.27479007840156555, 0.26778745651245117, 0.26341086626052856, 0.25949573516845703, 0.255403608083725, 0.2523528039455414, 0.24990499019622803, 0.24702319502830505, 0.2446451038122177, 0.2402370572090149, 0.2377038598060608, 0.23512408137321472, 0.23037272691726685, 0.2244117707014084, 0.21944236755371094, 0.21721234917640686, 0.20617853105068207, 0.19861425459384918, 0.19820542633533478, 0.1939242035150528, 0.1867286115884781, 0.18201150000095367, 0.1767382025718689, 0.1751764714717865, 0.169793501496315, 0.16366244852542877, 0.1597817838191986, 0.16099373996257782, 0.15209521353244781, 0.1486181616783142, 0.15162765979766846, 0.1412922739982605, 0.14119412004947662, 0.14028307795524597, 0.13409285247325897, 0.13397349417209625, 0.13108442723751068, 0.12859266996383667, 0.13045166432857513, 0.12919549643993378, 0.13148027658462524, 0.1249212771654129, 0.1251089721918106, 0.12474176287651062, 0.12179241329431534, 0.1146371141076088, 0.1175791546702385, 0.11817124485969543, 0.11632408946752548, 0.12072738260030746, 0.11392176896333694, 0.1130458191037178, 0.11286722868680954, 0.12110176682472229, 0.11172758787870407, 0.11245273053646088, 0.10915002226829529, 0.10788469016551971, 0.11286939680576324, 0.10978402197360992, 0.108280710875988, 0.10835517942905426, 0.1094801127910614, 0.10267367213964462, 0.10681197047233582, 0.10693056881427765, 0.10690551996231079, 0.10470877587795258, 0.10547318309545517, 0.10110130161046982, 0.10487273335456848, 0.09853488206863403, 0.10707183927297592, 0.09663267433643341, 0.10510765016078949, 0.09834021329879761, 0.10452166199684143, 0.1036267876625061, 0.09903382509946823, 0.10136020928621292, 0.09987200796604156, 0.0980069562792778, 0.1037827804684639, 0.09984321892261505, 0.09788909554481506, 0.09580155462026596, 0.10167328268289566, 0.09712190181016922, 0.09528236836194992, 0.09626669436693192, 0.09557454288005829, 0.08935000002384186, 0.09456448256969452, 0.10125941038131714, 0.09641628712415695, 0.1008584201335907, 0.09385007619857788, 0.09742522239685059, 0.09343524277210236, 0.0920664519071579, 0.09602337330579758, 0.09963888674974442, 0.09703749418258667, 0.08887717872858047, 0.0950467437505722, 0.09689410030841827, 0.0932474210858345, 0.0970243364572525, 0.09675813466310501, 0.09081610292196274, 0.09432724118232727, 0.09348525106906891, 0.09338320791721344, 0.0914452001452446, 0.08957816660404205, 0.09295200556516647, 0.08549477159976959, 0.09731777012348175, 0.09505999833345413, 0.09053652733564377, 0.08997796475887299, 0.09643759578466415, 0.08713921159505844, 0.09277653694152832, 0.0887976586818695, 0.08836418390274048, 0.09028242528438568, 0.09273501485586166, 0.08635678887367249, 0.08533282577991486, 0.08749332278966904, 0.08985978364944458, 0.09081126004457474, 0.09482746571302414, 0.08825959265232086, 0.09114187210798264, 0.0833243578672409, 0.08444959670305252, 0.08431576937437057, 0.08982789516448975, 0.08869486302137375, 0.08775485306978226, 0.08716373890638351, 0.08858158439397812, 0.08887846767902374, 0.08292821794748306, 0.08448224514722824, 0.08886689692735672, 0.08199628442525864, 0.08474510908126831, 0.08690391480922699, 0.08921821415424347, 0.08864027261734009, 0.0866975262761116, 0.08919460326433182, 0.08558320999145508, 0.0815080925822258, 0.09038738906383514, 0.08077875524759293, 0.08635693788528442, 0.0817294642329216, 0.08811916410923004, 0.08851443231105804, 0.08028987050056458, 0.0889832004904747, 0.08575974404811859, 0.0862743929028511, 0.0794731080532074, 0.08668269962072372, 0.08424435555934906, 0.08200937509536743, 0.08783642202615738]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:31 2016", "state": "available"}], "summary": "47b91a75ae91660339d3a0c4a7b78529"}