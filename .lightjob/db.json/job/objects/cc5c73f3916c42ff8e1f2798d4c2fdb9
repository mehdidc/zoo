{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 32, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5403519868850708, 1.0771875381469727, 0.893129289150238, 0.7751526236534119, 0.6877686381340027, 0.6168867349624634, 0.5553173422813416, 0.5015288591384888, 0.4531795382499695, 0.40950021147727966, 0.3771103322505951, 0.3427180051803589, 0.31522440910339355, 0.2927219867706299, 0.2745368480682373, 0.25600460171699524, 0.2454867660999298, 0.22718939185142517, 0.21900248527526855, 0.21067099273204803, 0.2002255618572235, 0.19569766521453857, 0.19048070907592773, 0.18647633492946625, 0.1817370504140854, 0.17551667988300323, 0.17271533608436584, 0.17317058145999908, 0.16973476111888885, 0.16607613861560822, 0.16524721682071686, 0.16283582150936127, 0.16025181114673615, 0.1565096378326416, 0.15875336527824402, 0.15632493793964386, 0.1562040150165558, 0.15615367889404297, 0.15845584869384766, 0.1527385711669922, 0.1528228223323822, 0.1514255404472351, 0.1521230936050415, 0.15127992630004883, 0.15040457248687744, 0.14947323501110077, 0.15202867984771729, 0.15089447796344757, 0.14761729538440704, 0.14799319207668304, 0.14608527719974518, 0.14998139441013336, 0.14822213351726532, 0.1471719890832901, 0.14903409779071808, 0.14932940900325775, 0.14570894837379456, 0.14405332505702972, 0.14811323583126068, 0.14662769436836243, 0.1450672745704651, 0.14522525668144226, 0.14982350170612335, 0.1437956839799881, 0.1458648294210434, 0.1463257521390915, 0.14712116122245789, 0.14302000403404236, 0.14297854900360107, 0.14090800285339355, 0.14417344331741333, 0.14256858825683594, 0.14730589091777802, 0.1430235505104065, 0.14294755458831787, 0.14434365928173065, 0.14389045536518097, 0.1437220275402069, 0.14564254879951477, 0.140446737408638, 0.14041659235954285, 0.14277289807796478, 0.14425116777420044, 0.14089706540107727, 0.14144979417324066, 0.143638476729393, 0.14497900009155273, 0.14181990921497345, 0.1393374651670456, 0.14162929356098175, 0.1413881927728653, 0.13927972316741943, 0.14348652958869934, 0.1381688266992569, 0.1395556479692459, 0.14074674248695374, 0.137534499168396, 0.14138580858707428, 0.13983087241649628, 0.13745401799678802, 0.14217641949653625, 0.1378985345363617, 0.13808347284793854, 0.13807670772075653, 0.13789938390254974, 0.13872431218624115, 0.13915953040122986, 0.13795176148414612, 0.13543595373630524, 0.13565386831760406, 0.13305294513702393, 0.13695989549160004, 0.13966825604438782, 0.13878187537193298, 0.13629493117332458, 0.13827918469905853, 0.13443492352962494, 0.13575515151023865, 0.1379692107439041, 0.13900218904018402, 0.13793517649173737, 0.13537247478961945, 0.13468782603740692, 0.13420747220516205, 0.13373394310474396, 0.1359003335237503, 0.1351337730884552, 0.13396629691123962, 0.1366090029478073, 0.1333373486995697, 0.13428500294685364, 0.13614210486412048, 0.13370198011398315, 0.13285398483276367, 0.1355765461921692, 0.131985604763031, 0.13521228730678558, 0.1351548433303833, 0.136697456240654, 0.12839771807193756, 0.1311061680316925, 0.13492004573345184, 0.1327386051416397, 0.13488081097602844, 0.13041704893112183, 0.12800535559654236, 0.13305343687534332, 0.13592080771923065, 0.1341031789779663, 0.1343560367822647, 0.133334219455719, 0.1343163549900055, 0.1309710144996643, 0.13134467601776123, 0.1339408904314041, 0.13020147383213043, 0.12869317829608917, 0.13139216601848602, 0.13233350217342377, 0.1333135962486267, 0.13105551898479462, 0.1341823935508728, 0.12636353075504303, 0.1291024386882782, 0.12806172668933868, 0.12939363718032837, 0.13160817325115204, 0.13325665891170502, 0.13019685447216034, 0.12712933123111725], "moving_avg_accuracy_train": [0.033970586326827236, 0.08074962617374952, 0.13308361577582822, 0.1925324048419331, 0.24944139629872353, 0.3036748821764185, 0.35394714153052825, 0.3982297796349966, 0.4409574971253083, 0.48097923267602033, 0.51691301650744, 0.5512666763853966, 0.5812271892101275, 0.6069412614251428, 0.6330757440577762, 0.6471724275942743, 0.6712855304420821, 0.6941126589800998, 0.7160496946059455, 0.7360349502918533, 0.7539564680960474, 0.7701440709865165, 0.7854080609843488, 0.8000523158228372, 0.8133880743429899, 0.8245556727837832, 0.8341414455697735, 0.8440962649985843, 0.8535647740249809, 0.8579506052465138, 0.8668798181218716, 0.8747767449763972, 0.8827606323442984, 0.8903088541896951, 0.8946098745708363, 0.9010428545602274, 0.9069998391185089, 0.9135261329209898, 0.9186022352527372, 0.9231404283191487, 0.9272852559479666, 0.9314271161043697, 0.9350805296785026, 0.937573292855909, 0.9413769185179464, 0.9428033113722241, 0.945151838900561, 0.9452730774392147, 0.947237228288178, 0.9498884845510454, 0.9529697986328549, 0.95395984451131, 0.9548135031768734, 0.9560142376056331, 0.9571391485165353, 0.9586910389089756, 0.9610083549954682, 0.9600322231746312, 0.9619247772333679, 0.9633328540850589, 0.965211565290848, 0.9649493885225143, 0.9662339686059863, 0.9675016978239683, 0.9647902790797774, 0.9657675744229994, 0.9673610330140605, 0.9691763620043211, 0.9685524746503268, 0.9695093703019884, 0.9699939383301597, 0.9703068887662467, 0.9713092321432211, 0.9719019882443752, 0.9729540490175752, 0.9737520941896457, 0.974421506719509, 0.9751657399761295, 0.9751753157916395, 0.9760232766970178, 0.9756983800154574, 0.976005753248454, 0.9759057871486455, 0.9761644818337902, 0.9767391399742392, 0.9764798407447186, 0.9763395494881685, 0.9775546819274561, 0.9779716828192435, 0.9784097986885281, 0.9795387778970562, 0.9804270480978452, 0.9809427510261559, 0.9810047410640442, 0.9816883583255338, 0.9822104997620373, 0.9820201208418045, 0.9806213896327163, 0.9808060884670636, 0.9814605986679763, 0.9819148713154828, 0.9824003745113156, 0.9830001238530504, 0.9824378858713536, 0.9830966630949418, 0.983099010847371, 0.9830663547388705, 0.9834669365268882, 0.9839391033277801, 0.9844477948545444, 0.9846451284643281, 0.9853807644274191, 0.9854057820692101, 0.9843565485409067, 0.9836679326368437, 0.9835549886660258, 0.9842810560196613, 0.9837465097915417, 0.9833328475017101, 0.9836463261443963, 0.9838749785001948, 0.9841553145775932, 0.983916902501986, 0.9836954282851484, 0.9836635482530991, 0.984541520064694, 0.9843111346118422, 0.9844780285161526, 0.9847374429264422, 0.9848267566695122, 0.98482350577877, 0.9847786912497117, 0.9846987945949787, 0.9850454504402519, 0.9838324314725002, 0.9841611279824115, 0.9842546308460843, 0.9843180373281979, 0.9846353035358543, 0.9852929029810875, 0.9858731167377499, 0.984730646766402, 0.9851578227516757, 0.9856050241074605, 0.9858982954312568, 0.9855810966155675, 0.9853907685385437, 0.98547523963827, 0.9851026541030606, 0.9848322870927638, 0.9850284099084966, 0.9852677355093321, 0.9846529783274465, 0.9846252525923394, 0.9845213164688567, 0.9848114235112937, 0.984493485698278, 0.9847025262653549, 0.984030537960294, 0.9844998230404644, 0.9842154064721599, 0.9844895294404385, 0.9845409276118893, 0.9851312347388048, 0.9851882168447047, 0.9855672746245199, 0.9858294076156486, 0.9861350817719502, 0.9867124218090408, 0.9865903588686314], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.03390539697853915, 0.08024027261389306, 0.13092029985175074, 0.18766454857516, 0.24198467833737292, 0.29277503298104524, 0.33843855354401, 0.3783669355766572, 0.4161132380656782, 0.4498753317572278, 0.4790933120415954, 0.5068534556115323, 0.5305771720797465, 0.5506079514361544, 0.5691442301347829, 0.5799092651879462, 0.5980597696273594, 0.6146037726627409, 0.629601179658515, 0.6438943620032811, 0.6560899279207993, 0.6667820870018369, 0.6769878791487918, 0.6869534596450572, 0.6958339443469672, 0.7027989309363668, 0.7082036611013898, 0.7149316230691876, 0.7213560883036845, 0.7246366951171413, 0.7296247269420236, 0.7340549794454869, 0.7385996121014654, 0.7434914745923731, 0.7461636938500033, 0.7496714420290089, 0.7534377374439545, 0.7570502184058844, 0.7597256617855519, 0.7624052040407316, 0.7650476961554837, 0.7671309112914414, 0.7685653222801436, 0.7697627539546141, 0.7720286425275262, 0.7721155526064755, 0.7731329777204514, 0.7727291243667045, 0.7727562813483322, 0.7748784200621435, 0.7767171617343931, 0.7768755648663002, 0.7769367965009051, 0.77763784811964, 0.7779117021442723, 0.7788316165025108, 0.7805558002494735, 0.7797160170053996, 0.7805856591866367, 0.7817061630627773, 0.7829922897440749, 0.7828029122851041, 0.7836141826153287, 0.7842537291504825, 0.781091763379636, 0.7815024771489767, 0.7824671755466543, 0.7837057330767028, 0.7831205689840174, 0.7834554729760524, 0.7839451395809321, 0.7838913812233661, 0.784500266444403, 0.785070618188517, 0.7854943675048309, 0.7858970674260346, 0.7868954395614431, 0.7874704146188531, 0.7864498062330221, 0.787475265180503, 0.7874489712490491, 0.7883002419987677, 0.7880867346475355, 0.7879779682328574, 0.7884254239039241, 0.7883235276919957, 0.7870771536031425, 0.7875892471660211, 0.7876431813067233, 0.7874964095333552, 0.7881008548469625, 0.7892430001604589, 0.7902740194685847, 0.7901296300405516, 0.7908349052236199, 0.7911806551003543, 0.78974504793934, 0.7880900261555717, 0.787962546506882, 0.7892588299774288, 0.789945439920198, 0.7903180716623649, 0.7908783143361133, 0.7903459645948966, 0.7908901814354823, 0.7910096476198708, 0.7907540447742994, 0.7907244622565834, 0.7907944647319793, 0.7916123909526821, 0.7918000905810585, 0.7927473287932839, 0.7920086640144826, 0.791277830086679, 0.7907278838155865, 0.7911403705187416, 0.7917303056054217, 0.7911258462045181, 0.790590068812982, 0.7922055665909459, 0.7922515879627248, 0.7925717098987566, 0.792122250222887, 0.7919304035891526, 0.7918798119312915, 0.7928067239045781, 0.7921334498574938, 0.7923087532151178, 0.7926618387369796, 0.7926113457518358, 0.7927754806137757, 0.792809220673633, 0.7928029656337547, 0.7930985409171414, 0.7916188061307736, 0.7915586350903618, 0.791468889568901, 0.7915488690231556, 0.7928009616012617, 0.793840336685488, 0.7945060429924664, 0.7927227485576775, 0.7929835474029641, 0.7930341313863123, 0.7930470068395485, 0.792051588205669, 0.7915483954524967, 0.791598069273211, 0.7912745057570345, 0.7910859553131081, 0.7917504560732129, 0.7923292402381055, 0.7920769849831202, 0.7931541956527298, 0.7921612946774418, 0.7925922202266404, 0.7924154412574402, 0.7922553106765005, 0.7916585034887449, 0.7925384221534246, 0.7919824279708382, 0.7919022198123087, 0.7910446644349935, 0.7920683276978195, 0.791448450170884, 0.7922302453081631, 0.7922249706832203, 0.7926433515798832, 0.7927991383157202, 0.7930309726486814], "moving_var_accuracy_train": [0.010386006618495796, 0.029041913077645712, 0.050787339978915505, 0.07751603267386004, 0.09891212918413547, 0.11549235517973722, 0.12668882020686534, 0.13166850652360065, 0.13493257644862758, 0.135854972652185, 0.13389060677095532, 0.13112311161695273, 0.12608949141374526, 0.11943146386128194, 0.11363541811743248, 0.10406032468624284, 0.09888726777815912, 0.09368824117596385, 0.08865051884681406, 0.08338016096561267, 0.07793277207270584, 0.07249784125149097, 0.0673449616422272, 0.06254055327597578, 0.05788708004614861, 0.05322080933594703, 0.04872571176149404, 0.04474502645408669, 0.041077397777324616, 0.037142777639126115, 0.03414607745837461, 0.03129272279624871, 0.02873713263415381, 0.026376200247984344, 0.02390506921005684, 0.021887011372946314, 0.020017681220900113, 0.01839924569597682, 0.016791222460320485, 0.015297456981060656, 0.013922327647608099, 0.01268448993284408, 0.011536167816252605, 0.010438475848955039, 0.009524836377651719, 0.008590664109059159, 0.007781237932115446, 0.007003246427953196, 0.0063376427821752275, 0.005767140941900243, 0.005275877315947034, 0.004757111301925342, 0.004287958769788431, 0.0038721387613252674, 0.0034963137062099417, 0.0031683576097002826, 0.002899851433332712, 0.0026184417899842954, 0.0023888334587730286, 0.0021677942366781352, 0.001982780815163138, 0.0017851213635675088, 0.0016214605411284365, 0.0014737787233467207, 0.001392566975469194, 0.0012619062336132244, 0.0011585676027847393, 0.0010723696165921908, 0.0009686357738072379, 0.0008800130400200354, 0.0007941249915833652, 0.0007155939342040524, 0.0006530767709919281, 0.0005909313320518337, 0.0005417996856812055, 0.0004933516019830697, 0.000448049460001006, 0.0004082294622632454, 0.00036740734130310496, 0.00033713794644624467, 0.0003043741724848208, 0.00027478705997560334, 0.0002473982929680413, 0.00022326077013233608, 0.00020390678092456076, 0.00018412122764597462, 0.00016588623961135682, 0.00016258653725530298, 0.00014789289122353632, 0.00013483111173545357, 0.00013281934704150776, 0.00012663862788384495, 0.00011636831068787466, 0.00010476606450226373, 9.849545109389666e-05, 9.109959110193208e-05, 8.231582919116026e-05, 9.169228722953942e-05, 8.283008144126915e-05, 7.840252572503111e-05, 7.24195458969815e-05, 6.729901148575695e-05, 6.380640379338332e-05, 6.0270767346606264e-05, 5.814957748481231e-05, 5.233466934380429e-05, 4.711080020222542e-05, 4.384391210202612e-05, 4.146599428260337e-05, 3.9648298478958824e-05, 3.603393361301517e-05, 3.7300982683449165e-05, 3.357651735671136e-05, 4.0126884593284976e-05, 4.0381922903912796e-05, 3.6458537678418765e-05, 3.755724812871441e-05, 3.63731803458159e-05, 3.427591072149229e-05, 3.173273938412608e-05, 2.9030002544023153e-05, 2.683429713624088e-05, 2.466243028277494e-05, 2.263764471301182e-05, 2.038302726970185e-05, 2.5282235060328054e-05, 2.3231708666266906e-05, 2.1159219977304017e-05, 1.964896050596627e-05, 1.7755856957680326e-05, 1.598036637652786e-05, 1.4400404817007487e-05, 1.3017815614244573e-05, 1.279756652837924e-05, 2.476054502066912e-05, 2.3256863079253017e-05, 2.1009861840962923e-05, 1.8945059094632852e-05, 1.7956473803855622e-05, 2.005275969681071e-05, 2.107731575791212e-05, 3.071672290100576e-05, 2.9287364512456964e-05, 2.8158529534753222e-05, 2.611674920552855e-05, 2.4410610083048267e-05, 2.2295572066875298e-05, 2.0130233160388487e-05, 1.93665896737753e-05, 1.8087815588709417e-05, 1.662521145949731e-05, 1.5478181002484858e-05, 1.7331700436355605e-05, 1.560544884020513e-05, 1.4142128416066173e-05, 1.3485374439103276e-05, 1.3046597071699608e-05, 1.2135218992684397e-05, 1.4985811632662002e-05, 1.5469286847629814e-05, 1.4650393221801586e-05, 1.3861644515262536e-05, 1.2499256011992649e-05, 1.4385492947578961e-05, 1.2976166296356133e-05, 1.2971712870666672e-05, 1.2292964928942562e-05, 1.1904598644524021e-05, 1.3714032445922559e-05, 1.2476723454122894e-05], "duration": 27588.344404, "accuracy_train": [0.33970586326827246, 0.5017609847960502, 0.6040895221945368, 0.7275715064368771, 0.7616223194098376, 0.7917762550756736, 0.8063974757175157, 0.7967735225752122, 0.8255069545381136, 0.841174852632429, 0.8403170709902179, 0.8604496152870063, 0.8508718046327058, 0.8383679113602805, 0.8682860877514765, 0.7740425794227574, 0.8883034560723514, 0.8995568158222591, 0.9134830152385567, 0.9159022514650241, 0.9152501283337948, 0.9158324970007383, 0.9227839709648394, 0.9318506093692323, 0.9334099010243633, 0.9250640587509228, 0.9204134006436876, 0.9336896398578812, 0.9387813552625508, 0.8974230862403102, 0.9472427340000923, 0.9458490866671282, 0.954615618655408, 0.9582428507982651, 0.9333190580011074, 0.9589396744647471, 0.9606127001430418, 0.9722627771433187, 0.9642871562384644, 0.9639841659168512, 0.9645887046073275, 0.9687038575119971, 0.9679612518456996, 0.9600081614525655, 0.9756095494762828, 0.9556408470607235, 0.9662885866555924, 0.9463642242870985, 0.9649145859288483, 0.9737497909168512, 0.98070162536914, 0.9628702574174051, 0.9624964311669435, 0.9668208474644703, 0.9672633467146549, 0.9726580524409376, 0.9818641997739018, 0.9512470367870985, 0.9789577637619971, 0.9760055457502769, 0.9821199661429494, 0.9625897976075121, 0.9777951893572352, 0.9789112607858066, 0.9403875103820598, 0.9745632325119971, 0.9817021603336102, 0.9855143229166666, 0.962937488464378, 0.9781214311669435, 0.9743550505837025, 0.97312344269103, 0.9803303225359912, 0.9772367931547619, 0.982422595976375, 0.9809345007382798, 0.9804462194882798, 0.9818638392857143, 0.9752614981312293, 0.9836549248454227, 0.9727743098814139, 0.9787721123454227, 0.9750060922503692, 0.9784927340000923, 0.9819110632382798, 0.974146147679033, 0.9750769281792175, 0.9884908738810447, 0.9817246908453304, 0.9823528415120893, 0.9896995907738095, 0.9884214799049464, 0.9855840773809523, 0.9815626514050388, 0.9878409136789406, 0.9869097726905685, 0.9803067105597084, 0.9680328087509228, 0.9824683779761905, 0.9873511904761905, 0.9860033251430418, 0.9867699032738095, 0.9883978679286637, 0.9773777440360835, 0.9890256581072352, 0.9831201406192323, 0.9827724497623662, 0.9870721726190477, 0.9881886045358066, 0.9890260185954227, 0.9864211309523809, 0.9920014880952381, 0.9856309408453304, 0.9749134467861758, 0.9774703895002769, 0.9825384929286637, 0.9908156622023809, 0.9789355937384644, 0.9796098868932264, 0.9864676339285714, 0.9859328497023809, 0.9866783392741787, 0.9817711938215209, 0.9817021603336102, 0.9833766279646549, 0.9924432663690477, 0.9822376655361758, 0.9859800736549464, 0.9870721726190477, 0.9856305803571429, 0.9847942477620893, 0.9843753604881875, 0.9839797247023809, 0.9881653530477114, 0.9729152607627353, 0.9871193965716132, 0.98509615661914, 0.9848886956672205, 0.9874906994047619, 0.9912112979881875, 0.9910950405477114, 0.974448417024271, 0.98900240661914, 0.9896298363095238, 0.9885377373454227, 0.9827263072743633, 0.9836778158453304, 0.9862354795358066, 0.9817493842861758, 0.9823989840000923, 0.9867935152500923, 0.9874216659168512, 0.9791201636904762, 0.984375720976375, 0.9835858913575121, 0.9874223868932264, 0.981632045381137, 0.9865838913690477, 0.9779826432147471, 0.9887233887619971, 0.9816556573574198, 0.9869566361549464, 0.9850035111549464, 0.9904439988810447, 0.9857010557978036, 0.9889787946428571, 0.9881886045358066, 0.9888861491786637, 0.9919084821428571, 0.9854917924049464], "end": "2016-01-29 23:23:37.039000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0], "moving_var_accuracy_valid": [0.010346183498450987, 0.028633851449899314, 0.04888665275237934, 0.0729771753457986, 0.09223554628767158, 0.1062289327823745, 0.1143724534959758, 0.11728368937208343, 0.11837837059920917, 0.11679944427322103, 0.11280271319297816, 0.10845807201289188, 0.10267759731918066, 0.09602092668188845, 0.08951117666563868, 0.08160303281633732, 0.07640769683735003, 0.07123026348152744, 0.06613153708274674, 0.06135703892833864, 0.056559921487941575, 0.05193282973147534, 0.04767697049844941, 0.04380308860025238, 0.04013254681709485, 0.036555891479100024, 0.0331632023046004, 0.03025427132430156, 0.02760030797381474, 0.024937138606013765, 0.02266734889878673, 0.02057725824410804, 0.018705415593497306, 0.017050246901817123, 0.015409489013483057, 0.013979278787720612, 0.012709015739322305, 0.011555564333692826, 0.010464429875823808, 0.009482606408517072, 0.00859719064885411, 0.007776529651692847, 0.0070173945004841495, 0.0063285596339709635, 0.005741911929797549, 0.005167788717074201, 0.004660326230129722, 0.004195761484898744, 0.0037761919739237298, 0.0034391040310172696, 0.003125622366350945, 0.002813285953685632, 0.0025319911021347567, 0.0022832152522704575, 0.002055568691284678, 0.0018576280039946495, 0.0016986204899348003, 0.001535105564014565, 0.0013884015053235907, 0.0012608611152192457, 0.0011496621002604307, 0.0010350186646320837, 0.0009374402341071988, 0.0008473773886321249, 0.0008526218975929531, 0.0007688778800365916, 0.0007003658790192686, 0.0006441355139144985, 0.0005828037156613631, 0.0005255327902501556, 0.00047513747168054964, 0.0004276497341615685, 0.0003882214316569858, 0.0003523269984994114, 0.00031871036999715926, 0.0002882988380362802, 0.000268439676519495, 0.0002445710757173372, 0.0002294887414406622, 0.0002160039617733142, 0.00019440978793346453, 0.000181490766144056, 0.00016375195803092207, 0.00014748323342448754, 0.00013453685928016635, 0.00012117661889419823, 0.00012303999232905988, 0.00011309615145042843, 0.00010181271632918506, 9.182532227738513e-05, 8.59309772839225e-05, 8.907834280980899e-05, 8.973751585238178e-05, 8.095139902949332e-05, 7.733297688121265e-05, 7.0675565988448e-05, 8.215672067640444e-05, 9.859292255149334e-05, 8.887988984381445e-05, 9.511505838354709e-05, 8.98464514667777e-05, 8.211149605753332e-05, 7.672519313317926e-05, 7.160324004262324e-05, 6.710846376455406e-05, 6.052606691100981e-05, 5.506145555188667e-05, 4.9563186124887813e-05, 4.465097063145299e-05, 4.620690329092418e-05, 4.19032933162654e-05, 4.578830606093997e-05, 4.612010635381979e-05, 4.6315159788698406e-05, 4.440561191962646e-05, 4.1496358250182534e-05, 4.0478933083630185e-05, 3.971938028133397e-05, 3.833095897273203e-05, 5.7986360710913934e-05, 5.220678633976625e-05, 4.790841019114803e-05, 4.493569517412877e-05, 4.077337183459314e-05, 3.6719070293739984e-05, 4.077965552036307e-05, 4.078137145062243e-05, 3.697981571030879e-05, 3.4403858611013074e-05, 3.09864186238503e-05, 2.813023903760158e-05, 2.532746065859403e-05, 2.2795066722449547e-05, 2.130184278354605e-05, 3.887819384707297e-05, 3.50229594493037e-05, 3.1593151831973915e-05, 2.8491407066702112e-05, 3.975188877736787e-05, 4.5499404991023816e-05, 4.4937948476277675e-05, 6.906540499898838e-05, 6.277100883841435e-05, 5.651693660891522e-05, 5.0866734943688045e-05, 5.469778575939097e-05, 5.150683370505813e-05, 4.6378357730731426e-05, 4.2682762098662906e-05, 3.873444731793998e-05, 3.883505392776541e-05, 3.7966468520762336e-05, 3.474251609169513e-05, 4.171170992301372e-05, 4.641321005126395e-05, 4.344316050670649e-05, 3.9380101691599164e-05, 3.567286774900831e-05, 3.5311190348316707e-05, 3.874838302155095e-05, 3.765571049902551e-05, 3.3948039587375344e-05, 3.71718466550978e-05, 4.2885640270523144e-05, 4.2055309579068763e-05, 4.335061135122058e-05, 3.901580061111311e-05, 3.668960372223392e-05, 3.3239068913575196e-05, 3.0398886443673384e-05], "accuracy_test": 0.6521045918367346, "start": "2016-01-29 15:43:48.695000", "learning_rate_per_epoch": [0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576, 0.002354467986151576], "accuracy_train_first": 0.33970586326827246, "accuracy_train_last": 0.9854917924049464, "batch_size_eval": 1024, "accuracy_train_std": [0.015039224794632985, 0.02052860394881461, 0.01816170564303047, 0.022981688355138256, 0.028264070124835988, 0.027666143051076605, 0.029300597116253414, 0.029798355824168066, 0.03107705091459417, 0.032662752950781124, 0.03412131486761617, 0.032079512289701614, 0.032920248658645485, 0.029236965671428215, 0.028182676624346443, 0.027098750175477616, 0.027418879318591882, 0.026485265831952404, 0.02367957197776984, 0.02232801762004718, 0.023309744337414284, 0.023027942285065807, 0.020203997203572917, 0.020946640753009376, 0.018669937607786413, 0.020145732559185676, 0.020466937688494835, 0.01777754414854926, 0.01768344626116588, 0.017971892442283914, 0.016320569832691447, 0.016053249286384418, 0.013831656266072214, 0.013369477962116235, 0.016861505796800274, 0.012233397169024891, 0.013213775373699346, 0.009752803112224297, 0.01036111446739043, 0.010539945534059804, 0.010874665809519209, 0.011767898147637143, 0.010031737890160422, 0.010970373584969372, 0.009118347053309196, 0.01057190205903546, 0.010804475725886555, 0.0125734922574874, 0.01127594139679507, 0.009073582622695213, 0.00731129508208945, 0.010106646209817837, 0.010212452029884614, 0.011155750852840134, 0.009774509640360966, 0.009314345759758784, 0.006900076100288445, 0.009906567113758513, 0.0067305113195788155, 0.0076776627346190375, 0.007060579592295339, 0.010088977135326457, 0.007761123040834543, 0.00761934375027872, 0.012596307692139258, 0.008428570712373863, 0.005648860679189504, 0.006424693732754451, 0.010555633513349717, 0.0058470526738347165, 0.006935505748396418, 0.007944174930931733, 0.007167470415249406, 0.00800908299446904, 0.006860588054427096, 0.006483808825944383, 0.006442583137478699, 0.006551475545147441, 0.007992278333094587, 0.006014105054486194, 0.008399603871491029, 0.007244293609151083, 0.00806182496477025, 0.006385278912515633, 0.0059847121189267015, 0.007432753167882316, 0.005861290958475397, 0.005126399701342555, 0.006982332340983735, 0.0060606461362493255, 0.0045913888775823, 0.004381934296368789, 0.006164582983545089, 0.006848556562341132, 0.004843031657531255, 0.005007868702027629, 0.005627779916351977, 0.007925172661762611, 0.0055445902383887205, 0.0052457748198839645, 0.004997661358807214, 0.005128679560479269, 0.004675450372414862, 0.006194161753504409, 0.004082211847854844, 0.006574156026334339, 0.005667977143627837, 0.004960365900681135, 0.0053146390330434615, 0.004565759449657313, 0.005126939943867356, 0.004160912101612265, 0.0045828776227201514, 0.0054942148024092865, 0.006783212153066225, 0.0060704506704400245, 0.0042942294546302546, 0.005613915449482479, 0.0069740612176349605, 0.005397751708190041, 0.005056704208347362, 0.004939097335811462, 0.005564917882039241, 0.006273545190193441, 0.004388324343635517, 0.0038480334710255704, 0.0055155342861807856, 0.004765818677063128, 0.004594390497314878, 0.005133473630852148, 0.004719661428169855, 0.005311438068443616, 0.005528624995980396, 0.003861580664920839, 0.007243722559463737, 0.00447275342055904, 0.0046895594940719095, 0.003856910811353036, 0.005159735203280866, 0.003430294027213548, 0.003831791706602362, 0.005933209275677532, 0.0039765532071352125, 0.003841494871291552, 0.004517088229096006, 0.004961336903787075, 0.005680371027313495, 0.004678564676531681, 0.004404413433277414, 0.006724540922611816, 0.004290829991534481, 0.004385366956396104, 0.006765663652643799, 0.004527494943924927, 0.004802096526419262, 0.0041678306380686396, 0.006037629034198667, 0.004149006377012661, 0.006513361955925083, 0.004274572439424663, 0.006314953586015713, 0.0049018974207500066, 0.00535807428972199, 0.003933224735718986, 0.005626704483296995, 0.004581900274089881, 0.004658959865116186, 0.004373024939196715, 0.003931083371541677, 0.005403373616506931], "accuracy_test_std": 0.013468970396692084, "error_valid": [0.6609460302146084, 0.5027458466679217, 0.4129594550075302, 0.3016372129141567, 0.2691341538027108, 0.25011177522590367, 0.2505897613893072, 0.2622776261295181, 0.24417003953313254, 0.24626582501882532, 0.25794486539909633, 0.2433052522590362, 0.2559093797063253, 0.2691150343561747, 0.26402926157756024, 0.32320541933358427, 0.23858569041792166, 0.23650020001882532, 0.2354221573795181, 0.22746699689382532, 0.2341499788215362, 0.23698848126882532, 0.23115999152861444, 0.2233563158885542, 0.22424169333584332, 0.2345161897590362, 0.24315376741340367, 0.22451671922063254, 0.22082372458584332, 0.24583784356174698, 0.2254829866340362, 0.22607274802334332, 0.22049869399472888, 0.21248176298945776, 0.22978633283132532, 0.21875882435993976, 0.2126656038215362, 0.21043745293674698, 0.21619534779743976, 0.21347891566265065, 0.21116987481174698, 0.21412015248493976, 0.2185249788215362, 0.21946036097515065, 0.2075783603162651, 0.2271022566829819, 0.2177101962537651, 0.2309055558170181, 0.2269993058170181, 0.2060223315135542, 0.20673416321536142, 0.2216988069465362, 0.22251211878765065, 0.21605268731174698, 0.2196236116340362, 0.21288915427334332, 0.20392654602786142, 0.2278420321912651, 0.21158756118222888, 0.20820930205195776, 0.20543257012424698, 0.21890148484563254, 0.20908438441265065, 0.20999035203313254, 0.2473659285579819, 0.21480109892695776, 0.20885053887424698, 0.20514724915286142, 0.22214590785015065, 0.21353039109563254, 0.21164786097515065, 0.21659244399472888, 0.2100197665662651, 0.20979621611445776, 0.21069188864834332, 0.21047863328313254, 0.20411921121987953, 0.20735480986445776, 0.22273566923945776, 0.20329560429216864, 0.2127876741340362, 0.2040383212537651, 0.2138348315135542, 0.21300092949924698, 0.20754747505647586, 0.21259353821536142, 0.2241402131965362, 0.2078019107680723, 0.21187141142695776, 0.21382453642695776, 0.2064591373305723, 0.2004776920180723, 0.2004468067582832, 0.21116987481174698, 0.2028176181287651, 0.2057075960090362, 0.22317541650978923, 0.22680516989834332, 0.21318477033132532, 0.19907461878765065, 0.20387507059487953, 0.20632824265813254, 0.20407950160015065, 0.2144451830760542, 0.20421186699924698, 0.20791515672063254, 0.21154638083584332, 0.20954178040286142, 0.20857551298945776, 0.20102627306099397, 0.2065106127635542, 0.19872752729668675, 0.21463931899472888, 0.2152996752635542, 0.21422163262424698, 0.20514724915286142, 0.20296027861445776, 0.21431428840361444, 0.21423192771084332, 0.19325495340737953, 0.2073342196912651, 0.20454719267695776, 0.21192288685993976, 0.20979621611445776, 0.20857551298945776, 0.19885106833584332, 0.2139260165662651, 0.2061135165662651, 0.2041603915662651, 0.20784309111445776, 0.2057473056287651, 0.20688711878765065, 0.20725332972515065, 0.20424128153237953, 0.2216988069465362, 0.20898290427334332, 0.20933882012424698, 0.2077313158885542, 0.1959302051957832, 0.19680528755647586, 0.19950260024472888, 0.22332690135542166, 0.20466926298945776, 0.2065106127635542, 0.20683711408132532, 0.21690717949924698, 0.2129803393260542, 0.20795486634036142, 0.2116375658885542, 0.21061099868222888, 0.20226903708584332, 0.20246170227786142, 0.21019331231174698, 0.1971509083207832, 0.21677481410015065, 0.2035294498305723, 0.20917556946536142, 0.20918586455195776, 0.2137127612010542, 0.19954230986445776, 0.21302151967243976, 0.20881965361445776, 0.21667333396084332, 0.19871870293674698, 0.2141304475715362, 0.20073359845632532, 0.2078225009412651, 0.20359122035015065, 0.20579878106174698, 0.20488251835466864], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.0974993424478295, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.002354467975484865, "optimization": "rmsprop", "nb_data_augmentation": 0, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 6.588108815254864e-06, "rotation_range": [0, 0], "momentum": 0.5221926247168533}, "accuracy_valid_max": 0.8067450465926205, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7951174816453314, "accuracy_valid_std": [0.00751737973119101, 0.01937260261858341, 0.01910065340535962, 0.010811282342325835, 0.013941331623113874, 0.008852636907567565, 0.010231413868707429, 0.01522047449410723, 0.014056430761522663, 0.014076448222937672, 0.013973790285039842, 0.015047221703516847, 0.01562960392545623, 0.013029316697661247, 0.012530110443457455, 0.02261333097339397, 0.018134080848453738, 0.014947967890714123, 0.012309801832021268, 0.015859535536020066, 0.01285812898187674, 0.015164083297914605, 0.013968206735471021, 0.01861722741053043, 0.01343497297394324, 0.016992462905133492, 0.019295857900447508, 0.016297267946292944, 0.013233443504443424, 0.018568815599215267, 0.011118755387823333, 0.014469534685156446, 0.014020595659636832, 0.014751454643825813, 0.008160454582662223, 0.010905821651804298, 0.010459894732106841, 0.0087476764274936, 0.014385539354088027, 0.01198994822864303, 0.014662901069412116, 0.01228443313614154, 0.013001092459757111, 0.011512340134702021, 0.017159994935246645, 0.02094671209502349, 0.01819471837947998, 0.015082913805521988, 0.012529847356962302, 0.015473938641244763, 0.01293893489995608, 0.012980362435595695, 0.017504321260279506, 0.014035517670527768, 0.017857132331184196, 0.01413477668431187, 0.008032697954612367, 0.01793280623746583, 0.014895859665448858, 0.018354356083699116, 0.013942968311703092, 0.01270842019536354, 0.015522852619958286, 0.017056034728219673, 0.019441399381755032, 0.017561811400353813, 0.012306593443246545, 0.011962157499360339, 0.011268985104467453, 0.013812594892086652, 0.012327936399996306, 0.012370057214923133, 0.014793170852845393, 0.010128069182208434, 0.012767512793598355, 0.012515987195824533, 0.016304302453660276, 0.011676650876385065, 0.01689249850542279, 0.015034650409229598, 0.018455249819378655, 0.0159142388240359, 0.017641867659422866, 0.00956391267660923, 0.012487989752605197, 0.014392259938113331, 0.017715298065198826, 0.015370543390589337, 0.012231882763007153, 0.013647842477276823, 0.015266405596535911, 0.015036120114499549, 0.014097143167858959, 0.011407175531708572, 0.012003820411870808, 0.010232955469752784, 0.016408451087902648, 0.012147410084360218, 0.008232648076542374, 0.012842842948966263, 0.014166545778697578, 0.010240080282997376, 0.006315487098562095, 0.012153392330600189, 0.011206867543624469, 0.016494278530842732, 0.012399930454732169, 0.015329451530669655, 0.010888843412073723, 0.017626972020168136, 0.010094892499398878, 0.013341058377574525, 0.014652147440247147, 0.015095820108761362, 0.0166894135942817, 0.011597850800551219, 0.012740273377484764, 0.013894881234927398, 0.015478916731138204, 0.013871132166949036, 0.01661554441094565, 0.013255076959775085, 0.016539432599240744, 0.016646326479495228, 0.01239412933531803, 0.014678942216674986, 0.013282192284386027, 0.008397875481471207, 0.00853814270519691, 0.01563677245310107, 0.012322874445913687, 0.011422530543760044, 0.01234906381034919, 0.011538246370212297, 0.016302287852717524, 0.01677968994037725, 0.017289465677217446, 0.011734096935956015, 0.014574993476864284, 0.010061666508430184, 0.015490895641989558, 0.009767837927095476, 0.009391110604061824, 0.007940064620698312, 0.01624544719571451, 0.012119135961503545, 0.013297233853414496, 0.012496431981900149, 0.011119203986816066, 0.013896601921062937, 0.018284776730332703, 0.013832100856325103, 0.013868234301911283, 0.01591544118019657, 0.015313988400022205, 0.019764702326599045, 0.01365685286822828, 0.015703663542140922, 0.015072164569408816, 0.0115246140813253, 0.009514845275695556, 0.01815128011428539, 0.014546128742987932, 0.014606052328244206, 0.013677640168915907, 0.016657105736524822, 0.015314504009219847, 0.013311383662380672, 0.014478572735566124, 0.014968427826302061], "accuracy_valid": [0.3390539697853916, 0.49725415333207834, 0.5870405449924698, 0.6983627870858433, 0.7308658461972892, 0.7498882247740963, 0.7494102386106928, 0.7377223738704819, 0.7558299604668675, 0.7537341749811747, 0.7420551346009037, 0.7566947477409638, 0.7440906202936747, 0.7308849656438253, 0.7359707384224398, 0.6767945806664157, 0.7614143095820783, 0.7634997999811747, 0.7645778426204819, 0.7725330031061747, 0.7658500211784638, 0.7630115187311747, 0.7688400084713856, 0.7766436841114458, 0.7757583066641567, 0.7654838102409638, 0.7568462325865963, 0.7754832807793675, 0.7791762754141567, 0.754162156438253, 0.7745170133659638, 0.7739272519766567, 0.7795013060052711, 0.7875182370105422, 0.7702136671686747, 0.7812411756400602, 0.7873343961784638, 0.789562547063253, 0.7838046522025602, 0.7865210843373494, 0.788830125188253, 0.7858798475150602, 0.7814750211784638, 0.7805396390248494, 0.7924216396837349, 0.7728977433170181, 0.7822898037462349, 0.7690944441829819, 0.7730006941829819, 0.7939776684864458, 0.7932658367846386, 0.7783011930534638, 0.7774878812123494, 0.783947312688253, 0.7803763883659638, 0.7871108457266567, 0.7960734539721386, 0.7721579678087349, 0.7884124388177711, 0.7917906979480422, 0.794567429875753, 0.7810985151543675, 0.7909156155873494, 0.7900096479668675, 0.7526340714420181, 0.7851989010730422, 0.791149461125753, 0.7948527508471386, 0.7778540921498494, 0.7864696089043675, 0.7883521390248494, 0.7834075560052711, 0.7899802334337349, 0.7902037838855422, 0.7893081113516567, 0.7895213667168675, 0.7958807887801205, 0.7926451901355422, 0.7772643307605422, 0.7967043957078314, 0.7872123258659638, 0.7959616787462349, 0.7861651684864458, 0.786999070500753, 0.7924525249435241, 0.7874064617846386, 0.7758597868034638, 0.7921980892319277, 0.7881285885730422, 0.7861754635730422, 0.7935408626694277, 0.7995223079819277, 0.7995531932417168, 0.788830125188253, 0.7971823818712349, 0.7942924039909638, 0.7768245834902108, 0.7731948301016567, 0.7868152296686747, 0.8009253812123494, 0.7961249294051205, 0.7936717573418675, 0.7959204983998494, 0.7855548169239458, 0.795788133000753, 0.7920848432793675, 0.7884536191641567, 0.7904582195971386, 0.7914244870105422, 0.798973726939006, 0.7934893872364458, 0.8012724727033133, 0.7853606810052711, 0.7847003247364458, 0.785778367375753, 0.7948527508471386, 0.7970397213855422, 0.7856857115963856, 0.7857680722891567, 0.8067450465926205, 0.7926657803087349, 0.7954528073230422, 0.7880771131400602, 0.7902037838855422, 0.7914244870105422, 0.8011489316641567, 0.7860739834337349, 0.7938864834337349, 0.7958396084337349, 0.7921569088855422, 0.7942526943712349, 0.7931128812123494, 0.7927466702748494, 0.7957587184676205, 0.7783011930534638, 0.7910170957266567, 0.790661179875753, 0.7922686841114458, 0.8040697948042168, 0.8031947124435241, 0.8004973997552711, 0.7766730986445783, 0.7953307370105422, 0.7934893872364458, 0.7931628859186747, 0.783092820500753, 0.7870196606739458, 0.7920451336596386, 0.7883624341114458, 0.7893890013177711, 0.7977309629141567, 0.7975382977221386, 0.789806687688253, 0.8028490916792168, 0.7832251858998494, 0.7964705501694277, 0.7908244305346386, 0.7908141354480422, 0.7862872387989458, 0.8004576901355422, 0.7869784803275602, 0.7911803463855422, 0.7833266660391567, 0.801281297063253, 0.7858695524284638, 0.7992664015436747, 0.7921774990587349, 0.7964087796498494, 0.794201218938253, 0.7951174816453314], "seed": 560068963, "model": "residualv3", "loss_std": [0.41230279207229614, 0.1368463635444641, 0.11571531742811203, 0.10706109553575516, 0.09956599771976471, 0.09553173184394836, 0.09082701057195663, 0.08482741564512253, 0.0801924616098404, 0.07838065177202225, 0.07084941864013672, 0.0712970644235611, 0.0672282725572586, 0.06541470438241959, 0.061554186046123505, 0.05596910044550896, 0.059104178100824356, 0.051879655569791794, 0.049660250544548035, 0.04722670465707779, 0.04456793889403343, 0.043286360800266266, 0.043234992772340775, 0.04453582316637039, 0.044121332466602325, 0.037730373442173004, 0.038754843175411224, 0.039883434772491455, 0.03475788235664368, 0.03312431275844574, 0.03635159879922867, 0.03462105244398117, 0.03423919901251793, 0.03172504901885986, 0.03236323967576027, 0.0330757275223732, 0.03287985548377037, 0.03550106659531593, 0.033211201429367065, 0.029179958626627922, 0.032910387963056564, 0.03166825696825981, 0.029077712446451187, 0.026633648201823235, 0.0274946391582489, 0.028993675485253334, 0.028484046459197998, 0.028130387887358665, 0.028090598061680794, 0.02700737677514553, 0.026316503062844276, 0.02880888618528843, 0.02689966931939125, 0.024828139692544937, 0.028626343235373497, 0.02937883324921131, 0.025482358410954475, 0.02480732649564743, 0.025005334988236427, 0.02693389169871807, 0.023942872881889343, 0.025169186294078827, 0.027471674606204033, 0.02572295442223549, 0.027053656056523323, 0.027126336470246315, 0.02252463810145855, 0.022569766268134117, 0.02258678339421749, 0.021190209314227104, 0.02469678781926632, 0.023424474522471428, 0.027262575924396515, 0.02292819321155548, 0.02366030402481556, 0.022765498608350754, 0.02276030369102955, 0.021330108866095543, 0.024392586201429367, 0.02205376885831356, 0.019783269613981247, 0.02184825949370861, 0.0247942004352808, 0.022421319037675858, 0.019410017877817154, 0.024936430156230927, 0.025610487908124924, 0.02199241705238819, 0.021446602419018745, 0.02379182167351246, 0.02153104357421398, 0.02296321466565132, 0.024846473708748817, 0.02151661179959774, 0.01922667771577835, 0.0213851947337389, 0.019954964518547058, 0.02441011369228363, 0.02131647802889347, 0.020905809476971626, 0.025355692952871323, 0.02160354144871235, 0.02159908227622509, 0.01996246911585331, 0.02155272848904133, 0.022629233077168465, 0.022302068769931793, 0.02275146171450615, 0.0197575893253088, 0.019442060962319374, 0.01940164715051651, 0.022665681317448616, 0.023221369832754135, 0.02148188278079033, 0.018862532451748848, 0.02234567143023014, 0.019627712666988373, 0.022901277989149094, 0.020597880706191063, 0.023104127496480942, 0.01934811659157276, 0.020298095420002937, 0.0214186143130064, 0.017971741035580635, 0.018966231495141983, 0.022692367434501648, 0.019739193841814995, 0.020743366330862045, 0.022635385394096375, 0.018930096179246902, 0.02090114913880825, 0.021332785487174988, 0.020279543474316597, 0.0207033883780241, 0.021282458677887917, 0.01851949840784073, 0.022628478705883026, 0.020535141229629517, 0.01929117925465107, 0.017447693273425102, 0.02093307487666607, 0.020488040521740913, 0.020010191947221756, 0.02340192347764969, 0.019837157800793648, 0.01792684569954872, 0.01983807608485222, 0.02373022586107254, 0.02182280272245407, 0.021312082186341286, 0.0203565526753664, 0.02426762320101261, 0.019943466410040855, 0.019680878147482872, 0.022725366055965424, 0.019271548837423325, 0.02010609395802021, 0.020300813019275665, 0.020042182877659798, 0.021873602643609047, 0.020726250484585762, 0.01997535489499569, 0.01757717691361904, 0.019710244610905647, 0.018271267414093018, 0.019460568204522133, 0.02048565447330475, 0.020447997376322746, 0.01910969614982605, 0.017795443534851074]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:22 2016", "state": "available"}], "summary": "865fa7bd8fe610f886e528ba4fd67ce5"}