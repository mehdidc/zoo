{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 16, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 7, "nbg3": 2, "nbg2": 8, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.017627828834263115, 0.01896658442365114, 0.018862482144932878, 0.02105441443552146, 0.01769416173857055, 0.012138479389312552, 0.014926754292808887, 0.011930270568401062, 0.010077489453519228, 0.008670306740794628, 0.014915933124048536, 0.01618117476032402, 0.014395264556294885, 0.013970943904203642, 0.013042427126528418, 0.010005147415787912, 0.009922587187880284, 0.009667940787896533, 0.00810938447051373, 0.00590007070518645, 0.008979842064413351, 0.008426938661974675, 0.009986612069890847, 0.008603023843620243, 0.010115435023452187, 0.009768317011167519, 0.009863488602208557, 0.009916462251087246, 0.009634246059948276, 0.011566015763970143, 0.009651291123482206, 0.00815216860614044, 0.010424394240406615, 0.0117277669139935, 0.009412972318367793, 0.011419649831656027, 0.011075260288227905, 0.01037401492177417, 0.009366967324228816, 0.009740105447181124, 0.007898383842245147, 0.010748145673435986, 0.009790552568293396, 0.0071452095363379745, 0.007327457029025855, 0.010925769184948843, 0.008033177391569876, 0.008937138064602708, 0.009718266061442143, 0.008959397846980577, 0.007145695091490091, 0.008367530479404622, 0.009298772306601744, 0.008333659995108182, 0.0095239952290581, 0.009923611584745497, 0.008192214685866131, 0.009773300660676682, 0.00893424487865363, 0.006992268862180356, 0.009218192634022697, 0.011751253978534154, 0.010669447020788836, 0.011694222557416456, 0.009260322450382429, 0.009995085950751256, 0.008761846249592442, 0.009722374472239576, 0.00989936127053508, 0.0109950218578262, 0.00986763278044803, 0.011150294530071603, 0.010525052555670879, 0.007680048277607255, 0.01054881603619167, 0.011798961960528024, 0.012163001570437012, 0.008642397803681527, 0.008757367870948116, 0.008703407824680686, 0.009181729908906165, 0.01090729758199348, 0.010496835588160912, 0.009708288717924304, 0.009706786017466815, 0.01120403196529278, 0.01030960168061602, 0.010096887588441185, 0.010511977084417767, 0.011860467104222769, 0.010404722854975436, 0.010827671437720064, 0.01114764538961853, 0.010753744126901874, 0.009490962288632087, 0.00934680150075937, 0.009711410133272107, 0.009962935440573502, 0.009173935834558552, 0.009405823495229882, 0.009180460041976984, 0.00859149268984344, 0.009177686927864378, 0.009336096360407473, 0.007919365687096563, 0.008601861541420657, 0.009031595167798417, 0.010239935376534703, 0.00847533924011498, 0.008687933291343639, 0.008967690754567298, 0.008369226995913779, 0.007631389461759552, 0.009130681281824829, 0.008614647405254578, 0.009321614988066535, 0.007845868803400142, 0.008178438791679316, 0.010071786560168692, 0.009623627476075777, 0.009180127821456637, 0.009329257631468958, 0.009183001955711754, 0.00963468161003752, 0.010179692960952976, 0.010216261474261164, 0.009754720362085184, 0.009523503627152553, 0.009451683187651749, 0.009244429380613527], "moving_avg_accuracy_train": [0.056585956966362115, 0.11733197104847265, 0.17694345977096018, 0.23332305568893613, 0.2870987228209931, 0.33738913388260916, 0.3843333708438776, 0.42821779977093316, 0.4682949288123891, 0.5057218353174551, 0.5401079216731219, 0.5713390675479839, 0.6006535266281339, 0.6279524681871275, 0.6524982641021264, 0.6754751097803228, 0.6966333236430989, 0.7164615803683978, 0.734527972655709, 0.751315534494051, 0.7668984542127891, 0.7813531984405966, 0.7944019957753852, 0.8064039327969146, 0.8177568445306045, 0.8283418386028302, 0.8381053902999485, 0.847171568635679, 0.8555542352770945, 0.8634729842127018, 0.8708579137237867, 0.8775300350671241, 0.8836371426772905, 0.8894311225252407, 0.8948618350812253, 0.9001005378030308, 0.904817731450284, 0.9090957578161453, 0.9131598952358966, 0.916994258125559, 0.9205311431345702, 0.9238725579081841, 0.9271868229449312, 0.9302903168743675, 0.933109038047765, 0.9357761675347936, 0.9383323690433575, 0.9405794719784458, 0.9427460598950346, 0.9447517565425743, 0.9466452752289406, 0.9485423933490421, 0.9503405886071611, 0.951724160358525, 0.9532110101668494, 0.9546469393907976, 0.9559996935125797, 0.9573984617316891, 0.9585899598622301, 0.9596763311702116, 0.9608701960402244, 0.9618377896756353, 0.9628038469022393, 0.9637966033907253, 0.9646040176755915, 0.9654935590950939, 0.9662591970428658, 0.9670853828779848, 0.967791855895096, 0.9686019235759352, 0.9692938181565569, 0.969944389016012, 0.9705369503335876, 0.9712003557062827, 0.9717161124310125, 0.9722570233939836, 0.972834524064229, 0.9733403598234115, 0.973863041322152, 0.974324190124599, 0.9747322125515541, 0.9751016497381668, 0.9754411186525468, 0.9757792288076409, 0.9761649802531965, 0.9765308298423102, 0.9768483966308275, 0.9771993109071596, 0.97755698643443, 0.9778067787470593, 0.9780502290677207, 0.9782785989027352, 0.9784910711518581, 0.9786939579689351, 0.9789765014542953, 0.9791517355315955, 0.9792815444154515, 0.9795076904537882, 0.9796391062263772, 0.9798806493574309, 0.9799956595301227, 0.980227051870069, 0.9803888380486491, 0.9805576610486475, 0.9806910366069886, 0.9807971597654573, 0.9809438238818887, 0.9810014168247722, 0.9810579007709863, 0.9811691541428079, 0.9812599815822093, 0.9813208359872037, 0.9814360227719273, 0.9815489914734167, 0.9816436878583286, 0.9817079882654636, 0.9818658400306947, 0.9819428024527359, 0.9820190440790016, 0.9820272076735932, 0.982120585414678, 0.982123245173321, 0.9821651664858615, 0.982207545964767, 0.9822596383886391, 0.982350699397505, 0.9823931267757223, 0.9824359256649182, 0.9824070513985371, 0.9824159417909369], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 80640042, "moving_var_accuracy_train": [0.02881773473219087, 0.05914666530074754, 0.08521376506007411, 0.10530031807693496, 0.12079668764872156, 0.13147914788656626, 0.1381650855527911, 0.14168116491779614, 0.14196863487586614, 0.1403787313631301, 0.1369824846405515, 0.13206269643040855, 0.12659046438782376, 0.12063850784121348, 0.11399712193099913, 0.10734882867377607, 0.10064297593116522, 0.09411711622092793, 0.0876429553713463, 0.08141505992649707, 0.0754590004164931, 0.0697935570510658, 0.0643466413529187, 0.059208395648045656, 0.05444755352673767, 0.05001117706964537, 0.04586800183836154, 0.042020961961062815, 0.03845128766514644, 0.03517051816097843, 0.03214430099983383, 0.029330525728832418, 0.02673314402620856, 0.024361961445893778, 0.022191199051095963, 0.020219075201853436, 0.018397434924819267, 0.016722405018620375, 0.015198819433457942, 0.013811258539038734, 0.012542718685237566, 0.011388932290917735, 0.0103488982364302, 0.009400693483917618, 0.00853213083700609, 0.007742939970610678, 0.0070274534689210655, 0.006370153366436905, 0.005775384958595991, 0.005234051834113962, 0.004742915367843136, 0.004301015345513379, 0.003900015366638938, 0.0035272422670955964, 0.003194414541558663, 0.0028935301220284915, 0.002620646603251631, 0.00237619091570358, 0.0021513488342889656, 0.001946835774429318, 0.0017649800169370425, 0.001596908152232928, 0.0014456167360953004, 0.0013099251514946486, 0.0011847998967918377, 0.0010734414625457487, 0.0009713731294947891, 0.0008803790638526734, 0.0007968330945825603, 0.0007230556719521666, 0.0006550585677531929, 0.0005933618929664223, 0.0005371858639055627, 0.00048742823771169336, 0.00044107945893246, 0.00039960477506797493, 0.0003626458607783827, 0.00032868410303795397, 0.0002982744562762884, 0.00027036093461064466, 0.00024482318185766453, 0.00022156921818556927, 0.00020044944866148574, 0.0001814333700881373, 0.00016462927067905752, 0.00014937095690784389, 0.00013534149920358174, 0.00012291561674722715, 0.00011177544111777769, 0.00010115946280103839, 9.15769290486056e-05, 8.288861117764626e-05, 7.500605016970786e-05, 6.787591269762986e-05, 6.180679881794168e-05, 5.590248177277297e-05, 5.0463886712447027e-05, 4.5877776317100896e-05, 4.1445429632957425e-05, 3.78259744270946e-05, 3.4162423042788425e-05, 3.122806247338235e-05, 2.8340829134259856e-05, 2.5763257068790348e-05, 2.3347032717976842e-05, 2.111368856904955e-05, 1.9195912979581948e-05, 1.7306174205253572e-05, 1.5604270710347556e-05, 1.4155239453987938e-05, 1.281396212232304e-05, 1.1565895237555603e-05, 1.0528717672174921e-05, 9.59070325260332e-06, 8.712339575181488e-06, 7.878316498882905e-06, 7.314739467073614e-06, 6.636574450024383e-06, 6.0252320752027135e-06, 5.423308666172336e-06, 4.959452422325926e-06, 4.463570848937682e-06, 4.033030332050015e-06, 3.645891480935762e-06, 3.3057249184660523e-06, 3.0497813926404876e-06, 2.7610039951780456e-06, 2.501389299907899e-06, 2.2587538792485215e-06, 2.0335898430168856e-06], "duration": 124128.989803, "accuracy_train": [0.5658595696636213, 0.6640460977874677, 0.7134468582733481, 0.7407394189507198, 0.7710797270095054, 0.7900028334371539, 0.8068315034952934, 0.8231776601144334, 0.8289890901854927, 0.8425639938630491, 0.8495826988741234, 0.8524193804217424, 0.8644836583494832, 0.8736429422180694, 0.8734104273371169, 0.8822667208840901, 0.8870572484080842, 0.8949158908960871, 0.8971255032415099, 0.9024035910391289, 0.9071447316814323, 0.9114458964908637, 0.9118411717884828, 0.9144213659906791, 0.9199330501338132, 0.9236067852528608, 0.9259773555740125, 0.9287671736572536, 0.9309982350498339, 0.9347417246331673, 0.9373222793235512, 0.9375791271571613, 0.9386011111687893, 0.9415769411567922, 0.9437382480850868, 0.9472488622992802, 0.9472724742755629, 0.9475979951088963, 0.9497371320136582, 0.9515035241325213, 0.9523631082156699, 0.9539452908707088, 0.9570152082756552, 0.9582217622392949, 0.9584775286083426, 0.959780332918051, 0.9613381826204319, 0.9608033983942414, 0.9622453511443337, 0.9628030263704319, 0.9636869434062385, 0.9656164564299556, 0.9665243459302326, 0.9641763061208011, 0.9665926584417681, 0.9675703024063308, 0.9681744806086194, 0.9699873757036729, 0.9693134430370985, 0.9694536729420451, 0.9716149798703396, 0.9705461323943337, 0.9714983619416758, 0.9727314117870985, 0.9718707462393872, 0.9734994318706165, 0.9731499385728128, 0.9745210553940569, 0.9741501130490956, 0.9758925327034883, 0.9755208693821521, 0.9757995267511074, 0.9758700021917681, 0.9771710040605389, 0.9763579229535806, 0.9771252220607235, 0.9780320300964378, 0.9778928816560539, 0.9785671748108158, 0.9784745293466224, 0.9784044143941492, 0.9784265844176817, 0.9784963388819674, 0.9788222202034883, 0.9796367432631967, 0.9798234761443337, 0.9797064977274824, 0.9803575393941492, 0.9807760661798633, 0.9800549095607235, 0.9802412819536729, 0.9803339274178663, 0.9804033213939645, 0.9805199393226283, 0.981519392822536, 0.9807288422272978, 0.980449824370155, 0.9815430047988187, 0.9808218481796788, 0.982054537536914, 0.9810307510843485, 0.9823095829295865, 0.9818449136558692, 0.9820770680486341, 0.9818914166320598, 0.9817522681916758, 0.9822638009297711, 0.9815197533107235, 0.981566256286914, 0.9821704344892026, 0.9820774285368217, 0.9818685256321521, 0.9824727038344407, 0.9825657097868217, 0.982495955322536, 0.9822866919296788, 0.983286505917774, 0.9826354642511074, 0.9827052187153931, 0.9821006800249169, 0.9829609850844407, 0.9821471830011074, 0.9825424582987264, 0.9825889612749169, 0.9827284702034883, 0.9831702484772978, 0.9827749731796788, 0.9828211156676817, 0.9821471830011074, 0.982495955322536], "end": "2016-01-26 14:02:07.055000", "learning_rate_per_epoch": [0.0006422974402084947, 0.0006148550310172141, 0.0005885850987397134, 0.0005634375265799463, 0.0005393644096329808, 0.0005163198220543563, 0.0004942598170600832, 0.0004731423396151513, 0.0004529271391220391, 0.0004335756239015609, 0.0004150509194005281, 0.0003973176935687661, 0.0003803421277552843, 0.00036409185850061476, 0.0003485358611214906, 0.00033364450791850686, 0.0003193893935531378, 0.00030574333504773676, 0.0002926803135778755, 0.000280175416264683, 0.00026820480707101524, 0.00025674563948996365, 0.00024577605654485524, 0.00023527516168542206, 0.0002252229314763099, 0.00021560018649324775, 0.00020638857677113265, 0.00019757053814828396, 0.0001891292486106977, 0.00018104861374013126, 0.00017331323761027306, 0.0001659083500271663, 0.0001588198501849547, 0.00015203420480247587, 0.0001455384772270918, 0.00013932028377894312, 0.00013336776464711875, 0.00012766956933774054, 0.0001222148275701329, 0.0001169931492768228, 0.00011199456639587879, 0.00010720954742282629, 0.00010262897558277473, 9.824410517467186e-05, 9.40465833991766e-05, 9.002839942695573e-05, 8.618189895059913e-05, 8.24997405288741e-05, 7.897490286268294e-05, 7.560066296719015e-05, 7.237058889586478e-05, 6.927852518856525e-05, 6.631857104366645e-05, 6.348508031805977e-05, 6.077265425119549e-05, 5.8176115999231115e-05, 5.5690517910989e-05, 5.33111160621047e-05, 5.1033373893005773e-05, 4.8852951294975355e-05, 4.6765686420258135e-05, 4.476760295801796e-05, 4.2854888306465e-05, 4.102389357285574e-05, 3.927112993551418e-05, 3.7593254091916606e-05, 3.598706462071277e-05, 3.4449501981725916e-05, 3.2977630326058716e-05, 3.156864841002971e-05, 3.021986412932165e-05, 2.8928707251907326e-05, 2.7692714866134338e-05, 2.6509531380725093e-05, 2.537689942982979e-05, 2.4292659873026423e-05, 2.3254744519363157e-05, 2.226117612735834e-05, 2.131005749106407e-05, 2.039957689703442e-05, 1.95279953913996e-05, 1.8693654055823572e-05, 1.7894959455588832e-05, 1.7130389096564613e-05, 1.6398485968238674e-05, 1.5697853086749092e-05, 1.5027155313873664e-05, 1.4385113900061697e-05, 1.3770503755949903e-05, 1.31821534523624e-05, 1.2618940672837198e-05, 1.2079791304131504e-05, 1.1563676707737613e-05, 1.106961371988291e-05, 1.0596660104056355e-05, 1.0143913641513791e-05, 9.710510312288534e-06, 9.295624295191374e-06, 8.898464329831768e-06, 8.518273716617841e-06, 8.15432667877758e-06, 7.805929271853529e-06, 7.472417564713396e-06, 7.153155365813291e-06, 6.847533768450376e-06, 6.5549697865208145e-06, 6.2749058997724205e-06, 6.006807780067902e-06, 5.750164291384863e-06, 5.504486125573749e-06, 5.269304438115796e-06, 5.044171302870382e-06, 4.828656983590918e-06, 4.622350388672203e-06, 4.424858616403071e-06, 4.235804681229638e-06, 4.05482796850265e-06, 3.8815837797301356e-06, 3.715741513588e-06, 3.556984893293702e-06, 3.405011057111551e-06, 3.259530558352708e-06, 3.1202657737594564e-06, 2.9869511308788788e-06, 2.8593324259418296e-06, 2.737166141741909e-06, 2.6202194476354634e-06, 2.5082695174205583e-06, 2.401102619842277e-06, 2.2985145733400714e-06], "accuracy_valid": [0.5645266613328314, 0.6510274496423193, 0.6941300357680723, 0.7188808946724398, 0.7444347703313253, 0.7568359375, 0.7744964231927711, 0.7854930464043675, 0.789562547063253, 0.7969985410391567, 0.8022166792168675, 0.7944953642695783, 0.8087172910391567, 0.8125014707266567, 0.814098679875753, 0.8191962184676205, 0.8196947948042168, 0.8239672557417168, 0.8267851680158133, 0.826927828501506, 0.8268969432417168, 0.831200289439006, 0.8289618434676205, 0.829857516001506, 0.8331328242658133, 0.8334181452371988, 0.8340284967996988, 0.8373449854103916, 0.8364801981362951, 0.8335093302899097, 0.8387995340737951, 0.8347506235881024, 0.8354830454631024, 0.837670016001506, 0.8406614740210843, 0.8394098856362951, 0.8406717691076807, 0.8392981104103916, 0.8399187570594879, 0.8396746164344879, 0.8423704583960843, 0.8421469079442772, 0.8451883706701807, 0.8437132318335843, 0.8414953760353916, 0.8422483880835843, 0.8467855798192772, 0.8453104409826807, 0.8436014566076807, 0.8421057275978916, 0.8428484445594879, 0.8467855798192772, 0.8431234704442772, 0.8417498117469879, 0.8446794992469879, 0.8475282967808735, 0.8454119211219879, 0.8452898508094879, 0.8443338784826807, 0.8460531579442772, 0.8481283532567772, 0.8470297204442772, 0.8488401849585843, 0.8471620858433735, 0.8475885965737951, 0.8473547510353916, 0.8474871164344879, 0.8489828454442772, 0.8508344903049698, 0.8508447853915663, 0.8472429758094879, 0.8498476327183735, 0.8492372811558735, 0.8498373376317772, 0.8504682793674698, 0.8490946206701807, 0.8483621987951807, 0.8492475762424698, 0.8499697030308735, 0.8509462655308735, 0.8499594079442772, 0.8485857492469879, 0.8494814217808735, 0.8483519037085843, 0.8483724938817772, 0.8491358010165663, 0.8486063394201807, 0.8499594079442772, 0.8486166345067772, 0.8473753412085843, 0.8468767648719879, 0.8463678934487951, 0.8462870034826807, 0.8484739740210843, 0.8477415521460843, 0.8488710702183735, 0.8480165780308735, 0.8493490563817772, 0.8475077066076807, 0.8468767648719879, 0.8483621987951807, 0.8499594079442772, 0.8490034356174698, 0.8476297769201807, 0.8477518472326807, 0.8476400720067772, 0.8478842126317772, 0.8477621423192772, 0.8473856362951807, 0.8476297769201807, 0.8469988351844879, 0.8482195383094879, 0.8487284097326807, 0.8500814782567772, 0.8487284097326807, 0.8478533273719879, 0.8504579842808735, 0.8491049157567772, 0.8498476327183735, 0.8496240822665663, 0.8498476327183735, 0.8477518472326807, 0.8481180581701807, 0.8476400720067772, 0.8481386483433735, 0.8494917168674698, 0.8487489999058735, 0.8481283532567772, 0.8481180581701807, 0.8472635659826807], "accuracy_test": 0.8417291135204081, "start": "2016-01-25 03:33:18.065000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0], "accuracy_train_last": 0.982495955322536, "batch_size_eval": 1024, "accuracy_train_std": [0.017076316958708482, 0.018980882823027057, 0.015448849480190353, 0.017603405832337164, 0.018134940606849347, 0.018081575602394772, 0.018063854200509336, 0.018588712306218034, 0.018213133428581513, 0.017955489884575973, 0.017010761527752615, 0.01693335478194195, 0.018135276007152888, 0.01727898102846401, 0.017823527857470697, 0.016469723569412977, 0.01670136809752471, 0.015571881113277985, 0.01608185385388358, 0.015071700572215779, 0.014256966041383481, 0.015066981139924867, 0.01401222191853883, 0.012940922437936854, 0.012436414667198928, 0.012387022408032236, 0.01201141545620631, 0.011621409232511972, 0.01138691966331948, 0.013267278075342807, 0.011235596424540665, 0.011128978575527583, 0.010858416481002615, 0.010589704188806543, 0.01114221923746583, 0.010936079923647285, 0.010186154632689131, 0.010403559129241754, 0.010142580953778962, 0.010193005290731588, 0.009814171749839715, 0.010539619242439681, 0.009699793730945434, 0.009499103755271708, 0.00983649023626373, 0.009080973444094082, 0.008984065073357632, 0.008825045776540755, 0.008607242309227998, 0.00818083566738375, 0.008290669419952618, 0.008731454767213844, 0.007967461152967769, 0.008619880609738841, 0.008749984292017409, 0.0075022162335876275, 0.007358202454611664, 0.0067982219644400115, 0.00778192690416833, 0.006887574121016694, 0.006056218944454107, 0.006754039871208032, 0.00608708153816313, 0.0056660566750832975, 0.006669274687096183, 0.006363904435382397, 0.0063025851027376, 0.005519536856197565, 0.006153848943756894, 0.00545658528635163, 0.005488087336151419, 0.005518920738282686, 0.004852938427008836, 0.004846414227032224, 0.005529183969667132, 0.005414858278198191, 0.004375446329619039, 0.004885981269441542, 0.004463296692832943, 0.004561256253245188, 0.00442900914933982, 0.004519802030854299, 0.004135009862339336, 0.004403952528029016, 0.004325757928255041, 0.004479661402534654, 0.004071294847346678, 0.004140543739913398, 0.0037984298586120047, 0.004132446884023471, 0.003752966810440789, 0.0042589277152263235, 0.003570222181339362, 0.003962539170832297, 0.0036381828128997803, 0.004161159904868805, 0.0035834669347605715, 0.0038085299777814957, 0.004186967849436164, 0.003983589538767067, 0.004097566363308697, 0.003743315956920621, 0.00378263581157592, 0.004066950905099885, 0.0036676383057493104, 0.0036851161339319237, 0.004011541807110726, 0.0040935131055812235, 0.004324410396316683, 0.0037460698062016305, 0.003864103036128277, 0.0035573939481452427, 0.0036839418470038052, 0.0037060727067018554, 0.0038315980013744486, 0.003581777161834439, 0.0034423892329015176, 0.003478200577844586, 0.003953244412847021, 0.004046885333488283, 0.0035849612722948045, 0.003831356814249636, 0.003947195467684897, 0.003695893117497559, 0.0037114950345074446, 0.0038451342870890652, 0.003768109381685077, 0.0036559659547289467, 0.003674038024879548, 0.0037234000746345338], "accuracy_test_std": 0.013265768429352119, "error_valid": [0.43547333866716864, 0.3489725503576807, 0.3058699642319277, 0.28111910532756024, 0.2555652296686747, 0.2431640625, 0.22550357680722888, 0.21450695359563254, 0.21043745293674698, 0.20300145896084332, 0.19778332078313254, 0.20550463573042166, 0.19128270896084332, 0.18749852927334332, 0.18590132012424698, 0.18080378153237953, 0.1803052051957832, 0.1760327442582832, 0.17321483198418675, 0.17307217149849397, 0.1731030567582832, 0.16879971056099397, 0.17103815653237953, 0.17014248399849397, 0.16686717573418675, 0.16658185476280118, 0.16597150320030118, 0.1626550145896084, 0.16351980186370485, 0.1664906697100903, 0.16120046592620485, 0.16524937641189763, 0.16451695453689763, 0.16232998399849397, 0.15933852597891573, 0.16059011436370485, 0.1593282308923193, 0.1607018895896084, 0.16008124294051207, 0.16032538356551207, 0.15762954160391573, 0.15785309205572284, 0.1548116293298193, 0.15628676816641573, 0.1585046239646084, 0.15775161191641573, 0.15321442018072284, 0.1546895590173193, 0.1563985433923193, 0.1578942724021084, 0.15715155544051207, 0.15321442018072284, 0.15687652955572284, 0.15825018825301207, 0.15532050075301207, 0.1524717032191265, 0.15458807887801207, 0.15471014919051207, 0.1556661215173193, 0.15394684205572284, 0.15187164674322284, 0.15297027955572284, 0.15115981504141573, 0.1528379141566265, 0.15241140342620485, 0.1526452489646084, 0.15251288356551207, 0.15101715455572284, 0.14916550969503017, 0.14915521460843373, 0.15275702419051207, 0.1501523672816265, 0.1507627188441265, 0.15016266236822284, 0.14953172063253017, 0.1509053793298193, 0.1516378012048193, 0.15075242375753017, 0.1500302969691265, 0.1490537344691265, 0.15004059205572284, 0.15141425075301207, 0.1505185782191265, 0.15164809629141573, 0.15162750611822284, 0.15086419898343373, 0.1513936605798193, 0.15004059205572284, 0.15138336549322284, 0.15262465879141573, 0.15312323512801207, 0.15363210655120485, 0.1537129965173193, 0.15152602597891573, 0.15225844785391573, 0.1511289297816265, 0.1519834219691265, 0.15065094361822284, 0.1524922933923193, 0.15312323512801207, 0.1516378012048193, 0.15004059205572284, 0.15099656438253017, 0.1523702230798193, 0.1522481527673193, 0.15235992799322284, 0.15211578736822284, 0.15223785768072284, 0.1526143637048193, 0.1523702230798193, 0.15300116481551207, 0.15178046169051207, 0.1512715902673193, 0.14991852174322284, 0.1512715902673193, 0.15214667262801207, 0.1495420157191265, 0.15089508424322284, 0.1501523672816265, 0.15037591773343373, 0.1501523672816265, 0.1522481527673193, 0.1518819418298193, 0.15235992799322284, 0.1518613516566265, 0.15050828313253017, 0.1512510000941265, 0.15187164674322284, 0.1518819418298193, 0.1527364340173193], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.791077376318944, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0006709646951574979, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 2.293860272297184e-07, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.04272543449444677}, "accuracy_valid_max": 0.8509462655308735, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8472635659826807, "loss_train": [1.5755202770233154, 1.1591103076934814, 1.0005674362182617, 0.8977777361869812, 0.8188690543174744, 0.7558214664459229, 0.7052299380302429, 0.6624117493629456, 0.6275675892829895, 0.5935584902763367, 0.5657072067260742, 0.5432972311973572, 0.5179776549339294, 0.4986687898635864, 0.48094555735588074, 0.46248987317085266, 0.4453275501728058, 0.430316686630249, 0.4184529483318329, 0.4053104519844055, 0.3921285569667816, 0.3813524842262268, 0.3681894540786743, 0.35864904522895813, 0.3507760167121887, 0.34178853034973145, 0.3290276825428009, 0.3215177059173584, 0.31564250588417053, 0.3093341290950775, 0.3034156262874603, 0.29433920979499817, 0.2890823781490326, 0.2832218110561371, 0.2749459147453308, 0.2710023522377014, 0.26504582166671753, 0.2606392800807953, 0.25490307807922363, 0.2515546381473541, 0.24701730906963348, 0.24319802224636078, 0.23910455405712128, 0.234434112906456, 0.2299252599477768, 0.22707264125347137, 0.22563737630844116, 0.22130821645259857, 0.21697065234184265, 0.216239795088768, 0.21249061822891235, 0.21101601421833038, 0.20636248588562012, 0.20438165962696075, 0.20451433956623077, 0.2011651247739792, 0.19795997440814972, 0.195714071393013, 0.1937064379453659, 0.19505397975444794, 0.19199292361736298, 0.188418909907341, 0.187898188829422, 0.1861778348684311, 0.1850687861442566, 0.18360576033592224, 0.18158456683158875, 0.18069684505462646, 0.1775854229927063, 0.1771472841501236, 0.17711083590984344, 0.1749427616596222, 0.17333321273326874, 0.17227281630039215, 0.17185179889202118, 0.16945238411426544, 0.17103774845600128, 0.17438186705112457, 0.16989746689796448, 0.16738778352737427, 0.16723917424678802, 0.16631744801998138, 0.16588540375232697, 0.16496260464191437, 0.1637084186077118, 0.16395020484924316, 0.16110190749168396, 0.16370432078838348, 0.1612670123577118, 0.1617857664823532, 0.16096702218055725, 0.1608530431985855, 0.16092364490032196, 0.1603250950574875, 0.1603134125471115, 0.16014768183231354, 0.15852238237857819, 0.1572684645652771, 0.1565946340560913, 0.15964050590991974, 0.15772312879562378, 0.15674491226673126, 0.15854299068450928, 0.15602804720401764, 0.15473707020282745, 0.15657296776771545, 0.15458247065544128, 0.15620149672031403, 0.15711897611618042, 0.1539338231086731, 0.1540464609861374, 0.15504813194274902, 0.15486688911914825, 0.15414443612098694, 0.15290828049182892, 0.15324734151363373, 0.15285056829452515, 0.15443584322929382, 0.15365126729011536, 0.15435056388378143, 0.15277184545993805, 0.15056972205638885, 0.1517176628112793, 0.15195779502391815, 0.15075533092021942, 0.15176096558570862, 0.15136639773845673, 0.15079402923583984, 0.15286681056022644, 0.15139992535114288], "accuracy_train_first": 0.5658595696636213, "model": "residualv5", "loss_std": [0.27643463015556335, 0.1791296750307083, 0.17205436527729034, 0.17042100429534912, 0.16894836723804474, 0.16548101603984833, 0.1626676619052887, 0.15730886161327362, 0.1559450626373291, 0.15177375078201294, 0.15094093978405, 0.1465093195438385, 0.14429785311222076, 0.14037933945655823, 0.13822878897190094, 0.13528881967067719, 0.1323227882385254, 0.1307908594608307, 0.12854711711406708, 0.1284078061580658, 0.12401372194290161, 0.12344193458557129, 0.11984504014253616, 0.11864538490772247, 0.11614242941141129, 0.11356256157159805, 0.1127202957868576, 0.11092282831668854, 0.11098641157150269, 0.10759404301643372, 0.10709977149963379, 0.10524141043424606, 0.10562274605035782, 0.10202645510435104, 0.09940316528081894, 0.09828930348157883, 0.09723693877458572, 0.09844055771827698, 0.0938815176486969, 0.09456318616867065, 0.09297487139701843, 0.09122326225042343, 0.09125354140996933, 0.08955658972263336, 0.08712101727724075, 0.08858532458543777, 0.08742281794548035, 0.09023529291152954, 0.08731916546821594, 0.086592897772789, 0.08348269015550613, 0.08423580229282379, 0.08330506086349487, 0.08076494187116623, 0.08287938684225082, 0.07936979830265045, 0.08000180125236511, 0.07895174622535706, 0.08214469254016876, 0.08047585189342499, 0.07777281850576401, 0.07750563323497772, 0.07744678854942322, 0.07528749853372574, 0.07710318267345428, 0.07494963705539703, 0.07701899111270905, 0.07375974953174591, 0.07570501416921616, 0.07350414246320724, 0.07589004188776016, 0.07363977283239365, 0.07087428122758865, 0.0730026587843895, 0.07284260541200638, 0.07083632797002792, 0.07376173883676529, 0.07249383628368378, 0.07100328803062439, 0.07019997388124466, 0.07105635851621628, 0.06979089230298996, 0.07217897474765778, 0.07076342403888702, 0.07168759405612946, 0.07081464678049088, 0.06777097284793854, 0.07148737460374832, 0.07029417157173157, 0.0686727985739708, 0.06821245700120926, 0.06928696483373642, 0.06785284727811813, 0.06821843981742859, 0.06815934926271439, 0.06854715198278427, 0.06796007603406906, 0.0675552636384964, 0.06851060688495636, 0.06877084076404572, 0.06737268716096878, 0.06792319566011429, 0.06898511946201324, 0.06903116405010223, 0.06791163235902786, 0.06830248981714249, 0.06657387316226959, 0.06854838877916336, 0.06809964776039124, 0.06532401591539383, 0.0666140615940094, 0.06658356636762619, 0.06967314332723618, 0.06730587035417557, 0.06296341121196747, 0.06636207550764084, 0.06534355878829956, 0.06794905662536621, 0.0651782900094986, 0.06573299318552017, 0.06473276019096375, 0.06499728560447693, 0.06582817435264587, 0.06709153205156326, 0.06762052327394485, 0.06692247092723846, 0.06555035710334778, 0.06539511680603027, 0.06661704182624817, 0.06625855714082718]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:15 2016", "state": "available"}], "summary": "0096d3b177933c99b4af0dcc611c9315"}