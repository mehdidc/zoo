{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 64, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 8, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.008634129911543623, 0.012904931422462863, 0.010015891274171261, 0.009902001304803646, 0.019772154236500868, 0.011491058243778276, 0.020877646107439037, 0.011315554031595032, 0.008490550717913886, 0.01626719150331801, 0.014315917594608344, 0.009885162398083626, 0.014461063725998206, 0.014666769062428853, 0.013331080497319271, 0.017202398521330448, 0.013054499868487158, 0.008738606688236488, 0.010023365901490784, 0.015302189899390223, 0.017631760073902016, 0.012252427730860466, 0.011113511214840409, 0.01621818018489313, 0.010597627980371234, 0.00981995003428319, 0.011305236074865901, 0.010240039726293185, 0.010269976302313175, 0.013448818319840636, 0.011901121559427187, 0.012822747352987447, 0.015606320060443192, 0.013803553123611723, 0.015062318221999865, 0.013282773247619431, 0.011819161220202497, 0.008880849171093122, 0.014208010871982146, 0.008975299060049624, 0.018829058479026266, 0.015070418213884449, 0.013958316096330017, 0.012571381836121378, 0.011133481223771825, 0.017365456445682045, 0.01504021398817412, 0.016576614103405777, 0.010786428818137713, 0.012114651775542875, 0.019744963613122533, 0.013924091164863813, 0.015489314359626551, 0.014581731915488136, 0.012956109191566085, 0.017257726369217675, 0.014970728194916317, 0.015427613898507608, 0.015587773291484246, 0.012120565867840915, 0.016870040169898067, 0.01815681857701388, 0.01323592276200491, 0.015063117798108426, 0.011122098240823356, 0.012257488424953413, 0.014617468707906806, 0.012842492960109494, 0.014051799908981016, 0.011351152943280785, 0.015229455699627673, 0.018090856160574823, 0.011769782718106466, 0.015506261381356648, 0.019483070736787275, 0.011196168913860364, 0.01701549150659256, 0.013899378479235277, 0.01536291949184773, 0.013044390017294906, 0.011556248625566425, 0.01240375671678357, 0.012646311758252744, 0.018576604365653587, 0.014215931572391148, 0.015338004311138418, 0.012520941278950034, 0.010899830501498814, 0.019426233104135545, 0.017791734274669044, 0.01174400422506441, 0.016261157833610686, 0.015953240371483657, 0.019128162055669577, 0.015150166452861392, 0.013497495314884846, 0.012559185871553453, 0.011260775269644804, 0.014267603512637847, 0.012917284972009953, 0.01909667874565644, 0.010020739170042368, 0.011516031271148572, 0.015863220357018183, 0.015273891586166518, 0.018776446454933957, 0.013986489489945245, 0.009882543802102177, 0.01738730741825526, 0.015611053989815523, 0.011898563689025273, 0.01206683307924134, 0.013069724705524317, 0.010322588169623599, 0.015508450382487798, 0.013793883058549656, 0.007460240053700738, 0.011706689668181664, 0.019897025987269473, 0.010443572306898734, 0.013928374297758703, 0.011386760425780598, 0.014739482259219016, 0.011289217726905602, 0.01677736015258472, 0.02058247502088843, 0.012084072956118479, 0.012999314138461721, 0.014856781238339393, 0.014072890769731413, 0.013717265530567005, 0.013523804887074557, 0.00899106569693047, 0.011888822679183793, 0.008642747565746116, 0.012337838790036689, 0.014385395005948565, 0.01454616078821085, 0.01783266296471117, 0.00502059811292525, 0.010245764553885818, 0.013551609571774864, 0.013993642861874067, 0.012785131635835558, 0.012219534882660067, 0.009816634085880075, 0.010838441540221231, 0.012612766250098783, 0.00947988728314705, 0.014964101034038613, 0.01111316554431365, 0.009262520188344343, 0.01225832793589551, 0.010590344616954631, 0.014655730252600137, 0.014633359147394876, 0.013792932200387088, 0.013789722540618997, 0.01811352682257019, 0.016416447792407425, 0.012755887256321947, 0.018720844834200934, 0.011316658095223973, 0.011758601172868404, 0.01683453369973033, 0.013261714420049739, 0.01654291590063841, 0.016472695254243625, 0.010922593192968108, 0.012605537601345484, 0.010902673179438934, 0.01129816075351742, 0.012195581855260927, 0.011053007709584102, 0.011672003923710692, 0.013747854904375398, 0.015135080455900868, 0.012007188247666346, 0.01230427316029834, 0.009985143565176356, 0.009815779885011096, 0.01710397959873275, 0.009700843013488558, 0.011745020774892367, 0.01224953735080899, 0.010326252444636573, 0.009830277012652754, 0.012432487914604546, 0.011416836314391481, 0.008626262576339798, 0.010071448089589234, 0.010785284916209375, 0.012452006405367444, 0.010273404162172449, 0.009485629638878824, 0.009801665278643876, 0.01249828843265364, 0.01404723474360022, 0.011913757337739707, 0.009963853006941565, 0.009923705746785087, 0.00855339308635401, 0.01104585669449669, 0.011757097348084203, 0.010217702372291696, 0.012091706015931602, 0.010795303573315627, 0.011793463424035911, 0.010943177278101294, 0.014577027110009464, 0.011283835109430504, 0.012863790216014222, 0.010092588829487792, 0.01451613848461459, 0.014692629670635553, 0.009169444527734862, 0.012552115579636817, 0.008992395504117344, 0.015238033021779809, 0.011099641406898876, 0.011242800982693734, 0.011357775806043283, 0.008996377615209232, 0.009368165269892995, 0.010047159654279644, 0.010691537333559839, 0.009197039304029321, 0.013013035251737397, 0.00848480754610134, 0.010472135527465561, 0.009818803684144967, 0.01421644820813149, 0.010410027996386654, 0.011783815881829716, 0.009554237014711472, 0.018235097963494224, 0.01563199705168361, 0.0131939077719221, 0.015485678711031242, 0.009747153344085406, 0.013849508540293142, 0.010710755272634447, 0.009440774381588017, 0.014290353683825325, 0.014189631072431558, 0.012476864148580975, 0.008501868702243552, 0.010127807770947886, 0.010376781522202179, 0.0120960142543459, 0.009246696044487081, 0.007974730197320277, 0.015598339160810725, 0.010782927742712127, 0.01021315955120927, 0.011878162114019595, 0.010736526912560121, 0.010442465727985973, 0.011554142437956135, 0.01339832395860994, 0.012271881323065538, 0.01380615286938441, 0.010216615472154243], "moving_avg_accuracy_train": [0.05162198055094129, 0.10901041306178476, 0.167394264128253, 0.22690951685210176, 0.2837227929723548, 0.3374074664828897, 0.3875019788958318, 0.4344118492971309, 0.4784089668141232, 0.51870892800808, 0.5559758772384643, 0.5897903188612403, 0.6216183335098155, 0.6509447431970861, 0.6775894116941457, 0.7016532826498234, 0.7245408783766276, 0.7456907387497899, 0.765520813978493, 0.7832726226807729, 0.7998768964961213, 0.8136162437489639, 0.8269978544527404, 0.8391527309849027, 0.8510314439340777, 0.8612619061240494, 0.8708042156212329, 0.8794990707698424, 0.8874221327024095, 0.8953853277643484, 0.9021010523534082, 0.908384839006218, 0.9141447344949004, 0.9194890757025717, 0.9241851946930841, 0.9284837453488219, 0.9326616496818338, 0.9368634697577165, 0.9400080531010202, 0.9437449140480703, 0.9462501450385198, 0.9492605983906571, 0.9518909873968757, 0.9540002099357965, 0.9561728056827116, 0.9582026187144773, 0.9597039296097332, 0.9610388694226155, 0.9630655628374968, 0.9642526763811743, 0.9650629149549893, 0.9666453871868805, 0.9676999495836871, 0.9687816613205934, 0.9701596696302099, 0.9713162078005407, 0.9722454129133438, 0.9733561371720278, 0.9742093046298435, 0.9751190254680773, 0.9753518006736689, 0.9761635479491868, 0.976940695570981, 0.9772679964746156, 0.9783414560902585, 0.9792680782633939, 0.9797160274680161, 0.9797843963724234, 0.9804271795399523, 0.9811242669800139, 0.9816377133844026, 0.9811048317019702, 0.9816132822365535, 0.9821662909165065, 0.9825407297927407, 0.982791694275399, 0.9832500050931064, 0.9835694428278434, 0.9841197507022205, 0.9840151754451213, 0.9842441813006184, 0.9846897768979467, 0.9848838386426758, 0.9850515187665034, 0.9855722016363001, 0.9856944050953168, 0.9850625575917376, 0.9851775967349725, 0.9856019304019608, 0.9860256833808215, 0.9865767608760727, 0.9865588727348941, 0.9865777587864324, 0.9865621681006648, 0.986845755531093, 0.9872661058327733, 0.9871118538804483, 0.9872450695340702, 0.9872580428259106, 0.9870952965790338, 0.9873232460128156, 0.9875609165377337, 0.9879212683363413, 0.9881712162420021, 0.9882589495285161, 0.988447263578064, 0.988251661810743, 0.9887498773260972, 0.9890262102780113, 0.989116835864505, 0.989219361280454, 0.9894696721762182, 0.9894624731502722, 0.989732722784073, 0.9899898262497133, 0.9903467774045039, 0.9904332294628723, 0.9904389567023085, 0.990327853777325, 0.9904557257281731, 0.9907103194125079, 0.9909184913403046, 0.9910314773622357, 0.9911261532867265, 0.9912416246021106, 0.9913757396716615, 0.9912918301390191, 0.9911861927715734, 0.9912514823110918, 0.9914125494442776, 0.9914366381748683, 0.9917071089550189, 0.9918644661023833, 0.9918665425576212, 0.9921799813078114, 0.9920901244710962, 0.9917557400001771, 0.9917733754120734, 0.992044977603009, 0.9921057328188986, 0.9921929645965325, 0.9922482217083078, 0.9923491424315339, 0.9925121227931701, 0.992491394404357, 0.9923331217793975, 0.9925464962824285, 0.9926432022339661, 0.9925093124046263, 0.9923936060511146, 0.9927426941067174, 0.9928871374936648, 0.992905565447879, 0.9931476900411955, 0.9931331593918655, 0.9932711083336313, 0.9932557895014679, 0.9931885601787205, 0.9933163187441818, 0.9934499386923918, 0.9931911973898286, 0.993358219763941, 0.993606196150642, 0.9935806200248728, 0.9937831048973854, 0.9936537713421707, 0.9937164436496295, 0.9938286162489522, 0.9936598903752567, 0.993735901672264, 0.9936531771669517, 0.9939042098966852, 0.9940627460867877, 0.9942960734126327, 0.9943038161082834, 0.9937387979272263, 0.9939394159023608, 0.9940525788133244, 0.9939614741308199, 0.9939863646641665, 0.9939111098941783, 0.9938038891202459, 0.9939747464879831, 0.9937797818463369, 0.993918208758141, 0.9940055545489935, 0.9943562081714751, 0.9942207536114797, 0.9942081985991689, 0.9942688705547281, 0.9940119774718928, 0.9941015721354178, 0.994340317451638, 0.9943645620826739, 0.9945352278232344, 0.9944609263575869, 0.9941638653063612, 0.9942987611043058, 0.9942713217498277, 0.9944233376403211, 0.9945323583025071, 0.9946676432306082, 0.9948079287587379, 0.9948645033674064, 0.9947851400842371, 0.9948276454210515, 0.9949844828134702, 0.9952210036166562, 0.9948688611788093, 0.9949262819430805, 0.9949918754749629, 0.9951322898619904, 0.9952679634055532, 0.9954319222733312, 0.9953167434388552, 0.9951038725914167, 0.9948052959346653, 0.9949574649269315, 0.995159485137819, 0.9950948375538082, 0.9946018158519988, 0.9946766045048941, 0.9947509618365661, 0.9949828969029095, 0.9951428103376185, 0.9951403922491318, 0.9952961818932662, 0.9954341034729964, 0.9953745100899825, 0.9954999125036033, 0.9955058538794426, 0.9956297837069837, 0.9955948361767708, 0.9956563533031413, 0.9955397298026075, 0.9956579829378415, 0.9957131853881049, 0.9958256826600179, 0.9958617899892542, 0.9959407895617574, 0.9959956131353436, 0.9958590145444467, 0.9958987641316687, 0.9960066183732637, 0.9960688099585564, 0.9962061625936531, 0.9961903070854876, 0.9961434489959865, 0.9962989504130637, 0.9963458596872335, 0.9963834998340049, 0.9963964135779946, 0.996361496922576, 0.9963626240160328, 0.9964496689060962, 0.9964303530571532, 0.9964734587109709, 0.9964006466565498, 0.9963862690813802, 0.9964267716375279, 0.9962842235285463, 0.996228009843558, 0.9963332385461253], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 983353862, "moving_var_accuracy_train": [0.023983459884015856, 0.051226003570079004, 0.07678146980123501, 0.1009819105821638, 0.11993345461559249, 0.1338785066834278, 0.14307579758009872, 0.1485730412916889, 0.15113745431075634, 0.15064049072979055, 0.14807587120127186, 0.14355903224148578, 0.13832033166556643, 0.13222864324532052, 0.12539522415465226, 0.11806733070753062, 0.11097517598016023, 0.10390350772638253, 0.09705224390592852, 0.09018315992515674, 0.08364616111305663, 0.07698047196815873, 0.07089403231658971, 0.06513429829653973, 0.05989080285884582, 0.054843683782545206, 0.05017881643915103, 0.045841339349733626, 0.041822179608245934, 0.03821067392777172, 0.034795515145799476, 0.0316713374035018, 0.028802791227516514, 0.026179569951260983, 0.023760094758292355, 0.02155038312212261, 0.019552438771452543, 0.017756092521858106, 0.01606947890929914, 0.014588208186007518, 0.013185873008246345, 0.011948851171890264, 0.010816236571617555, 0.009774652291924021, 0.008839668613247238, 0.00799278302041785, 0.00721379012801399, 0.0065084496939487564, 0.0058945721003351905, 0.005317798037391913, 0.004791926612571202, 0.0043352719165964445, 0.003911753641575626, 0.0035311091799539113, 0.003195088424070871, 0.002887617806518673, 0.00260662682514174, 0.0023570675180370295, 0.0021279118186330074, 0.0019225689648013583, 0.0017307997269882668, 0.0015636501570432373, 0.0014127207671734562, 0.0012724128233897911, 0.001155542380968558, 0.001047715800737419, 0.0009447501470729721, 0.0008503172011294836, 0.0007690040128206603, 0.0006964769896304192, 0.0006292019355589953, 0.0005688374079903446, 0.0005142803647063731, 0.00046560469563666594, 0.00042030606632131894, 0.00037884230823319104, 0.0003428485166605207, 0.00030948202919183366, 0.00028125937508206205, 0.00025323186143343194, 0.00022838066842675636, 0.00020732960051130636, 0.00018693558010708162, 0.00016849507171171518, 0.00015408556039864108, 0.00013881140752733785, 0.0001285233481846186, 0.00011579011940644269, 0.00010583163901425499, 9.686457439666986e-05, 8.991129460895374e-05, 8.092304501841183e-05, 7.2833950663055e-05, 6.555274322209385e-05, 5.972126537615611e-05, 5.533938822364493e-05, 5.001959238444495e-05, 4.517735083932947e-05, 4.0661130512107085e-05, 3.6833394528748644e-05, 3.36177035751265e-05, 3.076431872334766e-05, 2.885656761985037e-05, 2.6533176457763354e-05, 2.394913297805008e-05, 2.1873379311558876e-05, 2.0030381842814813e-05, 2.026131195619086e-05, 1.8922419863393784e-05, 1.710409484940053e-05, 1.5488288512699945e-05, 1.450335956227409e-05, 1.3053490039817812e-05, 1.2405454816961022e-05, 1.1759829063663276e-05, 1.1730573299453855e-05, 1.0624781595073607e-05, 9.562598647010293e-06, 8.71743352176833e-06, 7.992851291914932e-06, 7.77692765965186e-06, 7.389254857390524e-06, 6.765221942017913e-06, 6.169371523919666e-06, 5.672436993616716e-06, 5.2670749611806925e-06, 4.803734752076916e-06, 4.42379455747685e-06, 4.019779617464075e-06, 3.8512852482517676e-06, 3.471379125899833e-06, 3.7826311995477082e-06, 3.627219526033122e-06, 3.264536378427003e-06, 3.822277391671843e-06, 3.51271791244459e-06, 4.167762890727027e-06, 3.7537856714290884e-06, 4.042316855374914e-06, 3.6713059361575226e-06, 3.372659789804393e-06, 3.062873946439704e-06, 2.8482514831839715e-06, 2.802489719377174e-06, 2.52610774236455e-06, 2.498948982432262e-06, 2.6588121910829424e-06, 2.4770993415396265e-06, 2.3907277849913308e-06, 2.272146648679002e-06, 3.14169421889218e-06, 3.0152998252982334e-06, 2.716826148237089e-06, 2.9727624016118356e-06, 2.6773864193802034e-06, 2.580916972251004e-06, 2.3249372745955644e-06, 2.133121583669644e-06, 2.0667096847412136e-06, 2.0207273313041163e-06, 2.42117815304307e-06, 2.430128598825952e-06, 2.740546334195523e-06, 2.4723789446602593e-06, 2.594142162562687e-06, 2.4852724628467496e-06, 2.2720955796619926e-06, 2.158130250045212e-06, 2.1985330091295325e-06, 2.030679163671272e-06, 1.8892013413167886e-06, 2.2674380897617216e-06, 2.266897792935852e-06, 2.530182782516271e-06, 2.277704048288102e-06, 4.923143547786097e-06, 4.793057340531128e-06, 4.429004206237869e-06, 4.060804354182227e-06, 3.660299766616499e-06, 3.345239313608467e-06, 3.114182031511612e-06, 3.0654939893515956e-06, 3.1010454938465893e-06, 2.963399033666544e-06, 2.735722714916832e-06, 3.5687721100603865e-06, 3.377026339466319e-06, 3.0407423605267955e-06, 2.7697979001966253e-06, 3.086764614254626e-06, 2.850332986418596e-06, 3.0782936219301526e-06, 2.7757544789437434e-06, 2.7603201860592362e-06, 2.533974537629681e-06, 3.0747844972642838e-06, 2.9310779342656846e-06, 2.6447464044067398e-06, 2.5882512426286283e-06, 2.436395661417145e-06, 2.3574742012172743e-06, 2.2988470457190905e-06, 2.097768518261107e-06, 1.9446784428735567e-06, 1.7664709315055362e-06, 1.8112055473011516e-06, 2.1335638056288924e-06, 3.0362460938606313e-06, 2.7622957819999182e-06, 2.5247888066232233e-06, 2.449755726719734e-06, 2.370445947853908e-06, 2.3753439459759556e-06, 2.257205026579571e-06, 2.4393105031242654e-06, 2.997711632423929e-06, 2.9063390890473934e-06, 2.9830146706059523e-06, 2.7223269946112186e-06, 4.63772788124535e-06, 4.22429517653793e-06, 3.851626773844424e-06, 3.9506089714573905e-06, 3.78569883371566e-06, 3.407181574711466e-06, 3.2848971362161428e-06, 3.1276086819920572e-06, 2.8468101554842577e-06, 2.703661028013134e-06, 2.433612624733604e-06, 2.3284787816496732e-06, 2.106622872296578e-06, 1.93001979659892e-06, 1.8594271848297665e-06, 1.7993387022805901e-06, 1.6468306266883487e-06, 1.596048289710378e-06, 1.4481771137605475e-06, 1.359527794485614e-06, 1.2506256330238754e-06, 1.293495645036536e-06, 1.178366347691753e-06, 1.1652225497929552e-06, 1.0835104343446071e-06, 1.1449511082223207e-06, 1.0327185716527969e-06, 9.492078394527946e-07, 1.071913271924793e-06, 9.845262647605626e-07, 8.98824664125183e-07, 8.10443080767147e-07, 7.403713281209619e-07, 6.663456283658066e-07, 6.679023815045403e-07, 6.044700615375701e-07, 5.607459319032925e-07, 5.523856961342928e-07, 4.990075585306712e-07, 4.638709161680751e-07, 6.003634949194843e-07, 5.687669508471667e-07, 6.115479743586765e-07], "duration": 215673.710612, "accuracy_train": [0.516219805509413, 0.6255063056593761, 0.6928489237264673, 0.7625467913667405, 0.7950422780546327, 0.820569528077704, 0.8383525906123109, 0.8566006829088224, 0.8743830244670543, 0.8814085787536915, 0.8913784203119232, 0.8941202934662238, 0.9080704653469915, 0.9148824303825213, 0.9173914281676817, 0.9182281212509228, 0.9305292399178663, 0.9360394821082503, 0.9439914910368217, 0.943038901001292, 0.9493153608342562, 0.9372703690245479, 0.9474323507867294, 0.9485466197743633, 0.9579398604766519, 0.9533360658337948, 0.9566850010958842, 0.9577527671073275, 0.958729690095515, 0.9670540833217978, 0.9625425736549464, 0.9649389188815062, 0.9659837938930418, 0.9675881465716132, 0.9664502656076966, 0.9671707012504615, 0.9702627886789406, 0.9746798504406607, 0.968309303190753, 0.9773766625715209, 0.9687972239525655, 0.976354678559893, 0.9755644884528424, 0.9729832127860835, 0.9757261674049464, 0.9764709360003692, 0.9732157276670359, 0.9730533277385567, 0.9813058035714286, 0.974936698274271, 0.9723550621193245, 0.9808876372739018, 0.9771910111549464, 0.9785170669527501, 0.9825617444167589, 0.981725051333518, 0.9806082589285714, 0.9833526555001846, 0.9818878117501846, 0.9833065130121816, 0.9774467775239941, 0.9834692734288483, 0.9839350241671282, 0.9802137046073275, 0.9880025926310447, 0.9876076778216132, 0.9837475703096161, 0.9803997165120893, 0.9862122280477114, 0.9873980539405685, 0.9862587310239018, 0.9763088965600776, 0.9861893370478036, 0.9871433690360835, 0.9859106796788483, 0.9850503746193245, 0.9873748024524732, 0.9864443824404762, 0.9890725215716132, 0.9830739981312293, 0.9863052340000923, 0.9887001372739018, 0.9866303943452381, 0.9865606398809523, 0.9902583474644703, 0.9867942362264673, 0.9793759300595238, 0.9862129490240864, 0.9894209334048542, 0.9898394601905685, 0.9915364583333334, 0.9863978794642857, 0.9867477332502769, 0.986421851928756, 0.9893980424049464, 0.9910492585478959, 0.9857235863095238, 0.9884440104166666, 0.9873748024524732, 0.9856305803571429, 0.9893747909168512, 0.9896999512619971, 0.9911644345238095, 0.9904207473929494, 0.9890485491071429, 0.9901420900239941, 0.9864912459048542, 0.9932338169642857, 0.9915132068452381, 0.9899324661429494, 0.9901420900239941, 0.9917224702380952, 0.9893976819167589, 0.9921649694882798, 0.9923037574404762, 0.9935593377976191, 0.9912112979881875, 0.9904905018572352, 0.9893279274524732, 0.9916065732858066, 0.9930016625715209, 0.9927920386904762, 0.9920483515596161, 0.9919782366071429, 0.9922808664405685, 0.9925827752976191, 0.9905366443452381, 0.9902354564645626, 0.9918390881667589, 0.9928621536429494, 0.9916534367501846, 0.994141345976375, 0.9932806804286637, 0.9918852306547619, 0.9950009300595238, 0.9912814129406607, 0.9887462797619048, 0.99193209411914, 0.9944893973214286, 0.9926525297619048, 0.9929780505952381, 0.9927455357142857, 0.9932574289405685, 0.9939789460478959, 0.9923048389050388, 0.9909086681547619, 0.9944668668097084, 0.9935135557978036, 0.9913043039405685, 0.9913522488695091, 0.9958844866071429, 0.9941871279761905, 0.9930714170358066, 0.9953268113810447, 0.9930023835478959, 0.9945126488095238, 0.9931179200119971, 0.9925834962739941, 0.9944661458333334, 0.9946525182262828, 0.9908625256667589, 0.9948614211309523, 0.9958379836309523, 0.9933504348929494, 0.99560546875, 0.9924897693452381, 0.9942804944167589, 0.9948381696428571, 0.9921413575119971, 0.9944200033453304, 0.99290865661914, 0.9961635044642857, 0.9954895717977114, 0.9963960193452381, 0.99437350036914, 0.9886536342977114, 0.9957449776785714, 0.9950710450119971, 0.9931415319882798, 0.9942103794642857, 0.9932338169642857, 0.9928389021548542, 0.9955124627976191, 0.9920251000715209, 0.995164050964378, 0.9947916666666666, 0.9975120907738095, 0.9930016625715209, 0.9940952034883721, 0.9948149181547619, 0.991699939726375, 0.9949079241071429, 0.9964890252976191, 0.9945827637619971, 0.9960712194882798, 0.9937922131667589, 0.9914903158453304, 0.9955128232858066, 0.9940243675595238, 0.9957914806547619, 0.9955135442621816, 0.995885207583518, 0.9960704985119048, 0.9953736748454227, 0.9940708705357143, 0.9952101934523809, 0.9963960193452381, 0.9973496908453304, 0.9916995792381875, 0.9954430688215209, 0.9955822172619048, 0.9963960193452381, 0.9964890252976191, 0.9969075520833334, 0.9942801339285714, 0.9931880349644703, 0.9921181060239018, 0.9963269858573275, 0.9969776670358066, 0.9945130092977114, 0.9901646205357143, 0.9953497023809523, 0.9954201778216132, 0.9970703125, 0.99658203125, 0.9951186294527501, 0.9966982886904762, 0.9966753976905685, 0.9948381696428571, 0.9966285342261905, 0.9955593262619971, 0.9967451521548542, 0.9952803084048542, 0.9962100074404762, 0.9944901182978036, 0.9967222611549464, 0.9962100074404762, 0.9968381581072352, 0.9961867559523809, 0.9966517857142857, 0.9964890252976191, 0.994629627226375, 0.9962565104166666, 0.9969773065476191, 0.9966285342261905, 0.9974423363095238, 0.9960476075119971, 0.9957217261904762, 0.9976984631667589, 0.9967680431547619, 0.9967222611549464, 0.9965126372739018, 0.9960472470238095, 0.9963727678571429, 0.9972330729166666, 0.9962565104166666, 0.9968614095953304, 0.9957453381667589, 0.9962568709048542, 0.9967912946428571, 0.9950012905477114, 0.9957220866786637, 0.9972802968692323], "end": "2016-01-28 17:08:15.823000", "learning_rate_per_epoch": [0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165, 0.00017976305389311165], "accuracy_valid": [0.4951274825865964, 0.5990534403237951, 0.6525231786521084, 0.7101418368787651, 0.7362045839608433, 0.7548931075865963, 0.7641998658697289, 0.7732242446347892, 0.7854327466114458, 0.7905788191829819, 0.7958690229668675, 0.7955234022025602, 0.7983310193900602, 0.8088908367846386, 0.8084937405873494, 0.8059302640248494, 0.8149943524096386, 0.8177107845444277, 0.824974703501506, 0.8168357021837349, 0.8262145260730422, 0.8110675122364458, 0.8197756847703314, 0.8178931546498494, 0.8306708278426205, 0.8249232280685241, 0.8340887965926205, 0.8253100291792168, 0.8265307323042168, 0.8374052852033133, 0.8331019390060241, 0.8345770778426205, 0.8381068218185241, 0.8357374811746988, 0.8377612010542168, 0.8407423639871988, 0.8376803110881024, 0.8399790568524097, 0.838646578501506, 0.8465620293674698, 0.8360007412462349, 0.8467135142131024, 0.8443941782756024, 0.8351771343185241, 0.8449030496987951, 0.8425425334149097, 0.8364993175828314, 0.8435999858810241, 0.8528890954442772, 0.8377612010542168, 0.8388598338667168, 0.8497961572853916, 0.8442721079631024, 0.8479548075112951, 0.8522272684487951, 0.8505991740399097, 0.8518610575112951, 0.8506506494728916, 0.8547804499246988, 0.8521051981362951, 0.8461634624435241, 0.8487166439194277, 0.8530611704631024, 0.8462561182228916, 0.8624517601656627, 0.8570188958960843, 0.8538450677710843, 0.8557570124246988, 0.8576292474585843, 0.8594603021460843, 0.854515719126506, 0.8481268825301205, 0.8571409662085843, 0.8578425028237951, 0.8576586619917168, 0.8557878976844879, 0.8577204325112951, 0.8576189523719879, 0.8613722467996988, 0.8514742564006024, 0.8566115046121988, 0.8606810052710843, 0.8545671945594879, 0.8585646296121988, 0.8622164439006024, 0.8567541650978916, 0.8459810923381024, 0.8533464914344879, 0.8589308405496988, 0.8635798075112951, 0.8635400978915663, 0.8530199901167168, 0.8547495646649097, 0.8546171992658133, 0.8630709360881024, 0.8698259836219879, 0.8574865869728916, 0.8622679193335843, 0.854515719126506, 0.8569983057228916, 0.8613207713667168, 0.8663065347326807, 0.8654932228915663, 0.861351656626506, 0.8623591043862951, 0.8636812876506024, 0.8556349421121988, 0.8633665521460843, 0.8655123423381024, 0.862816500376506, 0.8607927804969879, 0.8665506753576807, 0.8642210443335843, 0.8721967949924698, 0.8672213267131024, 0.8685449807040663, 0.8642519295933735, 0.8639666086219879, 0.863426851939006, 0.8701730751129518, 0.8667036309299698, 0.8663477150790663, 0.8650549463478916, 0.8680258141942772, 0.8642916392131024, 0.8681670039533133, 0.8596735575112951, 0.8651976068335843, 0.8671301416603916, 0.8681272943335843, 0.8666624505835843, 0.8698568688817772, 0.871983539627259, 0.8672728021460843, 0.8746484963290663, 0.8640680887612951, 0.8624708796121988, 0.8649328760353916, 0.8710158014871988, 0.875035297439759, 0.8729189217808735, 0.8672419168862951, 0.8746382012424698, 0.8675581231174698, 0.8697347985692772, 0.8651167168674698, 0.8669271813817772, 0.8653196771460843, 0.8695215432040663, 0.8661432840737951, 0.8755338737763554, 0.8702230798192772, 0.8721864999058735, 0.874180805252259, 0.8686052804969879, 0.8737425287085843, 0.8683817300451807, 0.8732748376317772, 0.8655226374246988, 0.8726233057228916, 0.8696024331701807, 0.868553805064006, 0.8779546898531627, 0.8671610269201807, 0.8711275767131024, 0.8628576807228916, 0.8695509577371988, 0.8713923075112951, 0.8701010095067772, 0.8721659097326807, 0.867955219314759, 0.8748720467808735, 0.8748823418674698, 0.875819194747741, 0.874180805252259, 0.8643534097326807, 0.8758589043674698, 0.8681169992469879, 0.8677713784826807, 0.8723703407379518, 0.8692568124058735, 0.8724100503576807, 0.8780561699924698, 0.8683008400790663, 0.8741396249058735, 0.8731527673192772, 0.8769575371799698, 0.8729189217808735, 0.873204242752259, 0.8673139824924698, 0.8714849632906627, 0.8731733574924698, 0.8746382012424698, 0.8742616952183735, 0.8782503059111446, 0.8758088996611446, 0.8653608574924698, 0.8737631188817772, 0.877842914627259, 0.8795621940888554, 0.8772016778049698, 0.8734689735504518, 0.8757265389683735, 0.8724306405308735, 0.8731733574924698, 0.8742411050451807, 0.8780061652861446, 0.8754926934299698, 0.8697347985692772, 0.8742411050451807, 0.8752588478915663, 0.8769472420933735, 0.8850656532379518, 0.8804666909826807, 0.8725115304969879, 0.8758691994540663, 0.8736513436558735, 0.8834787391754518, 0.8780355798192772, 0.8770796074924698, 0.8650755365210843, 0.8766016213290663, 0.8798269248870482, 0.8825433570218373, 0.879307758377259, 0.8769987175263554, 0.8794092385165663, 0.8784223809299698, 0.8771413780120482, 0.8773340432040663, 0.8832551887236446, 0.8795313088290663, 0.8768457619540663, 0.8781179405120482, 0.8783414909638554, 0.878331195877259, 0.8813314782567772, 0.8824712914156627, 0.8720644295933735, 0.8796136695218373, 0.8770693124058735, 0.8789827277861446, 0.8744146507906627, 0.8813623635165663, 0.8770693124058735, 0.8834478539156627, 0.8789827277861446, 0.8737940041415663, 0.8841096809111446, 0.8787885918674698, 0.8849538780120482, 0.8823801063629518, 0.8760412744728916, 0.877720844314759, 0.8789724326995482, 0.8794092385165663, 0.8821462608245482, 0.8832551887236446, 0.8834890342620482, 0.8775781838290663, 0.8720541345067772, 0.8758177240210843, 0.8780061652861446], "accuracy_test": 0.7911172672193878, "start": "2016-01-26 05:13:42.112000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0], "accuracy_train_last": 0.9972802968692323, "batch_size_eval": 1024, "accuracy_train_std": [0.021029743666458774, 0.019277157099125364, 0.02201439912959055, 0.019488476576812554, 0.0199598573132133, 0.019707982099852837, 0.019239663617715102, 0.020046393282478106, 0.019528754874631297, 0.017721053400125995, 0.01868002748382982, 0.020619142373540365, 0.018484988122958547, 0.017990476365885393, 0.01878545995873595, 0.019056918267471388, 0.015955298274859777, 0.016669792370139748, 0.01398069503800933, 0.014352911873141087, 0.01300064887861727, 0.014913889525830296, 0.013090447864298083, 0.013334344438625521, 0.012610896272374886, 0.01394886387951754, 0.012273575600385894, 0.011401086954347587, 0.012062005285725812, 0.010312218076105174, 0.01220821652658664, 0.010655250220591363, 0.010521943205968765, 0.010242592467775386, 0.010884777620887885, 0.009402633988148633, 0.009519777094929585, 0.008556236615257803, 0.009666145520884746, 0.008137774222541905, 0.00969519710078471, 0.00786332267219413, 0.008003886189250445, 0.008553091019814591, 0.0075530613287536335, 0.007467167622379545, 0.007825131642586823, 0.008733149356881787, 0.006982418390760156, 0.00782958299601857, 0.008469424404170221, 0.007456839364033608, 0.008048490362964558, 0.006881854761811805, 0.006252387882460961, 0.006557017141810002, 0.0066321787522347735, 0.005358851322691858, 0.0067747243999423355, 0.005952900343160068, 0.007324690254880583, 0.0053949358321987, 0.005389896651049537, 0.0060858253240145335, 0.004757722948703318, 0.004366062073756693, 0.006456887572125224, 0.005621616477427071, 0.0061334936807401165, 0.004845367523503283, 0.00484818510537297, 0.006826645170275216, 0.005215099793384839, 0.004601449662184089, 0.004632418833388832, 0.005100798919254428, 0.004085600904631493, 0.0052727587753202045, 0.004374912416966007, 0.005421355964256544, 0.004744481316530816, 0.004072959590492749, 0.004457316938858125, 0.0050059325300587385, 0.003967810771122612, 0.004129969872867233, 0.006091934253859661, 0.005252819917136882, 0.004693375722105831, 0.004263080010157012, 0.0034471850452467904, 0.004828534146506508, 0.0037002513483652124, 0.004760928160019034, 0.004749790170047031, 0.0036853029752089033, 0.005301747190865177, 0.004474748732905653, 0.004927079887772234, 0.005286020120342407, 0.004216089304359364, 0.003825992739997909, 0.003975120537535632, 0.003994533950603668, 0.004254259909252333, 0.0037033362240236106, 0.004331298580210482, 0.0031275628408631187, 0.003955282475833106, 0.004627912521835725, 0.0038299312662986138, 0.0035991020407160246, 0.003921964681279989, 0.0033124615431027646, 0.0033573248703406534, 0.0033428007088745423, 0.003572954588604395, 0.0036860090146460947, 0.003880187667915913, 0.003417383379444997, 0.003349161854620794, 0.0034883431988634115, 0.0032852970276262537, 0.004001215626770835, 0.0028977455432815545, 0.003572642407695257, 0.003366009285260712, 0.0034440247175555783, 0.003974936174937293, 0.003537584173957109, 0.003317465445561082, 0.0033223804506332783, 0.00320561769516032, 0.003459161948516648, 0.002907923726385399, 0.003439796310651771, 0.00465053012811599, 0.0035284977731236695, 0.0030647013721608863, 0.003667843942840587, 0.0035610471947687215, 0.003096553536939452, 0.0034354337217544673, 0.002882222413535241, 0.0029550837113564714, 0.0035626409290078175, 0.0026248840128851904, 0.0028343862633325445, 0.003402432615074133, 0.0031209703028391295, 0.0021578834121453714, 0.0023629735892694383, 0.0029288242548930303, 0.0023975698142929764, 0.002548096499177023, 0.0027863006285240408, 0.002601527063369849, 0.0038250220019106973, 0.0029348499489378088, 0.002714807843673959, 0.0034092257047903534, 0.0022759157068526444, 0.002696069952403891, 0.0029243563444340424, 0.0020906954066837774, 0.0034460085984648198, 0.0025804996673197004, 0.0023372083097338288, 0.003036602134785137, 0.002885513144834119, 0.0032954719331890875, 0.0019692514248734797, 0.0028190375638459885, 0.0022298407414924636, 0.00268601579744939, 0.0043417780206335245, 0.002590950366574033, 0.0022248623321506353, 0.002750456983139592, 0.0032064299482592647, 0.0032064299482592647, 0.00280900260468228, 0.0023543782207170257, 0.003146523540201774, 0.0021627452383053084, 0.002792115523117202, 0.0017075206443207922, 0.0027866633030041664, 0.002493907695970376, 0.00292571729403485, 0.00335182603480879, 0.002260661727740855, 0.0018214984860197066, 0.0025327723363721966, 0.002396719974822535, 0.002582766702192479, 0.003024191375034402, 0.0025492359055090466, 0.0028207227281193403, 0.0026315302445379897, 0.0022043097854336758, 0.0023104068517712244, 0.0020159187678821344, 0.002945786617793122, 0.002715650382724026, 0.0026685578004842954, 0.002167881767141278, 0.001811966863766939, 0.0029401389447146877, 0.0023702246636485512, 0.002384836570862739, 0.0017376501480220775, 0.002235652042236118, 0.0020207402653113463, 0.0028556765472180132, 0.0027255655143380623, 0.0033184457365398497, 0.0019037019053189251, 0.0018433952522124313, 0.002548422817923588, 0.004023045011216685, 0.002253355922541986, 0.001616076756747707, 0.001857792443772152, 0.0020467913994309723, 0.0027933283173368737, 0.002314428411110362, 0.0018217714250793454, 0.002478656616673443, 0.0018724306571138616, 0.0020797189102964063, 0.002242020599800113, 0.0027287282890927555, 0.0023653746953191993, 0.002626216054637435, 0.0016060981184014974, 0.0020013845845429055, 0.0016205507004900337, 0.002263410253652894, 0.0018472864644755586, 0.0024857352400830695, 0.002404359743123531, 0.0020651981673908723, 0.001867660493677753, 0.002143807377113003, 0.0015360067489689122, 0.002285122905807821, 0.002274846504582405, 0.001903993782559805, 0.0017764197772427508, 0.001939146924750453, 0.0018352563734332467, 0.0022038682312263366, 0.0021969888041327595, 0.0016495451916422941, 0.0023238696258062576, 0.001947940684377391, 0.0019630755957015427, 0.0021830903093388323, 0.0023176963903231234, 0.0021966284997598103, 0.0021626522224448973, 0.0016575353977446702], "accuracy_test_std": 0.007651625777585861, "error_valid": [0.5048725174134037, 0.40094655967620485, 0.3474768213478916, 0.2898581631212349, 0.2637954160391567, 0.24510689241340367, 0.23580013413027112, 0.22677575536521077, 0.2145672533885542, 0.2094211808170181, 0.20413097703313254, 0.20447659779743976, 0.20166898060993976, 0.19110916321536142, 0.19150625941265065, 0.19406973597515065, 0.18500564759036142, 0.1822892154555723, 0.17502529649849397, 0.1831642978162651, 0.17378547392695776, 0.1889324877635542, 0.18022431522966864, 0.18210684535015065, 0.16932917215737953, 0.17507677193147586, 0.16591120340737953, 0.1746899708207832, 0.1734692676957832, 0.16259471479668675, 0.16689806099397586, 0.16542292215737953, 0.16189317818147586, 0.16426251882530118, 0.1622387989457832, 0.15925763601280118, 0.16231968891189763, 0.1600209431475903, 0.16135342149849397, 0.15343797063253017, 0.1639992587537651, 0.15328648578689763, 0.15560582172439763, 0.16482286568147586, 0.15509695030120485, 0.1574574665850903, 0.16350068241716864, 0.15640001411897586, 0.14711090455572284, 0.1622387989457832, 0.1611401661332832, 0.1502038427146084, 0.15572789203689763, 0.15204519248870485, 0.14777273155120485, 0.1494008259600903, 0.14813894248870485, 0.1493493505271084, 0.14521955007530118, 0.14789480186370485, 0.15383653755647586, 0.1512833560805723, 0.14693882953689763, 0.1537438817771084, 0.13754823983433728, 0.14298110410391573, 0.14615493222891573, 0.14424298757530118, 0.14237075254141573, 0.14053969785391573, 0.14548428087349397, 0.15187311746987953, 0.14285903379141573, 0.14215749717620485, 0.1423413380082832, 0.14421210231551207, 0.14227956748870485, 0.14238104762801207, 0.13862775320030118, 0.14852574359939763, 0.14338849538780118, 0.13931899472891573, 0.14543280544051207, 0.14143537038780118, 0.13778355609939763, 0.1432458349021084, 0.15401890766189763, 0.14665350856551207, 0.14106915945030118, 0.13642019248870485, 0.13645990210843373, 0.1469800098832832, 0.1452504353350903, 0.14538280073418675, 0.13692906391189763, 0.13017401637801207, 0.1425134130271084, 0.13773208066641573, 0.14548428087349397, 0.1430016942771084, 0.1386792286332832, 0.1336934652673193, 0.13450677710843373, 0.13864834337349397, 0.13764089561370485, 0.13631871234939763, 0.14436505788780118, 0.13663344785391573, 0.13448765766189763, 0.13718349962349397, 0.13920721950301207, 0.1334493246423193, 0.13577895566641573, 0.12780320500753017, 0.13277867328689763, 0.13145501929593373, 0.1357480704066265, 0.13603339137801207, 0.13657314806099397, 0.12982692488704817, 0.13329636907003017, 0.13365228492093373, 0.1349450536521084, 0.13197418580572284, 0.13570836078689763, 0.13183299604668675, 0.14032644248870485, 0.13480239316641573, 0.1328698583396084, 0.13187270566641573, 0.13333754941641573, 0.13014313111822284, 0.12801646037274095, 0.13272719785391573, 0.12535150367093373, 0.13593191123870485, 0.13752912038780118, 0.1350671239646084, 0.12898419851280118, 0.12496470256024095, 0.1270810782191265, 0.13275808311370485, 0.12536179875753017, 0.13244187688253017, 0.13026520143072284, 0.13488328313253017, 0.13307281861822284, 0.13468032285391573, 0.13047845679593373, 0.13385671592620485, 0.12446612622364461, 0.12977692018072284, 0.1278135000941265, 0.12581919474774095, 0.13139471950301207, 0.12625747129141573, 0.1316182699548193, 0.12672516236822284, 0.13447736257530118, 0.1273766942771084, 0.1303975668298193, 0.13144619493599397, 0.12204531014683728, 0.1328389730798193, 0.12887242328689763, 0.1371423192771084, 0.13044904226280118, 0.12860769248870485, 0.12989899049322284, 0.1278340902673193, 0.13204478068524095, 0.1251279532191265, 0.12511765813253017, 0.12418080525225905, 0.12581919474774095, 0.1356465902673193, 0.12414109563253017, 0.13188300075301207, 0.1322286215173193, 0.12762965926204817, 0.1307431875941265, 0.1275899496423193, 0.12194383000753017, 0.13169915992093373, 0.1258603750941265, 0.12684723268072284, 0.12304246282003017, 0.1270810782191265, 0.12679575724774095, 0.13268601750753017, 0.12851503670933728, 0.12682664250753017, 0.12536179875753017, 0.1257383047816265, 0.12174969408885539, 0.12419110033885539, 0.13463914250753017, 0.12623688111822284, 0.12215708537274095, 0.12043780591114461, 0.12279832219503017, 0.12653102644954817, 0.12427346103162651, 0.1275693594691265, 0.12682664250753017, 0.1257588949548193, 0.12199383471385539, 0.12450730657003017, 0.13026520143072284, 0.1257588949548193, 0.12474115210843373, 0.12305275790662651, 0.11493434676204817, 0.11953330901731929, 0.12748846950301207, 0.12413080054593373, 0.1263486563441265, 0.11652126082454817, 0.12196442018072284, 0.12292039250753017, 0.13492446347891573, 0.12339837867093373, 0.12017307511295183, 0.11745664297816272, 0.12069224162274095, 0.12300128247364461, 0.12059076148343373, 0.12157761907003017, 0.12285862198795183, 0.12266595679593373, 0.11674481127635539, 0.12046869117093373, 0.12315423804593373, 0.12188205948795183, 0.12165850903614461, 0.12166880412274095, 0.11866852174322284, 0.11752870858433728, 0.1279355704066265, 0.12038633047816272, 0.12293068759412651, 0.12101727221385539, 0.12558534920933728, 0.11863763648343373, 0.12293068759412651, 0.11655214608433728, 0.12101727221385539, 0.12620599585843373, 0.11589031908885539, 0.12121140813253017, 0.11504612198795183, 0.11761989363704817, 0.1239587255271084, 0.12227915568524095, 0.12102756730045183, 0.12059076148343373, 0.11785373917545183, 0.11674481127635539, 0.11651096573795183, 0.12242181617093373, 0.12794586549322284, 0.12418227597891573, 0.12199383471385539], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.8383044667187753, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00017976304827855747, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "l2_decay": 2.1033488640122287e-06, "optimization": "adam", "nb_data_augmentation": 1, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.012075219729670895}, "accuracy_valid_max": 0.8850656532379518, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        #nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        nb_data_augmentation=make_constant_param(1),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8780061652861446, "loss_train": [1.7925667762756348, 1.324258804321289, 1.0828396081924438, 0.9298882484436035, 0.8206656575202942, 0.7314272522926331, 0.6625732183456421, 0.6103580594062805, 0.5614119172096252, 0.5228049159049988, 0.4897806942462921, 0.4606136381626129, 0.43108102679252625, 0.40784674882888794, 0.38599833846092224, 0.36764052510261536, 0.3533358573913574, 0.333609938621521, 0.32153555750846863, 0.31006091833114624, 0.29900169372558594, 0.2900455594062805, 0.2771739661693573, 0.2679048180580139, 0.25665098428726196, 0.24890369176864624, 0.23807834088802338, 0.23494932055473328, 0.23093564808368683, 0.22077830135822296, 0.21774089336395264, 0.20916828513145447, 0.20456795394420624, 0.19995103776454926, 0.1972411721944809, 0.18646137416362762, 0.18527910113334656, 0.18253137171268463, 0.17650151252746582, 0.17290879786014557, 0.17134393751621246, 0.1661105453968048, 0.1633784919977188, 0.15843582153320312, 0.15691135823726654, 0.15592926740646362, 0.14853805303573608, 0.14786531031131744, 0.14632821083068848, 0.14248864352703094, 0.13828307390213013, 0.13736417889595032, 0.13544608652591705, 0.1321038007736206, 0.13067114353179932, 0.12705646455287933, 0.12685661017894745, 0.12263716757297516, 0.12068888545036316, 0.1197105199098587, 0.11618579179048538, 0.11747225373983383, 0.11456840485334396, 0.11161204427480698, 0.1147075667977333, 0.10811549425125122, 0.1075492650270462, 0.10801056772470474, 0.10825936496257782, 0.10442337393760681, 0.10148356854915619, 0.10131672769784927, 0.09839089959859848, 0.09702118486166, 0.09512823820114136, 0.09637876600027084, 0.09609168767929077, 0.09132229536771774, 0.0908133015036583, 0.09088730067014694, 0.09110388159751892, 0.09026867151260376, 0.08769737929105759, 0.08674301952123642, 0.08578313142061234, 0.08345954865217209, 0.08326005935668945, 0.08261853456497192, 0.08136174827814102, 0.08241654187440872, 0.08086967468261719, 0.07796233892440796, 0.08006474375724792, 0.08009897917509079, 0.07614026963710785, 0.07589784264564514, 0.0744103193283081, 0.07460880279541016, 0.074838787317276, 0.07329966872930527, 0.07478682696819305, 0.0731227770447731, 0.07175082713365555, 0.07191667705774307, 0.06971330940723419, 0.07310470193624496, 0.06765101104974747, 0.06948993355035782, 0.06713136285543442, 0.06850888580083847, 0.06970478594303131, 0.06792719662189484, 0.06899964064359665, 0.06678714603185654, 0.06630127131938934, 0.06345672905445099, 0.06520913541316986, 0.06576851010322571, 0.06213356927037239, 0.061051011085510254, 0.0653100237250328, 0.06347467750310898, 0.0655522346496582, 0.06094978749752045, 0.060956910252571106, 0.05832658335566521, 0.05892951041460037, 0.06424107402563095, 0.05850403383374214, 0.0600493922829628, 0.056576043367385864, 0.06202057749032974, 0.05716260150074959, 0.05622081458568573, 0.06074976921081543, 0.05582259222865105, 0.05776701122522354, 0.058263953775167465, 0.05447704344987869, 0.056166741997003555, 0.05767269432544708, 0.0583110935986042, 0.0547061450779438, 0.055278245359659195, 0.05372706800699234, 0.05369687080383301, 0.05759288743138313, 0.05352826789021492, 0.052976060658693314, 0.05167676880955696, 0.05236706882715225, 0.054384417831897736, 0.05398854240775108, 0.05135907605290413, 0.051552411168813705, 0.05217030644416809, 0.05121942237019539, 0.05295909196138382, 0.05223022773861885, 0.04982787370681763, 0.05526519939303398, 0.04962141066789627, 0.04919937252998352, 0.05118425935506821, 0.048701900988817215, 0.048708636313676834, 0.052251026034355164, 0.05437944456934929, 0.048686377704143524, 0.04604646563529968, 0.048018429428339005, 0.048209208995103836, 0.050788700580596924, 0.047064777463674545, 0.0499841570854187, 0.050086069852113724, 0.04663544520735741, 0.04684031754732132, 0.04983912408351898, 0.04705914109945297, 0.04668089747428894, 0.049687035381793976, 0.047475628554821014, 0.04604369401931763, 0.04550776258111, 0.04698275402188301, 0.04731115326285362, 0.04515611380338669, 0.04787451773881912, 0.04605432227253914, 0.04597798362374306, 0.04469471424818039, 0.044970180839300156, 0.049339912831783295, 0.045388780534267426, 0.04563239961862564, 0.04557400569319725, 0.04571079462766647, 0.046201467514038086, 0.04404068738222122, 0.04597032070159912, 0.04535548761487007, 0.04441404342651367, 0.04494829848408699, 0.04367320239543915, 0.04712877795100212, 0.0464862622320652, 0.04334684833884239, 0.043500158935785294, 0.04492885246872902, 0.04288475215435028, 0.04227502644062042, 0.0462784618139267, 0.04372618719935417, 0.04451615735888481, 0.046026960015296936, 0.04288531839847565, 0.041235700249671936, 0.04395241290330887, 0.044511716812849045, 0.042733483016490936, 0.041332393884658813, 0.04224659502506256, 0.04267076775431633, 0.043308865278959274, 0.04396397992968559, 0.04131508991122246, 0.04337906837463379, 0.04242575913667679, 0.043680090457201004, 0.04291575774550438, 0.0415172278881073, 0.04202158749103546, 0.042472321540117264, 0.042075712233781815, 0.041685931384563446, 0.03960144892334938, 0.04314548894762993, 0.04104828089475632, 0.041351959109306335, 0.04110196232795715, 0.04090292379260063, 0.04191170632839203, 0.04037420079112053, 0.039346639066934586, 0.041851140558719635, 0.04181571677327156, 0.03731994330883026, 0.03999510034918785, 0.04230201989412308, 0.04355153813958168, 0.039187364280223846, 0.04348558560013771, 0.0397660918533802, 0.040125470608472824, 0.04311652481555939, 0.040011852979660034, 0.04019661620259285, 0.03964662551879883, 0.03826287388801575, 0.04109349846839905, 0.04380330815911293, 0.03641868010163307], "accuracy_train_first": 0.516219805509413, "model": "residualv5", "loss_std": [0.24370384216308594, 0.14426442980766296, 0.12744250893592834, 0.1222592443227768, 0.11765548586845398, 0.11568184942007065, 0.11493052542209625, 0.10799825191497803, 0.10402324795722961, 0.09800000488758087, 0.09858132153749466, 0.0922684371471405, 0.09353319555521011, 0.08910524845123291, 0.08365626633167267, 0.08447129279375076, 0.08016892522573471, 0.07904383540153503, 0.07967939227819443, 0.07813121378421783, 0.07483644783496857, 0.07294121384620667, 0.06934045255184174, 0.06951847672462463, 0.06726548075675964, 0.07196744531393051, 0.06435079872608185, 0.06577343493700027, 0.06385641545057297, 0.06208112835884094, 0.06325994431972504, 0.05969567969441414, 0.057870298624038696, 0.05722974240779877, 0.05529313161969185, 0.05451226606965065, 0.054336145520210266, 0.05467137321829796, 0.056201688945293427, 0.05229085311293602, 0.05501769110560417, 0.05081823095679283, 0.052451640367507935, 0.05242012068629265, 0.04919690638780594, 0.05134039372205734, 0.04862389713525772, 0.05047730728983879, 0.049792591482400894, 0.04831928387284279, 0.0479123629629612, 0.04990839585661888, 0.04694623500108719, 0.04590412974357605, 0.045968592166900635, 0.04488510265946388, 0.04546039551496506, 0.04304015263915062, 0.043906934559345245, 0.04337283596396446, 0.04181991145014763, 0.0445544496178627, 0.0407407209277153, 0.039861537516117096, 0.04144575446844101, 0.03990842401981354, 0.040350548923015594, 0.041792742908000946, 0.04394281655550003, 0.03878660127520561, 0.039979979395866394, 0.04057542234659195, 0.035208266228437424, 0.0377223864197731, 0.03643747791647911, 0.038387514650821686, 0.037222541868686676, 0.035487934947013855, 0.03657754510641098, 0.0376635305583477, 0.036102645099163055, 0.03567343205213547, 0.03541341423988342, 0.03516067564487457, 0.03543928265571594, 0.03465647250413895, 0.033540163189172745, 0.0344015508890152, 0.03219027444720268, 0.03363938629627228, 0.03263384848833084, 0.03347823768854141, 0.03432071581482887, 0.03450464829802513, 0.032454174011945724, 0.03356196731328964, 0.03094256855547428, 0.03186791017651558, 0.031962502747774124, 0.03109913505613804, 0.03166443482041359, 0.03168671578168869, 0.030932610854506493, 0.030554136261343956, 0.03145940974354744, 0.030498569831252098, 0.028910094872117043, 0.031636517494916916, 0.029826747253537178, 0.02856193482875824, 0.031161263585090637, 0.031145574524998665, 0.029481563717126846, 0.02933969348669052, 0.029057659208774567, 0.026758331805467606, 0.02780982479453087, 0.029878510162234306, 0.0268204715102911, 0.02717927098274231, 0.02908429503440857, 0.02909042127430439, 0.028678048402071, 0.02688775584101677, 0.027583613991737366, 0.027567533776164055, 0.026946404948830605, 0.029394254088401794, 0.02605844847857952, 0.027657225728034973, 0.02615056373178959, 0.02829076535999775, 0.025444278493523598, 0.026422971859574318, 0.029287753626704216, 0.0253954716026783, 0.025685956701636314, 0.02735850401222706, 0.02581624872982502, 0.02558950148522854, 0.026149341836571693, 0.02757715806365013, 0.02495562471449375, 0.024491481482982635, 0.02515340968966484, 0.025332337245345116, 0.026552047580480576, 0.024536222219467163, 0.02441677637398243, 0.024410173296928406, 0.023012951016426086, 0.02551712840795517, 0.02524949051439762, 0.024170733988285065, 0.02491077221930027, 0.023014720529317856, 0.02338377758860588, 0.025540128350257874, 0.022739814594388008, 0.02399531938135624, 0.027252495288848877, 0.023331977427005768, 0.022979291155934334, 0.024877972900867462, 0.022420482710003853, 0.02362952008843422, 0.025254763662815094, 0.027339547872543335, 0.02424277365207672, 0.02083359658718109, 0.02349916286766529, 0.022507358342409134, 0.02306380122900009, 0.021714774891734123, 0.024157987907528877, 0.023690462112426758, 0.021595751866698265, 0.022357696667313576, 0.022868331521749496, 0.022098273038864136, 0.022898271679878235, 0.02331414259970188, 0.021624241024255753, 0.02107127010822296, 0.021149003878235817, 0.02266143076121807, 0.02391899935901165, 0.02163556218147278, 0.022211307659745216, 0.02172890119254589, 0.02260127291083336, 0.02064410038292408, 0.02231021411716938, 0.024891210719943047, 0.02143457904458046, 0.021239325404167175, 0.02195918932557106, 0.021828731521964073, 0.021250704303383827, 0.020238976925611496, 0.021331284195184708, 0.022018378600478172, 0.02114592306315899, 0.022779878228902817, 0.020162733271718025, 0.02056492678821087, 0.022099336609244347, 0.020195361226797104, 0.020445285364985466, 0.02143668569624424, 0.019672807306051254, 0.02060559205710888, 0.022686807438731194, 0.021178675815463066, 0.021988483145833015, 0.02199089713394642, 0.021355191245675087, 0.018535366281867027, 0.0205194354057312, 0.021806625649333, 0.020735152065753937, 0.01866036467254162, 0.0204133540391922, 0.020867107436060905, 0.020456550642848015, 0.02084028162062168, 0.01841464452445507, 0.0196217093616724, 0.018912388011813164, 0.021493397653102875, 0.02254977636039257, 0.018941247835755348, 0.022292613983154297, 0.020406324416399002, 0.021022729575634003, 0.021108468994498253, 0.01798097789287567, 0.020465945824980736, 0.019769733771681786, 0.020560383796691895, 0.018217051401734352, 0.018431726843118668, 0.01998971588909626, 0.019268766045570374, 0.01810619980096817, 0.020335771143436432, 0.019875185564160347, 0.01671093888580799, 0.01920713484287262, 0.02030218578875065, 0.020623713731765747, 0.017429420724511147, 0.021091261878609657, 0.0185776986181736, 0.018383288756012917, 0.02111200988292694, 0.018443312495946884, 0.019578298553824425, 0.018557453528046608, 0.016810107976198196, 0.01931418851017952, 0.02120109833776951, 0.01544922310858965]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:20 2016", "state": "available"}], "summary": "be6cf68c50677cc632c783d2770c6749"}