{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 16, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.541786789894104, 1.1830544471740723, 0.9755569100379944, 0.87766432762146, 0.8128423690795898, 0.7672969102859497, 0.7350082993507385, 0.7059305310249329, 0.6834429502487183, 0.6639116406440735, 0.6482293605804443, 0.6312974095344543, 0.6184394955635071, 0.6084672808647156, 0.5976031422615051, 0.5880467295646667, 0.5784263014793396, 0.5716571807861328, 0.5639601349830627, 0.5573629140853882, 0.5517163872718811, 0.5445110201835632, 0.5375077128410339, 0.5350871682167053, 0.5278710126876831, 0.522449791431427, 0.5190185904502869, 0.5146823525428772, 0.5098422169685364, 0.505379319190979, 0.5003142356872559, 0.49868643283843994, 0.4954642355442047, 0.49164724349975586, 0.48776769638061523, 0.4850545823574066, 0.4839589297771454, 0.47960197925567627, 0.47830870747566223, 0.47443363070487976, 0.47146981954574585, 0.46813464164733887, 0.4662378430366516, 0.46452370285987854, 0.46166566014289856, 0.46059247851371765, 0.4569855034351349, 0.45553550124168396, 0.45373570919036865, 0.4524609446525574, 0.45054861903190613, 0.44969436526298523, 0.44627508521080017, 0.44358882308006287, 0.4416219890117645, 0.4405075013637543, 0.4388676881790161, 0.43705499172210693, 0.4363044798374176, 0.4338596761226654, 0.4322466552257538, 0.4303645193576813, 0.42941248416900635, 0.4277057647705078, 0.42831748723983765, 0.42649227380752563, 0.42245984077453613, 0.4223495125770569, 0.4209338128566742, 0.4198247492313385, 0.4188918471336365, 0.41768261790275574, 0.41714808344841003, 0.416483998298645, 0.41305792331695557, 0.4138631522655487, 0.41246387362480164, 0.4105049967765808, 0.4099541902542114, 0.4075554311275482, 0.40764299035072327, 0.40623241662979126, 0.4050905704498291, 0.40391406416893005, 0.4042241871356964, 0.4038097858428955, 0.4025672674179077, 0.40120500326156616, 0.40054041147232056, 0.3992190659046173, 0.3992602527141571, 0.39794921875, 0.3969811201095581, 0.3958132863044739, 0.39425089955329895, 0.39301154017448425, 0.3925882875919342, 0.3914670944213867, 0.39161011576652527, 0.39026275277137756, 0.38918787240982056, 0.3904968202114105, 0.3881724774837494, 0.38856738805770874, 0.3876763880252838, 0.38478541374206543, 0.3855161964893341, 0.384448379278183, 0.38384947180747986, 0.3832899034023285, 0.38291439414024353, 0.3817291557788849, 0.3811696171760559, 0.38106679916381836, 0.37939026951789856, 0.3797090947628021, 0.3782566487789154, 0.3791104555130005, 0.3786953389644623, 0.3776673674583435, 0.3763416111469269, 0.3775838315486908, 0.37624531984329224, 0.37493976950645447, 0.37527236342430115, 0.3738422095775604, 0.3722248375415802, 0.37264782190322876, 0.3720187544822693, 0.37105828523635864, 0.37191277742385864, 0.37062862515449524, 0.36845487356185913, 0.369830846786499, 0.3695296347141266, 0.36782798171043396, 0.36768776178359985], "moving_avg_accuracy_train": [0.0565595872554448, 0.11508346743493905, 0.17789770931386115, 0.23818034315591538, 0.2941669134280407, 0.34631253102649723, 0.39446654304128015, 0.4387886197033758, 0.47948964119441695, 0.516657669911354, 0.550622609448227, 0.5817629695409182, 0.6100450239445692, 0.635998707804265, 0.6596801108160589, 0.6811978784754552, 0.70068960764872, 0.7186668225367642, 0.7348928189121944, 0.749798593141776, 0.7634064889090398, 0.7757840197258232, 0.7871283384585288, 0.7975428744620208, 0.8071251121115643, 0.8158561549390292, 0.8238511330682344, 0.8311674490249769, 0.8378637405289022, 0.8441346516538912, 0.8498482982283047, 0.8549462941714395, 0.8597250806250043, 0.8640631908141649, 0.8680280880487322, 0.871722053595557, 0.8751487849400433, 0.8783791112321686, 0.8812889102879846, 0.8839937599441714, 0.8864397864276058, 0.8887458059103067, 0.8909397339363856, 0.8929446402896555, 0.8948860595432665, 0.8966845622429634, 0.8982984562286154, 0.8997974637918926, 0.901237323500051, 0.9026192637921648, 0.9038373973693437, 0.9050986229101871, 0.9062570494826789, 0.9072508413217404, 0.9083173510376191, 0.9092607534961497, 0.9101447650386074, 0.9110402486791727, 0.9118881087318903, 0.9127116366483838, 0.913496989600609, 0.9142107466552214, 0.9148554892020009, 0.9155286192512083, 0.9161809753205042, 0.9167658066828798, 0.9173153703482942, 0.9178333012329001, 0.9183621819980836, 0.9189007734605122, 0.9194484289898301, 0.9199737989519121, 0.920560492111815, 0.92107689021168, 0.921534817250405, 0.9220561254328488, 0.9223650838221036, 0.9228477234188522, 0.9231820095106602, 0.9235410317623443, 0.9238896563281273, 0.9241825281468649, 0.9245111809015766, 0.9248859152938849, 0.9251766011731343, 0.9254570359478473, 0.9257813987117282, 0.9261663311516019, 0.9264987473569934, 0.9267212641287692, 0.9269936088364625, 0.9272828969007675, 0.92751066802649, 0.9277574786693928, 0.9280075460825384, 0.9282883021793418, 0.9285342235129487, 0.9287692873131396, 0.9290321782512148, 0.9293151749252165, 0.929576811329428, 0.9298959534015424, 0.9301739527688447, 0.9303428801375399, 0.9305948240705374, 0.9309494207459402, 0.9311848523966599, 0.9314362684120696, 0.9316323158914146, 0.9318366243597206, 0.9319739990050054, 0.9322325308655329, 0.9325000867721505, 0.9327454652880879, 0.9329640168524408, 0.9331747002020342, 0.9334038066976115, 0.9335914013531549, 0.9336695557395726, 0.9338329727373669, 0.9339312920080193, 0.9341057377599213, 0.9342115496140049, 0.9344277601184128, 0.9346408065675811, 0.9348674977016129, 0.9350204024972508, 0.9352137122382972, 0.9353946304028488, 0.9355389997557441, 0.9357200133495218, 0.9357597287458358, 0.93592804213348, 0.9360933669287606, 0.9362678079790554, 0.9364155764267201, 0.9365997573522467], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.057132288921310226, 0.11544140919145328, 0.17774260077419046, 0.23673027123140997, 0.29155566160638646, 0.3423227646381424, 0.3889632467869034, 0.4317657878838155, 0.47077738562969595, 0.506328306234648, 0.5386801977026742, 0.5680623661854188, 0.5946060330872083, 0.6188290411772073, 0.6407090206249986, 0.6606269057424685, 0.6785387362996222, 0.6949442635544492, 0.7097671847140645, 0.7235402073947665, 0.7357650293698983, 0.7470348943263573, 0.7570912940597607, 0.766217355024643, 0.7743300651170583, 0.7817668110526416, 0.7885097400283262, 0.7947594225578731, 0.8003241311868751, 0.805297806876546, 0.8101413554434094, 0.8141953733723365, 0.8180993076559613, 0.8218468411222929, 0.825071077849672, 0.8280726061716326, 0.830712946505147, 0.83326015124281, 0.8358120716889357, 0.8377405301356295, 0.8396093905727443, 0.841278128426238, 0.8427576374492014, 0.844368927779959, 0.8456248060863004, 0.8467774516071884, 0.8477049692947377, 0.8486485689861223, 0.8495598733732781, 0.8505174131914472, 0.8513038978229802, 0.8522579336336792, 0.8530687672469679, 0.8536499741066085, 0.8543958753687639, 0.8548677559700653, 0.8553178920823962, 0.8557606651859035, 0.8563534439704006, 0.8567181054562671, 0.8570585078247971, 0.8573516334165643, 0.8577975224092452, 0.8580655746675677, 0.8584543355837175, 0.8587543627745928, 0.8588656958401305, 0.8589669251077741, 0.8591892202664244, 0.8593048661991193, 0.8594832192347044, 0.8598187534388695, 0.8603028101827085, 0.8606285979709135, 0.8609818126278884, 0.8612752917566658, 0.8613939681062251, 0.8615709304823496, 0.8617444626694308, 0.8618131334017347, 0.8619756818367872, 0.8621087388884247, 0.8621684845873081, 0.8622344627475532, 0.8622317784268642, 0.8626088100156536, 0.8631730125513623, 0.8632117801027018, 0.8633331496263171, 0.863405761103821, 0.8634080172600052, 0.8636328628890498, 0.8636622665003707, 0.8636785817366288, 0.8635701656281014, 0.8636180459967672, 0.8636998184396356, 0.8637734136382172, 0.8637297860356907, 0.8637281717958265, 0.863811138690039, 0.8640465593183995, 0.8640712143805355, 0.8641697346499367, 0.8642451663524883, 0.8644127701521038, 0.8643784490856887, 0.8642488743672554, 0.8642685934817347, 0.8642598676049468, 0.8642805464129762, 0.8644547602204737, 0.8645891976020409, 0.8647813744156321, 0.8647956421416141, 0.8651045403709768, 0.8652095913225839, 0.8652054514203708, 0.8652292280968578, 0.8650532555883768, 0.8652499137456536, 0.8651918839674738, 0.8652393724344313, 0.8652078403585334, 0.8651571064450445, 0.8653036698969255, 0.8650937801286185, 0.865298592863121, 0.8652764343015831, 0.8653032607038795, 0.8652653398010368, 0.8652088559432976, 0.8652180261189226, 0.8650909724245756, 0.8650030971794824, 0.8650816713564891, 0.8653243160619546], "moving_var_accuracy_train": [0.028790882194556472, 0.056737194936475055, 0.08657413628824115, 0.1106226861458337, 0.12777088198877098, 0.13946628270241404, 0.14638893429025135, 0.1494300591779924, 0.14939621161392092, 0.14688975168085636, 0.14258333057246175, 0.13705249575553782, 0.13054611759160367, 0.12355384918544467, 0.11624574390436321, 0.10878829843932074, 0.10132881615086377, 0.09410455683195565, 0.08706364777413937, 0.08035692194517477, 0.07398780319557177, 0.06796785229809839, 0.06232930917587148, 0.057072541299796595, 0.05219166067516791, 0.04765857458734635, 0.04346799420618995, 0.03960295109818091, 0.03604621886751271, 0.03279551591779905, 0.029809776140614898, 0.02706270459027938, 0.024561965330970412, 0.022275141597993074, 0.02018911112891988, 0.018293008449178052, 0.016569389993625817, 0.015006366065845596, 0.013581931834168086, 0.01228958455571444, 0.011114473510161963, 0.010050885691837128, 0.009089117004305949, 0.008216382149243792, 0.007428665912784038, 0.0067149108291529895, 0.0060668616304100035, 0.005480398680441865, 0.004951017576210283, 0.004473103649327962, 0.004039147929101821, 0.003649549344975521, 0.003296671979592665, 0.0029758933816078637, 0.0026885410302136515, 0.002427697000981138, 0.002191960588547811, 0.00197998154824771, 0.0017884531934438885, 0.0016157116581626968, 0.0014596915056825453, 0.0013183073973113716, 0.0011902178941448833, 0.001075274041298709, 0.0009715767531391633, 0.0008774973273270094, 0.000792465776595403, 0.0007156334705469195, 0.0006465875572662582, 0.0005845395284102415, 0.0005287849147783498, 0.0004783905456740368, 0.00043364937088152496, 0.0003926844367712702, 0.0003553032676493018, 0.0003222188008741175, 0.000290856018363324, 0.000263866885350139, 0.00023848592153571162, 0.00021579740217697924, 0.0001953115137500873, 0.000176552327494975, 0.00015986920844409457, 0.00014514612038269242, 0.00013139199286797806, 0.00011896058654699604, 0.00010801142871562812, 9.854384269346936e-05, 8.96839632265855e-05, 8.116119032742041e-05, 7.371261605295586e-05, 6.709454270500444e-05, 6.085200560591981e-05, 5.531504448637899e-05, 5.0346343437797165e-05, 4.602112496704783e-05, 4.196330819125044e-05, 3.826427228356715e-05, 3.5059849863108576e-05, 3.227464893426258e-05, 2.9663266512914643e-05, 2.7613604821364502e-05, 2.5547797173212798e-05, 2.324984555893971e-05, 2.1496142711414394e-05, 2.0478177660133986e-05, 1.8929212453566246e-05, 1.7605181323449936e-05, 1.6190574718522355e-05, 1.4947194798663768e-05, 1.3622321457301663e-05, 1.286163781774185e-05, 1.221974950446149e-05, 1.1539670098768155e-05, 1.0815586165421227e-05, 1.0133514813042554e-05, 9.592571408579765e-06, 8.950040060817808e-06, 8.110009027782867e-06, 7.539354161517556e-06, 6.872418856200363e-06, 6.459058853790304e-06, 5.913918304592636e-06, 5.743249314080371e-06, 5.5774234882014e-06, 5.482180971619092e-06, 5.14438176321879e-06, 4.966261490747832e-06, 4.76421778205554e-06, 4.475378594348568e-06, 4.32273402510476e-06, 3.904656436933662e-06, 3.7691553613826323e-06, 3.6382304166556615e-06, 3.5482744952418625e-06, 3.389966672845032e-06, 3.3562735255110196e-06], "duration": 140903.424548, "accuracy_train": [0.5655958725544481, 0.6417983890503876, 0.7432258862241602, 0.7807240477344037, 0.7980460458771688, 0.8156230894126062, 0.8278526511743264, 0.837687309662237, 0.8457988346137875, 0.8511699283637875, 0.8563070652800849, 0.8620262103751385, 0.8645835135774271, 0.8695818625415282, 0.8728127379222037, 0.8748577874100221, 0.8761151702081026, 0.880461756529162, 0.8809267862910668, 0.8839505612080103, 0.885877550814415, 0.8871817970768733, 0.8892272070528792, 0.8912736984934477, 0.8933652509574567, 0.8944355403862125, 0.8958059362310816, 0.8970142926356589, 0.8981303640642304, 0.900572851778793, 0.9012711173980252, 0.900828257659653, 0.9027341587070875, 0.9031061825166113, 0.9037121631598376, 0.9049677435169805, 0.9059893670404209, 0.9074520478612956, 0.9074771017903286, 0.9083374068498523, 0.9084540247785161, 0.9094999812546143, 0.9106850861710963, 0.9109887974690846, 0.9123588328257659, 0.9128710865402363, 0.9128235020994832, 0.9132885318613879, 0.9141960608734773, 0.9150567264211886, 0.9148005995639534, 0.9164496527777777, 0.9166828886351052, 0.9161949678732927, 0.9179159384805279, 0.9177513756229235, 0.9181008689207272, 0.9190996014442598, 0.9195188492063492, 0.9201233878968254, 0.9205651661706349, 0.9206345601467331, 0.9206581721230158, 0.9215867896940754, 0.9220521799441677, 0.9220292889442598, 0.9222614433370249, 0.9224946791943521, 0.923122108884736, 0.9237480966223699, 0.9243773287536915, 0.9247021286106497, 0.9258407305509413, 0.9257244731104651, 0.9256561605989295, 0.9267478990748431, 0.9251457093253967, 0.9271914797895902, 0.9261905843369325, 0.926772232027501, 0.9270272774201735, 0.926818374515504, 0.9274690556939831, 0.9282585248246585, 0.9277927740863787, 0.9279809489202658, 0.9287006635866556, 0.9296307231104651, 0.9294904932055187, 0.9287239150747508, 0.9294447112057033, 0.9298864894795128, 0.9295606081579919, 0.9299787744555187, 0.930258152800849, 0.9308151070505721, 0.9307475155154117, 0.9308848615148578, 0.9313981966938908, 0.931862144991233, 0.9319315389673312, 0.9327682320505721, 0.9326759470745662, 0.9318632264557956, 0.9328623194675157, 0.9341407908245662, 0.9333037372531378, 0.9336990125507567, 0.9333967432055187, 0.933675400574474, 0.9332103708125692, 0.9345593176102805, 0.9349080899317092, 0.9349538719315246, 0.9349309809316169, 0.9350708503483758, 0.9354657651578073, 0.9352797532530455, 0.9343729452173312, 0.9353037257175157, 0.9348161654438908, 0.9356757495270396, 0.9351638563007567, 0.9363736546580842, 0.936558224610096, 0.9369077179078996, 0.9363965456579919, 0.936953499907715, 0.9370228938838132, 0.9368383239318014, 0.9373491356935216, 0.9361171673126615, 0.9374428626222776, 0.9375812900862864, 0.9378377774317092, 0.9377454924557033, 0.938257385681986], "end": "2016-01-31 06:51:43.883000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0], "moving_var_accuracy_valid": [0.029376885936492615, 0.05703877890294544, 0.08626784726631122, 0.10895696993340584, 0.1251136838079823, 0.13579800417931637, 0.14179621493700484, 0.14410511116247998, 0.14339174287440948, 0.140427380189705, 0.135804446104765, 0.12999380791702403, 0.12333552339866, 0.11628275814714743, 0.10896308383815462, 0.10163727478231377, 0.09436105036925609, 0.08734721725291031, 0.08058996645295682, 0.07423823519152932, 0.06815942812328937, 0.06248657401619183, 0.05714809719495462, 0.05218285237407191, 0.0475569117220569, 0.04329896726084495, 0.039378274355300606, 0.03579197370525166, 0.03249147017385768, 0.029464960205266125, 0.0267296038492156, 0.024204559016606605, 0.021921269440963693, 0.0198555385605988, 0.017963546026806543, 0.016248273974533666, 0.014686189150771349, 0.01327596450347437, 0.012006978734797119, 0.010839751429143028, 0.00978721004022944, 0.00883355121041964, 0.007969896611918952, 0.007196273259496992, 0.006490841006430345, 0.00585371423105872, 0.005276085409499298, 0.004756490291947599, 0.004288315543927284, 0.003867735932064971, 0.003486529361539212, 0.0031460680843381555, 0.0028373783362402897, 0.0025566807153395003, 0.0023060199620415158, 0.0020774220075543246, 0.0018715034094755108, 0.0016861175007186657, 0.0015206682308329467, 0.0013697982097431217, 0.0012338612527213165, 0.001111248430962125, 0.0010019129408100585, 0.0009023683148477786, 0.0008134916988123317, 0.0007329526757684789, 0.0006597689636549692, 0.0005938842935711212, 0.0005349406004520434, 0.0004815669062425789, 0.0004336965038660427, 0.0003913401022989208, 0.00035431489045033244, 0.0003198386405517911, 0.0002889776218417292, 0.0002608550296488079, 0.00023489628336742968, 0.0002116884961737593, 0.0001907906673359624, 0.00017175404162764256, 0.00015481643540852025, 0.00013949412947858252, 0.0001255768424675399, 0.00011305833627944984, 0.00010175256750170292, 9.285668612203879e-05, 8.643593802153646e-05, 7.78058705267147e-05, 7.01578585254068e-05, 6.318952451285372e-05, 5.687061787373489e-05, 5.163855609846583e-05, 4.6482481639847654e-05, 4.183662915827031e-05, 3.7758752715737186e-05, 3.40035102114956e-05, 3.066333978206013e-05, 2.764575208314253e-05, 2.48983071841482e-05, 2.2408499917666433e-05, 2.0229601475717254e-05, 1.8705447178464686e-05, 1.684037330941857e-05, 1.5243692169822802e-05, 1.3770532428588894e-05, 1.2646298488540397e-05, 1.1392270060085248e-05, 1.0404149522990424e-05, 9.367234161974003e-06, 8.431196014108066e-06, 7.5919249306109154e-06, 7.105886494055003e-06, 6.557958530713025e-06, 6.2345500267799405e-06, 5.612927136144217e-06, 5.910397467460399e-06, 5.418679042616445e-06, 4.8769653874678146e-06, 4.394356821823949e-06, 4.233618053311278e-06, 4.158326125391641e-06, 3.7728006092528746e-06, 3.415816938773293e-06, 3.0831836911898686e-06, 2.7980306918719533e-06, 2.7115552315304666e-06, 2.836883141937163e-06, 2.930729133673396e-06, 2.6420752369509614e-06, 2.384344615997434e-06, 2.15885210824939e-06, 1.9716807330903886e-06, 1.7752694888703e-06, 1.7430263112084401e-06, 1.6382222083891355e-06, 1.5299650991805615e-06, 1.9068566670767068e-06], "accuracy_test": 0.7657944036989796, "start": "2016-01-29 15:43:20.458000", "learning_rate_per_epoch": [0.0017058778321370482, 0.0008529389160685241, 0.0005686259246431291, 0.00042646945803426206, 0.00034117556060664356, 0.00028431296232156456, 0.0002436968352412805, 0.00021323472901713103, 0.00018954198458231986, 0.00017058778030332178, 0.00015507980424445122, 0.00014215648116078228, 0.00013122137170284986, 0.00012184841762064025, 0.00011372518929420039, 0.00010661736450856552, 0.00010034575097961351, 9.477099229115993e-05, 8.978303958429024e-05, 8.529389015166089e-05, 8.123227598844096e-05, 7.753990212222561e-05, 7.416860171360895e-05, 7.107824058039114e-05, 6.823510921094567e-05, 6.561068585142493e-05, 6.318066152743995e-05, 6.0924208810320124e-05, 5.882337427465245e-05, 5.6862594647100195e-05, 5.502831481862813e-05, 5.330868225428276e-05, 5.169326686882414e-05, 5.0172875489806756e-05, 4.873936632066034e-05, 4.7385496145579964e-05, 4.610480391420424e-05, 4.489151979214512e-05, 4.3740456021623686e-05, 4.2646945075830445e-05, 4.160677417530678e-05, 4.061613799422048e-05, 3.9671576814725995e-05, 3.8769951061112806e-05, 3.7908394006080925e-05, 3.7084300856804475e-05, 3.629527418524958e-05, 3.553912029019557e-05, 3.481383100734092e-05, 3.411755460547283e-05, 3.344858487253077e-05, 3.2805342925712466e-05, 3.2186373573495075e-05, 3.1590330763719976e-05, 3.101595939369872e-05, 3.0462104405160062e-05, 2.992767986143008e-05, 2.9411687137326226e-05, 2.8913182177348062e-05, 2.8431297323550098e-05, 2.796521039272193e-05, 2.7514157409314066e-05, 2.707742532948032e-05, 2.665434112714138e-05, 2.624427361297421e-05, 2.584663343441207e-05, 2.5460862161708064e-05, 2.5086437744903378e-05, 2.472286723786965e-05, 2.436968316033017e-05, 2.402644713583868e-05, 2.3692748072789982e-05, 2.3368189431494102e-05, 2.305240195710212e-05, 2.274503822263796e-05, 2.244575989607256e-05, 2.2154257749207318e-05, 2.1870228010811843e-05, 2.159339055651799e-05, 2.1323472537915222e-05, 2.1060219296487048e-05, 2.080338708765339e-05, 2.055274489976e-05, 2.030806899711024e-05, 2.0069150195922703e-05, 1.9835788407362998e-05, 1.960779081855435e-05, 1.9384975530556403e-05, 1.9167166101397015e-05, 1.8954197003040463e-05, 1.8745909983408637e-05, 1.8542150428402238e-05, 1.834277281886898e-05, 1.814763709262479e-05, 1.795660864445381e-05, 1.7769560145097785e-05, 1.758636972226668e-05, 1.740691550367046e-05, 1.7231088349944912e-05, 1.7058777302736416e-05, 1.6889878679648973e-05, 1.6724292436265387e-05, 1.6561920347157866e-05, 1.6402671462856233e-05, 1.624645483389031e-05, 1.6093186786747538e-05, 1.594278364791535e-05, 1.5795165381859988e-05, 1.5650255591026507e-05, 1.550797969684936e-05, 1.5368268577731214e-05, 1.5231052202580031e-05, 1.5096264178282581e-05, 1.496383993071504e-05, 1.483372034272179e-05, 1.4705843568663113e-05, 1.4580152310372796e-05, 1.4456591088674031e-05, 1.4335107152874116e-05, 1.4215648661775049e-05, 1.4098163774178829e-05, 1.3982605196360964e-05, 1.3868925634596962e-05, 1.3757078704657033e-05, 1.3647022569784895e-05, 1.353871266474016e-05, 1.3432108971755952e-05, 1.332717056357069e-05, 1.3223859241406899e-05, 1.3122136806487106e-05, 1.302196778851794e-05, 1.2923316717206035e-05, 1.282614903175272e-05, 1.2730431080854032e-05, 1.263613194169011e-05, 1.2543218872451689e-05, 1.2451662769308314e-05], "accuracy_train_first": 0.5655958725544481, "accuracy_train_last": 0.938257385681986, "batch_size_eval": 1024, "accuracy_train_std": [0.017907140090167705, 0.014298549553953084, 0.014736649476061075, 0.016037123199475815, 0.01484688923631022, 0.015570530655670866, 0.014418084093682703, 0.014122666181273107, 0.013140981956482408, 0.013575472767315414, 0.013583953752727947, 0.012233555968095201, 0.011958009913390097, 0.012409507282849968, 0.012282648689746713, 0.012728924053077621, 0.010977251459238217, 0.011163686549350573, 0.01197311138246521, 0.010435619024730452, 0.011096679137834728, 0.010615389713873142, 0.010738169785020015, 0.011183445023604301, 0.011161453775777201, 0.010621845288662151, 0.010483739533650852, 0.010481767662203195, 0.010760808034699407, 0.010638313375574372, 0.010995234029036209, 0.010633483060015044, 0.011315319272909347, 0.010744154925549334, 0.010334092903135158, 0.010776419467442304, 0.01137076455092192, 0.010629806852120005, 0.011060954369435527, 0.01086775618965702, 0.010863260229954408, 0.010885215105305303, 0.011400816960845018, 0.010942995222079293, 0.010891190599990953, 0.010839895881111563, 0.010692607022173966, 0.01088958620079851, 0.01053392935096643, 0.011312409532122886, 0.011123971524539662, 0.011097909259632359, 0.009825080433790576, 0.011294012192651144, 0.011061724077226753, 0.011089921688660244, 0.010585941846499011, 0.011285054929581671, 0.011318537392888703, 0.010859205579933367, 0.010426710365795951, 0.010478373322199844, 0.01059987091110034, 0.01085586287312299, 0.010442069242459707, 0.009948543866107528, 0.010255354049110713, 0.010599943450241404, 0.010066025353805389, 0.010187509950675569, 0.01038277373830562, 0.010901715300243213, 0.009977952203830003, 0.010230777783475087, 0.010220896056878037, 0.009927591051722705, 0.009574632922192948, 0.010018603597268987, 0.009214804114024826, 0.009705186259639889, 0.009946441482947606, 0.009754662940768254, 0.01042126907260126, 0.009929951425301268, 0.009733365631324972, 0.010075590006933538, 0.009717168599855356, 0.01007820244958543, 0.009823334007761447, 0.009662095826514176, 0.009331296153119733, 0.010106773360638864, 0.010006510886991053, 0.009759529483304889, 0.010097283347423922, 0.009403376093273021, 0.010048847001721609, 0.009565999111270382, 0.008695757123796971, 0.00904275041199365, 0.009176604457897442, 0.008863926860478918, 0.008541672791168712, 0.009079502818681664, 0.009154227458887639, 0.009554187388739985, 0.008924395417534377, 0.009520488555386498, 0.009608067480585817, 0.009029921765250184, 0.00996681162972746, 0.008324979882885714, 0.009274527323257353, 0.009240857462209403, 0.009382065057716892, 0.009137626975016374, 0.008962983477471896, 0.008620973633202406, 0.009761635955357022, 0.009323547669999864, 0.00957737562721035, 0.008561642495229714, 0.009109651643428123, 0.009011666090013366, 0.009316038084640097, 0.008633465304252361, 0.008820214640831535, 0.009214219023457088, 0.009179698132120434, 0.00890647506650796, 0.009176091342553356, 0.009195413312087355, 0.008948616657809395, 0.009706100114265816, 0.00982940032733187, 0.009117149525645518, 0.009596273032136074], "accuracy_test_std": 0.009350918737295483, "error_valid": [0.42867711078689763, 0.35977650837725905, 0.2615466749811747, 0.23238069465361444, 0.21501582501882532, 0.2007733080760542, 0.19127241387424698, 0.18301134224397586, 0.17811823465737953, 0.1737134083207832, 0.1701527790850903, 0.16749811746987953, 0.16650096479668675, 0.16316388601280118, 0.16237116434487953, 0.16011212820030118, 0.16025478868599397, 0.1574059911521084, 0.15682652484939763, 0.15250258847891573, 0.15421157285391573, 0.15153632106551207, 0.1524011083396084, 0.15164809629141573, 0.15265554405120485, 0.1513024755271084, 0.15080389919051207, 0.14899343467620485, 0.1495934911521084, 0.14993911191641573, 0.1462667074548193, 0.1493184652673193, 0.14676528379141573, 0.14442535768072284, 0.14591079160391573, 0.14491363893072284, 0.14552399049322284, 0.14381500611822284, 0.14122064429593373, 0.1449033438441265, 0.14357086549322284, 0.1437032308923193, 0.1439267813441265, 0.14112945924322284, 0.1430722891566265, 0.1428487387048193, 0.1439473715173193, 0.14285903379141573, 0.1422383871423193, 0.14086472844503017, 0.14161774049322284, 0.13915574407003017, 0.13963373023343373, 0.1411191641566265, 0.13889101327183728, 0.14088531861822284, 0.1406308829066265, 0.14025437688253017, 0.1383115469691265, 0.13999994117093373, 0.13987787085843373, 0.14001023625753017, 0.1381894766566265, 0.13952195500753017, 0.13804681617093373, 0.13854539250753017, 0.14013230657003017, 0.14012201148343373, 0.13881012330572284, 0.1396543204066265, 0.13891160344503017, 0.1371614387236446, 0.13534067912274095, 0.13643931193524095, 0.13583925545933728, 0.13608339608433728, 0.13753794474774095, 0.13683640813253017, 0.13669374764683728, 0.13756883000753017, 0.13656138224774095, 0.13669374764683728, 0.13729380412274095, 0.13717173381024095, 0.13779238045933728, 0.13399790568524095, 0.13174916462725905, 0.13643931193524095, 0.1355745246611446, 0.1359407355986446, 0.13657167733433728, 0.13434352644954817, 0.13607310099774095, 0.13617458113704817, 0.1374055793486446, 0.13595103068524095, 0.13556422957454817, 0.13556422957454817, 0.13666286238704817, 0.13628635636295183, 0.13544215926204817, 0.1338346550263554, 0.13570689006024095, 0.13494358292545183, 0.13507594832454817, 0.1340787956513554, 0.13593044051204817, 0.1369172980986446, 0.13555393448795183, 0.1358186652861446, 0.13553334431475905, 0.13397731551204817, 0.1342008659638554, 0.13348903426204817, 0.13507594832454817, 0.13211537556475905, 0.13384495011295183, 0.13483180769954817, 0.13455678181475905, 0.13653049698795183, 0.1329801628388554, 0.1353303840361446, 0.13433323136295183, 0.13507594832454817, 0.1352994987763554, 0.1333772590361446, 0.1367952277861446, 0.1328580925263554, 0.13492299275225905, 0.13445530167545183, 0.13507594832454817, 0.1352994987763554, 0.13469944230045183, 0.13605251082454817, 0.1357877800263554, 0.13421116105045183, 0.1324918815888554], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.07045245854956524, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.0017058778016757772, "optimization": "rmsprop", "nb_data_augmentation": 4, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.513513427828966e-05, "rotation_range": [0, 0], "momentum": 0.5482657301244491}, "accuracy_valid_max": 0.868250835372741, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8675081184111446, "accuracy_valid_std": [0.012889188555235003, 0.011831921182006705, 0.019472343564916863, 0.022161868122288684, 0.018880735783159866, 0.013316823542501214, 0.016748130903397277, 0.015500836052332191, 0.017718352742431748, 0.014945461183066427, 0.01503589217285514, 0.013796815823960236, 0.015999860834428625, 0.012064159019406063, 0.01599375444442015, 0.014040853851081694, 0.014674168491167079, 0.013526493340375648, 0.014766305400021442, 0.01619990878518791, 0.015106869807907973, 0.013535012242212164, 0.014247321888036273, 0.0156655114497091, 0.014900884557195291, 0.013779153826992265, 0.0131126127908033, 0.014465131637654048, 0.013777616289516396, 0.014228132809759791, 0.016563405879658353, 0.01582740946399499, 0.016835600630929887, 0.014541105320401588, 0.013392388481606532, 0.013916384622970086, 0.016181737460229526, 0.015279807182985253, 0.015680292455328385, 0.012881007855914242, 0.013314399233039686, 0.01458277143688177, 0.013522557914549312, 0.014284785679304971, 0.01343504155996634, 0.015045492005077878, 0.013069117638138585, 0.014705517400572719, 0.015400411844794215, 0.01455637953934925, 0.014573398826311052, 0.016038461250207307, 0.0141612239682965, 0.014779440987370971, 0.01353546993170861, 0.013686274004729883, 0.014212203874388225, 0.013037094495838203, 0.016262373578420305, 0.014762814478182338, 0.014985596998693137, 0.013592116480260192, 0.012297654522247528, 0.014805161475770356, 0.012399542059240948, 0.013392844584877033, 0.011819496491658803, 0.013176241019607406, 0.013668573376902807, 0.012800439694124987, 0.012626118279710294, 0.014557515966193726, 0.012879274086618547, 0.012504871538789711, 0.012231240212619893, 0.012496877859863961, 0.013947108549909073, 0.011137328900138091, 0.013630160735104473, 0.012994970221404804, 0.011415704043667088, 0.01297600575640939, 0.013507940760171793, 0.012442435132510013, 0.012117750601148414, 0.012883113876882546, 0.013355834449261111, 0.01374900569272364, 0.011766390866728179, 0.01316681302924367, 0.011996575168819558, 0.012415410577994813, 0.01192996089269413, 0.014521266844865391, 0.014073224607355585, 0.011997482066347799, 0.015188548936280728, 0.013179908132628275, 0.014004992560601852, 0.01353966999638667, 0.012428150902145815, 0.013290151997492379, 0.012254828785248114, 0.015191574612456526, 0.013344142698235681, 0.01316086674153697, 0.015262805283366003, 0.014265158751899529, 0.012816842015827765, 0.013100015209895788, 0.014997190318083968, 0.014077081379324537, 0.015251498683096188, 0.013335818804612994, 0.015017038376194142, 0.013865613748151168, 0.014567284641907605, 0.01373493655782161, 0.014839686188286788, 0.014448064162947008, 0.01556075370391145, 0.012090256236890333, 0.013172979703805848, 0.014491867855817828, 0.014996450907448434, 0.013622543235330511, 0.012864702225310085, 0.014212303029567525, 0.015035836931191707, 0.01496011927991755, 0.013852596216549707, 0.016189096273707026, 0.01399155806018099, 0.013330358685598823, 0.014353185991931148, 0.014622546885567057, 0.013856206872172268], "accuracy_valid": [0.5713228892131024, 0.640223491622741, 0.7384533250188253, 0.7676193053463856, 0.7849841749811747, 0.7992266919239458, 0.808727586125753, 0.8169886577560241, 0.8218817653426205, 0.8262865916792168, 0.8298472209149097, 0.8325018825301205, 0.8334990352033133, 0.8368361139871988, 0.8376288356551205, 0.8398878717996988, 0.839745211314006, 0.8425940088478916, 0.8431734751506024, 0.8474974115210843, 0.8457884271460843, 0.8484636789344879, 0.8475988916603916, 0.8483519037085843, 0.8473444559487951, 0.8486975244728916, 0.8491961008094879, 0.8510065653237951, 0.8504065088478916, 0.8500608880835843, 0.8537332925451807, 0.8506815347326807, 0.8532347162085843, 0.8555746423192772, 0.8540892083960843, 0.8550863610692772, 0.8544760095067772, 0.8561849938817772, 0.8587793557040663, 0.8550966561558735, 0.8564291345067772, 0.8562967691076807, 0.8560732186558735, 0.8588705407567772, 0.8569277108433735, 0.8571512612951807, 0.8560526284826807, 0.8571409662085843, 0.8577616128576807, 0.8591352715549698, 0.8583822595067772, 0.8608442559299698, 0.8603662697665663, 0.8588808358433735, 0.8611089867281627, 0.8591146813817772, 0.8593691170933735, 0.8597456231174698, 0.8616884530308735, 0.8600000588290663, 0.8601221291415663, 0.8599897637424698, 0.8618105233433735, 0.8604780449924698, 0.8619531838290663, 0.8614546074924698, 0.8598676934299698, 0.8598779885165663, 0.8611898766942772, 0.8603456795933735, 0.8610883965549698, 0.8628385612763554, 0.864659320877259, 0.863560688064759, 0.8641607445406627, 0.8639166039156627, 0.862462055252259, 0.8631635918674698, 0.8633062523531627, 0.8624311699924698, 0.863438617752259, 0.8633062523531627, 0.862706195877259, 0.862828266189759, 0.8622076195406627, 0.866002094314759, 0.868250835372741, 0.863560688064759, 0.8644254753388554, 0.8640592644013554, 0.8634283226656627, 0.8656564735504518, 0.863926899002259, 0.8638254188629518, 0.8625944206513554, 0.864048969314759, 0.8644357704254518, 0.8644357704254518, 0.8633371376129518, 0.8637136436370482, 0.8645578407379518, 0.8661653449736446, 0.864293109939759, 0.8650564170745482, 0.8649240516754518, 0.8659212043486446, 0.8640695594879518, 0.8630827019013554, 0.8644460655120482, 0.8641813347138554, 0.864466655685241, 0.8660226844879518, 0.8657991340361446, 0.8665109657379518, 0.8649240516754518, 0.867884624435241, 0.8661550498870482, 0.8651681923004518, 0.865443218185241, 0.8634695030120482, 0.8670198371611446, 0.8646696159638554, 0.8656667686370482, 0.8649240516754518, 0.8647005012236446, 0.8666227409638554, 0.8632047722138554, 0.8671419074736446, 0.865077007247741, 0.8655446983245482, 0.8649240516754518, 0.8647005012236446, 0.8653005576995482, 0.8639474891754518, 0.8642122199736446, 0.8657888389495482, 0.8675081184111446], "seed": 825562835, "model": "residualv3", "loss_std": [0.3463432788848877, 0.27059435844421387, 0.2575739920139313, 0.24944689869880676, 0.24657723307609558, 0.2395881712436676, 0.23748581111431122, 0.23456062376499176, 0.23106412589550018, 0.22967030107975006, 0.2251662015914917, 0.22351226210594177, 0.22129078209400177, 0.221949964761734, 0.21800121665000916, 0.2167520374059677, 0.2130747139453888, 0.21313518285751343, 0.2102799266576767, 0.20860999822616577, 0.21042145788669586, 0.20718958973884583, 0.2050996571779251, 0.20515300333499908, 0.20407019555568695, 0.2027955800294876, 0.20236337184906006, 0.20178435742855072, 0.19917380809783936, 0.20006079971790314, 0.19864629209041595, 0.1977057307958603, 0.1961725354194641, 0.19499152898788452, 0.19421012699604034, 0.19443096220493317, 0.19325116276741028, 0.19127173721790314, 0.19115369021892548, 0.18876467645168304, 0.1884603351354599, 0.18754854798316956, 0.18750692903995514, 0.18619117140769958, 0.18571984767913818, 0.18756870925426483, 0.1847415715456009, 0.18459054827690125, 0.1818036437034607, 0.18306387960910797, 0.1822705715894699, 0.1840568482875824, 0.18149664998054504, 0.18099649250507355, 0.17798319458961487, 0.1781541407108307, 0.17891396582126617, 0.17736484110355377, 0.17890776693820953, 0.17769157886505127, 0.1766737997531891, 0.17838062345981598, 0.1749805361032486, 0.17419691383838654, 0.17376478016376495, 0.17644797265529633, 0.1716681867837906, 0.17462748289108276, 0.1731196641921997, 0.1723625212907791, 0.17144283652305603, 0.17196311056613922, 0.17241854965686798, 0.1729133576154709, 0.17158976197242737, 0.170578271150589, 0.16999299824237823, 0.16934141516685486, 0.16890285909175873, 0.16906335949897766, 0.16856732964515686, 0.16749818623065948, 0.16543515026569366, 0.16746538877487183, 0.1659684032201767, 0.16686375439167023, 0.16585108637809753, 0.16684578359127045, 0.1648465394973755, 0.1669590026140213, 0.1650821566581726, 0.16528502106666565, 0.16508926451206207, 0.16273213922977448, 0.1638326495885849, 0.16245037317276, 0.16306741535663605, 0.16065604984760284, 0.162944495677948, 0.16104093194007874, 0.16154439747333527, 0.16097232699394226, 0.1618759036064148, 0.16129440069198608, 0.1607617884874344, 0.15932653844356537, 0.16014127433300018, 0.15991561114788055, 0.15900543332099915, 0.15788231790065765, 0.1596565991640091, 0.15825428068637848, 0.15917758643627167, 0.1595352292060852, 0.15746252238750458, 0.1573573797941208, 0.15553125739097595, 0.15955817699432373, 0.15765517950057983, 0.15901340544223785, 0.15509071946144104, 0.1571255922317505, 0.15533973276615143, 0.15655729174613953, 0.1567797213792801, 0.15642134845256805, 0.15380938351154327, 0.15467441082000732, 0.154880553483963, 0.15358039736747742, 0.15448804199695587, 0.15386702120304108, 0.15289971232414246, 0.15216292440891266, 0.15637871623039246, 0.1509939432144165, 0.14983077347278595]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:25 2016", "state": "available"}], "summary": "0df8506f94633c2cefe44e53a423fb97"}