{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 16, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 6, "nbg2": 4, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.015461623853892497, 0.01824699167268702, 0.017592772451340145, 0.016661222464560536, 0.009684757145012768, 0.007975988634800714, 0.009876089273843765, 0.006601585919186097, 0.011157127355572862, 0.009554843282802246, 0.008287451533095149, 0.010186350503699643, 0.01311466644575256, 0.014036158375122034, 0.010948983406702978, 0.01136278770099417, 0.01510431829799442, 0.014751060784724318, 0.017833160576037392, 0.018442382558598848, 0.014687326173011177, 0.01562761010031659, 0.01502406985884481, 0.015923483478978658, 0.0159877940039462, 0.018181275866538123, 0.01635854286565105, 0.014702183371255135, 0.016669005709846314, 0.015179088670967091, 0.018021395019190273, 0.01569159424672718, 0.017389637147738956, 0.015026115913544965, 0.013756058568490938, 0.015894002396377213, 0.013523133829887443, 0.011007701878337573, 0.013530493325760283, 0.015379054893707647, 0.01646906628740409, 0.014997815972499558, 0.013160594322490733, 0.010692913883692113, 0.01367860544590709, 0.014383580305980413, 0.013145907972126612, 0.014301454604031032, 0.016829616513741086, 0.015952534223690026, 0.017028412390990576, 0.014465228760015894, 0.015314043273926223, 0.01687578795726998, 0.013955476479071558, 0.018838608416175497, 0.016283823309802787, 0.015561332235799653, 0.017370701176108498, 0.01472411379762794, 0.01894089741023262, 0.017540840277478157, 0.018740446146456584, 0.018149953020034193, 0.017521896366238607, 0.018714715965152364, 0.0212144152812714, 0.019126262648318484, 0.01912539905447593, 0.017571964391201164, 0.01749923134768543, 0.018113540137399926, 0.020580292371219614, 0.021971623763815933, 0.01827620551275366, 0.01885520714009998, 0.021691051173161537, 0.020842777030864512, 0.020217032952026297, 0.01893730944283233, 0.020729549035365932, 0.021452205752475976, 0.020394091289421653, 0.0202066093056814, 0.017561543879434677, 0.019850602432293824, 0.018627435794178467, 0.020080728022877803, 0.021054957694114057, 0.022596424315642134, 0.022166084897120134, 0.021717504932991374, 0.01876154363500013, 0.021148952313427314, 0.01769095567433937, 0.019680640462523758, 0.021442252026226245, 0.020490558799220442, 0.015924462919056925, 0.02061202578672217, 0.020409633676248342, 0.019213061421969702, 0.0189058085264312, 0.019478027327573113, 0.019951605218204573, 0.019601996663482963, 0.018110442533800435, 0.018605887118828462, 0.02028766729647684, 0.019224173219705702, 0.02027968806524673, 0.019122540468432526, 0.020102030912401724, 0.02023082813514575, 0.018325745872308435, 0.018047607889085538, 0.018862737120921742, 0.019070085361123812, 0.01982790800896534, 0.01908232006447978, 0.0192631515047808, 0.020212077171866374, 0.019399245644770063, 0.01844439185489828, 0.02003264199870971, 0.018726554781719886, 0.01835368439277564, 0.02043602351981323, 0.018044941710425155, 0.020978020222550617, 0.022348369745880937, 0.018399469688340084, 0.02040620717629573, 0.019092257225519393, 0.01911222911011224, 0.020354092167362767, 0.02232702299864897, 0.020294909172474108, 0.02068731742774602, 0.020235655114621026, 0.018821370277419276, 0.019033409523925383, 0.02120040058741935, 0.018688441101914064, 0.02030141448232176, 0.018958102625427332, 0.021274584656282493, 0.020430189395188144, 0.019685014103706863], "moving_avg_accuracy_train": [0.036842253253045396, 0.07795803989594867, 0.12085040785633533, 0.16398703846697346, 0.20587393931758138, 0.24599430626391477, 0.2841159630429312, 0.319555115977287, 0.35291501712411344, 0.38408225249180544, 0.41317417864766365, 0.439889371265317, 0.46484897300806366, 0.4877680193038335, 0.5089229697497882, 0.5285202806213395, 0.5466741515879061, 0.563437849299409, 0.5788253016802839, 0.592924980699225, 0.6057729461305945, 0.6177754960461896, 0.6287543220356553, 0.6390003498380883, 0.6484031725162396, 0.6570934693634528, 0.6651125183700293, 0.6724110066354627, 0.6791190829052867, 0.6854098288171852, 0.6911667230926279, 0.6966500170416708, 0.7016317729625499, 0.7062734273615698, 0.7105648125635358, 0.7144712370726861, 0.7182518698022946, 0.7217707327482372, 0.7250354016984043, 0.7280619233594978, 0.7307998158449767, 0.7334102232128138, 0.7357945752224664, 0.7380078132001736, 0.7400345325146155, 0.741944682501203, 0.7437451255998276, 0.7452958420219415, 0.7468424772791882, 0.7483043116214521, 0.7496153122318705, 0.7507254222681427, 0.7517709521793405, 0.7528260055863604, 0.7537523382134017, 0.7547299805110169, 0.7556146530717647, 0.7563667886955129, 0.7571620770532594, 0.7577919142157352, 0.7583309019250679, 0.7589113219646579, 0.75937328218006, 0.7598982923191506, 0.7603335630145612, 0.7607717014701649, 0.76114517183856, 0.7614814033165719, 0.7618397791693924, 0.7621900029297326, 0.7624820249235811, 0.7627077865323673, 0.7628853953433701, 0.7631032998958732, 0.7633482060693073, 0.7635152153004164, 0.7636770772548247, 0.7638298005578583, 0.7639695406305793, 0.7640674049103139, 0.7641927572406649, 0.7643055743379809, 0.7643977730815084, 0.7645458921661685, 0.7646163842756868, 0.764733341645691, 0.7648315557346288, 0.7649037084218249, 0.7650104264212352, 0.7650925938254848, 0.7651386066547764, 0.7652264851285107, 0.7653427781358239, 0.7653963246174149, 0.7654910194270375, 0.7655436926723643, 0.7654980565919588, 0.7655733136577075, 0.7656014814383008, 0.7657105017491588, 0.7657365764646545, 0.7657228413276483, 0.7657245026948374, 0.7657515385133935, 0.7657223562786563, 0.7657147655555063, 0.7657171984510908, 0.765733338949974, 0.7657408539037215, 0.7657359916180466, 0.7657432413049869, 0.7657637890137278, 0.7657450074730046, 0.765772245864916, 0.7657688946807407, 0.7657240259364115, 0.7657999015069914, 0.765810096849094, 0.7658704980284334, 0.7658875125136114, 0.7659191376407571, 0.7659452751063787, 0.7659407528444487, 0.7659392242504339, 0.7659704005991539, 0.7659379333463168, 0.76592498886043, 0.7658993879302748, 0.7658670104490782, 0.7659099503290966, 0.7659207304842175, 0.7659490338143026, 0.765983843455436, 0.765926708331238, 0.7659358126861447, 0.766011435921037, 0.7659422770038595, 0.7659056466641232, 0.7659540235178751, 0.7659674078493656, 0.765956166210793, 0.7659809259682205, 0.7659474422272955, 0.7658940914211865, 0.7658856032254502, 0.7658895535445165, 0.7658767967411907, 0.7658467865253588, 0.765852257316806], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 37418308, "moving_var_accuracy_train": [0.012216164622853808, 0.02620911936195112, 0.040146004488998854, 0.052878324144046765, 0.06338110389546002, 0.07152978810109, 0.07745615573117515, 0.08101394220439959, 0.08292849502469395, 0.08337821456641056, 0.08265745461689067, 0.08081502280458648, 0.07834035599653652, 0.07523386454485165, 0.0717382654457037, 0.06802093024169933, 0.06418490449716617, 0.06029560809611337, 0.05639701050346527, 0.05254651798905322, 0.04877749813167889, 0.04519629915879816, 0.04176148082388102, 0.03853016251304699, 0.035472863930593096, 0.03260526887116794, 0.029923488306780023, 0.027410550854748042, 0.02507448035444922, 0.0229231936761569, 0.020929150793828847, 0.019106834327230454, 0.01741951192300433, 0.01587146533074338, 0.014450062680233912, 0.013142397784221735, 0.011956796660325241, 0.010872558562183665, 0.009881225276152962, 0.00897554124882328, 0.008145451621299322, 0.007392234498803917, 0.0067041772594769325, 0.00607784533464293, 0.005507029121794422, 0.004989164266356323, 0.0045194221978831555, 0.004089122470891165, 0.003701738949372673, 0.003350797691233405, 0.0030311864255147232, 0.0027391588815969406, 0.0024750811885941326, 0.002237591308959696, 0.002021555007287018, 0.0018280015667170926, 0.0016522452199030423, 0.0014921120698813384, 0.0013485932150409104, 0.0012173041471979398, 0.0010981883022354515, 0.0009914014588131243, 0.0008941819780973406, 0.000807244501102939, 0.0007282251961971935, 0.0006571303643339831, 0.0005926726489452073, 0.0005344228485119408, 0.0004821364629277099, 0.0004350267267757004, 0.00039229154570215145, 0.000353521105867952, 0.0003184528992888694, 0.0002870349509059968, 0.00025887126711947225, 0.000233235169157006, 0.00021014744587186934, 0.0001893426209502878, 0.00017058410444657565, 0.00015361189075714985, 0.0001383921205419548, 0.00012466745776478047, 0.00011227721746307486, 0.00010124694908593244, 9.116697641487819e-05, 8.217339001097512e-05, 7.404286507527043e-05, 6.668543266016986e-05, 6.01193879767362e-05, 5.4168212519952754e-05, 4.877044589209231e-05, 4.3962904938195806e-05, 3.968833101632575e-05, 3.574530294591026e-05, 3.225147661404416e-05, 2.9051299189599138e-05, 2.616491313715223e-05, 2.3599394456943006e-05, 2.1246595826020632e-05, 1.9228905097034717e-05, 1.731213360442488e-05, 1.558261812987962e-05, 1.4024381158160088e-05, 1.2628521461709074e-05, 1.137333374095653e-05, 1.0236518938562318e-05, 9.21292031553441e-06, 8.293972925318745e-06, 7.465083903555318e-06, 6.718788289597641e-06, 6.047382482284463e-06, 5.446444109066515e-06, 4.904974414607263e-06, 4.4211543430918186e-06, 3.979139982701025e-06, 3.599344822389994e-06, 3.2912242600485022e-06, 2.963037339048959e-06, 2.699568327334369e-06, 2.4322169289538025e-06, 2.1979965740612477e-06, 1.9843454206371973e-06, 1.7860949362501369e-06, 1.6075064720220828e-06, 1.4555035072954667e-06, 1.3194402591270395e-06, 1.189004270648188e-06, 1.0760025122066874e-06, 9.77836972583718e-07, 8.96647774989254e-07, 8.080289031902191e-07, 7.34435719316341e-07, 6.718975474272402e-07, 6.340875944386919e-07, 5.714248384992491e-07, 5.657522175495836e-07, 5.522235982211508e-07, 5.090772745017892e-07, 4.792324268620418e-07, 4.329214471408514e-07, 3.9076667236693705e-07, 3.5720741542107315e-07, 3.315771220359923e-07, 3.2403618644474494e-07, 2.9228101300197776e-07, 2.6319335688830787e-07, 2.3833864547930938e-07, 2.2261029841986574e-07, 2.006186346094015e-07], "duration": 88808.598434, "accuracy_train": [0.36842253253045404, 0.44800011968207826, 0.5068817194998154, 0.5522167139627169, 0.5828560469730528, 0.6070776087809154, 0.6272108740540789, 0.6385074923864895, 0.6531541274455519, 0.6645873708010336, 0.6750015140503876, 0.6803261048241971, 0.6894853886927833, 0.6940394359657622, 0.6993175237633813, 0.7048960784653009, 0.7100589902870063, 0.7143111287029347, 0.717312373108158, 0.7198220918696936, 0.72140463501292, 0.7257984452865449, 0.7275637559408453, 0.7312146000599852, 0.7330285766196013, 0.7353061409883721, 0.7372839594292174, 0.7380974010243632, 0.7394917693337025, 0.7420265420242709, 0.7429787715716132, 0.7459996625830565, 0.7464675762504613, 0.7480483169527501, 0.7491872793812293, 0.7496290576550388, 0.7522775643687707, 0.7534404992617202, 0.7544174222499077, 0.7553006183093393, 0.7554408482142857, 0.7569038895233481, 0.7572537433093393, 0.7579269549995387, 0.7582750063445921, 0.759136032380491, 0.7599491134874492, 0.7592522898209672, 0.7607621945944075, 0.7614608207018273, 0.7614143177256368, 0.7607164125945921, 0.7611807213801218, 0.7623214862495385, 0.7620893318567736, 0.7635287611895534, 0.7635767061184938, 0.763136009309247, 0.764319672272979, 0.7634604486780177, 0.7631817913090624, 0.764135102320967, 0.7635309241186784, 0.764623383570967, 0.7642509992732558, 0.7647149475705981, 0.7645064051541158, 0.7645074866186784, 0.7650651618447766, 0.7653420167727945, 0.7651102228682172, 0.7647396410114433, 0.7644838746423956, 0.7650644408684015, 0.765552361630214, 0.7650182983803986, 0.7651338348444997, 0.7652043102851606, 0.7652272012850683, 0.7649481834279254, 0.7653209282138242, 0.7653209282138242, 0.7652275617732558, 0.76587896392811, 0.765250813261351, 0.765785957975729, 0.7657154825350683, 0.765553082606589, 0.7659708884159283, 0.765832100463732, 0.7655527221184015, 0.7660173913921188, 0.7663894152016426, 0.7658782429517349, 0.7663432727136397, 0.7660177518803063, 0.7650873318683092, 0.7662506272494463, 0.7658549914636397, 0.7666916845468806, 0.7659712489041158, 0.765599225094592, 0.7657394549995385, 0.7659948608803986, 0.7654597161660206, 0.7656464490471576, 0.765739094511351, 0.7658786034399224, 0.7658084884874492, 0.7656922310469729, 0.7658084884874492, 0.7659487183923956, 0.7655759736064968, 0.7660173913921188, 0.7657387340231635, 0.7653202072374492, 0.7664827816422111, 0.7659018549280177, 0.7664141086424879, 0.766040642880214, 0.7662037637850683, 0.7661805122969729, 0.7659000524870802, 0.7659254669043004, 0.7662509877376338, 0.7656457280707825, 0.7658084884874492, 0.7656689795588777, 0.7655756131183092, 0.7662964092492617, 0.7660177518803063, 0.7662037637850683, 0.7662971302256367, 0.7654124922134552, 0.7660177518803063, 0.7666920450350683, 0.7653198467492617, 0.7655759736064968, 0.7663894152016426, 0.7660878668327795, 0.7658549914636397, 0.7662037637850683, 0.76564608855897, 0.7654139341662052, 0.7658092094638242, 0.7659251064161129, 0.7657619855112587, 0.7655766945828718, 0.7659014944398301], "end": "2016-01-26 17:31:21.122000", "learning_rate_per_epoch": [0.000666383421048522, 0.0006162754143588245, 0.0005699352477677166, 0.0005270795663818717, 0.00048744637751951814, 0.0004507933626882732, 0.00041689645149745047, 0.0003855483664665371, 0.000356557487975806, 0.00032974654459394515, 0.00030495162354782224, 0.00028202112298458815, 0.0002608148497529328, 0.00024120316084008664, 0.00022306614846456796, 0.0002062929270323366, 0.00019078094919677824, 0.00017643538012634963, 0.0001631685154279694, 0.00015089924272615463, 0.00013955253234598786, 0.0001290590298594907, 0.00011935457587242126, 0.0001103798349504359, 0.00010207994637312368, 9.440415306016803e-05, 8.73055323609151e-05, 8.074069046415389e-05, 7.466947863576934e-05, 6.905478949192911e-05, 6.386228778865188e-05, 5.906023216084577e-05, 5.461926048155874e-05, 5.051222615293227e-05, 4.6714012569282204e-05, 4.320140214986168e-05, 3.995291990577243e-05, 3.6948702472727746e-05, 3.417038533370942e-05, 3.1600979127688333e-05, 2.9224778700154275e-05, 2.702725396375172e-05, 2.499496804375667e-05, 2.311549906153232e-05, 2.1377354642027058e-05, 1.9769908249145374e-05, 1.828333188313991e-05, 1.6908536053961143e-05, 1.5637117030564696e-05, 1.4461301361734513e-05, 1.337389949185308e-05, 1.236826392414514e-05, 1.1438245564932004e-05, 1.057815916283289e-05, 9.782746019482147e-06, 9.047143066709395e-06, 8.366852853214368e-06, 7.737716259725858e-06, 7.155887033150066e-06, 6.617807684961008e-06, 6.120189027569722e-06, 5.659987891704077e-06, 5.234391210251488e-06, 4.840796918870183e-06, 4.4767984945792705e-06, 4.140170403843513e-06, 3.828854914900148e-06, 3.540948227964691e-06, 3.27469047078921e-06, 3.028453647857532e-06, 2.800732318064547e-06, 2.5901342723955167e-06, 2.3953718937264057e-06, 2.2152544261189178e-06, 2.0486806988628814e-06, 1.894632418952824e-06, 1.7521675772513845e-06, 1.6204152188947774e-06, 1.498569872637745e-06, 1.3858865486326977e-06, 1.2816763046430424e-06, 1.1853020396301872e-06, 1.096174514714221e-06, 1.013748828881944e-06, 9.375210652251553e-07, 8.67025164552615e-07, 8.018301400625205e-07, 7.415374057018198e-07, 6.857783318992006e-07, 6.342119718283357e-07, 5.865230718882231e-07, 5.424200821835257e-07, 5.016333943785867e-07, 4.639136079731543e-07, 4.2903013763861964e-07, 3.9676967844570754e-07, 3.6693501215268043e-07, 3.3934372822841397e-07, 3.138271438274387e-07, 2.9022925218669116e-07, 2.684057847091026e-07, 2.4822330146889726e-07, 2.2955842382543779e-07, 2.1229702440450637e-07, 1.9633357339898794e-07, 1.8157048486955318e-07, 1.6791749146705115e-07, 1.5529111863088474e-07, 1.4361417299824097e-07, 1.3281525923503068e-07, 1.2282835371024703e-07, 1.1359240659203351e-07, 1.0505095104917928e-07, 9.715176219060595e-08, 8.984653732113657e-08, 8.309062593525596e-08, 7.684271707830703e-08, 7.106461197281533e-08, 6.572098243395885e-08, 6.077916481217471e-08, 5.6208939724911033e-08, 5.1982368631797726e-08, 4.80736090935352e-08, 4.445876555791983e-08, 4.1115736593155816e-08, 3.802408343744901e-08, 3.516490210131451e-08, 3.2520716786166304e-08, 3.007535553933849e-08, 2.781387209438435e-08, 2.572243751330916e-08, 2.3788267355939752e-08, 2.19995346384394e-08, 2.0345304108104756e-08, 1.8815461189092275e-08, 1.7400653362642515e-08, 1.6092229770947597e-08, 1.488219236733812e-08, 1.3763141737399565e-08, 1.2728237130943398e-08, 1.1771151164907678e-08, 1.088603251986342e-08, 1.006746952469939e-08, 9.31045729402058e-09, 8.61036753008193e-09, 7.962920101078907e-09, 7.364156839884117e-09, 6.810417119140766e-09, 6.298315202712956e-09], "accuracy_valid": [0.35499370528990964, 0.43927663780120485, 0.5104583372552711, 0.5416980421686747, 0.5783823771649097, 0.593041109751506, 0.6125429452183735, 0.6222173851656627, 0.6292665780308735, 0.640416156814759, 0.6467035132718373, 0.651087749435241, 0.6592767554593373, 0.664393413497741, 0.6705484045557228, 0.6742311041039157, 0.6759092032191265, 0.6790933264307228, 0.6805375800075302, 0.6840776190700302, 0.6909856221762049, 0.6885236257530121, 0.6898663991905121, 0.6930402273155121, 0.6960919851280121, 0.6995202489646084, 0.6975671239646084, 0.700537991810994, 0.7022160909262049, 0.704444241810994, 0.7026940770896084, 0.7080048710466867, 0.7052781438253012, 0.7068959431475903, 0.7091137989457832, 0.707862210560994, 0.7115552051957832, 0.7102330219314759, 0.7111889942582832, 0.7109242634600903, 0.711646390248494, 0.7130200489457832, 0.7127759083207832, 0.7151261295180723, 0.7115449101091867, 0.7131421192582832, 0.7147393284073795, 0.7128876835466867, 0.712500882435994, 0.7159394413591867, 0.713355374623494, 0.7150952442582832, 0.7141083866716867, 0.7153187947100903, 0.7153393848832832, 0.7149319935993976, 0.7145863728350903, 0.7150849491716867, 0.715308499623494, 0.7149731739457832, 0.7145657826618976, 0.715186429310994, 0.7148099232868976, 0.715796780873494, 0.7172719197100903, 0.7155423451618976, 0.7158879659262049, 0.7157864857868976, 0.7152982045368976, 0.7166615681475903, 0.715552640248494, 0.715918851185994, 0.7157761907003012, 0.7147790380271084, 0.717261624623494, 0.7160306264118976, 0.7145348974021084, 0.7146672628012049, 0.7160203313253012, 0.7154202748493976, 0.7145451924887049, 0.7149114034262049, 0.7143010518637049, 0.7161424016378012, 0.715430569935994, 0.7172513295368976, 0.716895413685994, 0.7154099797628012, 0.7162541768637049, 0.7162438817771084, 0.7147790380271084, 0.7164983174887049, 0.7161526967243976, 0.7160100362387049, 0.715796780873494, 0.7174954701618976, 0.7141686864646084, 0.7172410344503012, 0.7181470020707832, 0.7158982610128012, 0.7155320500753012, 0.7165189076618976, 0.7154202748493976, 0.7176175404743976, 0.7149216985128012, 0.7149216985128012, 0.717017483998494, 0.717261624623494, 0.7160203313253012, 0.7168851185993976, 0.7158982610128012, 0.7156644154743976, 0.7156541203878012, 0.7162644719503012, 0.716529202748494, 0.715918851185994, 0.716895413685994, 0.7157864857868976, 0.7158982610128012, 0.7165189076618976, 0.7161526967243976, 0.7157761907003012, 0.7163968373493976, 0.716773343373494, 0.7151658391378012, 0.717994046498494, 0.716773343373494, 0.7162644719503012, 0.716407132435994, 0.7151555440512049, 0.7158776708396084, 0.717383694935994, 0.7161424016378012, 0.716773343373494, 0.7168851185993976, 0.7163865422628012, 0.7158776708396084, 0.7161424016378012, 0.7173631047628012, 0.7157761907003012, 0.7159085560993976, 0.7162747670368976, 0.7157658956137049, 0.717139554310994, 0.7162644719503012, 0.7162747670368976, 0.7163762471762049, 0.7162644719503012, 0.7176175404743976], "accuracy_test": 0.7101921237244898, "start": "2016-01-25 16:51:12.524000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0], "accuracy_train_last": 0.7659014944398301, "batch_size_eval": 1024, "accuracy_train_std": [0.01739256159262034, 0.018970791524827956, 0.016680766930465957, 0.019374141716582433, 0.01873685950394883, 0.017445286427906975, 0.015430411829764921, 0.016406549293428933, 0.013938943224140363, 0.013114310410083018, 0.01468120415988191, 0.014784538570346423, 0.016214359529187115, 0.014874246135582699, 0.01456570319818181, 0.015344004495953202, 0.014685026156151384, 0.015238741635690922, 0.015526173046315548, 0.01699978916719441, 0.015714180076428403, 0.014513722153550755, 0.01525055112954097, 0.015955068823707143, 0.015519221384556905, 0.015444299963230287, 0.016147260097305903, 0.015454420266370419, 0.015067153477000337, 0.015563520523743563, 0.015602222362035675, 0.015210181744812293, 0.015108506654851372, 0.015077167478366833, 0.015305925019769958, 0.015912550210789207, 0.015807759150567396, 0.01581167522692041, 0.015914812142211545, 0.016335717825926906, 0.015367316406830622, 0.015559714056014862, 0.015859031433678384, 0.01576973790262325, 0.016473381922066998, 0.01564645587843781, 0.01565089001970558, 0.01582025569584812, 0.014864667989178166, 0.01573799963298077, 0.015768879742707693, 0.015994352318498245, 0.015554064740767653, 0.01530070354250432, 0.015272581668279604, 0.015018319311732965, 0.014962820167165929, 0.015219448070581893, 0.015559775762176722, 0.01532227740766083, 0.015132629312273105, 0.015432848230548087, 0.015282555396744644, 0.015378739696400625, 0.014934437027427293, 0.015304910723451946, 0.015075722256207768, 0.014933970845872812, 0.01533392177521131, 0.015555421849198938, 0.015587018218178464, 0.01553932671104598, 0.015129282862923197, 0.015367553982517533, 0.01542372853295102, 0.015256285861338703, 0.015209964472923642, 0.015747288307864572, 0.01522793237823577, 0.015432918769661155, 0.015367005066511839, 0.015511135511542333, 0.015055719139649884, 0.015156653713374659, 0.01544418088052658, 0.015631389417329285, 0.0151078784251889, 0.014967025554801784, 0.015560742058543232, 0.0146560805403393, 0.014925772323521751, 0.01538942643286628, 0.015204656659320479, 0.015317244986514642, 0.015280216436230926, 0.01558473212680453, 0.015235242386850608, 0.015283032334669436, 0.014736617905523391, 0.01554333791192994, 0.014981158990109184, 0.014936570243493968, 0.01555900027331206, 0.015205737029901127, 0.01537171680328625, 0.015495062266155487, 0.015503312906569634, 0.01527144315881019, 0.015622793234624609, 0.015065104888423851, 0.016051485108704112, 0.015818763122690094, 0.015386553114146941, 0.01499030213823051, 0.015692339581031868, 0.015316305930001791, 0.015207464151702682, 0.014928937351508837, 0.015767904534543235, 0.01534966487491373, 0.015225111721763756, 0.015287741581033997, 0.01556869590257773, 0.015452539650185577, 0.015229573872130623, 0.015185339004278169, 0.015608252234794652, 0.015031240042020982, 0.01537146952641471, 0.015171677487383406, 0.015567238634774671, 0.0151773123093068, 0.015552065177712345, 0.015435273096659466, 0.015457455774174813, 0.01504478548956131, 0.015192430616292135, 0.015196484616866342, 0.015425567427749112, 0.015273225066589922, 0.015161911648782153, 0.015265326057202658, 0.015166117025026505, 0.015221591978936957, 0.01521224155492582, 0.01533884701925394, 0.015576692634850686, 0.014681598036267927, 0.015400373853774263], "accuracy_test_std": 0.013010518534669951, "error_valid": [0.6450062947100903, 0.5607233621987951, 0.4895416627447289, 0.4583019578313253, 0.4216176228350903, 0.40695889024849397, 0.3874570547816265, 0.3777826148343373, 0.3707334219691265, 0.35958384318524095, 0.3532964867281627, 0.34891225056475905, 0.3407232445406627, 0.33560658650225905, 0.32945159544427716, 0.32576889589608427, 0.3240907967808735, 0.32090667356927716, 0.3194624199924698, 0.3159223809299698, 0.30901437782379515, 0.31147637424698793, 0.31013360080948793, 0.30695977268448793, 0.30390801487198793, 0.3004797510353916, 0.3024328760353916, 0.29946200818900603, 0.29778390907379515, 0.29555575818900603, 0.2973059229103916, 0.29199512895331325, 0.2947218561746988, 0.2931040568524097, 0.2908862010542168, 0.29213778943900603, 0.2884447948042168, 0.28976697806852414, 0.2888110057417168, 0.2890757365399097, 0.28835360975150603, 0.2869799510542168, 0.2872240916792168, 0.2848738704819277, 0.28845508989081325, 0.2868578807417168, 0.2852606715926205, 0.28711231645331325, 0.28749911756400603, 0.28406055864081325, 0.28664462537650603, 0.2849047557417168, 0.28589161332831325, 0.2846812052899097, 0.2846606151167168, 0.28506800640060237, 0.2854136271649097, 0.28491505082831325, 0.28469150037650603, 0.2850268260542168, 0.28543421733810237, 0.28481357068900603, 0.28519007671310237, 0.28420321912650603, 0.2827280802899097, 0.28445765483810237, 0.28411203407379515, 0.28421351421310237, 0.28470179546310237, 0.2833384318524097, 0.28444735975150603, 0.28408114881400603, 0.2842238092996988, 0.2852209619728916, 0.28273837537650603, 0.28396937358810237, 0.2854651025978916, 0.28533273719879515, 0.2839796686746988, 0.28457972515060237, 0.28545480751129515, 0.28508859657379515, 0.28569894813629515, 0.2838575983621988, 0.28456943006400603, 0.28274867046310237, 0.28310458631400603, 0.2845900202371988, 0.28374582313629515, 0.2837561182228916, 0.2852209619728916, 0.28350168251129515, 0.28384730327560237, 0.28398996376129515, 0.28420321912650603, 0.28250452983810237, 0.2858313135353916, 0.2827589655496988, 0.2818529979292168, 0.2841017389871988, 0.2844679499246988, 0.28348109233810237, 0.28457972515060237, 0.28238245952560237, 0.2850783014871988, 0.2850783014871988, 0.28298251600150603, 0.28273837537650603, 0.2839796686746988, 0.28311488140060237, 0.2841017389871988, 0.28433558452560237, 0.2843458796121988, 0.2837355280496988, 0.28347079725150603, 0.28408114881400603, 0.28310458631400603, 0.28421351421310237, 0.2841017389871988, 0.28348109233810237, 0.28384730327560237, 0.2842238092996988, 0.28360316265060237, 0.28322665662650603, 0.2848341608621988, 0.28200595350150603, 0.28322665662650603, 0.2837355280496988, 0.28359286756400603, 0.28484445594879515, 0.2841223291603916, 0.28261630506400603, 0.2838575983621988, 0.28322665662650603, 0.28311488140060237, 0.2836134577371988, 0.2841223291603916, 0.2838575983621988, 0.2826368952371988, 0.2842238092996988, 0.28409144390060237, 0.28372523296310237, 0.28423410438629515, 0.28286044568900603, 0.2837355280496988, 0.28372523296310237, 0.28362375282379515, 0.2837355280496988, 0.28238245952560237], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.9471631670208764, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0007205655689448293, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "l2_decay": 1.753442322485463e-05, "optimization": "nesterov_momentum", "nb_data_augmentation": 2, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.07519391488965048}, "accuracy_valid_max": 0.7181470020707832, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7176175404743976, "loss_train": [2.0480995178222656, 1.703751564025879, 1.556395411491394, 1.435156226158142, 1.3379179239273071, 1.2691177129745483, 1.2182174921035767, 1.1785961389541626, 1.145455002784729, 1.1158610582351685, 1.091224193572998, 1.0705488920211792, 1.050008773803711, 1.0311464071273804, 1.0145515203475952, 1.0012736320495605, 0.9858712553977966, 0.9766794443130493, 0.9672413468360901, 0.9552394151687622, 0.9463653564453125, 0.939821720123291, 0.9314209222793579, 0.9262036085128784, 0.9160854816436768, 0.9129745364189148, 0.9084968566894531, 0.9012893438339233, 0.898230254650116, 0.8921844959259033, 0.8888020515441895, 0.8858491778373718, 0.8821414709091187, 0.8793118596076965, 0.876991331577301, 0.8737624287605286, 0.8710437417030334, 0.868587076663971, 0.8656485080718994, 0.8654221892356873, 0.863623321056366, 0.859977662563324, 0.8600696325302124, 0.8582497239112854, 0.8575618267059326, 0.8558085560798645, 0.8578061461448669, 0.85417240858078, 0.8526839017868042, 0.8527728319168091, 0.8519009947776794, 0.8490914702415466, 0.8513705134391785, 0.8491724729537964, 0.8489173650741577, 0.8463928699493408, 0.8451446294784546, 0.846418023109436, 0.8474783897399902, 0.8465739488601685, 0.8464546203613281, 0.8462182283401489, 0.8450892567634583, 0.844711422920227, 0.8452696204185486, 0.8458024263381958, 0.8419663310050964, 0.8442379236221313, 0.844254732131958, 0.8430333733558655, 0.8416547775268555, 0.8415940403938293, 0.8423610329627991, 0.8413653373718262, 0.839843213558197, 0.8419786095619202, 0.8418875336647034, 0.8413962721824646, 0.8403720259666443, 0.8430049419403076, 0.8434080481529236, 0.8404622673988342, 0.8388609290122986, 0.8431538939476013, 0.8408212065696716, 0.8425518870353699, 0.8427491784095764, 0.8417086005210876, 0.8419391512870789, 0.8401565551757812, 0.8386856317520142, 0.8411305546760559, 0.8423264622688293, 0.8389211297035217, 0.8398283123970032, 0.8408623337745667, 0.8399178385734558, 0.8409730195999146, 0.838711142539978, 0.8419453501701355, 0.8415789008140564, 0.8415107131004333, 0.8414714932441711, 0.8390089273452759, 0.8403828740119934, 0.840693473815918, 0.8412896990776062, 0.8412522673606873, 0.8404543995857239, 0.8401932716369629, 0.8387128114700317, 0.8396264314651489, 0.8400640487670898, 0.8393495678901672, 0.8389567732810974, 0.8396120071411133, 0.8403692841529846, 0.8410976529121399, 0.8403384685516357, 0.8370251059532166, 0.8386378288269043, 0.8417301774024963, 0.8426393270492554, 0.8411321640014648, 0.8405508399009705, 0.841545581817627, 0.8421878814697266, 0.8407586216926575, 0.8393499851226807, 0.840162992477417, 0.8400813937187195, 0.839599072933197, 0.8405875563621521, 0.8403204679489136, 0.8418996930122375, 0.84041428565979, 0.8388513922691345, 0.8392531275749207, 0.8403588533401489, 0.8383469581604004, 0.8404610753059387, 0.8411509394645691, 0.8420971632003784, 0.839888334274292, 0.841516375541687, 0.8425872325897217, 0.8410652875900269, 0.8409908413887024, 0.8401219248771667], "accuracy_train_first": 0.36842253253045404, "model": "residualv5", "loss_std": [0.1911327838897705, 0.10571162402629852, 0.11237391084432602, 0.11862390488386154, 0.12271489948034286, 0.12208126485347748, 0.12614697217941284, 0.1238129585981369, 0.12455674260854721, 0.11812946200370789, 0.12155862152576447, 0.11862290650606155, 0.12142027169466019, 0.11990167200565338, 0.11649060249328613, 0.11755085736513138, 0.11800889670848846, 0.11730017513036728, 0.11580073833465576, 0.11684525012969971, 0.11473973095417023, 0.1147276982665062, 0.11451895534992218, 0.1151510551571846, 0.1115085780620575, 0.11433479189872742, 0.11488964408636093, 0.11497171223163605, 0.11310768127441406, 0.11374829709529877, 0.11321850121021271, 0.11328290402889252, 0.11453980952501297, 0.10972447693347931, 0.11496488749980927, 0.11134220659732819, 0.11257049441337585, 0.11140869557857513, 0.11130683124065399, 0.11087443679571152, 0.11347232758998871, 0.11261953413486481, 0.11124728620052338, 0.10987493395805359, 0.1102173700928688, 0.11103668063879013, 0.11014862358570099, 0.1118583008646965, 0.11202928423881531, 0.11040540039539337, 0.11243317276239395, 0.10922844707965851, 0.1129433661699295, 0.1128711998462677, 0.11182411760091782, 0.11057240515947342, 0.10899239778518677, 0.11041434854269028, 0.11164059489965439, 0.1104777380824089, 0.11142038553953171, 0.11094018816947937, 0.1097409799695015, 0.10999846458435059, 0.11105958372354507, 0.11151950061321259, 0.1094028428196907, 0.1116735115647316, 0.11200679838657379, 0.10918478667736053, 0.10936495661735535, 0.11024713516235352, 0.11049120873212814, 0.11076053977012634, 0.10911870002746582, 0.11061059683561325, 0.11257477849721909, 0.11130266636610031, 0.10917972773313522, 0.11139082908630371, 0.11063718050718307, 0.10994117707014084, 0.10949324816465378, 0.10973656922578812, 0.10871132463216782, 0.11066669225692749, 0.1120142787694931, 0.11125952750444412, 0.11089298129081726, 0.10864278674125671, 0.11099914461374283, 0.11047640442848206, 0.11200687289237976, 0.11103764921426773, 0.11172644793987274, 0.10908037424087524, 0.11019447445869446, 0.11025918275117874, 0.1109212189912796, 0.1125110536813736, 0.11098497360944748, 0.11138682067394257, 0.11020364612340927, 0.11092016100883484, 0.1114916205406189, 0.11186812072992325, 0.11064427345991135, 0.11088329553604126, 0.11155696213245392, 0.11001318693161011, 0.10870196670293808, 0.11084606498479843, 0.11150012910366058, 0.11007767915725708, 0.11234553158283234, 0.11229448765516281, 0.10901729762554169, 0.11055106669664383, 0.11136900633573532, 0.1102171391248703, 0.11027990281581879, 0.11004640907049179, 0.11314070224761963, 0.11051291227340698, 0.1110851839184761, 0.1123640388250351, 0.11169541627168655, 0.1130901500582695, 0.10910134762525558, 0.11032377928495407, 0.11102757602930069, 0.11246198415756226, 0.11234239488840103, 0.11440505087375641, 0.11042670905590057, 0.11204364150762558, 0.11100122332572937, 0.11153049021959305, 0.1100335419178009, 0.10929231345653534, 0.11139998584985733, 0.11091317236423492, 0.11086081713438034, 0.11082214117050171, 0.112739197909832, 0.11246323585510254, 0.11130017787218094, 0.1104317381978035, 0.11052818596363068]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:16 2016", "state": "available"}], "summary": "3902de281c56a2c0d591e4a9f2aef042"}