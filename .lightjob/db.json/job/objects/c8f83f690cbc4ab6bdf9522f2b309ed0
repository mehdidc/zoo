{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 32, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.013353978727163693, 0.008969059720194964, 0.01290901944764346, 0.01660744482052395, 0.01729206502208662, 0.017203952665647507, 0.01980153813018794, 0.022135249757627807, 0.020429605118185926, 0.018003566712752543, 0.018926096710713863, 0.01954988921126506, 0.019377436652275655, 0.018135756717549495, 0.018955876260524255, 0.01907007356485218, 0.017188306867131242, 0.017519163022112073, 0.016875254745091772, 0.015297194102523236, 0.015936692459989316, 0.015403034795343032, 0.016267840380509973, 0.014685628540270177, 0.014135739589237426, 0.014572808459325773, 0.014295706330083317, 0.014818784894456058, 0.015443386179733733, 0.015278615256981692, 0.015412199010909188, 0.014485867011958274, 0.015055649341808672, 0.015891289046796278, 0.014852466995398956, 0.014827901745926478, 0.014154585089979835, 0.015718515457310008, 0.015676807546835397, 0.014332054840909063, 0.01543883075040168, 0.015397241279351586, 0.01565713771063186, 0.015288870678355037, 0.015437027160518839, 0.01604289124350038, 0.01607520113541664, 0.01631083762890219, 0.016770981038145498, 0.016529864276973526, 0.01622071277145682, 0.01637125390897884, 0.01582710914203188, 0.015120422737690738, 0.01573492117639105, 0.015251601860105459, 0.015141011999930998, 0.013740224571411514, 0.013224499886109468, 0.014546609860010338, 0.014203165367443872, 0.013779087189273054, 0.013842088009593453, 0.014432349115873001, 0.014016576279236364, 0.014150991564527444, 0.014604972995167623, 0.01339280453110432, 0.014112262585205749, 0.014161948794691324, 0.01424372700598885, 0.013364093516307182, 0.01361821148486557, 0.013985262814764265, 0.013781849127171624, 0.014055244893946198, 0.013760729685644343, 0.013992419441776772, 0.013446633604837053, 0.012902305913580854, 0.012647878139584984, 0.012648080625342622, 0.01230979260689472, 0.012991060011941025, 0.012871215898489654, 0.013575554891737965, 0.012245955055771094, 0.012835117648891529, 0.01247091765254108, 0.011827941024658744, 0.011735387874299247, 0.012821311489134932, 0.011981882801645437, 0.011709370733308173, 0.012413116908525495, 0.01230099308066145, 0.012035225642708676, 0.011644126173954291, 0.011697106103733272, 0.012056865985015945, 0.011274663836387665, 0.011935476558340985, 0.012305938271309524, 0.011046890593511167, 0.010633284564382044, 0.011349067681632043, 0.010140360601792532, 0.011080132429054326, 0.010622564552314504, 0.010930725075746096, 0.010899511096633082, 0.010784933839928356, 0.009953180435511457, 0.009924834749810396, 0.010210573383529075, 0.01002199968993195, 0.01118341545784922, 0.012175676108936804, 0.010793349304332537, 0.011139499138990485, 0.011169564577608005, 0.010749413759430441, 0.011200695412521923, 0.010752010539563928, 0.011314846827594906, 0.01147195744377445, 0.011611345039678042, 0.01030024366837527, 0.010960642483259066, 0.010955014610617068, 0.011184366340776035, 0.0112755873613221, 0.011606568416314765, 0.01191070895352759, 0.011209483681095042, 0.011454878793042411, 0.011843739028244085, 0.012443015669449452, 0.012135304845584066, 0.01259685194343434, 0.01253845384172946, 0.012581484002419643, 0.013753641774984674, 0.012816297567730651, 0.012972037138772971, 0.012698096556990232, 0.014432629003243175, 0.012712121275179115, 0.013178075893683716, 0.012602585235116159, 0.01268973087739134, 0.013143023586302635, 0.013623009336709718, 0.012803079250234594, 0.012986719912994008, 0.013152090615913842, 0.013217830728430722, 0.01309759428653215, 0.013615258120303493, 0.013361997661617808, 0.01318918801844195, 0.01317331360546567, 0.013137066530681068, 0.014080734783099279, 0.012963508939658698, 0.013254507103618036, 0.013425319379303509, 0.013377413319650228, 0.012183497404265959, 0.013219562392205137, 0.013659596687164864, 0.013031764247524367, 0.013393829983016156, 0.013857565552100534, 0.01314230642328571, 0.014263508307204393, 0.014171276049092105, 0.013445970922240097, 0.014213141369231689, 0.014514935409698126, 0.014179552646870035, 0.014480118824092605, 0.014020014953959752, 0.013905734693459947, 0.014113235762197564, 0.014666043670214861, 0.013939983202332126, 0.013544325110753722, 0.014243248034924485, 0.013877664431923096, 0.013786184611087484, 0.01452498156795372, 0.014827420930411814, 0.014331986095311786, 0.014406640176349622, 0.015085505283896378, 0.015282677220353645, 0.015043794243223023, 0.015186657582554825, 0.014606929005519489, 0.015217003114915579, 0.015402098946272378, 0.015410518348880667, 0.01537288376920159, 0.015610197123437428, 0.015395029299928963, 0.015169916309835982, 0.015479971948158965, 0.01583233987440704, 0.015265506146914342, 0.0156875975574511, 0.015860352201265263, 0.015735886442061094, 0.015703777040240178, 0.01559003714696489, 0.01459862741251277, 0.01537288376920159, 0.015154326291189986, 0.015175984750810056, 0.014917950946460805, 0.015464562533663385, 0.014897086354189643, 0.01522947018666273, 0.01488595264417446, 0.014916699292135472, 0.015332957265735098, 0.015720078431449288, 0.015699690936182287, 0.01549860403704062, 0.015414486196517032, 0.015179972508335046, 0.015137351998042227, 0.015481409996098785, 0.015714869794237793, 0.0157864118352262, 0.01583130944205697, 0.01560532266253884, 0.015493320943422753, 0.015451787075284109, 0.015582388766317044, 0.015535915206828926, 0.015344665997774681, 0.015287469417290051, 0.014959153191517423, 0.01566783813985179, 0.01516309047630018, 0.015380758893299037, 0.014927243211352846, 0.015049569172205702, 0.01501987747359516, 0.01477472967224612, 0.014928115153627172, 0.01450044597396805, 0.015071169711199638, 0.014775610614692835], "moving_avg_accuracy_train": [0.022019663909653008, 0.04647549257105943, 0.07152642017926356, 0.09553441313965022, 0.11809019542064644, 0.1390483805378195, 0.15854286318009697, 0.1765435185783571, 0.19312528864627812, 0.2083045399299984, 0.2221566003853651, 0.23499308135827193, 0.24672945679456545, 0.25749902478482095, 0.2674215373176438, 0.2765239317067266, 0.28492044741040795, 0.2925562945056075, 0.29959346221266947, 0.30605715753117735, 0.3119674171725779, 0.31741460613199973, 0.3223727355716329, 0.32697238004233964, 0.3312538941433567, 0.33520262398428124, 0.33888203887682766, 0.3422957827789197, 0.34545876099673645, 0.34835888376657187, 0.35106657841178623, 0.35356871590560207, 0.35589497231430356, 0.358046731802373, 0.36002058982022545, 0.3618969713374646, 0.3636298925303608, 0.3652499754730151, 0.3667475776511658, 0.36813026079482547, 0.36944675523721443, 0.3706804283603645, 0.3718279726009708, 0.37292121628656405, 0.37394466313335983, 0.3749401340085621, 0.37584539444030096, 0.37672061874673235, 0.37752223546655894, 0.3782925907370404, 0.3790022225709591, 0.3796687569583815, 0.3803337420737283, 0.3810020191906449, 0.381619672539899, 0.382231400174475, 0.38284011981465027, 0.38342284472295085, 0.3839658622820788, 0.38444763868768417, 0.38492080103130966, 0.3853722237774774, 0.3858086590859146, 0.3862223051051563, 0.386636475249864, 0.3870417444146155, 0.387394896967663, 0.3877499726951769, 0.3881020208356353, 0.3884281287084671, 0.388723986991644, 0.3890205224298458, 0.3893361603515899, 0.3896063196371212, 0.38988434022624224, 0.39015312389810863, 0.3903974624980541, 0.3906009470010634, 0.3908120569371235, 0.39106261789508157, 0.39132525304055865, 0.3916034413012407, 0.39185134139177, 0.3920513081316074, 0.39228228727599557, 0.3924949269500202, 0.39269789235187125, 0.39290849904807024, 0.3931073456698874, 0.3932885606806949, 0.39349586806662135, 0.393717321946098, 0.39391891953761776, 0.3941236088580808, 0.3943519710250597, 0.39454823242892134, 0.39472254254358735, 0.3949329000694058, 0.39512225789146116, 0.39531357022177793, 0.3954996661631014, 0.39566944161028333, 0.39587106763774704, 0.39606186770652124, 0.3962662119493888, 0.39650352809295125, 0.3967055229269285, 0.39691282281677537, 0.39715054599144706, 0.39733426991412774, 0.397513644435035, 0.39765876941336614, 0.39778469554742635, 0.3979213166049945, 0.39805822644966304, 0.3981675304658263, 0.39830310646132566, 0.39846926663583726, 0.3986117992976504, 0.39871918840281523, 0.39885543822486297, 0.39897337671826816, 0.3990981586016278, 0.3991918611061752, 0.399306384245973, 0.39944672955038085, 0.3995450664409962, 0.3996568571794639, 0.3997597939928944, 0.39988963950593426, 0.3999902244260034, 0.400092412646932, 0.4002099226338538, 0.40029944162923553, 0.4003985738667365, 0.40051108041740136, 0.4006262511570381, 0.40077172145246387, 0.4009096201647756, 0.4010337290058562, 0.4011570527068763, 0.4012959458235087, 0.40144423716539185, 0.40158231362188695, 0.40170658243273255, 0.4018114128672463, 0.4019243974976036, 0.4020284448625535, 0.40211042569814204, 0.40222369993111484, 0.4023302609895907, 0.40245177862794246, 0.40256346965126855, 0.40266860582106234, 0.4027260620417431, 0.4028126498724987, 0.40291154130828316, 0.40301674754451833, 0.4030975183130916, 0.4031701759559888, 0.403256494173882, 0.40328767759379536, 0.40332740446458376, 0.4034072643780368, 0.4034861137465731, 0.40358032966635105, 0.4036814360846366, 0.40377235976345605, 0.40387976771129835, 0.4039857354595945, 0.40409974367235596, 0.40416747383169843, 0.40424234581914503, 0.4043236815007041, 0.40440153391172634, 0.40449950286736064, 0.40458306067863126, 0.40467682785043224, 0.40475424285862455, 0.4048332890588732, 0.4049322963759926, 0.4050121384149806, 0.4051025613917273, 0.4051490648386565, 0.4052257951730356, 0.40528326277874793, 0.40534660936793665, 0.4054129218934446, 0.4055028301009255, 0.40559072293408693, 0.4056814161791611, 0.4057932670342516, 0.4058916076550235, 0.4059568987744418, 0.40604817681643285, 0.4061419888470912, 0.40623335907229347, 0.4063202065237758, 0.40636349199796706, 0.4064373982545196, 0.4065132144806549, 0.4066069896722627, 0.40669836279113836, 0.40681082553265024, 0.4068934408095347, 0.4070073220884926, 0.4071237661324119, 0.40721694002789166, 0.4072984713850139, 0.4073718496064239, 0.4074379260545117, 0.40749972000660023, 0.407564635158718, 0.4076370096884811, 0.4077114113116872, 0.4077853842678201, 0.40786591082119683, 0.40794074591686413, 0.40803832443748855, 0.40810289361795526, 0.4081796070708515, 0.40823934858322, 0.40830939198601834, 0.4083468544116321, 0.4083968466363511, 0.40844648993621735, 0.40850279465014455, 0.40856509463672663, 0.40860256343417434, 0.4086827883280677, 0.408729414095667, 0.4087690521376968, 0.4088279778636189, 0.4088949619098059, 0.40892502061685043, 0.40900555187580956, 0.40905248942078676, 0.4091063229064951, 0.4091943005733945, 0.40924096443908936, 0.40932477854796745, 0.40939323579952913, 0.4094478718795061, 0.40953424673243777, 0.4096027195536569, 0.4096922108296497, 0.4097634884316238, 0.40984620341505795, 0.4099299474953868, 0.4099890411260161, 0.4100608265840586, 0.41011845804986835, 0.4101726515179066, 0.4102632422688937, 0.4103331842495533], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 610106073, "moving_var_accuracy_train": [0.004363790388246677, 0.009310199349066621, 0.014027120180443314, 0.017811861696272767, 0.020609545355414816, 0.022501800530524592, 0.023671934158882656, 0.0242209530958966, 0.024273453673575557, 0.023919795332026917, 0.023254732008556434, 0.02241223600161098, 0.021410694976884585, 0.020313477831466753, 0.01916823634299631, 0.017997094961226333, 0.016831898748763206, 0.015673464321638287, 0.014551813453510304, 0.01347264632269378, 0.012439762211683313, 0.011462832798551802, 0.010537796946558037, 0.009674427815214366, 0.0088719673006678, 0.008125102776810495, 0.007434435344692876, 0.006795874637085225, 0.006206327054234153, 0.005661390757531797, 0.00516123617440412, 0.004701458785305339, 0.004280016126686029, 0.0038936851340678987, 0.0035393816599328704, 0.0032171307623237145, 0.0029224448288384425, 0.0026538223646243105, 0.0024086254387178947, 0.002184969208927952, 0.001982070706586727, 0.0017975611803011005, 0.0016296567823283268, 0.001477447739900299, 0.0013391299569442156, 0.0012141356216201769, 0.0011000975275016072, 0.0009969819330305604, 0.0009030670440170541, 0.0008181013648001752, 0.0007408234243775551, 0.0006707394947463481, 0.0006076453921044084, 0.0005509002016389166, 0.000499243642413628, 0.0004526871744624007, 0.00041075331341917633, 0.0003727340969460439, 0.000338114499877131, 0.0003063920264343999, 0.00027776776722178605, 0.00025182503296142543, 0.00022835681167133898, 0.0002070610577673155, 0.0001878987841694887, 0.00017058709361562533, 0.0001546508347855785, 0.00014032046025744624, 0.00012740385527050305, 0.00011562058684595881, 0.00010484631727488247, 9.515308494237995e-05, 8.653442212692838e-05, 7.853785427026458e-05, 7.137972787501494e-05, 6.489195704787111e-05, 5.894007350589399e-05, 5.341871964198899e-05, 4.847795432371996e-05, 4.419518603422349e-05, 4.0396462407559045e-05, 3.7053314542234855e-05, 3.390107318197131e-05, 3.087084613714495e-05, 2.82639238097111e-05, 2.5844472107463805e-05, 2.36307794858545e-05, 2.1666898161623755e-05, 1.9856068156534355e-05, 1.816601126215855e-05, 1.6736197306279605e-05, 1.5503953962269006e-05, 1.431933286620123e-05, 1.3264479040785642e-05, 1.2407374650472892e-05, 1.151330403323747e-05, 1.0635429774587602e-05, 9.970139395144983e-06, 9.295832918592421e-06, 8.695653296314326e-06, 8.137773261076637e-06, 7.583409257161373e-06, 7.190945826002485e-06, 6.79949323960043e-06, 6.495353041978667e-06, 6.3526883057388325e-06, 6.084636691746603e-06, 5.862932221546527e-06, 5.785249769375676e-06, 5.51051511032475e-06, 5.2490405680486754e-06, 4.913687845264303e-06, 4.565035581892026e-06, 4.276519844042274e-06, 4.017566609742442e-06, 3.7233362603129577e-06, 3.5164302892823305e-06, 3.4132700926975853e-06, 3.2547831205795687e-06, 3.0330965876945355e-06, 2.896863054997465e-06, 2.7323621435378548e-06, 2.599260594916981e-06, 2.4183559696514575e-06, 2.2945603186285394e-06, 2.24237552698972e-06, 2.105169270793698e-06, 2.007126866578811e-06, 1.901778067953959e-06, 1.8633389764677453e-06, 1.768061014128884e-06, 1.6852368051850318e-06, 1.6409904979037454e-06, 1.549014302920696e-06, 1.4825576772361353e-06, 1.448221424995039e-06, 1.4227779759118322e-06, 1.4709546399819501e-06, 1.49500366969891e-06, 1.484130342638293e-06, 1.4725959254741998e-06, 1.4989580135575603e-06, 1.5469751108993822e-06, 1.5638635703537368e-06, 1.546461849459248e-06, 1.490720444516403e-06, 1.4565381403375772e-06, 1.408317013680901e-06, 1.3279730289469013e-06, 1.3106551927524053e-06, 1.281787006128543e-06, 1.2865071333909741e-06, 1.2701303822765592e-06, 1.24259987183907e-06, 1.1480508403094237e-06, 1.1007228281930559e-06, 1.0786661900173471e-06, 1.0704147403005623e-06, 1.0220885197737628e-06, 9.673918654387718e-07, 9.377101915571906e-07, 8.526908234989279e-07, 7.816257595127984e-07, 7.608616355520523e-07, 7.407304782640045e-07, 7.465471862939749e-07, 7.638950380313432e-07, 7.619095725588065e-07, 7.895468206401451e-07, 8.116546116867466e-07, 8.474700037116854e-07, 8.040093737015335e-07, 7.740607668692473e-07, 7.56194128034481e-07, 7.3512369634882e-07, 7.479925731266337e-07, 7.360304862329436e-07, 7.415579801775982e-07, 7.213399336005461e-07, 7.054406562042535e-07, 7.231186301723251e-07, 7.081795278630067e-07, 7.109484075902818e-07, 6.593167020178892e-07, 6.463729297414472e-07, 6.114583681240601e-07, 5.864276445682313e-07, 5.673610394645643e-07, 5.833763074700344e-07, 5.945650278132999e-07, 6.091359073506732e-07, 6.608178406759715e-07, 6.817739558527064e-07, 6.519629327414737e-07, 6.617517680147856e-07, 6.747828650794222e-07, 6.824412410531387e-07, 6.820794354086502e-07, 6.30734182351448e-07, 6.168199769347416e-07, 6.068708805499192e-07, 6.25327871544679e-07, 6.379365060677247e-07, 6.87973669516219e-07, 6.806038583368902e-07, 7.292639837770697e-07, 7.783705236778571e-07, 7.786658444999004e-07, 7.606255197976647e-07, 7.330222382135796e-07, 6.99014887319285e-07, 6.63479831219817e-07, 6.350576408680776e-07, 6.186945298071725e-07, 6.066454906478555e-07, 5.952289257343932e-07, 5.940667653495623e-07, 5.850627127064468e-07, 6.12250550621034e-07, 5.885481071542358e-07, 5.826578811361906e-07, 5.565135277232844e-07, 5.450168794310986e-07, 5.031460914837797e-07, 4.753244851266304e-07, 4.499721516084023e-07, 4.33506923741355e-07, 4.2508782632036094e-07, 3.952142407279026e-07, 4.1361711905696244e-07, 3.918210669892733e-07, 3.6677952967397495e-07, 3.6135174728558237e-07, 3.6559833454929487e-07, 3.371702339170577e-07, 3.618207635512038e-07, 3.454668853524479e-07, 3.370025944687842e-07, 3.729629638794673e-07, 3.552643147458245e-07, 3.8296112689446837e-07, 3.8684257182745087e-07, 3.7502422576193934e-07, 4.046673401563992e-07, 4.063973513521126e-07, 4.3783581252619006e-07, 4.3977670016219805e-07, 4.573749465065854e-07, 4.7475509076702637e-07, 4.5870809631887295e-07, 4.5921565456438765e-07, 4.431865617703419e-07, 4.2530029339544515e-07, 4.5663042153559256e-07, 4.5499430530925107e-07], "duration": 127616.653232, "accuracy_train": [0.22019663909653012, 0.26657795052371724, 0.29698476865310075, 0.3116063497831303, 0.3210922359496124, 0.32767204659237725, 0.3339932069605943, 0.33854941716269843, 0.3423612192575674, 0.34491780148348095, 0.3468251444836656, 0.35052141011443333, 0.3523568357212071, 0.3544251366971207, 0.3567241501130491, 0.35844548120847175, 0.36048908874354, 0.3612789183624031, 0.36292797157622736, 0.36423041539774825, 0.3651597539451827, 0.36643930676679587, 0.3669959005283315, 0.36836918027870064, 0.36978752105251017, 0.37074119255260246, 0.3719967729097453, 0.37301947789774825, 0.37392556495708745, 0.37445998869509045, 0.3754358302187154, 0.3760879533499446, 0.37683127999261723, 0.37741256719499816, 0.377785311980897, 0.37878440499261723, 0.3792261832664267, 0.37983072195690293, 0.380225997254522, 0.380574409087763, 0.3812952052187154, 0.3817834864687154, 0.3821558707664267, 0.38276040945690293, 0.383155684754522, 0.38389937188538203, 0.38399273832595054, 0.3845976375046143, 0.38473678594499816, 0.3852257881713732, 0.3853889090762274, 0.38566756644518274, 0.3863186081118494, 0.3870165132428941, 0.3871785526831857, 0.38773694888565896, 0.3883185965762274, 0.388667368897656, 0.38885302031423036, 0.3887836263381322, 0.38917926212393866, 0.3894350284929863, 0.3897365768618494, 0.3899451192783315, 0.3903640065522333, 0.3906891668973791, 0.39057326994509045, 0.3909456542428018, 0.3912704540997601, 0.3913630995639535, 0.39138671154023624, 0.3916893413736619, 0.3921769016472868, 0.39203775320690293, 0.3923865255283315, 0.39257217694490587, 0.39259650989756373, 0.3924323075281469, 0.39271204636166485, 0.39331766651670363, 0.39368896934985237, 0.3941071356473791, 0.39408244220653377, 0.39385100879014395, 0.39436109957548915, 0.39440868401624213, 0.3945245809685308, 0.3948039593138612, 0.39489696526624213, 0.3949194957779623, 0.3953616345399594, 0.3957104068613879, 0.39573329786129563, 0.395965812742248, 0.3964072305278701, 0.3963145850636766, 0.3962913335755814, 0.39682611780177185, 0.3968264782899594, 0.397035381194629, 0.397174529635013, 0.39719742063492064, 0.39768570188492064, 0.39777906832548915, 0.39810531013519745, 0.398639373385013, 0.39852347643272423, 0.39877852182539686, 0.3992900545634921, 0.398987785218254, 0.3991280151232004, 0.3989648942183463, 0.39891803075396826, 0.3991509061231082, 0.3992904150516796, 0.39915126661129563, 0.39952329042081947, 0.39996470820644153, 0.39989459325396826, 0.3996856903492987, 0.4000816866232927, 0.4000348231589147, 0.40022119555186414, 0.4000351836471022, 0.4003370925041528, 0.40070983729005166, 0.40043009845653377, 0.4006629738256737, 0.4006862253137689, 0.4010582491232927, 0.40089548870662606, 0.40101210663528974, 0.40126751251614984, 0.40110511258767073, 0.40129076400424507, 0.401523639373385, 0.4016627878137689, 0.40208095411129563, 0.4021507085755814, 0.4021507085755814, 0.40226696601605755, 0.4025459838732004, 0.4027788592423403, 0.40282500173034325, 0.40282500173034325, 0.4027548867778701, 0.40294125917081947, 0.4029648711471022, 0.4028482532184385, 0.4032431680278701, 0.403289310515873, 0.4035454373731082, 0.40356868886120345, 0.4036148313492064, 0.4032431680278701, 0.4035919403492987, 0.40380156423034325, 0.40396360367063494, 0.40382445523025107, 0.40382409474206354, 0.40403335813492064, 0.4035683283730159, 0.4036849463016796, 0.4041260035991141, 0.4041957580633998, 0.4044282729443522, 0.4045913938492064, 0.4045906728728313, 0.4048464392418789, 0.4049394451942599, 0.40512581758720934, 0.40477704526578073, 0.40491619370616466, 0.40505570263473606, 0.4051022056109265, 0.40538122346806943, 0.4053350809800665, 0.4055207323966408, 0.4054509779323551, 0.40554470486111116, 0.4058233622300665, 0.405730716765873, 0.4059163681824474, 0.40556759586101887, 0.4059163681824474, 0.4058004712301588, 0.40591672867063494, 0.4060097346230159, 0.406312003968254, 0.4063817584325397, 0.40649765538482835, 0.4067999247300665, 0.4067766732419712, 0.4065445188492064, 0.4068696791943522, 0.4069862971230159, 0.4070556910991141, 0.40710183358711705, 0.40675306126568844, 0.4071025545634921, 0.407195560515873, 0.4074509663967331, 0.40752072086101887, 0.40782299020625695, 0.40763697830149503, 0.4080322535991141, 0.4081717625276855, 0.40805550508720934, 0.4080322535991141, 0.4080322535991141, 0.4080326140873016, 0.40805586557539686, 0.4081488715277778, 0.40828838045634924, 0.40838102592054265, 0.4084511408730159, 0.4085906498015873, 0.4086142617778701, 0.4089165311231082, 0.40868401624215583, 0.4088700281469177, 0.40877702219453677, 0.40893978261120345, 0.40868401624215583, 0.40884677665882246, 0.408893279635013, 0.40900953707548915, 0.4091257945159653, 0.40893978261120345, 0.4094048123731082, 0.4091490460040606, 0.4091257945159653, 0.4093583093969177, 0.40949781832548915, 0.40919554898025107, 0.40973033320644153, 0.4094749273255814, 0.4095908242778701, 0.40998609957548915, 0.40966093923034325, 0.4100791055278701, 0.4100093510635844, 0.4099395965992987, 0.41031162040882246, 0.410218974944629, 0.4104976323135844, 0.41040498684939086, 0.4105906382659653, 0.4106836442183463, 0.4105208838016796, 0.41070689570644153, 0.41063714124215583, 0.41066039273025107, 0.4110785590277778, 0.41096266207548915], "end": "2016-01-24 22:09:02.600000", "learning_rate_per_epoch": [0.0010148169239982963, 0.0005074084619991481, 0.0003382723079994321, 0.00025370423099957407, 0.00020296337606851012, 0.00016913615399971604, 0.00014497384836431593, 0.00012685211549978703, 0.0001127574359998107, 0.00010148168803425506, 9.225608664564788e-05, 8.456807699985802e-05, 7.80628397478722e-05, 7.248692418215796e-05, 6.765445868950337e-05, 6.342605774989352e-05, 5.969511403236538e-05, 5.637871799990535e-05, 5.341141513781622e-05, 5.074084401712753e-05, 4.832461490877904e-05, 4.612804332282394e-05, 4.412247290019877e-05, 4.228403849992901e-05, 4.0592676668893546e-05, 3.90314198739361e-05, 3.7585810787277296e-05, 3.624346209107898e-05, 3.499368540360592e-05, 3.3827229344751686e-05, 3.273602851550095e-05, 3.171302887494676e-05, 3.0752027669223025e-05, 2.984755701618269e-05, 2.8994769309065305e-05, 2.8189358999952674e-05, 2.742748438322451e-05, 2.670570756890811e-05, 2.602094718895387e-05, 2.5370422008563764e-05, 2.4751632736297324e-05, 2.416230745438952e-05, 2.3600392523803748e-05, 2.306402166141197e-05, 2.255148683616426e-05, 2.2061236450099386e-05, 2.1591849872493185e-05, 2.1142019249964505e-05, 2.0710549506475218e-05, 2.0296338334446773e-05, 1.9898370737791993e-05, 1.951570993696805e-05, 1.9147488274029456e-05, 1.8792905393638648e-05, 1.8451217329129577e-05, 1.812173104553949e-05, 1.7803806258598343e-05, 1.749684270180296e-05, 1.720028740237467e-05, 1.6913614672375843e-05, 1.663634247961454e-05, 1.6368014257750474e-05, 1.6108204363263212e-05, 1.585651443747338e-05, 1.561256794957444e-05, 1.5376013834611513e-05, 1.514652103651315e-05, 1.4923778508091345e-05, 1.4707491573062725e-05, 1.4497384654532652e-05, 1.4293195818027016e-05, 1.4094679499976337e-05, 1.390160196024226e-05, 1.3713742191612255e-05, 1.3530891919799615e-05, 1.3352853784454055e-05, 1.3179440429667011e-05, 1.3010473594476935e-05, 1.2845784112869296e-05, 1.2685211004281882e-05, 1.25286042020889e-05, 1.2375816368148662e-05, 1.2226710168761201e-05, 1.208115372719476e-05, 1.1939022442675196e-05, 1.1800196261901874e-05, 1.1664562407531776e-05, 1.1532010830705985e-05, 1.14024369395338e-05, 1.127574341808213e-05, 1.1151833859912585e-05, 1.1030618225049693e-05, 1.0912010111496784e-05, 1.0795924936246593e-05, 1.0682283573260065e-05, 1.0571009624982253e-05, 1.0462030331837013e-05, 1.0355274753237609e-05, 1.0250675586576108e-05, 1.0148169167223386e-05, 1.004769183055032e-05, 9.949185368895996e-06, 9.8525915745995e-06, 9.757854968484025e-06, 9.664922799856868e-06, 9.573744137014728e-06, 9.484269867243711e-06, 9.396452696819324e-06, 9.310247151006479e-06, 9.225608664564788e-06, 9.14249449124327e-06, 9.060865522769745e-06, 8.980680831882637e-06, 8.901903129299171e-06, 8.824495125736576e-06, 8.74842135090148e-06, 8.673649062984623e-06, 8.600143701187335e-06, 8.527873433195055e-06, 8.456807336187921e-06, 8.386916306335479e-06, 8.31817123980727e-06, 8.25054394226754e-06, 8.184007128875237e-06, 8.11853533377871e-06, 8.054102181631606e-06, 7.990684025571682e-06, 7.92825721873669e-06, 7.866798114264384e-06, 7.80628397478722e-06, 7.746693881927058e-06, 7.688006917305756e-06, 7.630202162545174e-06, 7.573260518256575e-06, 7.51716243030387e-06, 7.461889254045673e-06, 7.407422799587948e-06, 7.353745786531363e-06, 7.300840934476582e-06, 7.248692327266326e-06, 7.197283139248611e-06, 7.146597909013508e-06, 7.096621629898436e-06, 7.0473397499881685e-06, 6.998737262620125e-06, 6.95080098012113e-06, 6.903516350575956e-06, 6.856871095806127e-06, 6.810851573391119e-06, 6.765445959899807e-06, 6.7206419771537185e-06, 6.6764268922270276e-06, 6.632790245930664e-06, 6.5897202148335055e-06, 6.54720588499913e-06, 6.505236797238467e-06, 6.463802037615096e-06, 6.422892056434648e-06, 6.382496394508053e-06, 6.342605502140941e-06, 6.303210739133647e-06, 6.26430210104445e-06, 6.225870492926333e-06, 6.187908184074331e-06, 6.150405624794075e-06, 6.113355084380601e-06, 6.07674792263424e-06, 6.04057686359738e-06, 6.004833721817704e-06, 5.969511221337598e-06, 5.934601631452097e-06, 5.900098130950937e-06, 5.865993898623856e-06, 5.832281203765888e-06, 5.798953679914121e-06, 5.766005415352993e-06, 5.733428679377539e-06, 5.7012184697669e-06, 5.669368420058163e-06, 5.637871709041065e-06, 5.606723334494745e-06, 5.575916929956293e-06, 5.545447493204847e-06, 5.5153091125248466e-06, 5.485496785695432e-06, 5.456005055748392e-06, 5.426828465715516e-06, 5.397962468123296e-06, 5.369401606003521e-06, 5.341141786630033e-06, 5.31317755303462e-06, 5.285504812491126e-06, 5.258118562778691e-06, 5.231015165918507e-06, 5.204189164942363e-06, 5.177637376618804e-06, 5.15135479872697e-06, 5.125337793288054e-06, 5.099582267575897e-06, 5.074084583611693e-06, 5.048840193921933e-06, 5.02384591527516e-06, 4.999098109692568e-06, 4.974592684447998e-06, 4.950326456309995e-06, 4.92629578729975e-06, 4.902497039438458e-06, 4.878927484242013e-06, 4.855583483731607e-06, 4.832461399928434e-06, 4.809558959095739e-06, 4.786872068507364e-06, 4.764398454426555e-06, 4.7421349336218555e-06, 4.72007877760916e-06, 4.698226348409662e-06, 4.6765758270339575e-06, 4.655123575503239e-06, 4.633867320080753e-06, 4.612804332282394e-06, 4.591931883624056e-06, 4.571247245621635e-06, 4.5507485992857255e-06, 4.530432761384873e-06, 4.510297458182322e-06, 4.4903404159413185e-06, 4.470558906177757e-06, 4.450951564649586e-06, 4.431514753377996e-06, 4.412247562868288e-06, 4.393146809889004e-06, 4.37421067545074e-06, 4.3554373405640945e-06, 4.3368245314923115e-06, 4.318369974498637e-06, 4.3000718505936675e-06, 4.2819278860406484e-06, 4.263936716597527e-06, 4.2460960685275495e-06, 4.228403668093961e-06, 4.210858605802059e-06, 4.1934581531677395e-06, 4.1762014006963e-06, 4.159085619903635e-06, 4.1421099012950435e-06, 4.12527197113377e-06, 4.108570465177763e-06, 4.0920035644376185e-06, 4.0755699046712834e-06, 4.059267666889355e-06, 4.043095486849779e-06, 4.027051090815803e-06, 4.011134024040075e-06, 3.995342012785841e-06, 3.9796741475583985e-06], "accuracy_valid": [0.2192882859563253, 0.2546283767884036, 0.28611369305346385, 0.3007827207266566, 0.3084731504141566, 0.3147296216114458, 0.3193374082266566, 0.323131883000753, 0.32719108857304213, 0.33332548945783136, 0.33552275508283136, 0.3394495952560241, 0.3397040309676205, 0.34131153520331325, 0.3437220561935241, 0.34496334949171686, 0.3480562876506024, 0.34729298051581325, 0.34853427381400603, 0.35088449501129515, 0.35137277626129515, 0.35395684299698793, 0.3529493952371988, 0.3556761224585843, 0.3567747552710843, 0.35615410862198793, 0.3576292474585843, 0.3582395990210843, 0.35847344455948793, 0.3600809487951807, 0.3598368081701807, 0.3608236657567771, 0.3614443124058735, 0.3613119470067771, 0.3616884530308735, 0.3616781579442771, 0.3622988045933735, 0.3626650155308735, 0.3637739434299699, 0.3640180840549699, 0.3636518731174699, 0.36451666039156627, 0.3643842949924699, 0.36561529320406627, 0.36710072712725905, 0.36672422110316266, 0.36697865681475905, 0.36709043204066266, 0.36746693806475905, 0.36722279743975905, 0.36795521931475905, 0.36832143025225905, 0.36806699454066266, 0.36905385212725905, 0.36917592243975905, 0.37003041462725905, 0.3700407097138554, 0.3705289909638554, 0.3708952019013554, 0.3708952019013554, 0.37078342667545183, 0.3713834831513554, 0.37139377823795183, 0.37139377823795183, 0.37250270613704817, 0.37238063582454817, 0.37225856551204817, 0.37261448136295183, 0.37261448136295183, 0.37408962019954817, 0.37285862198795183, 0.37384547957454817, 0.37346897355045183, 0.37372340926204817, 0.3742219855986446, 0.3748323371611446, 0.3756868293486446, 0.3749544074736446, 0.3754426887236446, 0.3756868293486446, 0.3758088996611446, 0.37667368693524095, 0.3759309699736446, 0.37640895613704817, 0.37640895613704817, 0.37777231974774095, 0.37777231974774095, 0.3776399543486446, 0.3778840949736446, 0.3771516730986446, 0.37826060099774095, 0.3784944465361446, 0.3781282355986446, 0.3787385871611446, 0.37874888224774095, 0.37824001082454817, 0.3791047980986446, 0.3788606574736446, 0.3793489387236446, 0.3795930793486446, 0.3789827277861446, 0.37897243269954817, 0.37984751506024095, 0.37960337443524095, 0.37933864363704817, 0.3804475715361446, 0.38045786662274095, 0.38094614787274095, 0.38106821818524095, 0.3802034309111446, 0.38106821818524095, 0.38070200724774095, 0.3811799934111446, 0.38167856974774095, 0.38106821818524095, 0.3810579230986446, 0.38143442912274095, 0.38216685099774095, 0.3813020637236446, 0.3815462043486446, 0.3810579230986446, 0.3820344855986446, 0.3816682746611446, 0.3821565559111446, 0.3824006965361446, 0.3822786262236446, 0.38214626082454817, 0.3822786262236446, 0.3821565559111446, 0.3832551887236446, 0.3828889777861446, 0.3831331184111446, 0.3830110480986446, 0.3830110480986446, 0.3830110480986446, 0.3833772590361446, 0.3833772590361446, 0.3839876105986446, 0.38361110457454817, 0.3837434699736446, 0.3839876105986446, 0.38348903426204817, 0.38409938582454817, 0.3836213996611446, 0.38385524519954817, 0.38373317488704817, 0.38361110457454817, 0.38385524519954817, 0.38446559676204817, 0.38397731551204817, 0.38409938582454817, 0.38434352644954817, 0.38409938582454817, 0.38422145613704817, 0.38446559676204817, 0.38434352644954817, 0.38385524519954817, 0.38434352644954817, 0.38434352644954817, 0.38507594832454817, 0.38495387801204817, 0.38507594832454817, 0.38507594832454817, 0.38556422957454817, 0.38507594832454817, 0.38507594832454817, 0.38532008894954817, 0.38556422957454817, 0.38556422957454817, 0.38519801863704817, 0.38556422957454817, 0.38641872176204817, 0.38629665144954817, 0.38617458113704817, 0.38617458113704817, 0.38605251082454817, 0.38605251082454817, 0.38641872176204817, 0.38641872176204817, 0.38666286238704817, 0.38641872176204817, 0.38629665144954817, 0.38629665144954817, 0.38654079207454817, 0.38629665144954817, 0.38678493269954817, 0.38654079207454817, 0.3875276496611446, 0.38715114363704817, 0.38702907332454817, 0.38678493269954817, 0.38727321394954817, 0.38763942488704817, 0.38690700301204817, 0.3877717902861446, 0.38751735457454817, 0.38776149519954817, 0.38800563582454817, 0.38824977644954817, 0.3887483527861446, 0.38837184676204817, 0.38788356551204817, 0.38873805769954817, 0.38837184676204817, 0.38961314006024095, 0.3883821418486446, 0.3882600715361446, 0.3883821418486446, 0.38812770613704817, 0.3886262824736446, 0.38837184676204817, 0.38873805769954817, 0.38861598738704817, 0.38824977644954817, 0.3886262824736446, 0.38924692912274095, 0.38837184676204817, 0.38800563582454817, 0.38788356551204817, 0.3886262824736446, 0.3883821418486446, 0.3885042121611446, 0.3885042121611446, 0.3886262824736446, 0.3888704230986446, 0.3883821418486446, 0.3889924934111446, 0.3885042121611446, 0.3889924934111446, 0.3892366340361446, 0.3889924934111446, 0.38873805769954817, 0.3888704230986446, 0.3885042121611446, 0.3888704230986446, 0.3882600715361446, 0.3886262824736446, 0.3885042121611446, 0.3886262824736446, 0.3886262824736446, 0.38812770613704817, 0.38861598738704817, 0.38800563582454817, 0.38861598738704817, 0.38776149519954817, 0.38824977644954817, 0.38800563582454817, 0.38861598738704817, 0.38849391707454817, 0.38837184676204817, 0.38861598738704817, 0.38898219832454817, 0.3887483527861446, 0.38898219832454817, 0.38898219832454817], "accuracy_test": 0.3984, "start": "2016-01-23 10:42:05.946000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0], "accuracy_train_last": 0.41096266207548915, "batch_size_eval": 1024, "accuracy_train_std": [0.01176093152114375, 0.012123917951437777, 0.013945859670342415, 0.013372950521770604, 0.013218084162836553, 0.01322057009424623, 0.013274839776553965, 0.013531365039641427, 0.012927057972054191, 0.012364900951080317, 0.01286506453818924, 0.012680162014158633, 0.012744595168885238, 0.013216029180202675, 0.013070959341811088, 0.013093773194077533, 0.013449796886132105, 0.01392624064727998, 0.013926271061874922, 0.013872094579900377, 0.014167218296110784, 0.013967887354500429, 0.01371473739248327, 0.013761664061530102, 0.013790327281973325, 0.013482401834368971, 0.013626104539921656, 0.013713402204205829, 0.01376778399157692, 0.013591227781699843, 0.014191644144041187, 0.014051591498731145, 0.014094229881376376, 0.014237176559469378, 0.014110824716700719, 0.014001256541700929, 0.014173037464101087, 0.014310843626821541, 0.013865955081264987, 0.01401847542354913, 0.01406392581053263, 0.014243409971171949, 0.013810691675907826, 0.013719030724439018, 0.013965036116196138, 0.014092445164819457, 0.013994143342123976, 0.01330767663318311, 0.01339582339967104, 0.013386220841522167, 0.013367984400866381, 0.013386643960978925, 0.013294402211902128, 0.013570265475607598, 0.013305826152384168, 0.013294738412398385, 0.013187707353460332, 0.013279072642541733, 0.013460273943003738, 0.013772997101223641, 0.013551336647746349, 0.013166077476631195, 0.01335656822958521, 0.013347055723886735, 0.013675366981784674, 0.013256733909688923, 0.013202171280572387, 0.013349964165101917, 0.013422555327663158, 0.013024938086144822, 0.013288066519089701, 0.013444372853689636, 0.013161013083434769, 0.013224392851339195, 0.013386722030212645, 0.01340214558572883, 0.013375750362178295, 0.013296280072561582, 0.01290905433316433, 0.01277486756854183, 0.012737061028543514, 0.012915938355211785, 0.013034833736731921, 0.012936303387650937, 0.012818552785855296, 0.01273306016050307, 0.012573397605784513, 0.012908736805026374, 0.01290115767779344, 0.012938873702343588, 0.012932728557724758, 0.012752504287625028, 0.012894929783750649, 0.012812787825396172, 0.012776221509982161, 0.012668045631784267, 0.012756131481090227, 0.01291665883695532, 0.012829925513934935, 0.012799021033206546, 0.012804878760951256, 0.01284354245397009, 0.013027707777159907, 0.01259493604554613, 0.01281437864496369, 0.01291563136855767, 0.012602590448590112, 0.0128139867607354, 0.012966457094155255, 0.013124565122158578, 0.012768453793751591, 0.012757799362933931, 0.012558893821039658, 0.01214304344627731, 0.01237859089815309, 0.012518428474098686, 0.012702554929337848, 0.01274758958849467, 0.012535971517229606, 0.01251760887649738, 0.012426476977600343, 0.01241833089974896, 0.01223769210832635, 0.01232431401711079, 0.012098292402495187, 0.012266597924765643, 0.012178782711178986, 0.012277517440577231, 0.01225939658977988, 0.01212738386346312, 0.012072403918836482, 0.012519874970186846, 0.01227326551841288, 0.012337663842828382, 0.012583109916171112, 0.012411013843130463, 0.012531039999741624, 0.01224686464825375, 0.012105510774219562, 0.01204156746330898, 0.012371181097569224, 0.011970493873469379, 0.012324658147867457, 0.012214228149424947, 0.012341834167235418, 0.012137911813814723, 0.01237075576874654, 0.012201848923366301, 0.012109666553529649, 0.012315101064356761, 0.012306362461709628, 0.01225135285127434, 0.01227774924353248, 0.012199083789327837, 0.012157368296510969, 0.012298507463763297, 0.012048208839397214, 0.012373358031835676, 0.01207497380372785, 0.012366977178275566, 0.01218178774130052, 0.012220573573603545, 0.012106278425736607, 0.012434516068707066, 0.012505381448685161, 0.012497834698117204, 0.01264210204659882, 0.012765715709210507, 0.012623966125287418, 0.012808880894852692, 0.012298665124958398, 0.012406300001406094, 0.012350673913583745, 0.01252731422658231, 0.01256061801710392, 0.012781730486397296, 0.01235189976734093, 0.012459517330851785, 0.01238118484038334, 0.012364470554504585, 0.012448810698301694, 0.012505105307925186, 0.012709746864642931, 0.012365088604773054, 0.012464408884419798, 0.012272312706636698, 0.012547689242019325, 0.012515390503330401, 0.012211781502832824, 0.012431450143544886, 0.012381427963815509, 0.01268612788758144, 0.012389205966209165, 0.012476979512872055, 0.012674852934168908, 0.012582941645648571, 0.012531009313729146, 0.012555250991838724, 0.01243514286189314, 0.012906193387505191, 0.013117897132771979, 0.01274309547464158, 0.01277993314110413, 0.012770285852517205, 0.012920756313437778, 0.012772133204235911, 0.01274832605247106, 0.012657237038172512, 0.012668600118046066, 0.012920606945974732, 0.01275404822200447, 0.01279433189318088, 0.01277197324914336, 0.012524055771696914, 0.012780034963447609, 0.01253004742106874, 0.012674276938583395, 0.012538928438847798, 0.012482692200605362, 0.01263657107117888, 0.012597515745205725, 0.012759669050745276, 0.012635929142530528, 0.012605986925714812, 0.012742983435851825, 0.012697704020003172, 0.012844684904141476, 0.012747217422670697, 0.012757538695128504, 0.012585786137028569, 0.012565193659894936, 0.012611112530490815, 0.012710400837062095, 0.012583778712031852, 0.012501812741813528, 0.012685412095211441, 0.012298147632105037, 0.012683540713006671, 0.01272269085415565, 0.012690368758863481, 0.012610663722365217, 0.012697076239469852, 0.012518674009567559, 0.012588315775829131, 0.012496206684121311, 0.012620679593651771, 0.012519306241494218, 0.012531144206141336, 0.012662784185187069, 0.012503439324324782, 0.012561871369707328, 0.012691500602201334, 0.012664627693740027, 0.012748840373222792, 0.012778462392743228], "accuracy_test_std": 0.12222904728418692, "error_valid": [0.7807117140436747, 0.7453716232115963, 0.7138863069465362, 0.6992172792733433, 0.6915268495858433, 0.6852703783885542, 0.6806625917733433, 0.676868116999247, 0.6728089114269579, 0.6666745105421686, 0.6644772449171686, 0.6605504047439759, 0.6602959690323795, 0.6586884647966867, 0.6562779438064759, 0.6550366505082832, 0.6519437123493976, 0.6527070194841867, 0.651465726185994, 0.6491155049887049, 0.6486272237387049, 0.6460431570030121, 0.6470506047628012, 0.6443238775414157, 0.6432252447289157, 0.6438458913780121, 0.6423707525414157, 0.6417604009789157, 0.6415265554405121, 0.6399190512048193, 0.6401631918298193, 0.6391763342432228, 0.6385556875941265, 0.6386880529932228, 0.6383115469691265, 0.6383218420557228, 0.6377011954066265, 0.6373349844691265, 0.6362260565700302, 0.6359819159450302, 0.6363481268825302, 0.6354833396084337, 0.6356157050075302, 0.6343847067959337, 0.632899272872741, 0.6332757788968373, 0.633021343185241, 0.6329095679593373, 0.632533061935241, 0.632777202560241, 0.632044780685241, 0.631678569747741, 0.6319330054593373, 0.630946147872741, 0.630824077560241, 0.629969585372741, 0.6299592902861446, 0.6294710090361446, 0.6291047980986446, 0.6291047980986446, 0.6292165733245482, 0.6286165168486446, 0.6286062217620482, 0.6286062217620482, 0.6274972938629518, 0.6276193641754518, 0.6277414344879518, 0.6273855186370482, 0.6273855186370482, 0.6259103798004518, 0.6271413780120482, 0.6261545204254518, 0.6265310264495482, 0.6262765907379518, 0.6257780144013554, 0.6251676628388554, 0.6243131706513554, 0.6250455925263554, 0.6245573112763554, 0.6243131706513554, 0.6241911003388554, 0.623326313064759, 0.6240690300263554, 0.6235910438629518, 0.6235910438629518, 0.622227680252259, 0.622227680252259, 0.6223600456513554, 0.6221159050263554, 0.6228483269013554, 0.621739399002259, 0.6215055534638554, 0.6218717644013554, 0.6212614128388554, 0.621251117752259, 0.6217599891754518, 0.6208952019013554, 0.6211393425263554, 0.6206510612763554, 0.6204069206513554, 0.6210172722138554, 0.6210275673004518, 0.620152484939759, 0.620396625564759, 0.6206613563629518, 0.6195524284638554, 0.619542133377259, 0.619053852127259, 0.618931781814759, 0.6197965690888554, 0.618931781814759, 0.619297992752259, 0.6188200065888554, 0.618321430252259, 0.618931781814759, 0.6189420769013554, 0.618565570877259, 0.617833149002259, 0.6186979362763554, 0.6184537956513554, 0.6189420769013554, 0.6179655144013554, 0.6183317253388554, 0.6178434440888554, 0.6175993034638554, 0.6177213737763554, 0.6178537391754518, 0.6177213737763554, 0.6178434440888554, 0.6167448112763554, 0.6171110222138554, 0.6168668815888554, 0.6169889519013554, 0.6169889519013554, 0.6169889519013554, 0.6166227409638554, 0.6166227409638554, 0.6160123894013554, 0.6163888954254518, 0.6162565300263554, 0.6160123894013554, 0.6165109657379518, 0.6159006141754518, 0.6163786003388554, 0.6161447548004518, 0.6162668251129518, 0.6163888954254518, 0.6161447548004518, 0.6155344032379518, 0.6160226844879518, 0.6159006141754518, 0.6156564735504518, 0.6159006141754518, 0.6157785438629518, 0.6155344032379518, 0.6156564735504518, 0.6161447548004518, 0.6156564735504518, 0.6156564735504518, 0.6149240516754518, 0.6150461219879518, 0.6149240516754518, 0.6149240516754518, 0.6144357704254518, 0.6149240516754518, 0.6149240516754518, 0.6146799110504518, 0.6144357704254518, 0.6144357704254518, 0.6148019813629518, 0.6144357704254518, 0.6135812782379518, 0.6137033485504518, 0.6138254188629518, 0.6138254188629518, 0.6139474891754518, 0.6139474891754518, 0.6135812782379518, 0.6135812782379518, 0.6133371376129518, 0.6135812782379518, 0.6137033485504518, 0.6137033485504518, 0.6134592079254518, 0.6137033485504518, 0.6132150673004518, 0.6134592079254518, 0.6124723503388554, 0.6128488563629518, 0.6129709266754518, 0.6132150673004518, 0.6127267860504518, 0.6123605751129518, 0.6130929969879518, 0.6122282097138554, 0.6124826454254518, 0.6122385048004518, 0.6119943641754518, 0.6117502235504518, 0.6112516472138554, 0.6116281532379518, 0.6121164344879518, 0.6112619423004518, 0.6116281532379518, 0.610386859939759, 0.6116178581513554, 0.6117399284638554, 0.6116178581513554, 0.6118722938629518, 0.6113737175263554, 0.6116281532379518, 0.6112619423004518, 0.6113840126129518, 0.6117502235504518, 0.6113737175263554, 0.610753070877259, 0.6116281532379518, 0.6119943641754518, 0.6121164344879518, 0.6113737175263554, 0.6116178581513554, 0.6114957878388554, 0.6114957878388554, 0.6113737175263554, 0.6111295769013554, 0.6116178581513554, 0.6110075065888554, 0.6114957878388554, 0.6110075065888554, 0.6107633659638554, 0.6110075065888554, 0.6112619423004518, 0.6111295769013554, 0.6114957878388554, 0.6111295769013554, 0.6117399284638554, 0.6113737175263554, 0.6114957878388554, 0.6113737175263554, 0.6113737175263554, 0.6118722938629518, 0.6113840126129518, 0.6119943641754518, 0.6113840126129518, 0.6122385048004518, 0.6117502235504518, 0.6119943641754518, 0.6113840126129518, 0.6115060829254518, 0.6116281532379518, 0.6113840126129518, 0.6110178016754518, 0.6112516472138554, 0.6110178016754518, 0.6110178016754518], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5288174781322144, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0010148169131431349, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "adadelta", "nb_data_augmentation": 1, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.301822442177921e-10, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.09391890163533453}, "accuracy_valid_max": 0.38961314006024095, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.38898219832454817, "loss_train": [8.815215110778809, 2.2310922145843506, 2.0456323623657227, 1.9839386940002441, 1.9457478523254395, 1.922531247138977, 1.9028520584106445, 1.8834002017974854, 1.8745447397232056, 1.8632808923721313, 1.8522626161575317, 1.8439338207244873, 1.8361248970031738, 1.8323686122894287, 1.8254599571228027, 1.8185744285583496, 1.8157862424850464, 1.8098042011260986, 1.80532705783844, 1.8034001588821411, 1.7976038455963135, 1.7957918643951416, 1.7936161756515503, 1.7879341840744019, 1.7894256114959717, 1.7823927402496338, 1.779883861541748, 1.7741549015045166, 1.7751853466033936, 1.7719383239746094, 1.7713031768798828, 1.77060866355896, 1.7651503086090088, 1.7658315896987915, 1.7629059553146362, 1.7633345127105713, 1.7582851648330688, 1.759453535079956, 1.7521369457244873, 1.7538511753082275, 1.7537280321121216, 1.7496670484542847, 1.749341368675232, 1.748291254043579, 1.747492790222168, 1.7437759637832642, 1.7462332248687744, 1.7431684732437134, 1.7419719696044922, 1.742754578590393, 1.7394969463348389, 1.7354695796966553, 1.7371784448623657, 1.7375426292419434, 1.7347817420959473, 1.7352402210235596, 1.7348285913467407, 1.7321866750717163, 1.7332026958465576, 1.7312487363815308, 1.729841947555542, 1.7258435487747192, 1.7296359539031982, 1.726270079612732, 1.7283961772918701, 1.7258573770523071, 1.7256526947021484, 1.7266137599945068, 1.7231299877166748, 1.7212581634521484, 1.7205528020858765, 1.7209559679031372, 1.7211570739746094, 1.7187782526016235, 1.7169469594955444, 1.7183095216751099, 1.7154490947723389, 1.7157708406448364, 1.7181850671768188, 1.7137113809585571, 1.7169281244277954, 1.7137895822525024, 1.713858723640442, 1.7103725671768188, 1.7131879329681396, 1.7109650373458862, 1.7111706733703613, 1.7100909948349, 1.7111033201217651, 1.7094566822052002, 1.7067387104034424, 1.7078343629837036, 1.7084970474243164, 1.7100173234939575, 1.7087079286575317, 1.7060664892196655, 1.7049576044082642, 1.707783579826355, 1.7068077325820923, 1.703909158706665, 1.7035094499588013, 1.702476978302002, 1.7035733461380005, 1.7020894289016724, 1.7028346061706543, 1.7008922100067139, 1.702101230621338, 1.7041434049606323, 1.7011868953704834, 1.7003153562545776, 1.7001662254333496, 1.696716547012329, 1.697501540184021, 1.696683406829834, 1.695805549621582, 1.6979378461837769, 1.6983321905136108, 1.696425437927246, 1.6958218812942505, 1.69700026512146, 1.6948035955429077, 1.6944712400436401, 1.6942293643951416, 1.6923365592956543, 1.69459867477417, 1.6929630041122437, 1.691608190536499, 1.6921865940093994, 1.6936688423156738, 1.6912894248962402, 1.691262125968933, 1.6906037330627441, 1.6908632516860962, 1.690801978111267, 1.6901354789733887, 1.6927926540374756, 1.691074013710022, 1.6887463331222534, 1.6914775371551514, 1.6885427236557007, 1.6884032487869263, 1.6885349750518799, 1.687461256980896, 1.6880853176116943, 1.6858280897140503, 1.689062476158142, 1.6842265129089355, 1.6875617504119873, 1.6852285861968994, 1.687740683555603, 1.6869845390319824, 1.686079978942871, 1.6869744062423706, 1.6853300333023071, 1.6841017007827759, 1.6827298402786255, 1.6829965114593506, 1.6830148696899414, 1.683394193649292, 1.6853786706924438, 1.6809152364730835, 1.6851269006729126, 1.6820451021194458, 1.682879090309143, 1.6820067167282104, 1.678030252456665, 1.681714653968811, 1.681558609008789, 1.682092308998108, 1.6817162036895752, 1.6783415079116821, 1.6793826818466187, 1.681846261024475, 1.6799976825714111, 1.6769481897354126, 1.6819287538528442, 1.679246425628662, 1.6797354221343994, 1.6780147552490234, 1.6766122579574585, 1.6787652969360352, 1.6764439344406128, 1.6779783964157104, 1.6748820543289185, 1.6781221628189087, 1.67799973487854, 1.6766655445098877, 1.6755324602127075, 1.6763122081756592, 1.6772205829620361, 1.6743191480636597, 1.6744433641433716, 1.6746407747268677, 1.6762274503707886, 1.6750943660736084, 1.6733626127243042, 1.6740710735321045, 1.6740163564682007, 1.6726388931274414, 1.6727242469787598, 1.6736469268798828, 1.6741631031036377, 1.674361228942871, 1.6717897653579712, 1.674289345741272, 1.6733334064483643, 1.6720521450042725, 1.6742985248565674, 1.6712275743484497, 1.671025276184082, 1.6716581583023071, 1.6718631982803345, 1.6703025102615356, 1.6738640069961548, 1.6710667610168457, 1.6697272062301636, 1.671952724456787, 1.6696385145187378, 1.67082679271698, 1.6696891784667969, 1.670773983001709, 1.671925663948059, 1.6680110692977905, 1.670265555381775, 1.6712909936904907, 1.6678344011306763, 1.6670043468475342, 1.667298674583435, 1.6682595014572144, 1.6693992614746094, 1.6723153591156006, 1.6673542261123657, 1.6683429479599, 1.667165994644165, 1.6675924062728882, 1.666986107826233, 1.6691628694534302, 1.6636486053466797, 1.6697957515716553, 1.6657230854034424, 1.6667193174362183, 1.6667591333389282, 1.664581060409546, 1.6658486127853394, 1.6641141176223755, 1.6670281887054443, 1.666071891784668, 1.665740728378296, 1.6651779413223267, 1.6659035682678223, 1.6647428274154663, 1.665238857269287, 1.6647666692733765, 1.6620924472808838, 1.666426181793213], "accuracy_train_first": 0.22019663909653012, "model": "residualv2", "loss_std": [17.59677505493164, 0.28437021374702454, 0.2451007217168808, 0.23642100393772125, 0.23427334427833557, 0.23626618087291718, 0.23632805049419403, 0.23102927207946777, 0.23196180164813995, 0.23304928839206696, 0.2329946756362915, 0.23145125806331635, 0.23268543183803558, 0.23352067172527313, 0.23252661526203156, 0.23580215871334076, 0.2351851910352707, 0.23350630700588226, 0.23321793973445892, 0.23242893815040588, 0.23482675850391388, 0.23607327044010162, 0.23605729639530182, 0.2315833866596222, 0.23744748532772064, 0.23512718081474304, 0.23485448956489563, 0.2331792265176773, 0.23599639534950256, 0.23716627061367035, 0.23569120466709137, 0.23731128871440887, 0.23497700691223145, 0.23364804685115814, 0.2358771115541458, 0.2371753454208374, 0.23459355533123016, 0.2362428903579712, 0.2358197420835495, 0.2353108525276184, 0.23687775433063507, 0.2373703569173813, 0.23689480125904083, 0.23530445992946625, 0.23853503167629242, 0.23760350048542023, 0.23768547177314758, 0.23882794380187988, 0.23381805419921875, 0.23520740866661072, 0.238266721367836, 0.23740504682064056, 0.23668374121189117, 0.2370590716600418, 0.2379472404718399, 0.2391565591096878, 0.2378646284341812, 0.23477311432361603, 0.23809173703193665, 0.2383609563112259, 0.23788154125213623, 0.23875179886817932, 0.24025043845176697, 0.23781166970729828, 0.23759205639362335, 0.23773396015167236, 0.2374267429113388, 0.24051792919635773, 0.24051344394683838, 0.23606541752815247, 0.23774467408657074, 0.2372806966304779, 0.2406882494688034, 0.23591913282871246, 0.23936641216278076, 0.23910820484161377, 0.23805488646030426, 0.23925313353538513, 0.24046069383621216, 0.2381708323955536, 0.23858517408370972, 0.23877525329589844, 0.2397235929965973, 0.23813676834106445, 0.23884934186935425, 0.23861268162727356, 0.23928600549697876, 0.2391323298215866, 0.23700697720050812, 0.2380458116531372, 0.24089479446411133, 0.23990832269191742, 0.2379862666130066, 0.23889856040477753, 0.239078089594841, 0.23636853694915771, 0.23823338747024536, 0.2373402714729309, 0.23860986530780792, 0.23818479478359222, 0.23682576417922974, 0.23900409042835236, 0.23967072367668152, 0.2390931248664856, 0.23958657681941986, 0.23839791119098663, 0.23930497467517853, 0.23771809041500092, 0.2413850873708725, 0.23990435898303986, 0.2387874722480774, 0.23917263746261597, 0.23950889706611633, 0.23756156861782074, 0.2399858832359314, 0.24085882306098938, 0.23965606093406677, 0.24141764640808105, 0.23792587220668793, 0.23849841952323914, 0.24063187837600708, 0.23866228759288788, 0.24090798199176788, 0.238341823220253, 0.2410202920436859, 0.24048475921154022, 0.2386208325624466, 0.23934787511825562, 0.2389519363641739, 0.2382505238056183, 0.23956188559532166, 0.23861339688301086, 0.23955342173576355, 0.2417304813861847, 0.23753364384174347, 0.2401990443468094, 0.24026767909526825, 0.2403223067522049, 0.24128203094005585, 0.24097605049610138, 0.2414073646068573, 0.2380024492740631, 0.24142146110534668, 0.24062681198120117, 0.24158978462219238, 0.24110804498195648, 0.24010729789733887, 0.24088871479034424, 0.24206048250198364, 0.24371498823165894, 0.2411857396364212, 0.24113069474697113, 0.2389354109764099, 0.23854680359363556, 0.24037116765975952, 0.24319800734519958, 0.24255001544952393, 0.24340501427650452, 0.24127905070781708, 0.24218867719173431, 0.23976466059684753, 0.24096466600894928, 0.24067415297031403, 0.24071693420410156, 0.24137930572032928, 0.2390374094247818, 0.2401665300130844, 0.24061962962150574, 0.2400156855583191, 0.2409580945968628, 0.24029792845249176, 0.240284264087677, 0.2409730702638626, 0.24101130664348602, 0.24232837557792664, 0.23805701732635498, 0.23863524198532104, 0.24200643599033356, 0.24114084243774414, 0.2393444925546646, 0.24194370210170746, 0.24000945687294006, 0.2412566989660263, 0.24061621725559235, 0.2409997433423996, 0.24488642811775208, 0.23969705402851105, 0.24023915827274323, 0.24337391555309296, 0.2417086958885193, 0.24145172536373138, 0.2426784783601761, 0.24239185452461243, 0.24167966842651367, 0.24102933704853058, 0.2396632432937622, 0.24259357154369354, 0.24121668934822083, 0.24183939397335052, 0.2405809760093689, 0.23980124294757843, 0.23901820182800293, 0.2429761290550232, 0.2420734167098999, 0.24316655099391937, 0.24469535052776337, 0.24393634498119354, 0.24286632239818573, 0.23952290415763855, 0.2412993311882019, 0.24204425513744354, 0.24118711054325104, 0.2401699721813202, 0.24185481667518616, 0.24232812225818634, 0.24044880270957947, 0.24241520464420319, 0.24020694196224213, 0.24026313424110413, 0.24017101526260376, 0.24090856313705444, 0.2427905648946762, 0.24023525416851044, 0.24086196720600128, 0.2418619990348816, 0.24171259999275208, 0.24198393523693085, 0.24109698832035065, 0.24284112453460693, 0.24155303835868835, 0.2410060614347458, 0.24129684269428253, 0.24099889397621155, 0.24074405431747437, 0.2390541136264801, 0.24104897677898407, 0.2412433624267578, 0.24234427511692047, 0.24180012941360474, 0.2440183162689209, 0.24245506525039673, 0.24347007274627686, 0.2395731806755066, 0.2434871345758438, 0.24239644408226013, 0.2431085854768753, 0.24191729724407196, 0.24228861927986145, 0.24199804663658142, 0.24241718649864197, 0.24208594858646393, 0.24450349807739258, 0.24289968609809875, 0.2408151626586914, 0.24231122434139252]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:11 2016", "state": "available"}], "summary": "2610450eb903f216093dbfd2d6479020"}