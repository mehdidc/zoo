{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 64, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 4, "nbg3": 1, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.4221982955932617, 0.9795918464660645, 0.7445970773696899, 0.6242490410804749, 0.5483153462409973, 0.49387216567993164, 0.4546778202056885, 0.4208795726299286, 0.39042946696281433, 0.36591869592666626, 0.344659686088562, 0.32467392086982727, 0.30711829662323, 0.29153990745544434, 0.275768518447876, 0.26301392912864685, 0.2504415512084961, 0.23790457844734192, 0.2258198857307434, 0.21550416946411133, 0.206834077835083, 0.19726021587848663, 0.18828924000263214, 0.18102511763572693, 0.1716897189617157, 0.16445040702819824, 0.15776760876178741, 0.15182039141654968, 0.14659585058689117, 0.14039720594882965, 0.13393016159534454, 0.12853996455669403, 0.12386403232812881, 0.11865537613630295, 0.1141238883137703, 0.10997608304023743, 0.10736037790775299, 0.1010056585073471, 0.0984305813908577, 0.09438852220773697, 0.09089557826519012, 0.08892854303121567, 0.08556915819644928, 0.08199343830347061, 0.0811324343085289, 0.07805095613002777, 0.07557550072669983, 0.07254604995250702, 0.06991112232208252, 0.06870056688785553, 0.06643714755773544, 0.06483693420886993, 0.06405720114707947, 0.06214519217610359, 0.059119850397109985, 0.05935884639620781, 0.057320110499858856, 0.055538348853588104, 0.05374309420585632, 0.05222131684422493, 0.05136294290423393, 0.05055350810289383, 0.049611806869506836, 0.048809781670570374, 0.047939978539943695, 0.045978471636772156, 0.045530710369348526, 0.04491754248738289, 0.04305294528603554, 0.042680032551288605, 0.042183998972177505, 0.04134546220302582, 0.040922120213508606, 0.039801277220249176, 0.039549484848976135, 0.03894009813666344, 0.03786558657884598, 0.03697711229324341, 0.03733767196536064, 0.035750556737184525, 0.03574279695749283, 0.03594202175736427, 0.03453925997018814, 0.03367694839835167, 0.03360653668642044, 0.0331168994307518, 0.033001456409692764, 0.03221491351723671, 0.03188057243824005, 0.031577832996845245, 0.03117937408387661, 0.030521351844072342, 0.03031342662870884, 0.03003324382007122, 0.029929127544164658, 0.029627785086631775, 0.02950618416070938, 0.02834543213248253, 0.029250765219330788, 0.02806401066482067, 0.027576057240366936, 0.02763749286532402, 0.027577312663197517, 0.026502273976802826, 0.026723219081759453, 0.026643307879567146, 0.0261236485093832, 0.02605733461678028, 0.026310207322239876, 0.025486629456281662, 0.02543122135102749, 0.024778375402092934, 0.02536330744624138, 0.024495931342244148, 0.024358419701457024, 0.024716094136238098, 0.024390973150730133, 0.023102443665266037, 0.023812532424926758, 0.023794757202267647, 0.023250896483659744, 0.022600369527935982, 0.022246109321713448, 0.02292713150382042, 0.022927504032850266, 0.021793194115161896, 0.02225990779697895, 0.022168798372149467, 0.022232862189412117, 0.02201123721897602, 0.02132582478225231, 0.021527238190174103, 0.021184438839554787, 0.020991258323192596, 0.02095835655927658, 0.021210134029388428, 0.020322447642683983, 0.020806757733225822, 0.020252913236618042, 0.020500214770436287, 0.02048206329345703, 0.020107708871364594, 0.020528465509414673, 0.020454058423638344, 0.01943901553750038, 0.0197194442152977, 0.020389236509799957, 0.019342046231031418, 0.019389137625694275, 0.01943739503622055, 0.01916584186255932, 0.019092712551355362, 0.019614093005657196, 0.019361212849617004, 0.019260048866271973, 0.019231881946325302, 0.0186171755194664, 0.018852509558200836, 0.018742626532912254, 0.018396679311990738, 0.018837453797459602, 0.0184632558375597, 0.018657775595784187, 0.018200121819972992, 0.018414415419101715, 0.01783883571624756, 0.017848853021860123, 0.0177667997777462, 0.017976442351937294, 0.01770889386534691, 0.017621725797653198, 0.017929725348949432, 0.017880430445075035, 0.01767319068312645, 0.017070064321160316, 0.017237024381756783, 0.01759994588792324, 0.017125874757766724, 0.017551587894558907, 0.01706145703792572, 0.016825443133711815, 0.017038404941558838, 0.016815271228551865, 0.016896212473511696, 0.016718080267310143, 0.017128007486462593, 0.016708672046661377, 0.017015840858221054, 0.016811106353998184, 0.016861524432897568, 0.01665831170976162, 0.016196832060813904, 0.016747716814279556, 0.016388392075896263, 0.01611710898578167, 0.016170410439372063, 0.016123469918966293, 0.01643315516412258, 0.016352688893675804, 0.016456684097647667, 0.0162192452698946, 0.016080331057310104, 0.016386307775974274, 0.015808988362550735, 0.01610429212450981, 0.01565673016011715, 0.015990404412150383, 0.016135189682245255, 0.01560660358518362, 0.01606868952512741, 0.015958499163389206, 0.015799790620803833, 0.01547740027308464, 0.01541296299546957, 0.01562611758708954, 0.015305927954614162, 0.015424189157783985, 0.015157956629991531, 0.01569136418402195, 0.015383507125079632, 0.014697558246552944, 0.015191572718322277, 0.015346686355769634, 0.015571070834994316, 0.01518894825130701, 0.015027659013867378, 0.01511916983872652, 0.014889712445437908, 0.014779454097151756, 0.014847854152321815, 0.014916050247848034, 0.015323251485824585, 0.01494341716170311, 0.01497317012399435, 0.014593223109841347, 0.015011033043265343, 0.014939150772988796, 0.014792926609516144, 0.014612476341426373, 0.014719921164214611, 0.014707579277455807, 0.014651274308562279, 0.014602506533265114, 0.01466880552470684, 0.014364678412675858, 0.014781653881072998, 0.014594832435250282, 0.01436516921967268, 0.014448463916778564, 0.014587356708943844, 0.014409930445253849, 0.014547660946846008, 0.014133622869849205, 0.014068767428398132, 0.014360138215124607, 0.01437360979616642, 0.014574212953448296, 0.01440173014998436, 0.01419731043279171, 0.01433047279715538, 0.014593847095966339, 0.014353040605783463], "moving_avg_accuracy_train": [0.03050117647058823, 0.06448635294117647, 0.11819301176470587, 0.1816278282352941, 0.24417092776470586, 0.3027561879294117, 0.35452527501882347, 0.4072162769287058, 0.4536805315887764, 0.4973171843122517, 0.5364489952927912, 0.5734464487046886, 0.6072100391283374, 0.6371549175684448, 0.664872366988071, 0.691500424406911, 0.714420970201514, 0.7360588731813625, 0.7564223976279322, 0.773855451982786, 0.7898134361962722, 0.8053356219884097, 0.8189667656719216, 0.8318747949870824, 0.844101433135433, 0.855663054527772, 0.8656049843691125, 0.8750162506380835, 0.8831852138095693, 0.8900008100756712, 0.8973889643622218, 0.9033512443965879, 0.9089831787804585, 0.9145789785494716, 0.9204434336357008, 0.9257449726250719, 0.9300551812449176, 0.93468025135572, 0.9384428144554421, 0.9416102977157802, 0.9447857385324374, 0.9479353999733113, 0.9509724482112744, 0.9540704975077939, 0.9567175654040734, 0.9580716912166073, 0.9603162868008288, 0.96251524635604, 0.9641625452498478, 0.9658098201366276, 0.9674641322406119, 0.967809483722433, 0.969760300056072, 0.9714007406387002, 0.9727241959865949, 0.9741082469761706, 0.9743774222785535, 0.9756667388742276, 0.976405947339746, 0.9772924114293008, 0.9783914055804884, 0.9786534414930278, 0.9794045679319603, 0.9802711699622937, 0.981236994142535, 0.9820238829635756, 0.982541494667218, 0.9829085216710844, 0.9836223753863289, 0.9844883731418137, 0.9851571828864559, 0.9853026410683986, 0.9856547299027352, 0.9856163157359911, 0.9861111547506273, 0.9864882745696821, 0.9867735647597727, 0.9873715024014424, 0.9876555286318864, 0.9878852698863448, 0.988244978191828, 0.9885098921373511, 0.9890989029236159, 0.9893443067489014, 0.9896404643093053, 0.9896223002313159, 0.9900647760905373, 0.9903171220108954, 0.9904007039274529, 0.9907418100052958, 0.9910652760635897, 0.9913281602219366, 0.9917059324350371, 0.9919047509562392, 0.991973099390027, 0.9923381423922008, 0.9923325634470983, 0.992452248278859, 0.992656435215679, 0.9928213799294052, 0.9926992419364647, 0.9929893177428183, 0.9931256800861835, 0.993267229724624, 0.993472271458044, 0.9937438678416514, 0.993783598704545, 0.9939487682458552, 0.994130362009505, 0.9943267375732604, 0.9944540638159344, 0.994483951551988, 0.9946308505144362, 0.9947489419335808, 0.9944669889166933, 0.9946391135544357, 0.9948269669048745, 0.9949230937437987, 0.9950543137811836, 0.9950735882854183, 0.9952744647509941, 0.9953446653347182, 0.9954149046835994, 0.9954804730387689, 0.9956218374995978, 0.995659653749638, 0.9957642766099684, 0.9958631430666186, 0.9958909464070156, 0.9959042047074905, 0.9958431960014473, 0.9957271116954203, 0.9958296946435253, 0.9959008428262316, 0.9958472291318436, 0.9959778003363063, 0.9960176673614992, 0.9960911947429963, 0.9960397223275202, 0.9961439853888859, 0.9961460574382327, 0.9961855693414682, 0.9962399535837919, 0.9961147817548245, 0.9962021271087538, 0.996094855574349, 0.9960994876639728, 0.9961954212505167, 0.9963076438313474, 0.9963427618011539, 0.9963790738563326, 0.9964682252942287, 0.9965719910001001, 0.9965665566059724, 0.9963569597689046, 0.9963612637920141, 0.9964780785892833, 0.9964726236715313, 0.9965477142455547, 0.996631766350411, 0.9966768250094875, 0.9967032601555976, 0.9967411694341556, 0.9967352877848576, 0.9967394060651954, 0.996731347811617, 0.9968488012657494, 0.9968980387862332, 0.9969752937311394, 0.9970001172992019, 0.9971048114516347, 0.9971308008947065, 0.9971800737464123, 0.9972503016658887, 0.9972664479698881, 0.9972527443493698, 0.9972874699144328, 0.9972740170406366, 0.9973348506306905, 0.9972672479205626, 0.9972628760696829, 0.9973318825803617, 0.9973422237340902, 0.9974009425371517, 0.9974114365187308, 0.9972632340433283, 0.9972780871095837, 0.9973267489868606, 0.997387015264645, 0.997365960797004, 0.9974434823643624, 0.9974920753043968, 0.997382867773957, 0.9973904633495024, 0.9974796523086699, 0.9975199223719206, 0.9976220477817873, 0.9976763135918438, 0.9976427998797184, 0.9976220493035113, 0.9975680796672778, 0.9976136246417264, 0.9975510857069655, 0.9976100947833277, 0.9976349676579361, 0.9976761767744955, 0.9976897355676342, 0.9977231149520472, 0.9977437446333131, 0.9977223113464524, 0.9977218449176896, 0.9976978957200382, 0.9977398708539168, 0.9977705896508781, 0.9977841189210843, 0.9977210011466229, 0.9977300775025488, 0.9977005991640586, 0.997751715718241, 0.997767132381711, 0.9978021838494223, 0.997866671346833, 0.9978894159768555, 0.9978887096732876, 0.9979092504706647, 0.9979395018941865, 0.9979008458224149, 0.9979319377107616, 0.9979552145279208, 0.9979738107221876, 0.9979811355323218, 0.9979971396261484, 0.9979809550752984, 0.9980134478030627, 0.9980073971404035, 0.9977972456616573, 0.9978057563896092, 0.9978228278094718, 0.9978758391461717, 0.997925902290378, 0.9979474297083991, 0.9980185690905004, 0.9980073004167445, 0.9980677468456582, 0.9980821486316807, 0.9981351102391008, 0.9981451286269554, 0.9981588510583775, 0.9981547306584222, 0.9981816105337565, 0.9981681553627338, 0.9982313398264604, 0.9979234999614615, 0.9979264440829623, 0.9979479173217249, 0.9979578314719054, 0.9980020483247148, 0.9980394905510669, 0.9980143650253719, 0.9980693991110701, 0.9980624591999631, 0.9980938603387902], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.030746666666666658, 0.06427199999999998, 0.11700479999999998, 0.17842431999999997, 0.2387552213333333, 0.29514636586666665, 0.34437839594666664, 0.39400722301866664, 0.4370998340501333, 0.47777651731178666, 0.5132655322472747, 0.5471923123558805, 0.5770864144536257, 0.6041777730082631, 0.6288933290407701, 0.6519639961366932, 0.6716609298563572, 0.6901081702040548, 0.7074706865169826, 0.7221636178652844, 0.7355872560787559, 0.7481618638042136, 0.7589056774237922, 0.7694284430147463, 0.7788855987132717, 0.7879303721752778, 0.7954573349577501, 0.8026182681286417, 0.8084497746491108, 0.8132847971841998, 0.8186629841324464, 0.8231566857192018, 0.8270410171472815, 0.8309769154325534, 0.834959223889298, 0.8386233015003682, 0.8416143046836647, 0.8449995408819649, 0.8477529201271018, 0.8496042947810583, 0.8512038653029524, 0.8533234787726572, 0.8553244642287248, 0.8574320178058522, 0.859088816025267, 0.8600999344227404, 0.8613432743137996, 0.8629556135490863, 0.8637267188608444, 0.8652473803080932, 0.8659226422772839, 0.8659837113828889, 0.8672653402446, 0.8682721395534734, 0.8689115922647928, 0.8696604330383134, 0.8695877230678154, 0.8699489507610338, 0.8703940556849304, 0.8708879834497708, 0.8715191851047938, 0.8709139332609811, 0.8713692066015497, 0.8718322859413947, 0.8725957240139219, 0.8732561516125298, 0.8734372031179435, 0.8736668161394824, 0.8741668011922008, 0.8750701210729807, 0.8758564422990159, 0.8757107980691143, 0.8760197182622029, 0.8761510797693159, 0.876762638459051, 0.8768463746131459, 0.8770550704851645, 0.8772562301033148, 0.8771572737596499, 0.8774548797170182, 0.8778827250786497, 0.8779211192374514, 0.8785023406470396, 0.8784654399156689, 0.8788055625907687, 0.8784983396650251, 0.8789551723651893, 0.8792863217953371, 0.87914435628247, 0.879429920654223, 0.8795935952554673, 0.8798209023965873, 0.8799988121569285, 0.8804255976079024, 0.8804497045137788, 0.8807914007290676, 0.8806055939894941, 0.880558367923878, 0.8806891977981568, 0.8808469446850078, 0.8804689168831736, 0.8805153585281896, 0.8809171560087039, 0.8810387737411669, 0.8811748963670503, 0.881604073397012, 0.8816036660573108, 0.8818432994515797, 0.8822189695064218, 0.8822104058891129, 0.882322698633535, 0.8822637621035148, 0.8823173858931633, 0.8824323139705137, 0.8823090825734623, 0.8822381743161161, 0.8824143568845044, 0.8826662545293873, 0.8827996290764486, 0.8829329995021371, 0.8829596995519233, 0.8831970629300644, 0.8831706899703913, 0.8832402876400188, 0.8835295922093502, 0.8833099663217485, 0.8835789696895737, 0.8837144060539497, 0.8835429654485547, 0.8833353355703658, 0.8833351353466625, 0.8827082884786629, 0.8828374596307966, 0.8834203803343836, 0.8831450089676119, 0.8831505080708507, 0.8833154572637657, 0.8834505782040558, 0.8831588537169835, 0.8831896350119519, 0.8831773381774234, 0.8831929376930143, 0.8832203105903796, 0.8830182795313417, 0.8834497849115409, 0.8832114730870535, 0.8832769924450148, 0.8835092932005133, 0.8836916972137954, 0.8837891941590825, 0.8840502747431742, 0.8843385806021901, 0.8844913892086377, 0.8848022502877739, 0.8842553585923298, 0.8845098227330969, 0.8846588404597872, 0.8845396230804752, 0.8846189941057611, 0.8848637613618515, 0.8849507185589996, 0.884748980036433, 0.8846340820327897, 0.8844240071628441, 0.8844082731132263, 0.884300779135237, 0.8847640345550466, 0.885007631099542, 0.8851202013229211, 0.8851815145239623, 0.8854366964048994, 0.8852396934310762, 0.885569057421302, 0.8857321516791719, 0.8858389365112547, 0.8856950428601292, 0.8858055385741163, 0.8857449847167046, 0.8858238195783674, 0.8858281042871974, 0.8859252938584776, 0.8861727644726298, 0.8861021546920336, 0.8862386058894969, 0.8863214119672138, 0.8858492707704925, 0.8858243436934432, 0.8860685759907655, 0.8862083850583556, 0.8858142132191867, 0.8861261252306013, 0.8861935127075412, 0.8862274947701203, 0.8864314119597749, 0.8866816040971307, 0.8867734436874176, 0.8867760993186758, 0.8868451560534749, 0.886747307114794, 0.8867125764033146, 0.8862146520963164, 0.8862598535533515, 0.8861405348646829, 0.8860731480448814, 0.8861591665737265, 0.8863565832496871, 0.8862275915913851, 0.8862048324322466, 0.8863176825223552, 0.886139247603453, 0.8860319895097744, 0.8864554572254635, 0.8864099115029171, 0.8864222536859587, 0.8862333616506961, 0.8858766921522931, 0.8859023562703972, 0.886018787310024, 0.8860835752456884, 0.8860218843877862, 0.8861796959490075, 0.88633505968744, 0.8862348870520294, 0.8860380650134931, 0.8859275918454772, 0.8860681659942629, 0.88623468272817, 0.8864245477886863, 0.8864620930098177, 0.8866158837088359, 0.8866076286712856, 0.8865201991374904, 0.8864815125570746, 0.8863133613013672, 0.8863620251712304, 0.8856058226541074, 0.88545857372203, 0.8854860496831602, 0.8853641113815109, 0.8851877002433599, 0.8851889302190239, 0.8854300371971214, 0.885313700144076, 0.8853156634630017, 0.8852374304500349, 0.8853670207383648, 0.885176985331195, 0.8854326201314089, 0.8858493581182679, 0.8859177556397744, 0.8859659800757969, 0.8861160487348838, 0.8856377771947287, 0.8856873328085891, 0.8859452661943968, 0.8859107395749571, 0.8858663322841281, 0.885986365722382, 0.8856143958168105, 0.8852929562351294, 0.8852169939449497, 0.8852552945504547], "moving_var_accuracy_train": [0.008372895894809686, 0.017930536282961932, 0.04209712947254865, 0.0741031999912087, 0.10189763368080099, 0.12259796468981735, 0.13445851362347552, 0.14599973740153302, 0.15083010631142277, 0.15288451282846313, 0.1513777492211669, 0.14855927832973984, 0.143963170841429, 0.1376371154604214, 0.13078771693534505, 0.1240904262189199, 0.11640954637273035, 0.10898238134374538, 0.1018162013603452, 0.09436978368156426, 0.08722472065482868, 0.08067069285523654, 0.07427589627279782, 0.06834786163272749, 0.06285849159315106, 0.057775682236814084, 0.052887691733863894, 0.04839606995554677, 0.044157050593665904, 0.04015941670646184, 0.03663473844967253, 0.03329120365357908, 0.03024755145235919, 0.02750461308261724, 0.025063678275481123, 0.022810267288835406, 0.0206964416450712, 0.018819318942332616, 0.01706479897781387, 0.015448615631873186, 0.013994504888106706, 0.012684337704025186, 0.011498916891620093, 0.010435406387451076, 0.009454928464733586, 0.008525938528705762, 0.007718688559865551, 0.006990338512008089, 0.006315727003617143, 0.0057085759342289715, 0.005162349077642575, 0.004647187578692282, 0.004216719980131391, 0.0038192673898644505, 0.003453104457398845, 0.0031250343859346727, 0.002813183045431922, 0.0025468257764436546, 0.0022970610611987357, 0.0020744273223174945, 0.0018778546833848454, 0.001690687180421504, 0.0015266961807247248, 0.001380785554363054, 0.0012511023460509963, 0.001131564857596005, 0.0010208196687181332, 0.0009199500812404245, 0.0008325413572572974, 0.0007560367905441094, 0.0006844588697604533, 0.0006162034055286544, 0.0005556987639011696, 0.0005001421683449125, 0.00045233174236407623, 0.0004083785423489847, 0.0003682732025471437, 0.00033466364710236, 0.00030192332048834604, 0.00027220601783551257, 0.0002461499266372634, 0.00022216654856033053, 0.00020307229706132494, 0.00018330707469237515, 0.00016576575092839757, 0.00014919214523912065, 0.00013603499468915157, 0.0001230046013919285, 0.00011076701468371448, 0.00010073749342241565, 9.160541669798807e-05, 8.306684775457716e-05, 7.604456958403677e-05, 6.879587186499029e-05, 6.195832825410253e-05, 5.696180296961664e-05, 5.126590279431108e-05, 4.626823264546239e-05, 4.20166401274275e-05, 3.8059836941960746e-05, 3.438811245164052e-05, 3.170659696736131e-05, 2.8703289468817683e-05, 2.6013287223219573e-05, 2.3790337512892347e-05, 2.2075185121900486e-05, 1.988187348290693e-05, 1.8139214931005858e-05, 1.6622080092873697e-05, 1.530694234194851e-05, 1.3922155856414881e-05, 1.2537979761671093e-05, 1.1478395532019366e-05, 1.0456066228297744e-05, 1.01259371390556e-05, 9.379985443411716e-06, 8.759586830510225e-06, 7.96679146991344e-06, 7.325080606823599e-06, 6.595916104762636e-06, 6.2994866840864515e-06, 5.713891113274662e-06, 5.1869040971283824e-06, 4.70690657021222e-06, 4.4160711102602665e-06, 3.98733461813818e-06, 3.6871146424576356e-06, 3.4063743644670094e-06, 3.0726941596553705e-06, 2.767006786473182e-06, 2.5238046677434255e-06, 2.3927042959211996e-06, 2.248143217506359e-06, 2.0688874708773985e-06, 1.8878685778228942e-06, 1.8525212749541497e-06, 1.6815735647383152e-06, 1.5620726907329186e-06, 1.4297101076541313e-06, 1.384576170576685e-06, 1.246157194015473e-06, 1.1355921890895925e-06, 1.0486517824988082e-06, 1.0847984851523906e-06, 1.044981534314487e-06, 1.044048019725047e-06, 9.398363240410991e-07, 9.286819688815617e-07, 9.491589408280921e-07, 8.653424929752168e-07, 7.906753318394254e-07, 7.831396085660474e-07, 8.017315431440413e-07, 7.218241825854493e-07, 1.0450192713064483e-06, 9.406840657101506e-07, 9.694269308884934e-07, 8.727520429487592e-07, 8.362241874182762e-07, 8.161845756534234e-07, 7.528386629080687e-07, 6.83844149166008e-07, 6.283937548564143e-07, 5.658657235569454e-07, 5.094317932977144e-07, 4.590730330245406e-07, 5.373235547108215e-07, 5.054102000503583e-07, 5.085841186573041e-07, 4.6327159257377233e-07, 5.155922232989009e-07, 4.70112061329664e-07, 4.449511804337119e-07, 4.448437084562166e-07, 4.027056658061539e-07, 3.64125202163303e-07, 3.385654657674879e-07, 3.063377375111332e-07, 3.090104948697333e-07, 3.1924058313245387e-07, 2.8748854254024486e-07, 3.0159677493075485e-07, 2.723995525816156e-07, 2.76190677820301e-07, 2.495627228826822e-07, 4.222822140332529e-07, 3.820395148246151e-07, 3.651473680431826e-07, 3.61320849380885e-07, 3.291783799115971e-07, 3.5034688257179047e-07, 3.3656365870524193e-07, 4.1024385517731534e-07, 3.6973870457038694e-07, 4.04356868049636e-07, 3.785162831925791e-07, 4.345310489374264e-07, 4.1758094731354196e-07, 3.8593137268610205e-07, 3.512135131338434e-07, 3.4230665653703196e-07, 3.2674509316112337e-07, 3.292706490942388e-07, 3.27682224022971e-07, 3.0048194064224357e-07, 2.8571746816641923e-07, 2.5880028919217754e-07, 2.4294791000711145e-07, 2.2248337274858008e-07, 2.043695075446073e-07, 1.8393451479226427e-07, 1.7070313992630895e-07, 1.694900327107642e-07, 1.6103382982041176e-07, 1.4657781720921042e-07, 1.6777471656489732e-07, 1.517386670404587e-07, 1.4438555229768345e-07, 1.534631160712329e-07, 1.4025586607704125e-07, 1.3728772796775359e-07, 1.6098669107164978e-07, 1.4954388571826108e-07, 1.345939869290053e-07, 1.2493190744809663e-07, 1.2067505432912632e-07, 1.2205617585950124e-07, 1.1855090796225078e-07, 1.1157210911956249e-07, 1.0352726417846204e-07, 9.365741335213159e-08, 8.659685118982513e-08, 8.029462324683083e-08, 8.176715714021829e-08, 7.391993609373666e-08, 4.6400073865741933e-07, 4.182525572041209e-07, 3.790502018688356e-07, 3.664369980503151e-07, 3.5235016391572585e-07, 3.212860150640162e-07, 3.347047187293572e-07, 3.0237709393037705e-07, 3.050233214532029e-07, 2.763876922736005e-07, 2.7399330979098887e-07, 2.474972916687426e-07, 2.244423086190792e-07, 2.0215087701930356e-07, 1.884385385992349e-07, 1.7122405938456598e-07, 1.9003214155389597e-07, 1.023917369741844e-06, 9.216036434303692e-07, 8.335931789339284e-07, 7.511184744047392e-07, 6.936027976156226e-07, 6.368598006818415e-07, 5.788554489866722e-07, 5.482286593856355e-07, 4.938392547426302e-07, 4.5332961294518926e-07], "duration": 247191.629409, "accuracy_train": [0.30501176470588237, 0.3703529411764706, 0.6015529411764706, 0.7525411764705883, 0.8070588235294117, 0.8300235294117647, 0.8204470588235294, 0.8814352941176471, 0.8718588235294118, 0.8900470588235294, 0.8886352941176471, 0.9064235294117647, 0.9110823529411765, 0.9066588235294117, 0.9143294117647058, 0.9311529411764706, 0.9207058823529412, 0.9308, 0.9396941176470588, 0.9307529411764706, 0.933435294117647, 0.9450352941176471, 0.9416470588235294, 0.9480470588235295, 0.9541411764705883, 0.9597176470588236, 0.9550823529411765, 0.9597176470588236, 0.9567058823529412, 0.9513411764705882, 0.9638823529411765, 0.9570117647058823, 0.9596705882352942, 0.9649411764705882, 0.9732235294117647, 0.9734588235294117, 0.9688470588235294, 0.9763058823529411, 0.9723058823529411, 0.9701176470588235, 0.9733647058823529, 0.9762823529411765, 0.9783058823529411, 0.9819529411764706, 0.9805411764705882, 0.9702588235294117, 0.9805176470588235, 0.9823058823529411, 0.9789882352941176, 0.980635294117647, 0.9823529411764705, 0.9709176470588236, 0.9873176470588235, 0.9861647058823529, 0.984635294117647, 0.9865647058823529, 0.9768, 0.9872705882352941, 0.9830588235294118, 0.9852705882352941, 0.9882823529411765, 0.9810117647058824, 0.9861647058823529, 0.9880705882352941, 0.9899294117647058, 0.9891058823529412, 0.9872, 0.9862117647058823, 0.9900470588235294, 0.9922823529411765, 0.9911764705882353, 0.9866117647058823, 0.9888235294117647, 0.9852705882352941, 0.9905647058823529, 0.9898823529411764, 0.9893411764705883, 0.9927529411764706, 0.9902117647058823, 0.9899529411764706, 0.9914823529411765, 0.9908941176470588, 0.9944, 0.9915529411764706, 0.9923058823529411, 0.9894588235294117, 0.9940470588235294, 0.9925882352941177, 0.9911529411764706, 0.9938117647058824, 0.9939764705882352, 0.9936941176470588, 0.9951058823529412, 0.9936941176470588, 0.9925882352941177, 0.9956235294117647, 0.9922823529411765, 0.9935294117647059, 0.9944941176470589, 0.9943058823529412, 0.9916, 0.9956, 0.9943529411764706, 0.9945411764705883, 0.9953176470588235, 0.9961882352941176, 0.9941411764705882, 0.9954352941176471, 0.995764705882353, 0.9960941176470588, 0.9956, 0.9947529411764706, 0.9959529411764706, 0.9958117647058824, 0.9919294117647058, 0.9961882352941176, 0.9965176470588235, 0.9957882352941176, 0.9962352941176471, 0.9952470588235294, 0.9970823529411764, 0.9959764705882352, 0.9960470588235294, 0.9960705882352942, 0.9968941176470588, 0.996, 0.9967058823529412, 0.9967529411764706, 0.9961411764705882, 0.9960235294117648, 0.9952941176470588, 0.9946823529411765, 0.9967529411764706, 0.9965411764705883, 0.9953647058823529, 0.9971529411764706, 0.9963764705882353, 0.9967529411764706, 0.9955764705882353, 0.9970823529411764, 0.996164705882353, 0.9965411764705883, 0.9967294117647059, 0.9949882352941176, 0.9969882352941176, 0.9951294117647059, 0.9961411764705882, 0.9970588235294118, 0.9973176470588235, 0.9966588235294118, 0.9967058823529412, 0.9972705882352941, 0.9975058823529411, 0.9965176470588235, 0.9944705882352941, 0.9964, 0.9975294117647059, 0.9964235294117647, 0.9972235294117647, 0.9973882352941177, 0.9970823529411764, 0.9969411764705882, 0.9970823529411764, 0.9966823529411765, 0.9967764705882353, 0.9966588235294118, 0.9979058823529412, 0.9973411764705883, 0.9976705882352941, 0.9972235294117647, 0.9980470588235294, 0.9973647058823529, 0.9976235294117647, 0.9978823529411764, 0.9974117647058823, 0.9971294117647059, 0.9976, 0.9971529411764706, 0.9978823529411764, 0.9966588235294118, 0.9972235294117647, 0.9979529411764706, 0.9974352941176471, 0.9979294117647058, 0.9975058823529411, 0.9959294117647058, 0.9974117647058823, 0.9977647058823529, 0.9979294117647058, 0.9971764705882353, 0.9981411764705882, 0.9979294117647058, 0.9964, 0.9974588235294117, 0.9982823529411765, 0.9978823529411764, 0.9985411764705883, 0.998164705882353, 0.9973411764705883, 0.9974352941176471, 0.9970823529411764, 0.9980235294117648, 0.9969882352941176, 0.9981411764705882, 0.9978588235294118, 0.9980470588235294, 0.9978117647058824, 0.9980235294117648, 0.9979294117647058, 0.9975294117647059, 0.9977176470588235, 0.9974823529411765, 0.9981176470588236, 0.9980470588235294, 0.9979058823529412, 0.9971529411764706, 0.9978117647058824, 0.9974352941176471, 0.9982117647058824, 0.9979058823529412, 0.9981176470588236, 0.9984470588235295, 0.9980941176470588, 0.9978823529411764, 0.9980941176470588, 0.9982117647058824, 0.9975529411764706, 0.9982117647058824, 0.998164705882353, 0.9981411764705882, 0.9980470588235294, 0.9981411764705882, 0.997835294117647, 0.9983058823529412, 0.9979529411764706, 0.9959058823529412, 0.9978823529411764, 0.9979764705882352, 0.9983529411764706, 0.9983764705882353, 0.9981411764705882, 0.9986588235294117, 0.9979058823529412, 0.9986117647058823, 0.9982117647058824, 0.9986117647058823, 0.9982352941176471, 0.9982823529411765, 0.9981176470588236, 0.9984235294117647, 0.9980470588235294, 0.9988, 0.9951529411764706, 0.9979529411764706, 0.9981411764705882, 0.9980470588235294, 0.9984, 0.9983764705882353, 0.9977882352941176, 0.9985647058823529, 0.998, 0.9983764705882353], "end": "2016-02-08 16:13:21.175000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0], "moving_var_accuracy_valid": [0.008508217599999998, 0.017772927616, 0.04102236861696, 0.07087134868853759, 0.09654257272091543, 0.11550796608483749, 0.12577130454853597, 0.1353613583825647, 0.13853798067389175, 0.1395755156570224, 0.13695319572114176, 0.13361711382586683, 0.12829831850535392, 0.12207396202984186, 0.11536429421682162, 0.10861816591739759, 0.10124807210726937, 0.09418597098455393, 0.08748048664054871, 0.080675378060947, 0.07422958682082786, 0.06822971497378735, 0.062445609256238814, 0.057197605691754444, 0.05228278526773436, 0.04779077808377202, 0.04352159679395332, 0.039630947789459775, 0.03597391122519827, 0.0325869170889118, 0.029588549433673242, 0.02681143467586319, 0.024266083484065393, 0.021978896793466905, 0.019923736139922155, 0.018052191708589453, 0.016327487438112916, 0.01479787711136616, 0.013386319275637496, 0.012078535641057559, 0.01089370970964242, 0.009844773590026761, 0.008896331716182633, 0.008046674583288536, 0.007266711948018386, 0.006549241996939928, 0.00590823084400823, 0.005340804499894211, 0.0048120754805211835, 0.004351679633603407, 0.003920615478786384, 0.0035285874958286808, 0.003190511899098351, 0.002880583512823646, 0.002596205259471404, 0.0023416315960610476, 0.0021075160171132312, 0.0018979387844190395, 0.0017099279715166286, 0.0015411308560968869, 0.0013906035102509314, 0.0012548401273757863, 0.0011312215789698998, 0.0010200294033478315, 0.0009232720022283046, 0.0008348702835225, 0.0007516782719987631, 0.0006769849440558292, 0.000611536315126723, 0.0005577265648771609, 0.0005075186180240672, 0.0004569576663969932, 0.0004121207849285748, 0.0003710640090456764, 0.00033732364442002304, 0.0003036543856695442, 0.0002736809328055688, 0.0002466770262527809, 0.0002220974548490664, 0.00020068483311690992, 0.00018226381468644537, 0.00016405070022067165, 0.00015068599514127772, 0.00013562965060293113, 0.00012310783644969126, 0.00011164652613964386, 0.00010236013856913294, 9.311106421800417e-05, 8.398134565779616e-05, 7.631713418574872e-05, 6.892652514300637e-05, 6.24988894563428e-05, 5.653386745613074e-05, 5.251979310098391e-05, 4.727304407708394e-05, 4.359654640125969e-05, 3.954760906137187e-05, 3.561292086669685e-05, 3.22056768840616e-05, 2.920906591845603e-05, 2.7574304497246194e-05, 2.483628548504768e-05, 2.3805627874672176e-05, 2.1558182942849803e-05, 1.9569128972061114e-05, 1.9269952382275194e-05, 1.7342958637378366e-05, 1.612548024648014e-05, 1.578308413277728e-05, 1.4205435739372265e-05, 1.2898379109483536e-05, 1.1639802829672535e-05, 1.0501702144051705e-05, 9.57040809631752e-06, 8.750041081658858e-06, 7.920288802131878e-06, 7.407622598554077e-06, 7.237932150176605e-06, 6.6742378633931345e-06, 6.166903111088736e-06, 5.556628833907177e-06, 5.508038310059e-06, 4.963494276070373e-06, 4.510739369021595e-06, 4.8129396366439495e-06, 4.765765447523033e-06, 4.940454209882167e-06, 4.611495868052357e-06, 4.4148732118508575e-06, 4.3613773875160486e-06, 3.925240009570226e-06, 7.069148971901194e-06, 6.512400753603025e-06, 8.919329598276081e-06, 8.709861145187864e-06, 7.839147191896957e-06, 7.300106598896838e-06, 6.734414955551082e-06, 6.826902047214095e-06, 6.152739235572018e-06, 5.5388262212696135e-06, 4.987133703122715e-06, 4.495163812401948e-06, 4.412996370505779e-06, 5.647468771722574e-06, 5.593854625765047e-06, 5.073104239597516e-06, 5.051466584684444e-06, 4.845760942768409e-06, 4.446735737554468e-06, 4.615529806306213e-06, 4.902059240761746e-06, 4.622007548525869e-06, 5.029518288368929e-06, 7.2183811984431575e-06, 7.079311069025516e-06, 6.571236507934512e-06, 6.042027908911376e-06, 5.494522954914523e-06, 5.484269746310016e-06, 5.003896758901834e-06, 4.869792966397945e-06, 4.50162763092908e-06, 4.448647926680283e-06, 4.006011176868617e-06, 3.7094046569174324e-06, 5.269914447072733e-06, 5.276976490776084e-06, 4.863327338423133e-06, 4.410828382178117e-06, 4.555805675187699e-06, 4.449516652925888e-06, 4.980890730149872e-06, 4.722199289685782e-06, 4.352606363983805e-06, 4.103694173093397e-06, 3.8032084810696947e-06, 3.4558885597895723e-06, 3.1662341225312516e-06, 2.8497759388459415e-06, 2.649810659852125e-06, 2.9360049376868767e-06, 2.6872761139609383e-06, 2.586118866167374e-06, 2.3892185981124735e-06, 4.156552525074681e-06, 3.746489505099182e-06, 3.9086852900874275e-06, 3.6937359395024315e-06, 4.722705294696325e-06, 5.126036691009382e-06, 4.654302670343339e-06, 4.199265428503244e-06, 4.153578867782719e-06, 4.301585931356626e-06, 3.947337931317595e-06, 3.5526676095822534e-06, 3.2403203422140203e-06, 3.0024580412012785e-06, 2.71306823795994e-06, 4.6731189536599715e-06, 4.2241956037567794e-06, 3.929908588571183e-06, 3.577786581060864e-06, 3.2866006086970138e-06, 3.3087006433536084e-06, 3.1275802102218556e-06, 2.8194840031218973e-06, 2.6521518883475336e-06, 2.6734878820654614e-06, 2.5096777817950815e-06, 3.872634159695104e-06, 3.504040459306014e-06, 3.1550073787155075e-06, 3.1606284497145795e-06, 3.989483784562153e-06, 3.596463228728451e-06, 3.3588225887531293e-06, 3.060717619346548e-06, 2.788897714950249e-06, 2.7341483431513922e-06, 2.6779745298140193e-06, 2.5004880887986873e-06, 2.5990895136009883e-06, 2.449019449904274e-06, 2.3819673266750307e-06, 2.393320998047165e-06, 2.4784275690862998e-06, 2.2432716048459023e-06, 2.231808656301957e-06, 2.0092411014763676e-06, 1.877112301745581e-06, 1.7028709351093682e-06, 1.7870574447623932e-06, 1.6296652503567933e-06, 6.613278947450086e-06, 6.147091284686501e-06, 5.5391765121781765e-06, 5.119079405642539e-06, 4.887259472052159e-06, 4.398547140408149e-06, 4.481885600353581e-06, 4.155505829519982e-06, 3.739989938158821e-06, 3.4210745832037732e-06, 3.2301099103480826e-06, 3.2321200231170746e-06, 3.497050380528661e-06, 4.7103802896982455e-06, 4.2814462492625605e-06, 3.874231990403553e-06, 3.6894942133246716e-06, 5.3792377870927315e-06, 4.863415838169246e-06, 4.975840937980586e-06, 4.488985631231918e-06, 4.057835135417713e-06, 3.781723858567316e-06, 4.648805968568649e-06, 5.1138360137533665e-06, 4.654384838142004e-06, 4.2021487817662775e-06], "accuracy_test": 0.8564, "start": "2016-02-05 19:33:29.546000", "learning_rate_per_epoch": [0.0012340216198936105, 0.0008725850493647158, 0.0007124627009034157, 0.0006170108099468052, 0.0005518712569028139, 0.0005037871887907386, 0.0004664163279812783, 0.0004362925246823579, 0.00041134053026326, 0.000390231900382787, 0.00037207151763141155, 0.00035623135045170784, 0.00034225601120851934, 0.00032980614923872054, 0.0003186230023857206, 0.0003085054049734026, 0.00029929421725682914, 0.0002908616734202951, 0.00028310398920439184, 0.00027593562845140696, 0.0002692856069188565, 0.00026309429085813463, 0.0002573113015387207, 0.0002518935943953693, 0.0002468043239787221, 0.00024201154883485287, 0.00023748757666908205, 0.00023320816399063915, 0.00022915206500329077, 0.0002253004931844771, 0.0002216368302470073, 0.00021814626234117895, 0.0002148155908798799, 0.000211632956052199, 0.00020858772040810436, 0.00020567026513163, 0.0002028719027293846, 0.00020018474606331438, 0.00019760160648729652, 0.0001951159501913935, 0.00019272179633844644, 0.0001904136734083295, 0.0001881865318864584, 0.00018603575881570578, 0.00018395707593299448, 0.00018194655422121286, 0.00018000055570155382, 0.00017811567522585392, 0.0001762887986842543, 0.00017451701569370925, 0.00017279759049415588, 0.00017112800560425967, 0.00016950590361375362, 0.000167929072631523, 0.00016639544628560543, 0.00016490307461936027, 0.0001634501531952992, 0.00016203497943934053, 0.00016065592353697866, 0.0001593115011928603, 0.00015800027176737785, 0.0001567208964843303, 0.00015547210932709277, 0.0001542527024867013, 0.0001530615409137681, 0.0001518975623184815, 0.00015075973351486027, 0.00014964710862841457, 0.00014855874178465456, 0.00014749378897249699, 0.00014645142073277384, 0.00014543083671014756, 0.00014443130930885673, 0.0001434520963812247, 0.00014249254309106618, 0.00014155199460219592, 0.00014062981063034385, 0.0001397254382027313, 0.00013883828069083393, 0.00013796781422570348, 0.00013711351493839175, 0.00013627488806378096, 0.00013545146794058383, 0.00013464280345942825, 0.00013384844351094216, 0.00013306796608958393, 0.00013230100739747286, 0.00013154714542906731, 0.00013080603093840182, 0.00013007730012759566, 0.0001293606183025986, 0.00012865565076936036, 0.00012796207738574594, 0.00012727960711345077, 0.00012660794891417027, 0.00012594679719768465, 0.00012529590458143502, 0.0001246550091309473, 0.0001240238343598321, 0.00012340216198936105, 0.00012278974463697523, 0.000122186349472031, 0.00012159176549175754, 0.00012100577441742644, 0.0001204281797981821, 0.00011985877790721133, 0.00011929737229365855, 0.00011874378833454102, 0.00011819783685496077, 0.00011765934323193505, 0.00011712814739439636, 0.00011660408199531958, 0.00011608698696363717, 0.00011557671678019688, 0.00011507311137393117, 0.00011457603250164539, 0.00011408534192014486, 0.00011360090138623491, 0.00011312257265672088, 0.00011265024659223855, 0.00011218378494959325, 0.00011172307131346315, 0.00011126798199256882, 0.00011081841512350366, 0.00011037424701498821, 0.00010993538307957351, 0.00010950171417789534, 0.00010907313117058948, 0.00010864954674616456, 0.00010823085904121399, 0.00010781696619233117, 0.00010740779543993995, 0.00010700324492063373, 0.00010660323459887877, 0.00010620766988722607, 0.0001058164780260995, 0.0001054295789799653, 0.00010504689271328971, 0.00010466834646649659, 0.00010429386020405218, 0.00010392336844233796, 0.00010355679114582017, 0.0001031940701068379, 0.000102835132565815, 0.00010247992031509057, 0.00010212835331913084, 0.00010178038792219013, 0.0001014359513646923, 0.00010109499271493405, 0.00010075744648929685, 0.0001004232544801198, 0.00010009237303165719, 9.976473666029051e-05, 9.944029443431646e-05, 9.911900269798934e-05, 9.880080324364826e-05, 9.84856451395899e-05, 9.817349200602621e-05, 9.786427835933864e-05, 9.755797509569675e-05, 9.725453128339723e-05, 9.695389599073678e-05, 9.665603283792734e-05, 9.636089816922322e-05, 9.606845560483634e-05, 9.577865421306342e-05, 9.549145761411637e-05, 9.520683670416474e-05, 9.492474055150524e-05, 9.46451400523074e-05, 9.436799155082554e-05, 9.40932659432292e-05, 9.382092684973031e-05, 9.355093789054081e-05, 9.328326268587261e-05, 9.301787940785289e-05, 9.275474440073594e-05, 9.249382856069133e-05, 9.223510278388858e-05, 9.197853796649724e-05, 9.172410500468686e-05, 9.147176751866937e-05, 9.122150368057191e-05, 9.097327711060643e-05, 9.072707325685769e-05, 9.048285573953763e-05, 9.024059545481578e-05, 9.000027785077691e-05, 8.976186654763296e-05, 8.952533971751109e-05, 8.929066825658083e-05, 8.905783761292696e-05, 8.882681868271902e-05, 8.859758963808417e-05, 8.837012137519196e-05, 8.814439934212714e-05, 8.79204017110169e-05, 8.769809937803075e-05, 8.747747051529586e-05, 8.725850784685463e-05, 8.704117499291897e-05, 8.682545740157366e-05, 8.661134052090347e-05, 8.639879524707794e-05, 8.618780702818185e-05, 8.597836131229997e-05, 8.577042899560183e-05, 8.556400280212983e-05, 8.535906090401113e-05, 8.515558147337288e-05, 8.495354995829985e-05, 8.475295180687681e-05, 8.455376519123092e-05, 8.435598283540457e-05, 8.415957563556731e-05, 8.39645363157615e-05, 8.377084304811433e-05, 8.357848855666816e-05, 8.338745101355016e-05, 8.319772314280272e-05, 8.300927584059536e-05, 8.282210910692811e-05, 8.263620111392811e-05, 8.245153730968013e-05, 8.226811041822657e-05, 8.208589861169457e-05, 8.190489461412653e-05, 8.17250765976496e-05, 8.154644456226379e-05, 8.136897668009624e-05, 8.119265839923173e-05, 8.101748971967027e-05, 8.084344153758138e-05, 8.067051385296509e-05, 8.049869211390615e-05, 8.032796176848933e-05, 8.015831554075703e-05, 7.998973887879401e-05, 7.982222450664267e-05, 7.965575059643015e-05, 7.949031714815646e-05, 7.932591688586399e-05, 7.916252070572227e-05, 7.900013588368893e-05, 7.883874786784872e-05, 7.867834210628644e-05, 7.851891859900206e-05, 7.836044824216515e-05, 7.820294558769092e-05, 7.804638153174892e-05, 7.789075607433915e-05, 7.773605466354638e-05, 7.758227729937062e-05, 7.742940215393901e-05, 7.727742922725156e-05, 7.712635124335065e-05, 7.697615365032107e-05, 7.682682917220518e-05, 7.667837053304538e-05, 7.653077045688406e-05, 7.638402166776359e-05, 7.623810961376876e-05], "accuracy_train_first": 0.30501176470588237, "accuracy_train_last": 0.9983764705882353, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.6925333333333333, 0.634, 0.4084, 0.26880000000000004, 0.21826666666666672, 0.19733333333333336, 0.21253333333333335, 0.15933333333333333, 0.1750666666666667, 0.15613333333333335, 0.16733333333333333, 0.14746666666666663, 0.1538666666666667, 0.15200000000000002, 0.1486666666666666, 0.14039999999999997, 0.15106666666666668, 0.1438666666666667, 0.13626666666666665, 0.14559999999999995, 0.14359999999999995, 0.13866666666666672, 0.14439999999999997, 0.1358666666666667, 0.136, 0.1306666666666667, 0.13680000000000003, 0.13293333333333335, 0.13906666666666667, 0.1432, 0.13293333333333335, 0.13639999999999997, 0.138, 0.13360000000000005, 0.12919999999999998, 0.12839999999999996, 0.13146666666666662, 0.12453333333333338, 0.12746666666666662, 0.13373333333333337, 0.13439999999999996, 0.12760000000000005, 0.1266666666666667, 0.12360000000000004, 0.126, 0.13080000000000003, 0.12746666666666662, 0.12253333333333338, 0.1293333333333333, 0.12106666666666666, 0.128, 0.13346666666666662, 0.12119999999999997, 0.1226666666666667, 0.1253333333333333, 0.12360000000000004, 0.13106666666666666, 0.12680000000000002, 0.12560000000000004, 0.1246666666666667, 0.12280000000000002, 0.13453333333333328, 0.12453333333333338, 0.124, 0.12053333333333338, 0.12080000000000002, 0.12493333333333334, 0.12426666666666664, 0.1213333333333333, 0.11680000000000001, 0.11706666666666665, 0.12560000000000004, 0.12119999999999997, 0.1226666666666667, 0.11773333333333336, 0.12239999999999995, 0.12106666666666666, 0.12093333333333334, 0.12373333333333336, 0.11986666666666668, 0.11826666666666663, 0.12173333333333336, 0.11626666666666663, 0.12186666666666668, 0.11813333333333331, 0.12426666666666664, 0.11693333333333333, 0.11773333333333336, 0.12213333333333332, 0.118, 0.11893333333333334, 0.11813333333333331, 0.11839999999999995, 0.11573333333333335, 0.11933333333333329, 0.11613333333333331, 0.12106666666666666, 0.11986666666666668, 0.11813333333333331, 0.11773333333333336, 0.12293333333333334, 0.11906666666666665, 0.11546666666666672, 0.11786666666666668, 0.11760000000000004, 0.11453333333333338, 0.11839999999999995, 0.11599999999999999, 0.11439999999999995, 0.11786666666666668, 0.1166666666666667, 0.11826666666666663, 0.11719999999999997, 0.11653333333333338, 0.11880000000000002, 0.11839999999999995, 0.11599999999999999, 0.11506666666666665, 0.11599999999999999, 0.11586666666666667, 0.11680000000000001, 0.1146666666666667, 0.11706666666666665, 0.11613333333333331, 0.11386666666666667, 0.1186666666666667, 0.11399999999999999, 0.11506666666666665, 0.118, 0.11853333333333338, 0.1166666666666667, 0.12293333333333334, 0.11599999999999999, 0.11133333333333328, 0.11933333333333329, 0.11680000000000001, 0.11519999999999997, 0.11533333333333329, 0.11946666666666672, 0.11653333333333338, 0.11693333333333333, 0.1166666666666667, 0.11653333333333338, 0.11880000000000002, 0.11266666666666669, 0.11893333333333334, 0.11613333333333331, 0.11439999999999995, 0.1146666666666667, 0.11533333333333329, 0.11360000000000003, 0.11306666666666665, 0.11413333333333331, 0.11240000000000006, 0.1206666666666667, 0.11319999999999997, 0.11399999999999999, 0.11653333333333338, 0.1146666666666667, 0.11293333333333333, 0.11426666666666663, 0.11706666666666665, 0.11639999999999995, 0.11746666666666672, 0.11573333333333335, 0.1166666666666667, 0.11106666666666665, 0.11280000000000001, 0.11386666666666667, 0.11426666666666663, 0.11226666666666663, 0.11653333333333338, 0.11146666666666671, 0.11280000000000001, 0.11319999999999997, 0.11560000000000004, 0.11319999999999997, 0.11480000000000001, 0.11346666666666672, 0.11413333333333331, 0.11319999999999997, 0.11160000000000003, 0.11453333333333338, 0.11253333333333337, 0.11293333333333333, 0.11839999999999995, 0.11439999999999995, 0.11173333333333335, 0.11253333333333337, 0.11773333333333336, 0.11106666666666665, 0.11319999999999997, 0.11346666666666672, 0.11173333333333335, 0.11106666666666665, 0.11240000000000006, 0.11319999999999997, 0.11253333333333337, 0.11413333333333331, 0.11360000000000003, 0.11826666666666663, 0.11333333333333329, 0.11493333333333333, 0.11453333333333338, 0.11306666666666665, 0.11186666666666667, 0.11493333333333333, 0.11399999999999999, 0.11266666666666669, 0.11546666666666672, 0.11493333333333333, 0.10973333333333335, 0.11399999999999999, 0.11346666666666672, 0.11546666666666672, 0.11733333333333329, 0.11386666666666667, 0.11293333333333333, 0.11333333333333329, 0.11453333333333338, 0.11240000000000006, 0.11226666666666663, 0.1146666666666667, 0.11573333333333335, 0.11506666666666665, 0.11266666666666669, 0.11226666666666663, 0.11186666666666667, 0.11319999999999997, 0.11199999999999999, 0.11346666666666672, 0.11426666666666663, 0.11386666666666667, 0.11519999999999997, 0.11319999999999997, 0.12119999999999997, 0.11586666666666667, 0.11426666666666663, 0.11573333333333335, 0.11639999999999995, 0.11480000000000001, 0.11240000000000006, 0.11573333333333335, 0.1146666666666667, 0.11546666666666672, 0.11346666666666672, 0.11653333333333338, 0.11226666666666663, 0.11040000000000005, 0.11346666666666672, 0.11360000000000003, 0.11253333333333337, 0.1186666666666667, 0.11386666666666667, 0.11173333333333335, 0.11439999999999995, 0.11453333333333338, 0.11293333333333333, 0.11773333333333336, 0.11760000000000004, 0.11546666666666672, 0.11439999999999995], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.06252054538101597, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.0012340216173942088, "optimization": "rmsprop", "nb_data_augmentation": 4, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.93726826718368e-06, "rotation_range": [0, 0], "momentum": 0.7870393899639447}, "accuracy_valid_max": 0.8902666666666667, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8856, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.30746666666666667, 0.366, 0.5916, 0.7312, 0.7817333333333333, 0.8026666666666666, 0.7874666666666666, 0.8406666666666667, 0.8249333333333333, 0.8438666666666667, 0.8326666666666667, 0.8525333333333334, 0.8461333333333333, 0.848, 0.8513333333333334, 0.8596, 0.8489333333333333, 0.8561333333333333, 0.8637333333333334, 0.8544, 0.8564, 0.8613333333333333, 0.8556, 0.8641333333333333, 0.864, 0.8693333333333333, 0.8632, 0.8670666666666667, 0.8609333333333333, 0.8568, 0.8670666666666667, 0.8636, 0.862, 0.8664, 0.8708, 0.8716, 0.8685333333333334, 0.8754666666666666, 0.8725333333333334, 0.8662666666666666, 0.8656, 0.8724, 0.8733333333333333, 0.8764, 0.874, 0.8692, 0.8725333333333334, 0.8774666666666666, 0.8706666666666667, 0.8789333333333333, 0.872, 0.8665333333333334, 0.8788, 0.8773333333333333, 0.8746666666666667, 0.8764, 0.8689333333333333, 0.8732, 0.8744, 0.8753333333333333, 0.8772, 0.8654666666666667, 0.8754666666666666, 0.876, 0.8794666666666666, 0.8792, 0.8750666666666667, 0.8757333333333334, 0.8786666666666667, 0.8832, 0.8829333333333333, 0.8744, 0.8788, 0.8773333333333333, 0.8822666666666666, 0.8776, 0.8789333333333333, 0.8790666666666667, 0.8762666666666666, 0.8801333333333333, 0.8817333333333334, 0.8782666666666666, 0.8837333333333334, 0.8781333333333333, 0.8818666666666667, 0.8757333333333334, 0.8830666666666667, 0.8822666666666666, 0.8778666666666667, 0.882, 0.8810666666666667, 0.8818666666666667, 0.8816, 0.8842666666666666, 0.8806666666666667, 0.8838666666666667, 0.8789333333333333, 0.8801333333333333, 0.8818666666666667, 0.8822666666666666, 0.8770666666666667, 0.8809333333333333, 0.8845333333333333, 0.8821333333333333, 0.8824, 0.8854666666666666, 0.8816, 0.884, 0.8856, 0.8821333333333333, 0.8833333333333333, 0.8817333333333334, 0.8828, 0.8834666666666666, 0.8812, 0.8816, 0.884, 0.8849333333333333, 0.884, 0.8841333333333333, 0.8832, 0.8853333333333333, 0.8829333333333333, 0.8838666666666667, 0.8861333333333333, 0.8813333333333333, 0.886, 0.8849333333333333, 0.882, 0.8814666666666666, 0.8833333333333333, 0.8770666666666667, 0.884, 0.8886666666666667, 0.8806666666666667, 0.8832, 0.8848, 0.8846666666666667, 0.8805333333333333, 0.8834666666666666, 0.8830666666666667, 0.8833333333333333, 0.8834666666666666, 0.8812, 0.8873333333333333, 0.8810666666666667, 0.8838666666666667, 0.8856, 0.8853333333333333, 0.8846666666666667, 0.8864, 0.8869333333333334, 0.8858666666666667, 0.8876, 0.8793333333333333, 0.8868, 0.886, 0.8834666666666666, 0.8853333333333333, 0.8870666666666667, 0.8857333333333334, 0.8829333333333333, 0.8836, 0.8825333333333333, 0.8842666666666666, 0.8833333333333333, 0.8889333333333334, 0.8872, 0.8861333333333333, 0.8857333333333334, 0.8877333333333334, 0.8834666666666666, 0.8885333333333333, 0.8872, 0.8868, 0.8844, 0.8868, 0.8852, 0.8865333333333333, 0.8858666666666667, 0.8868, 0.8884, 0.8854666666666666, 0.8874666666666666, 0.8870666666666667, 0.8816, 0.8856, 0.8882666666666666, 0.8874666666666666, 0.8822666666666666, 0.8889333333333334, 0.8868, 0.8865333333333333, 0.8882666666666666, 0.8889333333333334, 0.8876, 0.8868, 0.8874666666666666, 0.8858666666666667, 0.8864, 0.8817333333333334, 0.8866666666666667, 0.8850666666666667, 0.8854666666666666, 0.8869333333333334, 0.8881333333333333, 0.8850666666666667, 0.886, 0.8873333333333333, 0.8845333333333333, 0.8850666666666667, 0.8902666666666667, 0.886, 0.8865333333333333, 0.8845333333333333, 0.8826666666666667, 0.8861333333333333, 0.8870666666666667, 0.8866666666666667, 0.8854666666666666, 0.8876, 0.8877333333333334, 0.8853333333333333, 0.8842666666666666, 0.8849333333333333, 0.8873333333333333, 0.8877333333333334, 0.8881333333333333, 0.8868, 0.888, 0.8865333333333333, 0.8857333333333334, 0.8861333333333333, 0.8848, 0.8868, 0.8788, 0.8841333333333333, 0.8857333333333334, 0.8842666666666666, 0.8836, 0.8852, 0.8876, 0.8842666666666666, 0.8853333333333333, 0.8845333333333333, 0.8865333333333333, 0.8834666666666666, 0.8877333333333334, 0.8896, 0.8865333333333333, 0.8864, 0.8874666666666666, 0.8813333333333333, 0.8861333333333333, 0.8882666666666666, 0.8856, 0.8854666666666666, 0.8870666666666667, 0.8822666666666666, 0.8824, 0.8845333333333333, 0.8856], "seed": 382581526, "model": "residualv5", "loss_std": [0.2675301432609558, 0.14780420064926147, 0.12976835668087006, 0.11423981934785843, 0.10835011303424835, 0.10119914263486862, 0.09935465455055237, 0.09332971274852753, 0.09155622124671936, 0.08884236961603165, 0.08684537559747696, 0.08330781012773514, 0.07946543395519257, 0.0775526762008667, 0.07602768391370773, 0.07434386014938354, 0.07060938328504562, 0.07022598385810852, 0.06569232046604156, 0.06537897884845734, 0.061381492763757706, 0.06046184152364731, 0.057699136435985565, 0.05590134486556053, 0.052849188446998596, 0.05357300117611885, 0.049879997968673706, 0.04816417768597603, 0.0494731180369854, 0.04570809751749039, 0.04488063231110573, 0.0432451032102108, 0.04095424711704254, 0.03988732397556305, 0.03684890642762184, 0.035984061658382416, 0.03703494369983673, 0.03354107588529587, 0.033071644604206085, 0.03130917623639107, 0.03021862730383873, 0.02921554446220398, 0.02773069404065609, 0.02709287591278553, 0.02699662558734417, 0.027206141501665115, 0.025901885703206062, 0.02472720481455326, 0.023662960156798363, 0.023717675358057022, 0.02175697311758995, 0.02144702523946762, 0.02293276973068714, 0.021136362105607986, 0.020735103636980057, 0.021330688148736954, 0.02011982910335064, 0.018918678164482117, 0.01890762522816658, 0.017375513911247253, 0.017851954326033592, 0.01681876741349697, 0.01788926310837269, 0.017451170831918716, 0.01708367094397545, 0.016571221873164177, 0.016040107235312462, 0.014686372131109238, 0.014787852764129639, 0.016105275601148605, 0.01576729491353035, 0.01440454926341772, 0.013830321840941906, 0.01356832217425108, 0.01492240559309721, 0.013745337724685669, 0.01380095910280943, 0.013543564826250076, 0.013165741227567196, 0.012850373983383179, 0.013270825147628784, 0.013144991360604763, 0.01262801792472601, 0.012223675847053528, 0.01154599990695715, 0.012101319618523121, 0.01199853140860796, 0.011124138720333576, 0.012164578773081303, 0.011922360397875309, 0.010763362050056458, 0.010732180438935757, 0.01051650196313858, 0.01061682403087616, 0.011285974644124508, 0.010434892028570175, 0.010557188652455807, 0.010247808881103992, 0.010862637311220169, 0.010444278828799725, 0.009633451700210571, 0.009998654015362263, 0.010083829052746296, 0.009220602922141552, 0.00969642587006092, 0.009696689434349537, 0.009498695842921734, 0.009473045356571674, 0.010577566921710968, 0.009325613267719746, 0.00910108257085085, 0.009080402553081512, 0.009802768938243389, 0.008224773220717907, 0.0089154914021492, 0.008753211237490177, 0.009129652753472328, 0.007910968735814095, 0.008014251478016376, 0.008954852819442749, 0.008077013306319714, 0.007634023670107126, 0.0074848271906375885, 0.007803968619555235, 0.0081710796803236, 0.007055243011564016, 0.007408228702843189, 0.007863737642765045, 0.007524475455284119, 0.007600930519402027, 0.007296664174646139, 0.007505771238356829, 0.007026678416877985, 0.007231502793729305, 0.007376598194241524, 0.007341116201132536, 0.0066057369112968445, 0.006817901041358709, 0.006559216883033514, 0.00683511421084404, 0.0066702705807983875, 0.006677665747702122, 0.006917568854987621, 0.006985141895711422, 0.005989939905703068, 0.006466117687523365, 0.007119057234376669, 0.006074265111237764, 0.006161937955766916, 0.0063313585706055164, 0.005942344665527344, 0.006442362908273935, 0.006686541251838207, 0.006733722519129515, 0.006719372700899839, 0.007177797146141529, 0.005606361664831638, 0.005920935422182083, 0.005858746822923422, 0.005922986194491386, 0.0064994837157428265, 0.0060071879997849464, 0.005704646930098534, 0.005672763101756573, 0.006531130522489548, 0.005641339346766472, 0.005426404066383839, 0.0054116933606565, 0.005972008686512709, 0.00523739168420434, 0.0051965778693556786, 0.005501235369592905, 0.005645885597914457, 0.005942083429545164, 0.004976780619472265, 0.0052291820757091045, 0.005634596571326256, 0.005016591865569353, 0.0056283376179635525, 0.005168368574231863, 0.004910132382065058, 0.0054165758192539215, 0.0054958355613052845, 0.005313713103532791, 0.005400140304118395, 0.005581764504313469, 0.004664037376642227, 0.005264958832412958, 0.0047551426105201244, 0.0050376830622553825, 0.004890107084065676, 0.004771765787154436, 0.005246013402938843, 0.0047064791433513165, 0.004571814555674791, 0.00475521432235837, 0.004533496219664812, 0.0050339470617473125, 0.004315726459026337, 0.00484099006280303, 0.004773624241352081, 0.004324354697018862, 0.0046391915529966354, 0.004444220568984747, 0.0045265574008226395, 0.004650043789297342, 0.004784223157912493, 0.004725825507193804, 0.004347709473222494, 0.004861998837441206, 0.004527045879513025, 0.0049985311925411224, 0.004163644276559353, 0.00414213677868247, 0.004198284354060888, 0.003873686073347926, 0.004170579835772514, 0.0037305313162505627, 0.004643953870981932, 0.004406208172440529, 0.003761235624551773, 0.003938049543648958, 0.004028669558465481, 0.004350114613771439, 0.004171987064182758, 0.0037404168397188187, 0.003964600618928671, 0.0035426791291683912, 0.003927212208509445, 0.003608281956985593, 0.003868668107315898, 0.004297048319131136, 0.004013546742498875, 0.004080811515450478, 0.0035070236772298813, 0.007672596722841263, 0.003960847854614258, 0.003843466052785516, 0.0033520597498863935, 0.0036065089516341686, 0.003520761616528034, 0.003639098023995757, 0.0036475681699812412, 0.0038560880348086357, 0.0032070428133010864, 0.0036243698559701443, 0.003711555851623416, 0.003225884633138776, 0.003402693197131157, 0.003951051738113165, 0.003538870019838214, 0.0037083826027810574, 0.0031760833226144314, 0.0034319027327001095, 0.003905851161107421, 0.003661869326606393, 0.003969745244830847, 0.0037813305389136076, 0.003229213412851095, 0.0037485200446099043, 0.003898607101291418, 0.0034935367293655872]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:46 2016", "state": "available"}], "summary": "bdbcfd1cd6b0c956ec8ce2af5c4e06cc"}