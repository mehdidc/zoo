{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 16, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.027621030203442445, 0.028366062549646395, 0.033169706835390916, 0.030734829088257892, 0.03490868957817202, 0.034391084024957864, 0.03252900613819737, 0.0360931061610298, 0.039084092204161265, 0.03917844712351559, 0.03948291155293376, 0.039149029088459394, 0.03844638973792983, 0.03656580636962807, 0.03822892742450042, 0.038336747927274914, 0.04059583467859826, 0.03986272527921122, 0.038892363209179305, 0.03814411642333589, 0.03808508892713775, 0.038548425048645626, 0.03875309426593799, 0.03641838856302741, 0.03832278392462798, 0.03639272224084183, 0.03823343590252652, 0.03704496461696823, 0.038351889571085165, 0.03827920245561557, 0.03841641112128682, 0.04096887420041561, 0.04243914959257085, 0.042951645403818496, 0.04432944611478178, 0.04306238783780429, 0.04386057991275051, 0.044104584560714165, 0.04429198019250372, 0.04505048224704305, 0.04608256183620797, 0.044698116419032784, 0.044667053145521204, 0.04511246155278348, 0.045393318965517244, 0.04537013055022922, 0.04465730335208167, 0.04606543183307492, 0.045126133823835846, 0.044788746488565005, 0.04537952728427537, 0.0452644331884336, 0.04570858525379763, 0.04574528758926897, 0.046029773122931134, 0.04596922797604728, 0.04525160461063095, 0.04555829436019751, 0.044555005319752546, 0.04438527912056383, 0.045115075689653124, 0.04583384706995979, 0.04587578866725614, 0.04644571562427871, 0.04633307427261571, 0.04620680985219285, 0.04596745183370649, 0.04642305288637341, 0.0460157779064592, 0.04623762348913901, 0.04635891208948278, 0.046008482937305493, 0.045810881640415045, 0.04597869957642101, 0.045879743442538465, 0.04539951386409676, 0.04519383094723276, 0.04537013055022921, 0.045300494086310004, 0.04546281324760074, 0.04465750649448943, 0.0446103526682392, 0.04471008930113091, 0.04490768602169155, 0.044651614989441084, 0.04437710285259225, 0.04454848934617796, 0.04477497119464813, 0.04403294672846858, 0.043927336151032455, 0.043518175878590554, 0.04383409730125837, 0.044005949400609506, 0.044269649356996704, 0.04453036178109529, 0.04438997979271138, 0.04453219524102662, 0.04455398726169833, 0.045212294447803836, 0.044973896573384214, 0.044948271741551894, 0.04516491640213832, 0.044901423267327545, 0.04471353851965805, 0.044754908407403406, 0.0446899973863123, 0.044606285349424374, 0.04420730926027158, 0.044322282950567136, 0.043724686985060435, 0.044003475534811584, 0.043848788850185144, 0.04369147825215209, 0.04377279489662732, 0.04390771249907449, 0.04380656325280491, 0.044278869874239665, 0.0440751612682676, 0.04406445701806327, 0.043974397186822375, 0.043787714191307794, 0.043565678904598164, 0.04340880523611942, 0.04329915833194401, 0.043565678904598164, 0.04329915833194401, 0.04329915833194401, 0.04349836764035543, 0.04340880523611942, 0.04329915833194401, 0.04344473587923887, 0.04323143209762692, 0.04361396220866936, 0.04341486539386521, 0.043347738503588, 0.043347738503588, 0.043347738503588, 0.043347738503588, 0.04372883630342703, 0.04341486539386521, 0.043529848091040615, 0.043529848091040615, 0.043529848091040615, 0.043463316236662435, 0.043463316236662435, 0.043529848091040615, 0.043529848091040615, 0.04353589139973085, 0.043619993863815765, 0.043468951404159385, 0.04344473587923887, 0.043511713128866966, 0.043511713128866966, 0.0436264405730109, 0.0436264405730109, 0.0436264405730109, 0.043560056250302334, 0.04365055529578973, 0.043505666461608514, 0.043505666461608514, 0.043505666461608514, 0.04359586223621435, 0.04359586223621435, 0.04359586223621435, 0.04359586223621435, 0.04359586223621435, 0.04359586223621435, 0.04359586223621435, 0.043505666461608514, 0.04359586223621435, 0.04359586223621435, 0.04359586223621435, 0.04359586223621435, 0.04359586223621435, 0.043564845964567216, 0.043564845964567216, 0.043564845964567216, 0.043564845964567216, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837, 0.04358170989742837], "moving_avg_accuracy_train": [0.011954066265060239, 0.021573795180722888, 0.030965737951807223, 0.04426129518072289, 0.058173362198795174, 0.07200022778614457, 0.0854939173569277, 0.09856067772966866, 0.11122672968561745, 0.12342625174717618, 0.13509059193390435, 0.14613207867424885, 0.15662476312610107, 0.1665858749159006, 0.1760520991712985, 0.18499056395296382, 0.19346110469622166, 0.20163993775069586, 0.20938680617442146, 0.21681314814734076, 0.2238286518567031, 0.23054499600838216, 0.23688855740151984, 0.2429201459384763, 0.24850859068197806, 0.25383939577040676, 0.2587924290849324, 0.26347606268246326, 0.2678537011431326, 0.27202653886014466, 0.27595858000425066, 0.2797068485098497, 0.2832238330865756, 0.28661266965743615, 0.2897520427519335, 0.29270454932011364, 0.295550058243524, 0.298242793383027, 0.3007486157013508, 0.3031662240107338, 0.3054785549229134, 0.3076514260872486, 0.3097576125447888, 0.3117096562601894, 0.3135512094594717, 0.3152697895677414, 0.3168847533820516, 0.31840881569444884, 0.3198675387936787, 0.3212768692516602, 0.3226111552180605, 0.3239061390938448, 0.32510692202180974, 0.32622763042203845, 0.3272974502111599, 0.32834500187679094, 0.3293372147915215, 0.3303031544569477, 0.3312125039208915, 0.33210857280591083, 0.332957391730139, 0.33375897936435406, 0.334548649952015, 0.3352805319447653, 0.335972170015349, 0.3366158227427298, 0.33727747089014354, 0.3378870731987196, 0.3385274886198115, 0.33912504096264967, 0.33972166713746904, 0.340225686417698, 0.3407310733482174, 0.3412000405615884, 0.3416762337945862, 0.34211892668018784, 0.3425455882290365, 0.34295076208685576, 0.3433342438600979, 0.34372173438372666, 0.3440869479935468, 0.3444274060556379, 0.34475734993802587, 0.3450590057574763, 0.3453422618082347, 0.34563248969367627, 0.3459031074411761, 0.3461725482030826, 0.3464032790755454, 0.34665329378847276, 0.3468783070301074, 0.3470855252728798, 0.3472884938299291, 0.34748057818187594, 0.3476511009359775, 0.34780221825201835, 0.34794057699910563, 0.34808863149799024, 0.3482430590108418, 0.3483514526579504, 0.348467832241553, 0.3485937523306507, 0.34870708041083864, 0.3488067225203572, 0.34889404725627327, 0.349000877470405, 0.3490946715004729, 0.3491861456154859, 0.34927317864429874, 0.3493609210208327, 0.3494445954850145, 0.34953402147868173, 0.3496239175235846, 0.3497071771266479, 0.34977975760675417, 0.34984743320150047, 0.3499295197006275, 0.35000104438719126, 0.3500771824183517, 0.3501504129716972, 0.35023043944561183, 0.350302463272135, 0.35037669736660826, 0.3504458612142848, 0.35051046183984425, 0.3505756618907996, 0.3506343419366594, 0.3506871539779332, 0.3507535101162845, 0.3508202901287524, 0.35087803897732295, 0.3509347192663376, 0.35099279101440267, 0.3510450555876612, 0.35108268105299145, 0.35111183764648746, 0.3511451380685857, 0.35117275528582353, 0.35119996394398817, 0.35122680489898694, 0.35126508073438945, 0.3512924694982999, 0.3513171193858193, 0.3513393042845867, 0.35136397701877864, 0.35137912299159957, 0.35140452018039137, 0.3514273776503041, 0.35144559621057486, 0.3514643460774691, 0.3514835741203246, 0.3514961730335934, 0.35150986521818584, 0.35153160083492146, 0.35154410340203174, 0.3515624152003828, 0.35158595530685055, 0.3516094945653221, 0.3516165609220429, 0.3516252738057422, 0.3516354685637222, 0.3516493501712054, 0.3516712562685427, 0.3516956780814474, 0.3517153045504111, 0.351735321535129, 0.35175098365872454, 0.3517650795699605, 0.35177541272742224, 0.35178471256913785, 0.3517930824266819, 0.35180296846112213, 0.35180951272946775, 0.35182010889628, 0.3518272922837605, 0.3518314041698422, 0.351839811192617, 0.3518497306757649, 0.35185395188529683, 0.35186245729917676, 0.3518701121716687, 0.35187700155691143, 0.35188320200362994, 0.35188878240567656, 0.3518938047675185, 0.3518983248931763, 0.3518976866809671, 0.3518994654526294, 0.3519081258350773, 0.35191356701662985, 0.3519161109173765, 0.35191604726539794, 0.35191598997861717, 0.3519159384205145, 0.3519135388555715, 0.35191137924712274, 0.35190943559951887, 0.3519076863166754, 0.3519061119621163, 0.35190704820566365, 0.3519055376622057, 0.35190653133574423, 0.3519074256419289, 0.3519082305174951, 0.35190895490550467, 0.35191196001736386, 0.3519146646180371, 0.351917098758643, 0.35191928948518836, 0.3519212611390792, 0.35192303562758087, 0.3519246326672324, 0.3519260700029188, 0.35192736360503657, 0.35192852784694256, 0.35192957566465793, 0.35193051870060177, 0.3519313674329512, 0.35192977812941517, 0.3519283477562327, 0.3519270604203685, 0.3519259018180907, 0.35192485907604065, 0.35192392060819566, 0.3519230759871351, 0.3519223158281807, 0.3519216316851217, 0.3519210159563686, 0.35192046180049075], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.0012860973024250615, 0.0019903402318800804, 0.0025851835098299825, 0.003917611737093328, 0.005267761041822008, 0.006461624845375449, 0.007454179284931858, 0.008245423396186374, 0.008864740905924885, 0.009317721862106521, 0.009610461163821363, 0.009746644912374059, 0.009762848264191926, 0.009679577170572683, 0.009518104068396765, 0.009285359035434733, 0.009002573676239979, 0.008704356099812623, 0.00837404622320201, 0.008032996596770496, 0.007672652567758133, 0.007311370819856465, 0.006942400678207381, 0.006575581152898943, 0.006199099469469602, 0.0058349468685399974, 0.005472245032819201, 0.0051224483426205595, 0.004782676974789487, 0.004461122448823202, 0.004154158731971364, 0.0038651885098848183, 0.0035899922835126857, 0.0033343509748974317, 0.003089616848245773, 0.0028591108187375177, 0.002646072026162636, 0.0024467222263300017, 0.002258562313116089, 0.0020853095512428596, 0.001924900464345367, 0.001774902739782024, 0.0016373366583491533, 0.0015078972645157541, 0.0013876294017362593, 0.001275448119859496, 0.0011713762809673284, 0.0010751435462592225, 0.00098678004935534, 0.0009059779554779557, 0.0008314030312913538, 0.0007633555773090907, 0.0006999969363390096, 0.0006413011285701965, 0.0005874716451439398, 0.0005386007610590424, 0.0004936010631665608, 0.0004526383117850972, 0.0004148167286347611, 0.0003805615107915823, 0.00034898980180757527, 0.0003198737062447556, 0.0002934985523534316, 0.00026896955837989785, 0.0002463778715280349, 0.00022546868387641363, 0.00020686181992755692, 0.000189520172706392, 0.00017425934263990363, 0.00016004702759779525, 0.00014724598997033206, 0.00013480770988688192, 0.00012362568244405166, 0.00011324248642459941, 0.00010395907773851541, 9.532696288332447e-05, 8.743262729038565e-05, 8.016685725688823e-05, 7.347369596487978e-05, 6.747766652151066e-05, 6.193032869654006e-05, 5.678050105527153e-05, 5.208221763947188e-05, 4.769296197619947e-05, 4.364577169120071e-05, 4.0039284551471714e-05, 3.6694461783681536e-05, 3.3678400522904114e-05, 3.078969109018081e-05, 2.8273288191284213e-05, 2.5901638002354203e-05, 2.3697928803357877e-05, 2.1698902039378435e-05, 1.9861079419806155e-05, 1.813667356482297e-05, 1.652853419720706e-05, 1.5047969063546513e-05, 1.3740453368951648e-05, 1.258103874258621e-05, 1.1428677512929099e-05, 1.0407707628951712e-05, 9.509639685601926e-06, 8.674265000873494e-06, 7.896195450689892e-06, 7.1752063911462296e-06, 6.560400003894447e-06, 5.983535884192463e-06, 5.460489919229839e-06, 4.9826136602459084e-06, 4.5536408159798005e-06, 4.1612894779868045e-06, 3.817133605278458e-06, 3.508151934753287e-06, 3.219726194798165e-06, 2.94516491015049e-06, 2.6918682942537905e-06, 2.4833252048788432e-06, 2.2810347114833527e-06, 2.1051042384358466e-06, 1.94285824008193e-06, 1.806210544818599e-06, 1.6722763746200757e-06, 1.5546450441984325e-06, 1.4422332802073811e-06, 1.3355691195907225e-06, 1.2402716274328522e-06, 1.1472345947285404e-06, 1.0576131405872546e-06, 9.914800604005515e-07, 9.324681849474595e-07, 8.69235732053724e-07, 8.11226055313455e-07, 7.60454401092064e-07, 7.089932315439203e-07, 6.508349891613996e-07, 5.934024527438996e-07, 5.440424704767991e-07, 4.965026196207641e-07, 4.535151573707672e-07, 4.146475734209067e-07, 3.863681722606586e-07, 3.54482654531475e-07, 3.245029416708028e-07, 2.96482175103629e-07, 2.72312651905818e-07, 2.471459911494671e-07, 2.2823654682128063e-07, 2.101150675164285e-07, 1.9209080420985268e-07, 1.7604574136586045e-07, 1.6176862591774978e-07, 1.470203568659551e-07, 1.3400560444958755e-07, 1.2485697731853477e-07, 1.1377810724580059e-07, 1.0541819415086455e-07, 9.986360424839267e-08, 9.486411402807751e-08, 8.582710320102037e-08, 7.79276219621401e-08, 7.107025757836286e-08, 6.569752305738537e-08, 6.344666465660377e-08, 6.246982270094349e-08, 5.968962498669338e-08, 5.7326779582803594e-08, 5.380182066421886e-08, 5.020989101995038e-08, 4.6149869206124866e-08, 4.2313265788930026e-08, 3.871242984780531e-08, 3.572078995560781e-08, 3.2534157993661774e-08, 3.029125095431477e-08, 2.772653536012915e-08, 2.5106050288462337e-08, 2.3231547547042726e-08, 2.179395810563908e-08, 1.9774929784288175e-08, 1.8448515393283717e-08, 1.713103750976952e-08, 1.584510642000245e-08, 1.4606605633582327e-08, 1.342621305324406e-08, 1.2310608814166012e-08, 1.1263431756409104e-08, 1.0140754414183915e-08, 9.155155230405793e-09, 8.914659724661232e-09, 8.289651862381538e-09, 7.518929555222813e-09, 6.767073063869969e-09, 6.090395293460219e-09, 5.4813796882557664e-09, 4.9850629266720044e-09, 4.528531811870616e-09, 4.1096785247547796e-09, 3.72625058647694e-09, 3.3759328583293167e-09, 3.046228540316964e-09, 2.7621413601300196e-09, 2.494813708026348e-09, 2.2525303891901908e-09, 2.033107772363942e-09, 1.834519637022623e-09, 1.7323439488955524e-09, 1.624943337221862e-09, 1.5157743679045667e-09, 1.4073904762821045e-09, 1.3016382002399369e-09, 1.1998136652006605e-09, 1.1027871195182997e-09, 1.011101812445023e-09, 9.250522891521294e-10, 8.447461931777066e-10, 7.701528715419772e-10, 7.011414355102527e-10, 6.375104113684439e-10, 5.964923418000201e-10, 5.552568145905214e-10, 5.146462357776019e-10, 4.752628453432355e-10, 4.375223596550812e-10, 4.01696620755002e-10, 3.6794742130252403e-10, 3.363532538969369e-10, 3.069303940342734e-10, 2.796494517077921e-10, 2.544483051693432e-10], "duration": 37433.847074, "accuracy_train": [0.11954066265060241, 0.10815135542168675, 0.11549322289156627, 0.16392131024096385, 0.18338196536144577, 0.19644201807228914, 0.20693712349397592, 0.21616152108433734, 0.22522119728915663, 0.23322195030120482, 0.24006965361445784, 0.2455054593373494, 0.25105892319277107, 0.2562358810240964, 0.2612481174698795, 0.26543674698795183, 0.2696959713855422, 0.27524943524096385, 0.27910862198795183, 0.28365022590361444, 0.28696818524096385, 0.29099209337349397, 0.29398060993975905, 0.2972044427710843, 0.29880459337349397, 0.3018166415662651, 0.30336972891566266, 0.30562876506024095, 0.3072524472891566, 0.309582078313253, 0.3113469503012048, 0.31344126506024095, 0.31487669427710846, 0.3171121987951807, 0.31800640060240964, 0.3192771084337349, 0.32115963855421686, 0.3224774096385542, 0.3233010165662651, 0.3249246987951807, 0.3262895331325301, 0.3272072665662651, 0.3287132906626506, 0.3292780496987952, 0.33012518825301207, 0.3307370105421687, 0.3314194277108434, 0.3321253765060241, 0.332996046686747, 0.33396084337349397, 0.33461972891566266, 0.3355609939759036, 0.33591396837349397, 0.3363140060240964, 0.336925828313253, 0.3377729668674699, 0.3382671310240964, 0.33899661144578314, 0.33939664909638556, 0.3401731927710843, 0.3405967620481928, 0.34097326807228917, 0.34165568524096385, 0.34186746987951805, 0.3421969126506024, 0.3424086972891566, 0.34323230421686746, 0.3433734939759036, 0.3442912274096386, 0.3445030120481928, 0.3450913027108434, 0.34476185993975905, 0.34527955572289154, 0.3454207454819277, 0.34596197289156627, 0.3461031626506024, 0.3463855421686747, 0.34659732680722893, 0.3467855798192771, 0.34720914909638556, 0.3473738704819277, 0.3474915286144578, 0.34772684487951805, 0.3477739081325301, 0.34789156626506024, 0.3482445406626506, 0.3483386671686747, 0.34859751506024095, 0.34847985692771083, 0.3489034262048193, 0.3489034262048193, 0.3489504894578313, 0.3491152108433735, 0.3492093373493976, 0.34918580572289154, 0.34916227409638556, 0.34918580572289154, 0.34942112198795183, 0.34963290662650603, 0.3493269954819277, 0.3495152484939759, 0.3497270331325301, 0.3497270331325301, 0.3497035015060241, 0.34967996987951805, 0.34996234939759036, 0.3499388177710843, 0.3500094126506024, 0.35005647590361444, 0.3501506024096386, 0.3501976656626506, 0.35033885542168675, 0.35043298192771083, 0.35045651355421686, 0.35043298192771083, 0.35045651355421686, 0.35066829819277107, 0.3506447665662651, 0.3507624246987952, 0.3508094879518072, 0.3509506777108434, 0.3509506777108434, 0.35104480421686746, 0.3510683358433735, 0.3510918674698795, 0.3511624623493976, 0.3511624623493976, 0.3511624623493976, 0.3513507153614458, 0.35142131024096385, 0.3513977786144578, 0.3514448418674699, 0.35151543674698793, 0.35151543674698793, 0.35142131024096385, 0.35137424698795183, 0.3514448418674699, 0.35142131024096385, 0.3514448418674699, 0.3514683734939759, 0.35160956325301207, 0.35153896837349397, 0.35153896837349397, 0.35153896837349397, 0.35158603162650603, 0.35151543674698793, 0.35163309487951805, 0.35163309487951805, 0.35160956325301207, 0.35163309487951805, 0.3516566265060241, 0.35160956325301207, 0.35163309487951805, 0.3517272213855422, 0.3516566265060241, 0.3517272213855422, 0.35179781626506024, 0.35182134789156627, 0.3516801581325301, 0.35170368975903615, 0.3517272213855422, 0.3517742846385542, 0.3518684111445783, 0.35191547439759036, 0.3518919427710843, 0.35191547439759036, 0.3518919427710843, 0.3518919427710843, 0.3518684111445783, 0.3518684111445783, 0.3518684111445783, 0.3518919427710843, 0.3518684111445783, 0.35191547439759036, 0.3518919427710843, 0.3518684111445783, 0.35191547439759036, 0.3519390060240964, 0.3518919427710843, 0.3519390060240964, 0.3519390060240964, 0.3519390060240964, 0.3519390060240964, 0.3519390060240964, 0.3519390060240964, 0.3519390060240964, 0.3518919427710843, 0.35191547439759036, 0.35198606927710846, 0.3519625376506024, 0.3519390060240964, 0.35191547439759036, 0.35191547439759036, 0.35191547439759036, 0.3518919427710843, 0.3518919427710843, 0.3518919427710843, 0.3518919427710843, 0.3518919427710843, 0.35191547439759036, 0.3518919427710843, 0.35191547439759036, 0.35191547439759036, 0.35191547439759036, 0.35191547439759036, 0.3519390060240964, 0.3519390060240964, 0.3519390060240964, 0.3519390060240964, 0.3519390060240964, 0.3519390060240964, 0.3519390060240964, 0.3519390060240964, 0.3519390060240964, 0.3519390060240964, 0.3519390060240964, 0.3519390060240964, 0.3519390060240964, 0.35191547439759036, 0.35191547439759036, 0.35191547439759036, 0.35191547439759036, 0.35191547439759036, 0.35191547439759036, 0.35191547439759036, 0.35191547439759036, 0.35191547439759036, 0.35191547439759036, 0.35191547439759036], "end": "2016-01-18 11:48:04.242000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0], "accuracy_valid": [0.11705280172413793, 0.11126077586206896, 0.11557112068965517, 0.16109913793103448, 0.17739762931034483, 0.18723060344827586, 0.19692887931034483, 0.21120689655172414, 0.21996228448275862, 0.22467672413793102, 0.2335668103448276, 0.23801185344827586, 0.24407327586206898, 0.24932650862068967, 0.25525323275862066, 0.25969827586206895, 0.26104525862068967, 0.2633351293103448, 0.2684536637931034, 0.27532327586206895, 0.2777478448275862, 0.2794989224137931, 0.28327047413793105, 0.2847521551724138, 0.2878502155172414, 0.28973599137931033, 0.2941810344827586, 0.2951239224137931, 0.2974137931034483, 0.30051185344827586, 0.30105064655172414, 0.30482219827586204, 0.3063038793103448, 0.3098060344827586, 0.31169181034482757, 0.3134428879310345, 0.3142510775862069, 0.31627155172413796, 0.3169450431034483, 0.31842672413793105, 0.3203125, 0.31963900862068967, 0.32058189655172414, 0.32058189655172414, 0.3220635775862069, 0.3220635775862069, 0.3230064655172414, 0.32421875, 0.3254310344827586, 0.32677801724137934, 0.3270474137931034, 0.32799030172413796, 0.32893318965517243, 0.32879849137931033, 0.32893318965517243, 0.3298760775862069, 0.33122306034482757, 0.3324353448275862, 0.33230064655172414, 0.3341864224137931, 0.3349946120689655, 0.33459051724137934, 0.33566810344827586, 0.33566810344827586, 0.3359375, 0.3370150862068966, 0.33620689655172414, 0.33620689655172414, 0.3368803879310345, 0.3371497844827586, 0.33728448275862066, 0.3375538793103448, 0.33728448275862066, 0.33741918103448276, 0.33728448275862066, 0.33782327586206895, 0.33741918103448276, 0.3376885775862069, 0.3376885775862069, 0.3376885775862069, 0.33795797413793105, 0.33795797413793105, 0.3380926724137931, 0.33836206896551724, 0.33782327586206895, 0.3387661637931034, 0.33903556034482757, 0.3386314655172414, 0.33970905172413796, 0.3389008620689655, 0.3389008620689655, 0.33903556034482757, 0.3393049568965517, 0.3394396551724138, 0.3394396551724138, 0.33970905172413796, 0.33984375, 0.3402478448275862, 0.33984375, 0.34051724137931033, 0.34065193965517243, 0.34011314655172414, 0.34065193965517243, 0.34065193965517243, 0.3409213362068966, 0.34065193965517243, 0.34119073275862066, 0.3410560344827586, 0.34119073275862066, 0.34119073275862066, 0.3414601293103448, 0.3410560344827586, 0.3414601293103448, 0.34119073275862066, 0.3414601293103448, 0.3415948275862069, 0.34172952586206895, 0.3414601293103448, 0.34172952586206895, 0.3415948275862069, 0.3414601293103448, 0.3415948275862069, 0.34186422413793105, 0.3415948275862069, 0.3415948275862069, 0.3415948275862069, 0.3415948275862069, 0.3414601293103448, 0.34186422413793105, 0.3415948275862069, 0.3415948275862069, 0.3414601293103448, 0.3415948275862069, 0.34172952586206895, 0.3415948275862069, 0.3415948275862069, 0.3415948275862069, 0.3415948275862069, 0.34172952586206895, 0.34172952586206895, 0.34186422413793105, 0.34186422413793105, 0.34186422413793105, 0.34172952586206895, 0.34172952586206895, 0.34186422413793105, 0.34186422413793105, 0.34172952586206895, 0.3419989224137931, 0.3415948275862069, 0.3415948275862069, 0.34172952586206895, 0.34172952586206895, 0.34186422413793105, 0.34186422413793105, 0.34186422413793105, 0.34172952586206895, 0.34186422413793105, 0.34186422413793105, 0.34186422413793105, 0.34186422413793105, 0.3419989224137931, 0.3419989224137931, 0.3419989224137931, 0.3419989224137931, 0.3419989224137931, 0.3419989224137931, 0.3419989224137931, 0.34186422413793105, 0.3419989224137931, 0.3419989224137931, 0.3419989224137931, 0.3419989224137931, 0.3419989224137931, 0.3421336206896552, 0.3421336206896552, 0.3421336206896552, 0.3421336206896552, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724, 0.34226831896551724], "accuracy_test": 0.3467548076923077, "start": "2016-01-18 01:24:10.395000", "learning_rate_per_epoch": [0.0007305509061552584, 0.0007024612277746201, 0.0006754515925422311, 0.0006494804983958602, 0.0006245080148801208, 0.0006004957249388099, 0.0005774066667072475, 0.0005552053917199373, 0.0005338577320799232, 0.0005133309168741107, 0.000493593339342624, 0.000474614673294127, 0.00045636575669050217, 0.00043881850433535874, 0.0004219459369778633, 0.00040572212310507894, 0.0003901221207343042, 0.0003751219483092427, 0.00036069852649234235, 0.00034682967816479504, 0.00033349409932270646, 0.0003206712717656046, 0.00030834146309643984, 0.0002964857267215848, 0.0002850858436431736, 0.0002741242933552712, 0.0002635842247400433, 0.00025344942696392536, 0.00024370430037379265, 0.00023433387104887515, 0.0002253237325930968, 0.0002166600461350754, 0.00020832946756854653, 0.0002003192057600245, 0.00019261693523731083, 0.0001852108252933249, 0.00017808946722652763, 0.00017124193254858255, 0.00016465768567286432, 0.00015832659846637398, 0.000152238950249739, 0.00014638536958955228, 0.0001407568488502875, 0.00013534474419429898, 0.00013014074647799134, 0.00012513683759607375, 0.000120325326861348, 0.0001156988219008781, 0.00011125020682811737, 0.00010697264224290848, 0.00010285954340361059, 9.890459477901459e-05, 9.510171366855502e-05, 9.14450574782677e-05, 8.792899461695924e-05, 8.454812632407993e-05, 8.129725028993562e-05, 7.817137520760298e-05, 7.516568439314142e-05, 7.227556488942355e-05, 6.94965710863471e-05, 6.682443199679255e-05, 6.425503670470789e-05, 6.178443436510861e-05, 5.94088232901413e-05, 5.712455458706245e-05, 5.492811760632321e-05, 5.2816132665611804e-05, 5.0785354687832296e-05, 4.88326586491894e-05, 4.695504321716726e-05, 4.5149623474571854e-05, 4.3413620005594566e-05, 4.17443661717698e-05, 4.013929719803855e-05, 3.85959428967908e-05, 3.711193130584434e-05, 3.56849777745083e-05, 3.4312892239540815e-05, 3.299356103525497e-05, 3.172495780745521e-05, 3.0505134418490343e-05, 2.93322118523065e-05, 2.8204389309394173e-05, 2.711993147386238e-05, 2.6077170332428068e-05, 2.5074503355426714e-05, 2.4110389858833514e-05, 2.3183345547295175e-05, 2.229194615210872e-05, 2.1434821974253282e-05, 2.0610654246411286e-05, 1.9818175132968463e-05, 1.9056165911024436e-05, 1.8323456970392726e-05, 1.7618920537643135e-05, 1.694147431408055e-05, 1.629007419978734e-05, 1.5663721569580957e-05, 1.5061451449582819e-05, 1.4482338883681223e-05, 1.3925493476563133e-05, 1.339005848421948e-05, 1.287521081394516e-05, 1.2380159205349628e-05, 1.1904142411367502e-05, 1.1446428288763855e-05, 1.1006313798134215e-05, 1.0583121365925763e-05, 1.017620070342673e-05, 9.784926078282297e-06, 9.408696314494591e-06, 9.046932063938584e-06, 8.699077625351492e-06, 8.364598215848673e-06, 8.042979970923625e-06, 7.733727215963881e-06, 7.436365649482468e-06, 7.150437340897042e-06, 6.875503004266648e-06, 6.6111401793023106e-06, 6.356941867124988e-06, 6.112517439760268e-06, 5.877491275896318e-06, 5.651501851389185e-06, 5.4342017392627895e-06, 5.225256700214231e-06, 5.0243456826137844e-06, 4.831159913010197e-06, 4.645401986635989e-06, 4.466786322154803e-06, 4.295038706914056e-06, 4.129894477955531e-06, 3.971100341004785e-06, 3.818411641987041e-06, 3.6715937312692404e-06, 3.5304210541653447e-06, 3.3946764688153053e-06, 3.2641512461850652e-06, 3.1386446153192082e-06, 3.0179637633409584e-06, 2.901923153331154e-06, 2.790344296954572e-06, 2.6830555270862533e-06, 2.579891997811501e-06, 2.480695229678531e-06, 2.3853126549511217e-06, 2.293597390234936e-06, 2.205408691224875e-06, 2.1206108158366987e-06, 2.0390734789543785e-06, 1.9606711703090696e-06, 1.885283381852787e-06, 1.8127942666978925e-06, 1.7430924117434188e-06, 1.6760704966145568e-06, 1.6116256347231683e-06, 1.5496586911467602e-06, 1.4900743963153218e-06, 1.4327811186376493e-06, 1.3776907508145086e-06, 1.324718596151797e-06, 1.2737831411868683e-06, 1.2248061693753698e-06, 1.1777124200307298e-06, 1.132429360950482e-06, 1.0888875294767786e-06, 1.0470198503753636e-06, 1.0067619768960867e-06, 9.680519497123896e-07, 9.308303674515628e-07, 8.950399887908134e-07, 8.606257324572653e-07, 8.27534677227959e-07, 7.957160050864331e-07, 7.651207170056296e-07, 7.357018603215693e-07, 7.074141308294202e-07, 6.802141001571727e-07, 6.540598747051263e-07, 6.289112661761465e-07, 6.047296210454078e-07, 5.814777637169755e-07, 5.591199396803859e-07, 5.376218155106471e-07, 5.169502514945634e-07, 4.970735290044104e-07, 4.779610662808409e-07, 4.5958347527630394e-07, 4.419125048116257e-07, 4.249209837325907e-07, 4.0858279248823237e-07, 3.928728062874143e-07, 3.7776686667712056e-07, 3.632417531207466e-07, 3.4927512615468004e-07, 3.3584549896659155e-07, 3.2293223739543464e-07, 3.105155030880269e-07, 2.9857619665563107e-07, 2.8709595767395513e-07, 2.760571362614428e-07, 2.654427362358547e-07, 2.5523647195768717e-07, 2.4542262622162525e-07, 2.359861355216708e-07, 2.269124763643049e-07, 2.181876936901972e-07, 2.0979838666335127e-07, 2.01731637616831e-07, 1.9397505468532472e-07, 1.865167149617264e-07, 1.7934515028628084e-07, 1.7244933303572907e-07, 1.6581866191245354e-07, 1.5944293352276873e-07, 1.5331235658777587e-07, 1.4741749509994406e-07, 1.4174929674481973e-07, 1.3629903605760774e-07, 1.3105834284488083e-07, 1.2601915955201548e-07, 1.211737270523372e-07, 1.1651459885797522e-07, 1.1203461269815307e-07, 1.0772688341376124e-07, 1.0358478874650245e-07, 9.960195512803693e-08, 9.577226478540979e-08], "accuracy_train_last": 0.35191547439759036, "error_valid": [0.8829471982758621, 0.888739224137931, 0.8844288793103449, 0.8389008620689655, 0.8226023706896551, 0.8127693965517242, 0.8030711206896551, 0.7887931034482758, 0.7800377155172413, 0.775323275862069, 0.7664331896551724, 0.7619881465517242, 0.755926724137931, 0.7506734913793103, 0.7447467672413793, 0.740301724137931, 0.7389547413793103, 0.7366648706896552, 0.7315463362068966, 0.724676724137931, 0.7222521551724138, 0.7205010775862069, 0.716729525862069, 0.7152478448275862, 0.7121497844827587, 0.7102640086206897, 0.7058189655172413, 0.7048760775862069, 0.7025862068965517, 0.6994881465517242, 0.6989493534482758, 0.6951778017241379, 0.6936961206896552, 0.6901939655172413, 0.6883081896551724, 0.6865571120689655, 0.6857489224137931, 0.6837284482758621, 0.6830549568965517, 0.681573275862069, 0.6796875, 0.6803609913793103, 0.6794181034482758, 0.6794181034482758, 0.6779364224137931, 0.6779364224137931, 0.6769935344827587, 0.67578125, 0.6745689655172413, 0.6732219827586207, 0.6729525862068966, 0.6720096982758621, 0.6710668103448276, 0.6712015086206897, 0.6710668103448276, 0.6701239224137931, 0.6687769396551724, 0.6675646551724138, 0.6676993534482758, 0.6658135775862069, 0.6650053879310345, 0.6654094827586207, 0.6643318965517242, 0.6643318965517242, 0.6640625, 0.6629849137931034, 0.6637931034482758, 0.6637931034482758, 0.6631196120689655, 0.6628502155172413, 0.6627155172413793, 0.6624461206896552, 0.6627155172413793, 0.6625808189655172, 0.6627155172413793, 0.662176724137931, 0.6625808189655172, 0.6623114224137931, 0.6623114224137931, 0.6623114224137931, 0.662042025862069, 0.662042025862069, 0.6619073275862069, 0.6616379310344828, 0.662176724137931, 0.6612338362068966, 0.6609644396551724, 0.6613685344827587, 0.6602909482758621, 0.6610991379310345, 0.6610991379310345, 0.6609644396551724, 0.6606950431034483, 0.6605603448275862, 0.6605603448275862, 0.6602909482758621, 0.66015625, 0.6597521551724138, 0.66015625, 0.6594827586206897, 0.6593480603448276, 0.6598868534482758, 0.6593480603448276, 0.6593480603448276, 0.6590786637931034, 0.6593480603448276, 0.6588092672413793, 0.6589439655172413, 0.6588092672413793, 0.6588092672413793, 0.6585398706896552, 0.6589439655172413, 0.6585398706896552, 0.6588092672413793, 0.6585398706896552, 0.6584051724137931, 0.658270474137931, 0.6585398706896552, 0.658270474137931, 0.6584051724137931, 0.6585398706896552, 0.6584051724137931, 0.658135775862069, 0.6584051724137931, 0.6584051724137931, 0.6584051724137931, 0.6584051724137931, 0.6585398706896552, 0.658135775862069, 0.6584051724137931, 0.6584051724137931, 0.6585398706896552, 0.6584051724137931, 0.658270474137931, 0.6584051724137931, 0.6584051724137931, 0.6584051724137931, 0.6584051724137931, 0.658270474137931, 0.658270474137931, 0.658135775862069, 0.658135775862069, 0.658135775862069, 0.658270474137931, 0.658270474137931, 0.658135775862069, 0.658135775862069, 0.658270474137931, 0.6580010775862069, 0.6584051724137931, 0.6584051724137931, 0.658270474137931, 0.658270474137931, 0.658135775862069, 0.658135775862069, 0.658135775862069, 0.658270474137931, 0.658135775862069, 0.658135775862069, 0.658135775862069, 0.658135775862069, 0.6580010775862069, 0.6580010775862069, 0.6580010775862069, 0.6580010775862069, 0.6580010775862069, 0.6580010775862069, 0.6580010775862069, 0.658135775862069, 0.6580010775862069, 0.6580010775862069, 0.6580010775862069, 0.6580010775862069, 0.6580010775862069, 0.6578663793103448, 0.6578663793103448, 0.6578663793103448, 0.6578663793103448, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828, 0.6577316810344828], "accuracy_train_std": [0.02825269444471085, 0.027554238459555087, 0.027788529524284043, 0.031173108664647276, 0.03364863789274692, 0.03503068125046473, 0.036776461790131665, 0.03893416941679584, 0.03888138986120014, 0.03875483144456283, 0.03871647002793116, 0.03882911613254468, 0.03977350986144128, 0.03943599425403611, 0.04013905078832842, 0.040905791941731946, 0.041203676427090434, 0.041226623900387095, 0.041317750273831354, 0.041994322039215076, 0.04181989791021682, 0.041781647017203584, 0.04224913990962566, 0.04270354226147344, 0.04188272562460327, 0.04179797167970221, 0.04177902281578129, 0.04104795305791146, 0.04158199539530545, 0.04193084937492445, 0.04164948323458072, 0.0412846409509009, 0.04162410829811987, 0.04187513600166296, 0.04177136132512293, 0.04139356911727783, 0.04165462150534312, 0.04145201375681098, 0.04127403686995793, 0.04102355586293925, 0.041688984430232924, 0.041527147438748725, 0.041639218089332165, 0.041847641852255764, 0.04178492703722213, 0.041210449141618866, 0.04123068674396305, 0.04144413148213562, 0.04137528489118699, 0.041256947902960986, 0.04150087713048613, 0.04139592346895833, 0.04167425808465436, 0.04156876308900667, 0.04138139391218114, 0.041285740775550465, 0.041578159991590494, 0.04156847002599854, 0.041746796640179176, 0.041739082800976296, 0.04220230416133946, 0.04215467420841841, 0.042222394227747984, 0.04240396484661485, 0.042404304369487555, 0.042113558652982004, 0.04193272458041895, 0.041727671929543374, 0.04194776940883447, 0.04157249280136728, 0.04177607370586776, 0.04150281846087681, 0.04181359471717717, 0.04168488655160297, 0.04171039046029381, 0.04203636435800444, 0.0419188566411422, 0.0421870219619071, 0.04239128955729261, 0.04238486230582026, 0.04258100971790332, 0.042399909949892234, 0.042581887501041915, 0.04265942514037282, 0.04279662178550584, 0.04275454349089771, 0.04252030775949612, 0.04251536528410332, 0.042883760075771195, 0.04284779653770861, 0.04271458224714387, 0.042734697115815165, 0.04264325495701564, 0.04236309777684646, 0.04273982800196684, 0.04275356563797591, 0.042718315618911346, 0.04275226395799484, 0.04271718138248206, 0.04266134619993952, 0.04272246342712902, 0.04281589627510757, 0.04265242162647764, 0.04266217690160905, 0.042706368977049686, 0.04265758837076762, 0.042848055004103915, 0.04287426186482064, 0.04277720916461915, 0.04270299764339714, 0.042773351474904894, 0.04288064158859893, 0.04287893697800974, 0.042739983473749875, 0.04275012030906384, 0.042774380657698195, 0.04285876060483747, 0.04275734093336188, 0.04274035271696802, 0.04278419872283781, 0.042868010364436875, 0.0428079288135653, 0.042779099046705196, 0.042781525998847575, 0.042779642695951525, 0.042674946904219455, 0.04276101877365742, 0.04278680657102166, 0.04273656296387105, 0.042713188630645736, 0.04273677027529323, 0.04268096720586186, 0.042661716123774444, 0.04264878635592163, 0.04273470359459519, 0.04282476738543606, 0.04274552826425479, 0.042790591874288714, 0.042719715549208254, 0.04276493584370172, 0.042889499307240904, 0.04273708124054094, 0.042775778753144936, 0.04282302822365238, 0.04282302822365238, 0.042928059422750846, 0.04286159646258606, 0.04293016838946799, 0.04295802669411532, 0.042981525542004156, 0.043009273332304185, 0.042936623634509744, 0.04299863096163158, 0.04289135196601788, 0.042962228692970866, 0.04296843426781624, 0.04305145636641056, 0.04301928235456062, 0.04300280323931921, 0.042979212954343815, 0.042985544896910075, 0.04304090161834479, 0.043065956081454006, 0.043048517248055376, 0.043008114581234186, 0.04296302135526538, 0.04294822902675238, 0.04298673646035234, 0.04294626277818991, 0.04294626277818991, 0.042860562914294624, 0.04281103319567706, 0.04281764865903592, 0.042855911638525125, 0.04282391397658361, 0.04279617539416178, 0.04287735498710078, 0.0427977603858208, 0.04274657107089426, 0.042806350666349094, 0.042806350666349094, 0.04282352605784666, 0.04282352605784666, 0.04282352605784666, 0.04282352605784666, 0.04286643442915025, 0.04283679088431232, 0.04287735498710078, 0.04284883685545831, 0.04290265984153256, 0.042930716574860334, 0.04295874209411668, 0.04295874209411668, 0.04295874209411668, 0.04293110352504055, 0.04293110352504055, 0.04293110352504055, 0.04293110352504055, 0.04293110352504055, 0.04295874209411668, 0.04293110352504055, 0.04295874209411668, 0.04295874209411668, 0.04295874209411668, 0.04295874209411668, 0.04296496106066245, 0.04296496106066245, 0.04296496106066245, 0.04296496106066245, 0.04296496106066245, 0.04296496106066245, 0.04296496106066245, 0.04296496106066245, 0.04296496106066245, 0.04296496106066245, 0.04296496106066245, 0.04296496106066245, 0.04296496106066245, 0.04294590175227856, 0.04294590175227856, 0.04294590175227856, 0.04294590175227856, 0.04294590175227856, 0.04294590175227856, 0.04294590175227856, 0.04294590175227856, 0.04294590175227856, 0.04294590175227856, 0.04294590175227856], "accuracy_test_std": 0.03560918982458064, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5946363365262823, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0007597638017156532, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adadelta", "nb_data_augmentation": 1, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 4.521910525779313e-09, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.03844997630614303}, "accuracy_valid_max": 0.34226831896551724, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.34226831896551724, "loss_train": [197.650146484375, 34.22460174560547, 10.874947547912598, 4.587069511413574, 2.7757489681243896, 2.466247320175171, 2.3554461002349854, 2.2771573066711426, 2.2237069606781006, 2.1802713871002197, 2.146185874938965, 2.1190435886383057, 2.0926127433776855, 2.071028709411621, 2.0509352684020996, 2.0333306789398193, 2.018529176712036, 2.0047149658203125, 1.9918458461761475, 1.9767359495162964, 1.9661985635757446, 1.9582622051239014, 1.9470863342285156, 1.9379409551620483, 1.9293266534805298, 1.921627163887024, 1.9143755435943604, 1.907071828842163, 1.9018980264663696, 1.8946056365966797, 1.8887730836868286, 1.8830488920211792, 1.8805428743362427, 1.8726060390472412, 1.8700827360153198, 1.864825963973999, 1.8605420589447021, 1.8580751419067383, 1.855205774307251, 1.849172830581665, 1.8471662998199463, 1.8448864221572876, 1.8415933847427368, 1.8369961977005005, 1.8377385139465332, 1.8334715366363525, 1.8301477432250977, 1.8283154964447021, 1.8265384435653687, 1.8236240148544312, 1.8222806453704834, 1.8199611902236938, 1.818957805633545, 1.8182264566421509, 1.8153042793273926, 1.8151136636734009, 1.813720703125, 1.8105348348617554, 1.8105857372283936, 1.8091979026794434, 1.806586742401123, 1.8076339960098267, 1.8042168617248535, 1.8053492307662964, 1.8046510219573975, 1.8019484281539917, 1.7993392944335938, 1.801342248916626, 1.799935221672058, 1.7982100248336792, 1.7970905303955078, 1.798282265663147, 1.796140432357788, 1.796878457069397, 1.794073462486267, 1.793583869934082, 1.7935490608215332, 1.792231559753418, 1.7921644449234009, 1.792965054512024, 1.7948331832885742, 1.7902164459228516, 1.7923070192337036, 1.7904363870620728, 1.7887146472930908, 1.7893239259719849, 1.7909425497055054, 1.7887723445892334, 1.789001703262329, 1.7909444570541382, 1.788087248802185, 1.7879133224487305, 1.7874561548233032, 1.7875709533691406, 1.7874029874801636, 1.7852915525436401, 1.7860621213912964, 1.784843921661377, 1.7864667177200317, 1.7861884832382202, 1.7851020097732544, 1.7854957580566406, 1.7875925302505493, 1.7847720384597778, 1.787002682685852, 1.7822259664535522, 1.7848811149597168, 1.783509612083435, 1.7836642265319824, 1.7840890884399414, 1.7843959331512451, 1.7829331159591675, 1.7845308780670166, 1.7846661806106567, 1.7837367057800293, 1.7837984561920166, 1.7827225923538208, 1.783607840538025, 1.7836464643478394, 1.78233802318573, 1.7803423404693604, 1.782263994216919, 1.7827789783477783, 1.7823728322982788, 1.7827588319778442, 1.7820444107055664, 1.7827558517456055, 1.7807708978652954, 1.782479166984558, 1.7815691232681274, 1.7813669443130493, 1.7814699411392212, 1.7800228595733643, 1.7819985151290894, 1.7812210321426392, 1.779341459274292, 1.7806884050369263, 1.781065583229065, 1.783251166343689, 1.7814708948135376, 1.7790615558624268, 1.7795277833938599, 1.7820963859558105, 1.7811381816864014, 1.7809072732925415, 1.7793797254562378, 1.781311273574829, 1.7805523872375488, 1.7811506986618042, 1.7828519344329834, 1.782545804977417, 1.7812302112579346, 1.7818937301635742, 1.7800545692443848, 1.780617594718933, 1.7808797359466553, 1.779240608215332, 1.7814031839370728, 1.7815073728561401, 1.7808443307876587, 1.7787972688674927, 1.7822760343551636, 1.7793196439743042, 1.7818841934204102, 1.7804615497589111, 1.7804116010665894, 1.7788389921188354, 1.7797136306762695, 1.7815736532211304, 1.7807804346084595, 1.7811321020126343, 1.7801202535629272, 1.7813005447387695, 1.781292200088501, 1.7826930284500122, 1.780121088027954, 1.7796704769134521, 1.781273603439331, 1.7815947532653809, 1.7823714017868042, 1.779859185218811, 1.782346487045288, 1.7801891565322876, 1.7795597314834595, 1.7796494960784912, 1.7806453704833984, 1.7821272611618042, 1.781984567642212, 1.7796766757965088, 1.780644416809082, 1.7795485258102417, 1.7821499109268188, 1.781660795211792, 1.7799134254455566, 1.7803362607955933, 1.779983639717102, 1.778754711151123, 1.779388666152954, 1.780207872390747, 1.781490445137024, 1.781134843826294, 1.7806205749511719, 1.7797588109970093, 1.7789796590805054, 1.7820408344268799, 1.7819041013717651, 1.780945062637329, 1.778771162033081, 1.7803207635879517, 1.7804912328720093, 1.7804816961288452, 1.77942955493927, 1.7807235717773438, 1.77857506275177, 1.7805498838424683, 1.7805390357971191, 1.7797054052352905, 1.7810536623001099, 1.7821123600006104, 1.7801398038864136, 1.7821160554885864, 1.7802866697311401, 1.7794145345687866, 1.7807302474975586, 1.7804902791976929, 1.779719591140747, 1.778879165649414, 1.7795222997665405, 1.7794687747955322], "accuracy_train_first": 0.11954066265060241, "model": "residualv2", "loss_std": [119.83804321289062, 12.692391395568848, 3.0649728775024414, 1.058793067932129, 0.23464281857013702, 0.11674437671899796, 0.10457684099674225, 0.09427032619714737, 0.09373504668474197, 0.0901578962802887, 0.08691191673278809, 0.08492778986692429, 0.083973728120327, 0.08555874228477478, 0.07812589406967163, 0.0804743617773056, 0.08253578096628189, 0.07890713214874268, 0.0771254375576973, 0.07730734348297119, 0.07629142701625824, 0.0787421241402626, 0.07916992902755737, 0.07805590331554413, 0.07556064426898956, 0.07720579952001572, 0.07629302144050598, 0.07671192288398743, 0.07799123972654343, 0.0786423534154892, 0.07682214677333832, 0.08062838762998581, 0.07816503196954727, 0.08080203086137772, 0.0772898867726326, 0.07618784159421921, 0.07736325263977051, 0.07709100842475891, 0.07544441521167755, 0.07663903385400772, 0.0777512639760971, 0.07800460606813431, 0.07660901546478271, 0.0783766508102417, 0.07820352911949158, 0.07815402746200562, 0.07910758256912231, 0.0780055969953537, 0.079218789935112, 0.08286783844232559, 0.07733358442783356, 0.07549863308668137, 0.08000677824020386, 0.07780998945236206, 0.07853738218545914, 0.07923667132854462, 0.07704200595617294, 0.07800854742527008, 0.07926476746797562, 0.07686666399240494, 0.07678935676813126, 0.07934369146823883, 0.08024337142705917, 0.07745305448770523, 0.07674335688352585, 0.08105447143316269, 0.07880164682865143, 0.07701282948255539, 0.07777245342731476, 0.07749024778604507, 0.07893022149801254, 0.07946968078613281, 0.0781191736459732, 0.07766573131084442, 0.07832368463277817, 0.07750316709280014, 0.0774359256029129, 0.07974846661090851, 0.07768711447715759, 0.08077280968427658, 0.07598461210727692, 0.0806487649679184, 0.07894280552864075, 0.07975134253501892, 0.07912451773881912, 0.08035232126712799, 0.07702846825122833, 0.07739118486642838, 0.08161589503288269, 0.0771474614739418, 0.07989290356636047, 0.07819357514381409, 0.07758408039808273, 0.07905278354883194, 0.07857757806777954, 0.07856109738349915, 0.0806160569190979, 0.08287970721721649, 0.08002239465713501, 0.08001838624477386, 0.07655870169401169, 0.07888876646757126, 0.07706662267446518, 0.0801500454545021, 0.07935634255409241, 0.08316720277070999, 0.07955926656723022, 0.07839871197938919, 0.07619211822748184, 0.07903723418712616, 0.07817883044481277, 0.07931038737297058, 0.07770045846700668, 0.08119917660951614, 0.07886126637458801, 0.07870940864086151, 0.07719280570745468, 0.07556772232055664, 0.07745935767889023, 0.08334439247846603, 0.08015641570091248, 0.07627087831497192, 0.07969735562801361, 0.07976367324590683, 0.0796550065279007, 0.07914406061172485, 0.08080754429101944, 0.07862971723079681, 0.07673497498035431, 0.08018709719181061, 0.07735132426023483, 0.07687217742204666, 0.07915429770946503, 0.0800081118941307, 0.08024513721466064, 0.07713105529546738, 0.07717059552669525, 0.07961858063936234, 0.08042168617248535, 0.07958149164915085, 0.07996775954961777, 0.0780474841594696, 0.07782134413719177, 0.07992830127477646, 0.08064129948616028, 0.07798172533512115, 0.07999338954687119, 0.078653983771801, 0.07972706854343414, 0.07969853281974792, 0.07749546319246292, 0.07817526161670685, 0.07749541848897934, 0.08057262748479843, 0.07663492113351822, 0.07871425896883011, 0.07962256669998169, 0.07659003883600235, 0.07775859534740448, 0.08084709942340851, 0.08314558863639832, 0.07960133999586105, 0.07817529886960983, 0.07662782818078995, 0.07960104942321777, 0.07768379151821136, 0.07780608534812927, 0.07815912365913391, 0.07761020213365555, 0.07972043752670288, 0.07916677743196487, 0.07856873422861099, 0.07871614396572113, 0.07762086391448975, 0.07641936093568802, 0.07941258698701859, 0.08121250569820404, 0.07685407251119614, 0.08078984171152115, 0.07768557220697403, 0.07998714596033096, 0.07821450382471085, 0.07629430294036865, 0.07740126550197601, 0.07908684015274048, 0.07902920246124268, 0.0776105597615242, 0.07821716368198395, 0.07937929779291153, 0.07742597162723541, 0.0787024274468422, 0.0776284784078598, 0.07759067416191101, 0.07772055268287659, 0.08073152601718903, 0.0790368840098381, 0.07635553181171417, 0.07884706556797028, 0.07992277294397354, 0.07718639075756073, 0.07526475191116333, 0.0801343098282814, 0.0778796374797821, 0.07929136604070663, 0.07645721733570099, 0.07838094979524612, 0.08017420768737793, 0.07895149290561676, 0.0781751424074173, 0.07685210555791855, 0.08049114793539047, 0.08000834286212921, 0.07652582973241806, 0.07972847670316696, 0.07796879857778549, 0.07789421826601028, 0.07775740325450897, 0.07959874719381332, 0.07979637384414673, 0.07852114737033844, 0.0778256207704544, 0.07817201316356659, 0.08108353614807129, 0.07675676792860031, 0.0789395198225975, 0.07618118822574615, 0.07813281565904617, 0.07851529121398926, 0.08131106197834015]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:00 2016", "state": "available"}], "summary": "d62afb40f5784ae8fac0c34f9d03f4e5"}