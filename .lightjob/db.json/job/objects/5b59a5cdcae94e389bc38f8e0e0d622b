{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 64, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.751617193222046, 1.4706412553787231, 1.363033413887024, 1.2814823389053345, 1.2125701904296875, 1.1525620222091675, 1.1001750230789185, 1.0532526969909668, 1.010596513748169, 0.9714892506599426, 0.9355422258377075, 0.9019661545753479, 0.8700471520423889, 0.839477002620697, 0.8101205229759216, 0.781733512878418, 0.7539876103401184, 0.7268746495246887, 0.7003170847892761, 0.674219012260437, 0.6480526924133301, 0.6221003532409668, 0.5961158871650696, 0.5698723196983337, 0.5433381199836731, 0.515990138053894, 0.4881677031517029, 0.4597330093383789, 0.43091022968292236, 0.401729553937912, 0.37201714515686035, 0.34178656339645386, 0.3120284378528595, 0.28288498520851135, 0.25449052453041077, 0.2267063856124878, 0.20070554316043854, 0.17665418982505798, 0.15470458567142487, 0.1354055255651474, 0.1175168827176094, 0.10182122141122818, 0.08861421793699265, 0.07994521409273148, 0.06930331140756607, 0.06317257881164551, 0.05980227515101433, 0.050878431648015976, 0.04252490773797035, 0.03908345103263855, 0.036710772663354874, 0.03058306872844696, 0.025239761918783188, 0.02191063016653061, 0.018260259181261063, 0.015123636461794376, 0.013224293477833271, 0.011201844550669193, 0.009673364460468292, 0.00828423723578453, 0.007430309895426035, 0.0067154038697481155, 0.0061586396768689156, 0.005683138966560364, 0.005286050960421562, 0.004938291851431131, 0.004639836493879557, 0.00438303267583251, 0.0041595930233597755, 0.003962760791182518, 0.003785913111642003, 0.003627197118476033, 0.0034839962609112263, 0.003354277228936553, 0.0032350472174584866, 0.0031262345146387815, 0.003026568563655019, 0.0029347215313464403, 0.00284931855276227, 0.0027701775543391705, 0.0026964496355503798, 0.0026276051066815853, 0.0025634111370891333, 0.002502918243408203, 0.00244619557633996, 0.0023923637345433235, 0.002341379877179861, 0.0022934796288609505, 0.0022479542531073093, 0.0022051639389246702, 0.002164433477446437, 0.0021256590262055397, 0.0020888240542262793, 0.0020538175012916327, 0.0020203888416290283, 0.0019885716028511524, 0.0019581697415560484, 0.0019288428593426943, 0.0019009598763659596, 0.0018741778330877423, 0.0018485481850802898, 0.001823792583309114, 0.0018000229028984904, 0.001777143683284521, 0.0017551133641973138, 0.001733875717036426, 0.0017135362140834332, 0.0016939017223194242, 0.001674946746788919, 0.001656611799262464, 0.001638851361349225, 0.0016216812655329704, 0.001605113036930561, 0.0015890422509983182, 0.001573496381752193, 0.0015584181528538465, 0.0015438030241057277, 0.0015296449419111013, 0.0015158896567299962, 0.0015025672037154436, 0.001489617396146059, 0.001477057347074151, 0.0014648394426330924, 0.0014529480831697583, 0.0014414112083613873, 0.0014301714254543185, 0.0014192310627549887, 0.0014085392467677593, 0.0013981370721012354, 0.0013879972975701094, 0.0013781493762508035, 0.0013685249723494053, 0.0013591645983979106, 0.0013500290224328637, 0.0013410886749625206, 0.0013323687016963959, 0.00132386339828372, 0.0013155854539945722, 0.001307463739067316, 0.0012995441211387515, 0.001291782595217228, 0.0012842187425121665, 0.0012768107699230313, 0.001269586500711739, 0.0012625140370801091, 0.0012555891880765557, 0.001248802524060011, 0.0012421577703207731, 0.0012356664519757032, 0.0012292962055653334, 0.0012230462161824107, 0.0012169283581897616, 0.0012109460076317191, 0.0012050720397382975, 0.0011993063380941749, 0.0011936526279896498, 0.0011881167301908135, 0.0011826993431895971, 0.0011773841688409448, 0.001172157353721559, 0.001167028909549117, 0.0011619923170655966, 0.0011570504866540432, 0.0011521890992298722, 0.0011474239872768521, 0.0011427529389038682, 0.0011381494114175439, 0.0011336348252370954, 0.001129194046370685, 0.001124837901443243, 0.001120555680245161, 0.0011163456365466118, 0.0011122068390250206, 0.0011081267148256302, 0.001104117021895945, 0.001100176596082747, 0.0010962971718981862, 0.0010924898087978363, 0.0010887273820117116, 0.0010850413236767054, 0.0010814131237566471, 0.0010778375435620546, 0.0010743202874436975, 0.0010708532063290477, 0.001067439909093082, 0.0010640791151672602, 0.0010607558069750667, 0.0010574900079518557, 0.0010542742675170302, 0.0010511039290577173, 0.0010479817865416408, 0.0010449144756421447, 0.0010418841848149896, 0.0010388885857537389, 0.0010359428124502301, 0.0010330338263884187, 0.0010301738511770964, 0.0010273524094372988, 0.00102457485627383, 0.0010218278039246798, 0.001019118819385767, 0.0010164452251046896, 0.0010138098150491714, 0.0010112138697877526, 0.0010086551774293184, 0.001006134320050478, 0.0010036381427198648, 0.0010011770064011216, 0.0009987493976950645, 0.0009963525226339698, 0.0009939903393387794, 0.0009916499257087708, 0.0009893436217680573, 0.0009870636276900768, 0.000984810059890151, 0.0009825913002714515, 0.0009804015280678868, 0.0009782351553440094, 0.0009760986431501806, 0.0009739893139339983, 0.0009719015215523541, 0.0009698336361907423, 0.0009677926427684724, 0.0009657819173298776, 0.0009637916227802634, 0.000961821700911969, 0.0009598747128620744, 0.0009579511824995279, 0.0009560538455843925, 0.000954175484366715, 0.0009523184271529317, 0.0009504853514954448, 0.0009486691560596228, 0.0009468774078413844, 0.0009451057412661612], "moving_avg_accuracy_train": [0.04187704757290512, 0.08557102410368214, 0.12819680960744276, 0.16860785983209897, 0.2065886645246458, 0.24261708826805609, 0.27702079449293004, 0.31037420282741335, 0.3426450150855081, 0.3735273620450267, 0.4028650126299297, 0.43064985630510577, 0.45689755668649995, 0.4817362154177669, 0.5049441936937273, 0.5268961481015787, 0.547475757405485, 0.5669878470742205, 0.5854459107771897, 0.60288570479314, 0.6195298917312309, 0.6352349622088092, 0.6500832742255158, 0.6642324029964397, 0.6775127404699592, 0.6902530893984617, 0.7025656134055056, 0.7142279919701322, 0.7252309790699536, 0.7355265094621461, 0.7455667253198721, 0.7553281857763031, 0.7646320083716148, 0.7737912048097394, 0.782769084432586, 0.7916653033252907, 0.8001532061322965, 0.8077853432121732, 0.8148214970542542, 0.8212028996859457, 0.8279854314746878, 0.8331785401417724, 0.8353903658410503, 0.8407008167869379, 0.8476189630059923, 0.8515367462746529, 0.8549050376344062, 0.8589846012389981, 0.8619588080355487, 0.8664907745118942, 0.8732013604512547, 0.8779902100788868, 0.8851410740031687, 0.8905843374327245, 0.8989636257501756, 0.9068002791346911, 0.9133789728724309, 0.9203925450792354, 0.9271930413153595, 0.9335134507254902, 0.9393576041648459, 0.9446894218733614, 0.9495322356384062, 0.9539395961519465, 0.9579457481438947, 0.9615977879128386, 0.9649357769786976, 0.9679864701141612, 0.9707762717634594, 0.9733289459263992, 0.9756589047563783, 0.9777860946378834, 0.9797121912752854, 0.9814782303322808, 0.9830862666740526, 0.9845427999768854, 0.985862980544673, 0.9870627687997295, 0.9881449033780899, 0.9891281250938524, 0.9900200000844671, 0.9908250127248299, 0.9915541743987755, 0.992212745054136, 0.992812434090389, 0.9933568045206358, 0.9938490630566674, 0.9942967460367149, 0.9947019858675672, 0.9950713520129534, 0.9954061066926104, 0.9957097110531113, 0.9959852801263717, 0.996233292292306, 0.9964611535392658, 0.9966685538103393, 0.9968621895007339, 0.9970364616220891, 0.9971933065313088, 0.9973344669496065, 0.997468486772503, 0.9975914297619194, 0.9977044036012036, 0.9978060800565594, 0.9979045643128082, 0.9979932001434321, 0.9980729723909936, 0.9981494177114181, 0.9982182184998001, 0.9982824643581534, 0.9983379604818619, 0.9983879069931995, 0.9984351840022129, 0.9984777333103249, 0.9985183528364352, 0.9985549104099345, 0.9985924625237029, 0.998628584574904, 0.9986610944209849, 0.9986903532824578, 0.9987166862577834, 0.9987403859355765, 0.9987640407943998, 0.9987853301673408, 0.9988068157517972, 0.998826152777808, 0.9988435561012177, 0.9988592190922864, 0.9988756409330577, 0.9988904205897519, 0.9989060474295861, 0.9989201115854369, 0.9989350944745123, 0.9989509042234896, 0.9989674581463788, 0.9989846818257886, 0.9990001831372574, 0.9990141343175792, 0.9990266903798689, 0.9990379908359297, 0.9990504863951939, 0.9990640575473412, 0.9990762715842737, 0.9990872642175129, 0.9990971575874282, 0.999106061620352, 0.9991140752499834, 0.9991212875166517, 0.9991301037054628, 0.9991380382753928, 0.9991451793883298, 0.999151606389973, 0.9991620409890709, 0.999171432128259, 0.9991798841535283, 0.9991874909762707, 0.9991943371167388, 0.9992004986431601, 0.9992060440169392, 0.9992110348533405, 0.9992155266061017, 0.9992195691835867, 0.9992232075033232, 0.9992264819910861, 0.9992294290300727, 0.9992320813651606, 0.9992344684667398, 0.999236616858161, 0.9992385504104401, 0.9992402906074913, 0.9992418567848373, 0.9992432663444488, 0.9992445349480992, 0.999248001840194, 0.9992511220430794, 0.9992539302256763, 0.9992564575900135, 0.999258732217917, 0.99926077938303, 0.9992626218316318, 0.9992642800353735, 0.9992657724187409, 0.9992671155637717, 0.9992683243942994, 0.9992694123417742, 0.9992703914945016, 0.9992712727319563, 0.9992720658456654, 0.9992727796480037, 0.999273422070108, 0.999274000250002, 0.9992745206119066, 0.9992749889376207, 0.9992754104307634, 0.9992757897745919, 0.9992761311840375, 0.9992764384525386, 0.9992767149941895, 0.9992769638816753, 0.9992771878804125, 0.9992773894792761, 0.9992775709182533, 0.9992777342133328, 0.9992778811789043, 0.9992780134479187, 0.9992781324900316, 0.9992782396279333, 0.9992783360520447, 0.9992784228337451, 0.9992808260860849, 0.9992829890131907, 0.9992849356475859, 0.9992890127673512, 0.9992926821751398, 0.9992959846421496, 0.9992989568624585, 0.9993016318607364, 0.9993040393591865, 0.9993062061077916, 0.9993081561815362, 0.9993099112479064, 0.999313815956449, 0.9993173301941374, 0.9993228181568665, 0.9993277573233228, 0.9993345277219429, 0.999340621080701, 0.9993461051035832, 0.9993510407241772, 0.9993554827827119, 0.999359480635393, 0.999363078702806, 0.9993663169634778, 0.9993692313980823, 0.9993718543892265], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04112828266189758, 0.08321789109563252, 0.1242745729009789, 0.16309943935899845, 0.19931546845585463, 0.23360373053346192, 0.26617523930428744, 0.2975862651197171, 0.3278276974368116, 0.3567347043779949, 0.3838161108697888, 0.4089716562210629, 0.4325068783530982, 0.45440879311567994, 0.4746688032995939, 0.49332799954154716, 0.5106980951540341, 0.5267462202677722, 0.5417928249446847, 0.555371390247656, 0.5680224336400591, 0.5794338162643815, 0.589938053237341, 0.5993653934331853, 0.6078276445642643, 0.6154883806725969, 0.6225305570537559, 0.6288064511318894, 0.6342706208247999, 0.6391883735484194, 0.6436387650621769, 0.6473358531173297, 0.6503336425232172, 0.6530326824971756, 0.6554332863765996, 0.6575541202483522, 0.6590600387016796, 0.6601457811135146, 0.6608065959803258, 0.6611052720844769, 0.6617555870729419, 0.6614171071495333, 0.6601797403728329, 0.6601138559439833, 0.6610962756489073, 0.6607790167775407, 0.660175071472151, 0.6599794946242582, 0.6599784919332933, 0.6612319722768616, 0.6624414357701844, 0.6632899303238136, 0.665312958658149, 0.6668082123499396, 0.6694824480614817, 0.6715230492643697, 0.6731622188296496, 0.6752376749869707, 0.6770027817352616, 0.6784815145274734, 0.6799079712731447, 0.6812416399779085, 0.682490769937196, 0.6834929165880548, 0.6843704345113276, 0.6852588864009328, 0.6860961437039872, 0.6867876106118265, 0.6875320011413818, 0.6882385737117316, 0.6888500749625464, 0.68937498251712, 0.6898718133787363, 0.6904298539441006, 0.6909565045154286, 0.6914304900296236, 0.6918936980861493, 0.6923349993995224, 0.6927077565190581, 0.6930432379266404, 0.6933939993184642, 0.6936974775398557, 0.6939584009078581, 0.6942064684789698, 0.6943931081992205, 0.694548876916196, 0.694676861730224, 0.6947920480628492, 0.6948702721910522, 0.6949539104463447, 0.6950658059698578, 0.6951420978785195, 0.6951364889001556, 0.6951436478508779, 0.6951622979377781, 0.6952034970784882, 0.6952649903676273, 0.6953823989927622, 0.6954758597241336, 0.6955965954761179, 0.6956930506216537, 0.695816481346386, 0.6959763971236449, 0.696120321323178, 0.6962376460715078, 0.6963676524075046, 0.6964602440474017, 0.6965557835545592, 0.6966783902047509, 0.696801972729833, 0.6968755463999974, 0.6969163191319856, 0.6969530145907751, 0.6969860405036855, 0.6970035567940549, 0.6970193214553874, 0.6970213026193366, 0.6970352926981409, 0.6970478837690648, 0.6970714227641464, 0.6971048148909696, 0.697121631265201, 0.6971123519395093, 0.6971162075776366, 0.6971074706207013, 0.6970996073594595, 0.6970925304243419, 0.6970983682139861, 0.6971036222246657, 0.6971205578655275, 0.6971235929110531, 0.6971141174207761, 0.6970933824482768, 0.6970747209730275, 0.697057925645303, 0.697042809850351, 0.6970414126661443, 0.6970523622316082, 0.6970866309030257, 0.6971296797385514, 0.6971928377530245, 0.6972496799660504, 0.6973008379577736, 0.6973346731190746, 0.6973651247642455, 0.6974047382761492, 0.6974403904368626, 0.6974724773815046, 0.6974891486004325, 0.6975041526974677, 0.6975054493535492, 0.6975066163440226, 0.6975320806979486, 0.6975427915852321, 0.6975890524775372, 0.6976306872806117, 0.6976559515721289, 0.6976786894344943, 0.6976991535106232, 0.6977175711791392, 0.6977097330183036, 0.6977026786735516, 0.6976841227320247, 0.6976796294159007, 0.697675585431389, 0.6976719458453284, 0.6976808772491239, 0.6976767084812898, 0.6976729565902392, 0.6976573728570435, 0.6976433474971675, 0.697630724673279, 0.6976193641317794, 0.6975592820107701, 0.6974930010706117, 0.6974333482244692, 0.6974173112653507, 0.697415085033394, 0.6974130814246329, 0.6974112781767479, 0.6974473058560611, 0.697479730767443, 0.6975211202189366, 0.6975583707252809, 0.6976041032122408, 0.6976452624505046, 0.6976578917024421, 0.6976692580291858, 0.6976794877232552, 0.6976764874166677, 0.6976859941719888, 0.697694550251778, 0.6976778366610882, 0.6976383803669673, 0.6975652190998489, 0.6974993739594424, 0.6974401133330764, 0.6973867787693471, 0.6973387776619907, 0.6972955766653699, 0.6972566957684112, 0.6972217029611484, 0.6971902094346119, 0.697174072291979, 0.6971473418323595, 0.6971476984812018, 0.69716022649641, 0.6971715017100973, 0.6971816494024159, 0.6971907823255027, 0.6971990019562807, 0.697206399623981, 0.6972130575249113, 0.6972312566669985, 0.697259842926127, 0.6972855705593426, 0.6972965183979867, 0.6973063714527663, 0.6973152392020681, 0.6972988061139396, 0.6972962233658739, 0.6972938988926148, 0.6972918068666817, 0.6972899240433419, 0.697288229502336, 0.6972867044154307, 0.6972975388684659, 0.6973072898761976, 0.6973160657831561, 0.6973117570681688, 0.6973200862559303, 0.6973275825249157, 0.6973343291670024, 0.6973404011448805, 0.6973458659249707], "moving_var_accuracy_train": [0.01578318402081023, 0.03138733788437806, 0.044601222404253724, 0.05483857698616555, 0.06233759301338949, 0.06778625956896311, 0.07166016863013369, 0.07450620039486133, 0.07642822826955, 0.07736887962614777, 0.07737827134010974, 0.07658842204858453, 0.07513005582152904, 0.07316968094749124, 0.0707002051536592, 0.06796717935921273, 0.06498214429520432, 0.06191042465485082, 0.05878568323033157, 0.05564443264516739, 0.05257325001012171, 0.049535768157460844, 0.046566442669424064, 0.043711579007267326, 0.040927727377235705, 0.03829580305689207, 0.03583060697801913, 0.033471645944279184, 0.031214072875920793, 0.029046647102837754, 0.027049235802781568, 0.025201887214685612, 0.02346074852718235, 0.021869691588993507, 0.020408143332795083, 0.019079613394797846, 0.017820052501868657, 0.016562292899336046, 0.015351630757407317, 0.01418296837759643, 0.01317869617702446, 0.012103541957975352, 0.010937217318493694, 0.01009730358988243, 0.009518319954868145, 0.008704629191043107, 0.007936274752096491, 0.007292432829722037, 0.006642802701367661, 0.006163370912515367, 0.00595232149410973, 0.005563487071503397, 0.005467352058125419, 0.005187278903184731, 0.0053004632672290055, 0.005323135166927446, 0.005180334551889497, 0.005105012852601096, 0.005010732308858826, 0.004869187253977963, 0.00468965569338477, 0.004476544644737839, 0.0042399657867302236, 0.003990792648324032, 0.0037361566675349453, 0.0034825775510469795, 0.00323459933497643, 0.0029948999589396686, 0.002765456902227543, 0.002547556520444051, 0.002341659241744225, 0.002148217748697599, 0.0019667846081373457, 0.001798176192881106, 0.0016416306014811286, 0.001496560944693366, 0.0013625907408081064, 0.0012392870934400397, 0.0011258975213071845, 0.0010220082936575876, 0.0009269664332817857, 0.0008401021981139023, 0.0007608770690232721, 0.000688692799893863, 0.0006230601623662959, 0.0005634211986176102, 0.0005092599449525135, 0.0004601377309128804, 0.00041560193170617535, 0.0003752696206797746, 0.00033875120487176775, 0.0003057056648540272, 0.00027581854319586236, 0.0002487902791863388, 0.00022437853799849993, 0.0002023278180506219, 0.0001824324892709112, 0.0001644625772943548, 0.00014823772249485242, 0.000133593286618613, 0.00012039560977311457, 0.00010849208360362281, 9.775774303852413e-05, 8.807501164883526e-05, 7.935480282251172e-05, 7.149002933449412e-05, 6.439829890437392e-05, 5.8011063997069706e-05, 5.22525595337006e-05, 4.7064451353170546e-05, 4.238572459557349e-05, 3.816960402196934e-05, 3.437275966000368e-05, 3.0951777686590644e-05, 2.7871449431044465e-05, 2.5096332593561458e-05, 2.2599390785441626e-05, 2.0351194930144267e-05, 1.8325587447959755e-05, 1.6500733431936015e-05, 1.4856900919047926e-05, 1.33762658996906e-05, 1.2043675280835095e-05, 1.0843386889353563e-05, 9.763202873473103e-06, 8.79024787130026e-06, 7.913948965161551e-06, 7.124762032248363e-06, 6.414712920712407e-06, 5.775207572909161e-06, 5.1998845987271196e-06, 4.681676343172598e-06, 4.215529091540737e-06, 3.796225715851201e-06, 3.419069435533254e-06, 3.0798323881716126e-06, 2.774011765269714e-06, 2.4983623076341052e-06, 2.249944969172699e-06, 2.026099775020052e-06, 1.8248950485299658e-06, 1.6440631292124092e-06, 1.4809994605748747e-06, 1.3339870563871905e-06, 1.2014692596630132e-06, 1.082035869917492e-06, 9.744102472645758e-07, 8.77437373652573e-07, 7.903931629536776e-07, 7.119204632580618e-07, 6.411873763780536e-07, 5.774403958913441e-07, 5.20676284027221e-07, 4.6940239708175903e-07, 4.231050879539643e-07, 3.8131535292867687e-07, 3.436056443895982e-07, 3.095867596212083e-07, 2.789048441922497e-07, 2.512385358048866e-07, 2.2629626481020624e-07, 2.0381372022369065e-07, 1.835514845358712e-07, 1.6529283651326942e-07, 1.4884171821104055e-07, 1.340208603227061e-07, 1.20670058575979e-07, 1.0864459298967141e-07, 9.781378131044962e-08, 8.805965775139848e-08, 7.927576817957372e-08, 7.136607308630169e-08, 6.424394997466668e-08, 5.7927729044376014e-08, 5.222257713435087e-08, 4.7071292426389575e-08, 4.242165131818418e-08, 3.8226051575256795e-08, 3.444116438273271e-08, 3.102759949611063e-08, 2.7949586303336874e-08, 2.5174672546041282e-08, 2.267344163859788e-08, 2.0419248915938103e-08, 1.8387976691716163e-08, 1.6557807683115713e-08, 1.4909016129866727e-08, 1.3423775781080693e-08, 1.2085983826975127e-08, 1.0881099799719634e-08, 9.795998447655706e-09, 8.818835591495645e-09, 7.938925993116547e-09, 7.146632302028967e-09, 6.43326418748755e-09, 5.790986812424576e-09, 5.2127378565675734e-09, 4.6921523484730185e-09, 4.223494618451095e-09, 3.801596735514541e-09, 3.4218028408790003e-09, 3.079918837712972e-09, 2.7721669414883837e-09, 2.4951446372523766e-09, 2.2457876293565233e-09, 2.02133640564266e-09, 1.8193060718480332e-09, 1.6374591431466293e-09, 1.4737810084035096e-09, 1.378383503840927e-09, 1.2826494364418794e-09, 1.1884889620156344e-09, 1.219246216033677e-09, 1.218502576108402e-09, 1.1948089136569744e-09, 1.154834864370276e-09, 1.103751920017392e-09, 1.045541167104031e-09, 9.832402460554018e-10, 9.191413099361236e-10, 8.549495006165881e-10, 9.066752897862771e-10, 9.271565595854392e-10, 1.1055005178729288e-09, 1.2145087536244953e-09, 1.5056025555382238e-09, 1.6892034885781532e-09, 1.7909537024813498e-09, 1.8311014880700564e-09, 1.825578295491065e-09, 1.7868659004868766e-09, 1.7246941124200278e-09, 1.6466016907836885e-09, 1.5583868832862467e-09, 1.464468937838272e-09], "duration": 43903.616313, "accuracy_train": [0.4187704757290513, 0.4788168128806755, 0.5118288791412883, 0.5323073118540051, 0.5484159067575674, 0.5668729019587486, 0.5866541505167958, 0.610554877837763, 0.633082325408361, 0.651468484680694, 0.6669038678940569, 0.6807134493816906, 0.6931268601190477, 0.7052841439991694, 0.7138159981773717, 0.7244637377722407, 0.7326922411406424, 0.7425966540928387, 0.7515684841039129, 0.7598438509366925, 0.7693275741740495, 0.7765805965070136, 0.7837180823758766, 0.7915745619347545, 0.7970357777316353, 0.8049162297549834, 0.8133783294689, 0.8191893990517718, 0.8242578629683462, 0.8281862829918788, 0.8359286680394058, 0.8431813298841824, 0.8483664117294205, 0.8562239727528608, 0.8635700010382059, 0.8717312733596345, 0.8765443313953488, 0.8764745769310631, 0.8781468816329827, 0.8786355233711702, 0.8890282175733666, 0.8799165181455334, 0.8552967971345515, 0.8884948752999261, 0.9098822789774824, 0.8867967956925988, 0.8852196598721853, 0.8957006736803249, 0.8887266692045036, 0.9072784727990033, 0.9335966339055003, 0.9210898567275747, 0.9494988493217055, 0.9395737082987264, 0.9743772206072352, 0.9773301595953304, 0.9725872165120893, 0.9835146949404762, 0.9883975074404762, 0.9903971354166666, 0.9919549851190477, 0.99267578125, 0.9931175595238095, 0.9936058407738095, 0.9940011160714286, 0.9944661458333334, 0.9949776785714286, 0.9954427083333334, 0.9958844866071429, 0.9963030133928571, 0.9966285342261905, 0.9969308035714286, 0.9970470610119048, 0.9973725818452381, 0.99755859375, 0.9976515997023809, 0.9977446056547619, 0.9978608630952381, 0.9978841145833334, 0.9979771205357143, 0.998046875, 0.9980701264880952, 0.9981166294642857, 0.9981398809523809, 0.9982096354166666, 0.9982561383928571, 0.9982793898809523, 0.9983258928571429, 0.9983491443452381, 0.9983956473214286, 0.9984188988095238, 0.9984421502976191, 0.9984654017857143, 0.9984654017857143, 0.9985119047619048, 0.99853515625, 0.9986049107142857, 0.9986049107142857, 0.9986049107142857, 0.9986049107142857, 0.9986746651785714, 0.9986979166666666, 0.9987211681547619, 0.9987211681547619, 0.9987909226190477, 0.9987909226190477, 0.9987909226190477, 0.9988374255952381, 0.9988374255952381, 0.9988606770833334, 0.9988374255952381, 0.9988374255952381, 0.9988606770833334, 0.9988606770833334, 0.9988839285714286, 0.9988839285714286, 0.9989304315476191, 0.9989536830357143, 0.9989536830357143, 0.9989536830357143, 0.9989536830357143, 0.9989536830357143, 0.9989769345238095, 0.9989769345238095, 0.9990001860119048, 0.9990001860119048, 0.9990001860119048, 0.9990001860119048, 0.9990234375, 0.9990234375, 0.9990466889880952, 0.9990466889880952, 0.9990699404761905, 0.9990931919642857, 0.9991164434523809, 0.9991396949404762, 0.9991396949404762, 0.9991396949404762, 0.9991396949404762, 0.9991396949404762, 0.9991629464285714, 0.9991861979166666, 0.9991861979166666, 0.9991861979166666, 0.9991861979166666, 0.9991861979166666, 0.9991861979166666, 0.9991861979166666, 0.9992094494047619, 0.9992094494047619, 0.9992094494047619, 0.9992094494047619, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992559523809523, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9992792038690477, 0.9993024553571429, 0.9993024553571429, 0.9993024553571429, 0.9993257068452381, 0.9993257068452381, 0.9993257068452381, 0.9993257068452381, 0.9993257068452381, 0.9993257068452381, 0.9993257068452381, 0.9993257068452381, 0.9993257068452381, 0.9993489583333334, 0.9993489583333334, 0.9993722098214286, 0.9993722098214286, 0.9993954613095238, 0.9993954613095238, 0.9993954613095238, 0.9993954613095238, 0.9993954613095238, 0.9993954613095238, 0.9993954613095238, 0.9993954613095238, 0.9993954613095238, 0.9993954613095238], "end": "2016-02-04 00:03:53.311000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0], "moving_var_accuracy_valid": [0.015223820712452513, 0.02964525488415344, 0.04185158948352724, 0.051232762834522, 0.0579138934229688, 0.06270366832739607, 0.06598143014712812, 0.06826316001741363, 0.06966774207297696, 0.07022150331835784, 0.06979997618468604, 0.06851519172349753, 0.06664883267838596, 0.06430119424295394, 0.06156528693252921, 0.05854224867883779, 0.055403505805236425, 0.052181036101708524, 0.04900053530226725, 0.04575987869322403, 0.04262433091414981, 0.039533874703323015, 0.036573538182447504, 0.033716057052716374, 0.030988938595293777, 0.028418226635257997, 0.02602273420538238, 0.023774942403163694, 0.021666162516743616, 0.019717204891725248, 0.017923738264184242, 0.0162543805785538, 0.014709823192596883, 0.013304404224366424, 0.012025829892802932, 0.0108637283303268, 0.00979776561078677, 0.00882859857897181, 0.007949668807668418, 0.007155504793638295, 0.006443760500532465, 0.005800415568406175, 0.0052341537004262926, 0.004710777397205347, 0.004248385993774418, 0.003824453273120124, 0.003445290695195231, 0.003101105878406593, 0.0027909952996144362, 0.0025260366863984005, 0.0022865982352336875, 0.0020644178987781637, 0.0018948099016740663, 0.0017254509639319782, 0.0016172696973067683, 0.001493019206999142, 0.0013678991780728866, 0.0012698769246142598, 0.001170929648648592, 0.0010735165398205952, 0.000984477895463977, 0.0009020381558441776, 0.0008258772711564653, 0.0007523282252292647, 0.000684025742057324, 0.000622727288692879, 0.000566763557947251, 0.0005143903405142566, 0.00046793836180725564, 0.00042563772880106624, 0.00038643935993869216, 0.00035027517541245967, 0.0003174692260167035, 0.0002885249868683624, 0.00026216873560004606, 0.00023797382244904257, 0.00021610749553681018, 0.00019624946762579194, 0.0001778750516946938, 0.00016110047649872404, 0.0001460977308347999, 0.00013231684902905062, 0.00011969789316187293, 0.00010828194152422131, 9.776725683837618e-05, 8.820890619323241e-05, 7.953543658750517e-05, 7.17013039497674e-05, 6.458624468288879e-05, 5.8190578434335054e-05, 5.248420606454202e-05, 4.728816955603317e-05, 4.2559635746174445e-05, 3.8304133426736014e-05, 3.447685051573487e-05, 3.104444178691867e-05, 2.7974030429709144e-05, 2.5300690454042734e-05, 2.2849235583414692e-05, 2.0695506121337986e-05, 1.870968786510718e-05, 1.6975835372867742e-05, 1.55084093379281e-05, 1.414399598103647e-05, 1.2853482252068642e-05, 1.1720248853455598e-05, 1.062538287411959e-05, 9.644994763558565e-06, 8.815786803243708e-06, 8.071661887470445e-06, 7.313213463196381e-06, 6.596853857940788e-06, 5.9492874824085825e-06, 5.364175132479839e-06, 4.8305190030866025e-06, 4.349703823700288e-06, 3.9147687664256025e-06, 3.5250533905275953e-06, 3.173974867077923e-06, 2.861564138975139e-06, 2.585443032281687e-06, 2.329443843034119e-06, 2.0972744116983548e-06, 1.8876807640368443e-06, 1.6995996973815704e-06, 1.5301962055396266e-06, 1.377627332081597e-06, 1.2401713169647988e-06, 1.1164026269223221e-06, 1.007343707612672e-06, 9.066922403634853e-07, 8.168310805710408e-07, 7.390174242748537e-07, 6.682499377737113e-07, 6.03963691296678e-07, 5.456237074802837e-07, 4.910789058456245e-07, 4.4305005211569455e-07, 4.0931412347061127e-07, 3.850615312846576e-07, 3.8245579128594046e-07, 3.732895467924493e-07, 3.595148531676398e-07, 3.338667311132304e-07, 3.0882578224441325e-07, 2.9206627694812356e-07, 2.7429933832511745e-07, 2.5613555264077034e-07, 2.3302336324155683e-07, 2.1174713326793884e-07, 1.9058755179408942e-07, 1.7154105341556558e-07, 1.602228479618563e-07, 1.452330711232607e-07, 1.499703954227168e-07, 1.5057446732399323e-07, 1.4126158042436353e-07, 1.317885158464709e-07, 1.2237866996810503e-07, 1.131936975933813e-07, 1.0242725872160646e-07, 9.263240686837231e-08, 8.64680728750629e-08, 7.800297459568129e-08, 7.034986143269466e-08, 6.34340945696565e-08, 5.78086148765118e-08, 5.218416101614991e-08, 4.709243509263948e-08, 4.456886624617976e-08, 4.188237609843236e-08, 3.9128159634853595e-08, 3.637690079984138e-08, 6.522796210460305e-08, 9.824363314859727e-08, 1.2044542830984326e-07, 1.1071554199880557e-07, 9.96885927774524e-08, 8.97558635323144e-08, 8.080954250549466e-08, 8.441053134517483e-08, 8.54318521137406e-08, 9.230644715687721e-08, 9.556420444734239e-08, 1.0483092727441327e-07, 1.0959458059713579e-07, 1.0007060457792808e-07, 9.122628457294587e-08, 8.304547588242817e-08, 7.482194485075993e-08, 6.81531559363162e-08, 6.199669885489631e-08, 5.831112599313625e-08, 6.649120570559406e-08, 1.0801522419238333e-07, 1.3623394440959968e-07, 1.5421694650416775e-07, 1.643964330475248e-07, 1.686937465097287e-07, 1.6862130683998898e-07, 1.6536469349078613e-07, 1.5984869318289087e-07, 1.527904037879593e-07, 1.3985502976034527e-07, 1.3230018402757976e-07, 1.1907131041039266e-07, 1.0857673985485096e-07, 9.886323986261836e-08, 8.990369681089089e-08, 8.16640196867743e-08, 7.410567868924437e-08, 6.718764020694922e-08, 6.086782498942354e-08, 5.776192144486949e-08, 5.934029719902291e-08, 5.93634674770188e-08, 5.4505817268103996e-08, 4.992897973771115e-08, 4.5643814563037843e-08, 4.3509850575681604e-08, 3.9218900806248466e-08, 3.534563930901294e-08, 3.185046453065697e-08, 2.869732329115311e-08, 2.585343418502301e-08, 2.3289023777138712e-08, 2.201658975258225e-08, 2.0670670143382334e-08, 1.9296752015552284e-08, 1.7534162037572438e-08, 1.640512415270187e-08, 1.5270358175729103e-08, 1.4152976973176687e-08, 1.3069499514025631e-08, 1.2031323955538376e-08], "accuracy_test": 0.7003009406887755, "start": "2016-02-03 11:52:09.695000", "learning_rate_per_epoch": [0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777, 0.002290736185386777], "accuracy_train_first": 0.4187704757290513, "accuracy_train_last": 0.9993954613095238, "batch_size_eval": 1024, "accuracy_train_std": [0.0173970389574951, 0.015151639184794993, 0.0165871529271894, 0.016506294157024166, 0.017244216324784494, 0.01830676782429226, 0.01840104937997027, 0.016448136958582403, 0.014850838421599708, 0.01717757565898689, 0.017386861470545845, 0.01682812147131987, 0.016630000049628207, 0.017621532163784204, 0.01771742421648661, 0.017540305464605097, 0.017502915360643502, 0.017483844278828427, 0.01879662684614348, 0.019096609911215933, 0.019954627286650827, 0.019686903019473, 0.019966115454751774, 0.020687787569324486, 0.021884562925937694, 0.022288132571376087, 0.023353188245200485, 0.02377660116874936, 0.023927877285884122, 0.024182186877929363, 0.024679789218520033, 0.02388124256970271, 0.023484641974242354, 0.023957685143471626, 0.023500631547898224, 0.022941340602191225, 0.021326900346945465, 0.02263082382656979, 0.021694876624574286, 0.022957146251542222, 0.022510619255823424, 0.02065762587404028, 0.02077824598932559, 0.01873394893924448, 0.01954317692717289, 0.0199526602230924, 0.018417678451439146, 0.01765873896988301, 0.019212717757530536, 0.01738992232241349, 0.01648552475302773, 0.014498990663249924, 0.013421138660197647, 0.012806197322386302, 0.008768850316306413, 0.008214479190153594, 0.00943482635294676, 0.007398103221469762, 0.006035765652596072, 0.0050018809580099025, 0.004164029279301271, 0.004144573676626343, 0.003911782151192911, 0.00377140283708843, 0.0035440027684631044, 0.0034799640874013595, 0.003143339772343971, 0.0030187560206691747, 0.0027583164965251762, 0.0024900813142820365, 0.002336398566426191, 0.0021477126560185064, 0.0020714714075095223, 0.0018999468023708724, 0.001897099158680548, 0.0017837089298745042, 0.0017245332627221594, 0.001626274967452168, 0.0016495451916422944, 0.0015277130106933782, 0.0014609633623888587, 0.0014529854724672614, 0.001451496376024331, 0.00145799994480942, 0.001412228854079261, 0.001389847854713398, 0.0013613542753259345, 0.0013857574800735992, 0.0013559824508764258, 0.0013447724833446108, 0.001346981804125766, 0.0012796770745737548, 0.0012633070784231887, 0.0012633070784231887, 0.001282841724714818, 0.0013006293034734928, 0.001189240313666706, 0.001169991222601295, 0.001169991222601295, 0.001169991222601295, 0.0011830872612731166, 0.00118011323850907, 0.001157214205555181, 0.001157214205555181, 0.0011034049960662827, 0.0011034049960662827, 0.0011034049960662827, 0.0011121894292319009, 0.0011121894292319009, 0.001105607647690545, 0.0011121894292319009, 0.0011121894292319009, 0.0011259579625003025, 0.0011259579625003025, 0.0010984944020663797, 0.0010984944020663797, 0.0011034049960662827, 0.0011153445828879702, 0.0011153445828879702, 0.0011153445828879702, 0.0011153445828879702, 0.0011153445828879702, 0.001106340890380424, 0.001106340890380424, 0.001117281787270644, 0.001117281787270644, 0.001117281787270644, 0.001117281787270644, 0.0010655170421679315, 0.0011276372445109878, 0.001117281787270644, 0.001117281787270644, 0.001106340890380424, 0.0010738563280092532, 0.0010398381591795896, 0.0010041305283523616, 0.0010041305283523616, 0.0010041305283523616, 0.0010041305283523616, 0.0010041305283523616, 0.0009897598890342616, 0.000974622949922558, 0.000974622949922558, 0.000974622949922558, 0.000974622949922558, 0.000974622949922558, 0.000974622949922558, 0.000974622949922558, 0.0009586834136945826, 0.0009586834136945826, 0.0009586834136945826, 0.0009586834136945826, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009721235565646796, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009779455377982276, 0.0009831833385354342, 0.0009831833385354342, 0.0009831833385354342, 0.0009878462512206102, 0.0009878462512206102, 0.0009878462512206102, 0.0009878462512206102, 0.0009878462512206102, 0.0009878462512206102, 0.0009878462512206102, 0.0009878462512206102, 0.0009878462512206102, 0.0009919423831706445, 0.0009919423831706445, 0.0009724015841855694, 0.0009724015841855694, 0.0009277314612751119, 0.0009277314612751119, 0.0009277314612751119, 0.0009277314612751119, 0.0009277314612751119, 0.0009277314612751119, 0.0009277314612751119, 0.0009277314612751119, 0.0009277314612751119, 0.0009277314612751119], "accuracy_test_std": 0.007922288636411322, "error_valid": [0.5887171733810241, 0.537975633000753, 0.5062152908509037, 0.4874767625188253, 0.47474026967243976, 0.4578019107680723, 0.4406811817582832, 0.41971450254141573, 0.3999994117093373, 0.3831022331513554, 0.37245123070406627, 0.3646284356174698, 0.35567612245858427, 0.34847397402108427, 0.3429911050451807, 0.3387392342808735, 0.33297104433358427, 0.32882065370858427, 0.32278773296310237, 0.32242152202560237, 0.31811817582831325, 0.3178637401167168, 0.31552381400602414, 0.3157885448042168, 0.31601209525602414, 0.3155649943524097, 0.31408985551581325, 0.3147105021649097, 0.31655185193900603, 0.31655185193900603, 0.31630771131400603, 0.31939035438629515, 0.32268625282379515, 0.3226759577371988, 0.32296127870858427, 0.3233583749058735, 0.3273866952183735, 0.3300825371799698, 0.3332460702183735, 0.3362066429781627, 0.3323915780308735, 0.3416292121611446, 0.3509565606174698, 0.3404791039156627, 0.33006194700677716, 0.34207631306475905, 0.3452604362763554, 0.34178069700677716, 0.3400305322853916, 0.32748670463102414, 0.3266733927899097, 0.32907361869352414, 0.31647978633283136, 0.3197345044239458, 0.3064494305346386, 0.3101115399096386, 0.31208525508283136, 0.3060832195971386, 0.3071112575301205, 0.3082098903426205, 0.30725391801581325, 0.3067553416792168, 0.3062670604292168, 0.3074877635542168, 0.3077319041792168, 0.3067450465926205, 0.30636854056852414, 0.3069891872176205, 0.3057684840926205, 0.3054022731551205, 0.3056464137801205, 0.3059008494917168, 0.3056567088667168, 0.3045477809676205, 0.3043036403426205, 0.3043036403426205, 0.3039374294051205, 0.3036932887801205, 0.3039374294051205, 0.3039374294051205, 0.3034491481551205, 0.3035712184676205, 0.3036932887801205, 0.30356092338102414, 0.30392713431852414, 0.30404920463102414, 0.30417127494352414, 0.30417127494352414, 0.3044257106551205, 0.30429334525602414, 0.30392713431852414, 0.30417127494352414, 0.3049139919051205, 0.3047919215926205, 0.3046698512801205, 0.3044257106551205, 0.3041815700301205, 0.30356092338102414, 0.30368299369352414, 0.30331678275602414, 0.30343885306852414, 0.30307264213102414, 0.30258436088102414, 0.30258436088102414, 0.30270643119352414, 0.30246229056852414, 0.30270643119352414, 0.30258436088102414, 0.30221814994352414, 0.3020857845444277, 0.30246229056852414, 0.3027167262801205, 0.3027167262801205, 0.3027167262801205, 0.3028387965926205, 0.3028387965926205, 0.3029608669051205, 0.3028387965926205, 0.3028387965926205, 0.3027167262801205, 0.3025946559676205, 0.3027270213667168, 0.3029711619917168, 0.3028490916792168, 0.3029711619917168, 0.3029711619917168, 0.3029711619917168, 0.3028490916792168, 0.3028490916792168, 0.3027270213667168, 0.3028490916792168, 0.3029711619917168, 0.3030932323042168, 0.3030932323042168, 0.3030932323042168, 0.3030932323042168, 0.3029711619917168, 0.3028490916792168, 0.3026049510542168, 0.3024828807417168, 0.3022387401167168, 0.3022387401167168, 0.3022387401167168, 0.3023608104292168, 0.3023608104292168, 0.3022387401167168, 0.3022387401167168, 0.3022387401167168, 0.3023608104292168, 0.3023608104292168, 0.3024828807417168, 0.3024828807417168, 0.3022387401167168, 0.3023608104292168, 0.3019945994917168, 0.3019945994917168, 0.3021166698042168, 0.3021166698042168, 0.3021166698042168, 0.3021166698042168, 0.3023608104292168, 0.3023608104292168, 0.3024828807417168, 0.3023608104292168, 0.3023608104292168, 0.3023608104292168, 0.3022387401167168, 0.3023608104292168, 0.3023608104292168, 0.3024828807417168, 0.3024828807417168, 0.3024828807417168, 0.3024828807417168, 0.30298145707831325, 0.30310352739081325, 0.30310352739081325, 0.3027270213667168, 0.3026049510542168, 0.3026049510542168, 0.3026049510542168, 0.3022284450301205, 0.3022284450301205, 0.3021063747176205, 0.3021063747176205, 0.3019843044051205, 0.3019843044051205, 0.3022284450301205, 0.3022284450301205, 0.3022284450301205, 0.3023505153426205, 0.3022284450301205, 0.3022284450301205, 0.3024725856551205, 0.3027167262801205, 0.3030932323042168, 0.3030932323042168, 0.3030932323042168, 0.3030932323042168, 0.3030932323042168, 0.3030932323042168, 0.3030932323042168, 0.3030932323042168, 0.3030932323042168, 0.3029711619917168, 0.3030932323042168, 0.3028490916792168, 0.3027270213667168, 0.3027270213667168, 0.3027270213667168, 0.3027270213667168, 0.3027270213667168, 0.3027270213667168, 0.3027270213667168, 0.3026049510542168, 0.3024828807417168, 0.3024828807417168, 0.3026049510542168, 0.3026049510542168, 0.3026049510542168, 0.3028490916792168, 0.3027270213667168, 0.3027270213667168, 0.3027270213667168, 0.3027270213667168, 0.3027270213667168, 0.3027270213667168, 0.3026049510542168, 0.3026049510542168, 0.3026049510542168, 0.3027270213667168, 0.3026049510542168, 0.3026049510542168, 0.3026049510542168, 0.3026049510542168, 0.3026049510542168], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.016582874272360028, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.002290736089600165, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.344335004930743e-07, "rotation_range": [0, 0], "momentum": 0.7272140859786154}, "accuracy_valid_max": 0.6980156955948795, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6973950489457832, "accuracy_valid_std": [0.013508577825400863, 0.013368120493658365, 0.011437995050437117, 0.012126422939485447, 0.011757873705893932, 0.01855416989398872, 0.014979447890661423, 0.017088061508471784, 0.013904886375711363, 0.014961356892376544, 0.01419020727581763, 0.013625651229174316, 0.01285001738727488, 0.010381483498832186, 0.010678820569801321, 0.012775934796815299, 0.013938037882721598, 0.015959349251768548, 0.015358669330075982, 0.016736942500869094, 0.017413595404107553, 0.017124067477642977, 0.016708545399999726, 0.014928321557766536, 0.015614019958382967, 0.016489984559117755, 0.013690517783138735, 0.0146233574908437, 0.01359968835695213, 0.013988579608587485, 0.01152169018134848, 0.01307564891790924, 0.012453986540887136, 0.012344890141671404, 0.015429111711658438, 0.016303433663071475, 0.016994470720514118, 0.017840976134279075, 0.014604871988731904, 0.01429969724514297, 0.014872953593878876, 0.0168855364257137, 0.010291199782636014, 0.015616974283219025, 0.015994170107142946, 0.014666593266694694, 0.016634453908156478, 0.0140209844289549, 0.012241106586362202, 0.012800524522533309, 0.01525316297426635, 0.01589492779089809, 0.012984376902436484, 0.01612246966501408, 0.014553225148200203, 0.015670419088695126, 0.017151434179654486, 0.015461551107210526, 0.013279415686441634, 0.013154044953177442, 0.013177337165169771, 0.013475708735007258, 0.013837699073941546, 0.013979763067462772, 0.01355562081528069, 0.013689467390363858, 0.013759773313844686, 0.013457044349508836, 0.01353054000592706, 0.012938371849995394, 0.013096671491537797, 0.013173854135479363, 0.013085192432493873, 0.013017010645620032, 0.013533034283098577, 0.01368196377543253, 0.013460239168849187, 0.013863487303960193, 0.013827205748266258, 0.014533367794472785, 0.014671559570002705, 0.014563139141385358, 0.014328618289201907, 0.01374268992012225, 0.014497439506401527, 0.014342152033108612, 0.014458800379969118, 0.014655338765318004, 0.014930456285840787, 0.014334313754011936, 0.01488690176089863, 0.014833211488631416, 0.014859003761002819, 0.014824220177723299, 0.015193898781995792, 0.015105089660231647, 0.015106777311836192, 0.014746923602325239, 0.014441356667435382, 0.014307435735103891, 0.014281091705875272, 0.013667760586312404, 0.013530886984790354, 0.013964448845201415, 0.013943869924883298, 0.014060445210161577, 0.013952416531238111, 0.01402407792348424, 0.01420546358133735, 0.013778520166949493, 0.014111223582048617, 0.01418526404678144, 0.01418526404678144, 0.014176857814685475, 0.013658271525726528, 0.013658271525726528, 0.013929040584264646, 0.013960409425909641, 0.013960409425909641, 0.0136717446337626, 0.013744960870169258, 0.014285689945410133, 0.01457290545778099, 0.014651875584574271, 0.014670739464343363, 0.014670739464343363, 0.01491251668242784, 0.014893959027702826, 0.014789542724187593, 0.014802071061165415, 0.014643737213114077, 0.014630054802875277, 0.014393431263788212, 0.014393431263788212, 0.014558132954724214, 0.014696678229183415, 0.014888515643487902, 0.014733012045636127, 0.014885832401936326, 0.014864234290624466, 0.014615419908369093, 0.014825969855445958, 0.014825969855445958, 0.014769134919158712, 0.014769134919158712, 0.014882147432689383, 0.014882147432689383, 0.014696757631837276, 0.014929692489878833, 0.014929692489878833, 0.014848185878698215, 0.014848185878698215, 0.014938113745115684, 0.014761061197665012, 0.01497585189134478, 0.01497585189134478, 0.014897598896623801, 0.014897598896623801, 0.014897598896623801, 0.014897598896623801, 0.014785269135816105, 0.01510433469513235, 0.014999952562968415, 0.014761061197665012, 0.014761061197665012, 0.014761061197665012, 0.0149141538990554, 0.015151615031315579, 0.015088541659960902, 0.014904279718996286, 0.014904279718996286, 0.014904279718996286, 0.014904279718996286, 0.015357405214268831, 0.01529661830725243, 0.01529661830725243, 0.01485031377360088, 0.014845737213187818, 0.014845737213187818, 0.014845737213187818, 0.014451085617701974, 0.014451085617701974, 0.014442169055247747, 0.014442169055247747, 0.014190654579483828, 0.014190654579483828, 0.014108816881191305, 0.014108816881191305, 0.014108816881191305, 0.014015188797343387, 0.013766697883767417, 0.013766697883767417, 0.013528994119350202, 0.013389817280290513, 0.013801505823941334, 0.013801505823941334, 0.013801505823941334, 0.013801505823941334, 0.013801505823941334, 0.013801505823941334, 0.013801505823941334, 0.013801505823941334, 0.013801505823941334, 0.013799821165655708, 0.014041270712550567, 0.013805693812937682, 0.013862179058911123, 0.013862179058911123, 0.013862179058911123, 0.013862179058911123, 0.013862179058911123, 0.013862179058911123, 0.013862179058911123, 0.013623024658797379, 0.013721593045622003, 0.013721593045622003, 0.013579201300470276, 0.013579201300470276, 0.013579201300470276, 0.013384808174184299, 0.013469638903910666, 0.013469638903910666, 0.013469638903910666, 0.013469638903910666, 0.013469638903910666, 0.013469638903910666, 0.013473443695264953, 0.013473443695264953, 0.013473443695264953, 0.013425314868048195, 0.013438006202300573, 0.013438006202300573, 0.013438006202300573, 0.013438006202300573, 0.013438006202300573], "accuracy_valid": [0.4112828266189759, 0.462024366999247, 0.4937847091490964, 0.5125232374811747, 0.5252597303275602, 0.5421980892319277, 0.5593188182417168, 0.5802854974585843, 0.6000005882906627, 0.6168977668486446, 0.6275487692959337, 0.6353715643825302, 0.6443238775414157, 0.6515260259789157, 0.6570088949548193, 0.6612607657191265, 0.6670289556664157, 0.6711793462914157, 0.6772122670368976, 0.6775784779743976, 0.6818818241716867, 0.6821362598832832, 0.6844761859939759, 0.6842114551957832, 0.6839879047439759, 0.6844350056475903, 0.6859101444841867, 0.6852894978350903, 0.683448148060994, 0.683448148060994, 0.683692288685994, 0.6806096456137049, 0.6773137471762049, 0.6773240422628012, 0.6770387212914157, 0.6766416250941265, 0.6726133047816265, 0.6699174628200302, 0.6667539297816265, 0.6637933570218373, 0.6676084219691265, 0.6583707878388554, 0.6490434393825302, 0.6595208960843373, 0.6699380529932228, 0.657923686935241, 0.6547395637236446, 0.6582193029932228, 0.6599694677146084, 0.6725132953689759, 0.6733266072100903, 0.6709263813064759, 0.6835202136671686, 0.6802654955760542, 0.6935505694653614, 0.6898884600903614, 0.6879147449171686, 0.6939167804028614, 0.6928887424698795, 0.6917901096573795, 0.6927460819841867, 0.6932446583207832, 0.6937329395707832, 0.6925122364457832, 0.6922680958207832, 0.6932549534073795, 0.6936314594314759, 0.6930108127823795, 0.6942315159073795, 0.6945977268448795, 0.6943535862198795, 0.6940991505082832, 0.6943432911332832, 0.6954522190323795, 0.6956963596573795, 0.6956963596573795, 0.6960625705948795, 0.6963067112198795, 0.6960625705948795, 0.6960625705948795, 0.6965508518448795, 0.6964287815323795, 0.6963067112198795, 0.6964390766189759, 0.6960728656814759, 0.6959507953689759, 0.6958287250564759, 0.6958287250564759, 0.6955742893448795, 0.6957066547439759, 0.6960728656814759, 0.6958287250564759, 0.6950860080948795, 0.6952080784073795, 0.6953301487198795, 0.6955742893448795, 0.6958184299698795, 0.6964390766189759, 0.6963170063064759, 0.6966832172439759, 0.6965611469314759, 0.6969273578689759, 0.6974156391189759, 0.6974156391189759, 0.6972935688064759, 0.6975377094314759, 0.6972935688064759, 0.6974156391189759, 0.6977818500564759, 0.6979142154555723, 0.6975377094314759, 0.6972832737198795, 0.6972832737198795, 0.6972832737198795, 0.6971612034073795, 0.6971612034073795, 0.6970391330948795, 0.6971612034073795, 0.6971612034073795, 0.6972832737198795, 0.6974053440323795, 0.6972729786332832, 0.6970288380082832, 0.6971509083207832, 0.6970288380082832, 0.6970288380082832, 0.6970288380082832, 0.6971509083207832, 0.6971509083207832, 0.6972729786332832, 0.6971509083207832, 0.6970288380082832, 0.6969067676957832, 0.6969067676957832, 0.6969067676957832, 0.6969067676957832, 0.6970288380082832, 0.6971509083207832, 0.6973950489457832, 0.6975171192582832, 0.6977612598832832, 0.6977612598832832, 0.6977612598832832, 0.6976391895707832, 0.6976391895707832, 0.6977612598832832, 0.6977612598832832, 0.6977612598832832, 0.6976391895707832, 0.6976391895707832, 0.6975171192582832, 0.6975171192582832, 0.6977612598832832, 0.6976391895707832, 0.6980054005082832, 0.6980054005082832, 0.6978833301957832, 0.6978833301957832, 0.6978833301957832, 0.6978833301957832, 0.6976391895707832, 0.6976391895707832, 0.6975171192582832, 0.6976391895707832, 0.6976391895707832, 0.6976391895707832, 0.6977612598832832, 0.6976391895707832, 0.6976391895707832, 0.6975171192582832, 0.6975171192582832, 0.6975171192582832, 0.6975171192582832, 0.6970185429216867, 0.6968964726091867, 0.6968964726091867, 0.6972729786332832, 0.6973950489457832, 0.6973950489457832, 0.6973950489457832, 0.6977715549698795, 0.6977715549698795, 0.6978936252823795, 0.6978936252823795, 0.6980156955948795, 0.6980156955948795, 0.6977715549698795, 0.6977715549698795, 0.6977715549698795, 0.6976494846573795, 0.6977715549698795, 0.6977715549698795, 0.6975274143448795, 0.6972832737198795, 0.6969067676957832, 0.6969067676957832, 0.6969067676957832, 0.6969067676957832, 0.6969067676957832, 0.6969067676957832, 0.6969067676957832, 0.6969067676957832, 0.6969067676957832, 0.6970288380082832, 0.6969067676957832, 0.6971509083207832, 0.6972729786332832, 0.6972729786332832, 0.6972729786332832, 0.6972729786332832, 0.6972729786332832, 0.6972729786332832, 0.6972729786332832, 0.6973950489457832, 0.6975171192582832, 0.6975171192582832, 0.6973950489457832, 0.6973950489457832, 0.6973950489457832, 0.6971509083207832, 0.6972729786332832, 0.6972729786332832, 0.6972729786332832, 0.6972729786332832, 0.6972729786332832, 0.6972729786332832, 0.6973950489457832, 0.6973950489457832, 0.6973950489457832, 0.6972729786332832, 0.6973950489457832, 0.6973950489457832, 0.6973950489457832, 0.6973950489457832, 0.6973950489457832], "seed": 900223766, "model": "residualv3", "loss_std": [0.23448017239570618, 0.09752961248159409, 0.09694986045360565, 0.098194420337677, 0.09886720776557922, 0.09912150353193283, 0.09921865910291672, 0.0985817089676857, 0.09778590500354767, 0.09713657200336456, 0.09647247195243835, 0.09541101008653641, 0.09445685893297195, 0.09339714795351028, 0.09251800924539566, 0.091333769261837, 0.0901445522904396, 0.08881678432226181, 0.08759255707263947, 0.0861353948712349, 0.08466801047325134, 0.08288589864969254, 0.08120524138212204, 0.07963898777961731, 0.07779451459646225, 0.0760621577501297, 0.07403325289487839, 0.07197177410125732, 0.06959114968776703, 0.06683000177145004, 0.06393861025571823, 0.060877036303281784, 0.05756114050745964, 0.05421363189816475, 0.05076492205262184, 0.04727891832590103, 0.044041600078344345, 0.04029093682765961, 0.036524444818496704, 0.03341019153594971, 0.030913345515727997, 0.027753718197345734, 0.02518715336918831, 0.023855699226260185, 0.020064551383256912, 0.018362581729888916, 0.019649041816592216, 0.01591349206864834, 0.012755175121128559, 0.01357996929436922, 0.013392969965934753, 0.010836979374289513, 0.008551348932087421, 0.007631510030478239, 0.00607788423076272, 0.004705517552793026, 0.004536167252808809, 0.0034832500386983156, 0.002858777530491352, 0.0018879950512200594, 0.0017664278857409954, 0.001437519327737391, 0.0012594560394063592, 0.0010760584846138954, 0.0009440514259040356, 0.0008374333847314119, 0.0007559568039141595, 0.0006935927085578442, 0.000642762694042176, 0.0005985577590763569, 0.000559979525860399, 0.0005266745574772358, 0.00049673265311867, 0.00047021560021676123, 0.00044587304000742733, 0.00042396789649501443, 0.0004043509252369404, 0.0003864278260152787, 0.0003699383232742548, 0.00035439454950392246, 0.0003402208676561713, 0.0003271167224738747, 0.0003148191317450255, 0.00030321130179800093, 0.0002926225424744189, 0.0002825256378855556, 0.0002728794643189758, 0.0002639650192577392, 0.00025565907708369195, 0.0002478935057297349, 0.00024059310089796782, 0.0002335581521037966, 0.00022706201707478613, 0.00022082592477090657, 0.0002149493375327438, 0.00020941697584930807, 0.00020414681057445705, 0.00019911439449060708, 0.00019442031043581665, 0.0001898195332614705, 0.00018544774502515793, 0.00018128135707229376, 0.00017727860540617257, 0.0001734310935717076, 0.00016975794278550893, 0.00016619778762105852, 0.0001628061872906983, 0.00015957474533934146, 0.00015646207612007856, 0.000153423345182091, 0.0001505221880506724, 0.00014773235307075083, 0.0001450416020816192, 0.00014241921599023044, 0.00013988862338010222, 0.00013744061288889498, 0.00013509055133908987, 0.00013281554856803268, 0.00013061011850368232, 0.00012848405458498746, 0.00012639205669984221, 0.00012440145656000823, 0.00012245272228028625, 0.00012056536070303991, 0.00011874980555148795, 0.00011694982822518796, 0.00011522678687470034, 0.00011351924331393093, 0.0001118760701501742, 0.00011028296285076067, 0.00010873524297494441, 0.0001072222730726935, 0.000105753329989966, 0.00010432445560581982, 0.00010293570085195825, 0.00010157885117223486, 0.00010025608207797632, 9.89601612673141e-05, 9.770417091203853e-05, 9.646081161918119e-05, 9.526203211862594e-05, 9.408297046320513e-05, 9.293202310800552e-05, 9.181354835163802e-05, 9.072475222637877e-05, 8.966102177510038e-05, 8.861887909006327e-05, 8.759607590036467e-05, 8.661724132252857e-05, 8.56483238749206e-05, 8.468040323350579e-05, 8.374213939532638e-05, 8.282304770546034e-05, 8.19107735878788e-05, 8.10227429610677e-05, 8.016073115868494e-05, 7.93172002886422e-05, 7.85008815000765e-05, 7.769888907205313e-05, 7.690644270041957e-05, 7.612595072714612e-05, 7.53560452722013e-05, 7.459863991243765e-05, 7.385884237010032e-05, 7.312785601243377e-05, 7.2418712079525e-05, 7.171264587668702e-05, 7.102515519363806e-05, 7.035040471237153e-05, 6.969366950215772e-05, 6.904010660946369e-05, 6.840373680461198e-05, 6.777186354156584e-05, 6.715233757859096e-05, 6.654352910118178e-05, 6.594383739866316e-05, 6.535997817991301e-05, 6.478561408584937e-05, 6.42167724436149e-05, 6.366091838572174e-05, 6.311402103165165e-05, 6.257314089452848e-05, 6.204732926562428e-05, 6.152843707241118e-05, 6.101397957536392e-05, 6.051258242223412e-05, 6.000940265948884e-05, 5.9522648371057585e-05, 5.904501449549571e-05, 5.8576930314302444e-05, 5.8113822888117284e-05, 5.765927562606521e-05, 5.72077369724866e-05, 5.675811553373933e-05, 5.63137473363895e-05, 5.587722625932656e-05, 5.545193562284112e-05, 5.5027099733706564e-05, 5.4612424719380215e-05, 5.4201042075874284e-05, 5.378923378884792e-05, 5.3385905630420893e-05, 5.299085387378e-05, 5.2605704695452005e-05, 5.2222367230569944e-05, 5.1845931011484936e-05, 5.147334377397783e-05, 5.110331767355092e-05, 5.0743874453473836e-05, 5.038763993070461e-05, 5.003946353099309e-05, 4.968992652720772e-05, 4.934798562317155e-05, 4.900670319329947e-05, 4.867053939960897e-05, 4.833923958358355e-05, 4.801101749762893e-05, 4.768659709952772e-05, 4.736766641144641e-05, 4.705283572548069e-05, 4.6742345148231834e-05, 4.6432371163973585e-05, 4.612774864654057e-05, 4.58273061667569e-05, 4.552723476081155e-05, 4.523132156464271e-05, 4.4941021769773215e-05, 4.465140955289826e-05, 4.4368571252562106e-05, 4.408857421367429e-05, 4.3811171053675935e-05, 4.353858093963936e-05, 4.3268330045975745e-05, 4.299855572753586e-05, 4.27378763561137e-05]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:34 2016", "state": "available"}], "summary": "82c81bf31cab636ce6e5eb1db767e741"}