{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 64, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.01321954962953157, 0.00800569882941189, 0.010065733070328521, 0.010028286108361003, 0.008870380432125995, 0.011390145781924014, 0.007237680036107827, 0.008154433834672877, 0.011735867919308487, 0.008483753980073455, 0.011847107730520548, 0.008153895208500836, 0.005487823141745303, 0.008118678508690233, 0.006559868077868721, 0.009581626032461286, 0.009453805208755646, 0.008641320522666479, 0.009228847973988503, 0.010078444129033033, 0.009916292983971974, 0.008967857786319769, 0.011512712899550843, 0.011688325825298048, 0.012066602645086444, 0.010698056803958094, 0.010521595256346258, 0.010668249213691394, 0.01065816429787673, 0.012953235644183127, 0.011364918312285015, 0.010174039171009612, 0.012948615593968446, 0.011581206217036177, 0.013330658627169692, 0.010333726969521777, 0.01333473195576096, 0.016046130572570313, 0.012687908660404429, 0.01328724162183257, 0.01106165177435398, 0.010604463318930748, 0.013956770399523455, 0.011823740824593861, 0.014068001646601679, 0.015253528270063639, 0.010884722990100458, 0.010870084007153757, 0.011537986257242802, 0.013091818790770073, 0.012819299592194222, 0.011955927599046345, 0.014235390473328021, 0.014299012908060876, 0.013383428248344844, 0.015707874061626714, 0.013298377768223084, 0.014412068201208121, 0.012702624158671585, 0.014993705617559767, 0.01617151431055876, 0.013122090520153258, 0.014227721119150492, 0.013869394908606123, 0.01125755938741247, 0.01217895370807513, 0.010451035260799421, 0.013346809646210203, 0.0107575883826508, 0.011692920322265819, 0.01142750703504148, 0.01438054286291666, 0.011450617121845431, 0.012302784433221808, 0.01478579228375722, 0.013958777261661766, 0.01368431324634364, 0.010782496347453607, 0.01064165084032689, 0.012188866842129024, 0.012643336664482356, 0.013506865038919251, 0.011967296569246284, 0.01394905074373093, 0.012328415040534299, 0.014056159618169186, 0.012806399212079761, 0.012124807478181784, 0.013390303759467375, 0.01162910142586223, 0.012301999561733659, 0.012965485696685571, 0.013639693060589713, 0.010097288936782804, 0.010677786583291257, 0.01563377538175988, 0.012805308137856962, 0.011240274577940882, 0.012483394788390575, 0.011276385648731289, 0.012563138128627747, 0.011562332446664682, 0.012190130647074074, 0.011462948761432461, 0.011336770816087822, 0.013629626636977981, 0.01147323498703921, 0.01283769219834291, 0.013020420755483758, 0.013094506322076078, 0.013606004454669054, 0.015725084723248822, 0.010739749872547317, 0.01420888189019786, 0.013164558347066074, 0.012182698544686451, 0.015622494414030722, 0.01582286753631338, 0.013467214558131187, 0.012141993988488285, 0.01467931141363183, 0.013311946615343866, 0.012743714342722548, 0.013631287422277571, 0.012574958806928572, 0.01308177680251228, 0.010948025119595967, 0.012819401590793582, 0.01410972866893253, 0.01145830405400896, 0.011232165754830586, 0.010598767747689681, 0.011801771896383142, 0.011772761763487556, 0.009054227983526472, 0.010991913104099891, 0.012860798744941346, 0.011270508285182279, 0.011674678224910244, 0.011144298165960579, 0.01060367564331258, 0.013314831771684242, 0.010997657707119085, 0.011899328272141305, 0.010924270802039003, 0.012290783650833527, 0.012079732972663066, 0.011221062638014779, 0.011693278358898801, 0.01368473962514901, 0.010582447150893655, 0.010664401960559556, 0.013593335033135222, 0.011398570616028713, 0.010567805867348049, 0.009967395693984145, 0.010953077877338057, 0.014137743384189005, 0.01273870511286059, 0.0110816292565334, 0.011890585534454042, 0.011188099077737088, 0.011134341860523247, 0.010219798255760075, 0.01126351408246065, 0.007656133999940897, 0.010933358521549813, 0.010861144092613035, 0.011274067936567265, 0.011717090146743648, 0.009654823823698361, 0.011471443162283053, 0.011349235019817789, 0.009734630196122411, 0.011262221968212263], "moving_avg_accuracy_train": [0.017891208990863786, 0.05285086675779807, 0.11449791024997691, 0.17989077261991276, 0.24108276352767966, 0.29984248433138083, 0.35360724816060896, 0.405417902312802, 0.4523772295949031, 0.4969866271999661, 0.5376535571802279, 0.5754881597897706, 0.6081863539217662, 0.6403581518940692, 0.6700404695488852, 0.6963406429013057, 0.7215915396207728, 0.744791677025436, 0.7659786482348525, 0.7854258855304611, 0.8030865813131939, 0.8189439690878823, 0.833752691411283, 0.84739211144282, 0.8601208853426041, 0.8717860812940857, 0.8823428503218385, 0.8920275931539496, 0.9007508371492783, 0.9084320208819787, 0.9156008526104568, 0.9219202676839442, 0.927849592775092, 0.9335416970761635, 0.9380996518840511, 0.9427620639277888, 0.9472860807492957, 0.9514739893779467, 0.955080346727066, 0.9583632707222257, 0.9616271110607174, 0.9643367027820265, 0.9670613286347763, 0.9696181235986796, 0.9718494846019069, 0.9739181633738591, 0.9757986115079109, 0.9774561375964147, 0.979052506723678, 0.980538067063215, 0.981972763667617, 0.9832453533722838, 0.9843348805350554, 0.9853898597434546, 0.9863579422214901, 0.9871920140707697, 0.988110089449407, 0.9888735782723235, 0.9896141966355673, 0.9901970478053438, 0.9906588348402856, 0.9912418899348376, 0.9917526525782586, 0.992268142528766, 0.992750684674699, 0.9931686965643719, 0.9935332815210299, 0.9939125973046504, 0.994239994568233, 0.9944998109221332, 0.9947592222775482, 0.9950252806295737, 0.9951879671868638, 0.9953645759741298, 0.9955072478410025, 0.9957030818366642, 0.995867706688712, 0.9960786480734123, 0.9962638450220235, 0.996379369001964, 0.9964717148398629, 0.9965967148213619, 0.996725454797559, 0.9968064435439936, 0.9968886340110228, 0.9969788814730157, 0.9970438281471428, 0.9971627340229047, 0.9972209932837278, 0.9973012923553644, 0.9974154141984086, 0.9974878969226246, 0.9974833769101333, 0.9975188724774717, 0.9975345424464096, 0.9976044129410635, 0.9976300940052997, 0.997683433897636, 0.9977453546447772, 0.9977871684731658, 0.9978178615211059, 0.9978594001082902, 0.9978874842415181, 0.9979267108542803, 0.9979550033105189, 0.9979851528675715, 0.998030888659395, 0.9980557748303696, 0.9980990987235323, 0.9981311147809503, 0.9981855058695311, 0.9982414332956825, 0.998275491937552, 0.9983037835176064, 0.9983292819884739, 0.9983382436705789, 0.9983207685963873, 0.9983515079569867, 0.9983629333886782, 0.9983871671700577, 0.998409013622118, 0.9984309284801443, 0.9984390621571391, 0.9984579721616632, 0.998474991165735, 0.9984996449134564, 0.9985102435911768, 0.9985151321035062, 0.9985171705669743, 0.9985259806305242, 0.998540849085329, 0.9985705427851387, 0.9986019174125864, 0.9986347688260897, 0.9986713465934899, 0.9987089168817692, 0.9987497055876492, 0.998772464530084, 0.9987999230247039, 0.9988106487281859, 0.9988086761172721, 0.9988464282972115, 0.9988362274317761, 0.9988549844874173, 0.9988671794910565, 0.9988828052919508, 0.9988875679175176, 0.9988988297269563, 0.9989322168435464, 0.9989459892068108, 0.9989560591849391, 0.9989697724628738, 0.9989797892642055, 0.9989748534925469, 0.9989564604051969, 0.998944556924201, 0.998961745577019, 0.9989562890252696, 0.9989583535751236, 0.9989671871164207, 0.9989890881964453, 0.9990204249125151, 0.9990323519153113, 0.9990198347297325, 0.9990550722389022, 0.9990519087650119, 0.9990653376801774, 0.9990518470669215, 0.9990559815566579, 0.9990574134974298, 0.9990656416417345, 0.9990754081692369, 0.9990702111023132, 0.9990818097837485, 0.999064346811326], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 805869726, "moving_var_accuracy_train": [0.002880858232392887, 0.013592371449784119, 0.04643635604674502, 0.08027875848247089, 0.10595102039552987, 0.12643026145633715, 0.1398030837772175, 0.14998187035259922, 0.15483028908642665, 0.1572572453699634, 0.1564157135791427, 0.15365725661282656, 0.14791407804698695, 0.1424378915052243, 0.1361234621869545, 0.12873640803356537, 0.12160123729644357, 0.1142853309471565, 0.10689678759369867, 0.09961086418021416, 0.09245687934196486, 0.0854743021310999, 0.07890055622965421, 0.072684804615859, 0.06687451931919951, 0.06141175855655771, 0.05627359105164982, 0.051490380140001984, 0.0470261969982201, 0.04285458255021761, 0.03903165363035695, 0.03548790332916043, 0.032255525060573026, 0.02932157301688421, 0.026576390283472494, 0.024114394029715564, 0.0218871551805555, 0.01985628687063732, 0.01798771050353951, 0.01628593776280752, 0.01475321787032326, 0.01334397306895662, 0.01207638803639821, 0.010927584037145362, 0.009879636380771335, 0.008930187629447932, 0.008068993633166873, 0.0072868208044568235, 0.006581074273525456, 0.005942828851874557, 0.005367071155807246, 0.004844939401234338, 0.004371129086056659, 0.003944033007622386, 0.0035580643600186613, 0.003208519006664641, 0.0028952528676059173, 0.0026109738174897906, 0.002354813075780578, 0.00212238920757751, 0.0019120695122105222, 0.0017239221401790185, 0.0015538778324623464, 0.0014008816182177796, 0.0012628890786994156, 0.0011381727762886458, 0.0010255517983753734, 0.000924291542711169, 0.0008328270891538644, 0.000750151921078264, 0.0006757423772323016, 0.0006088052229292144, 0.0005481629028795987, 0.0004936273285652949, 0.000444447793063138, 0.0004003481723415355, 0.000360557267184588, 0.0003249020068761424, 0.0002927204873765021, 0.00026356855074832357, 0.00023728844545748638, 0.0002137002258701108, 0.00019247936911634088, 0.00017329046479814817, 0.00015602221577416768, 0.00014049329563631645, 0.00012648192870700622, 0.00011396098330192197, 0.0001025954322449748, 9.23939204886284e-05, 8.327174259530385e-05, 7.49918520435614e-05, 6.749285071382155e-05, 6.0754905060145516e-05, 5.4681624485469616e-05, 4.9257399011131386e-05, 4.4337594763560985e-05, 3.992944158423502e-05, 3.59710050361502e-05, 3.238964009873582e-05, 2.9159154657588846e-05, 2.6258768279857408e-05, 2.3639989918724092e-05, 2.1289839471190836e-05, 1.916805969179189e-05, 1.7259434684726902e-05, 1.555231708013772e-05, 1.4002659265675892e-05, 1.2619285976577299e-05, 1.1366582630312845e-05, 1.0256549881934664e-05, 9.259045786704439e-06, 8.34358112780798e-06, 7.516426736544911e-06, 6.770635611039711e-06, 6.094294855651099e-06, 5.48761377404798e-06, 4.947356571253701e-06, 4.453795778532368e-06, 4.013701686118677e-06, 3.616626924715418e-06, 3.259286581264691e-06, 2.933953333451324e-06, 2.6437762945461353e-06, 2.382005483587876e-06, 2.149275200719499e-06, 1.9353586683723444e-06, 1.7420378795102574e-06, 1.56787148955903e-06, 1.4117828955809048e-06, 1.2725942445573324e-06, 1.153270262377077e-06, 1.0468025413667325e-06, 9.518352255525213e-07, 8.686931006091589e-07, 7.9452752960074e-07, 7.300482433868941e-07, 6.617051441949547e-07, 6.023203501165679e-07, 5.431236815415638e-07, 4.888463341317635e-07, 4.5278874453021765e-07, 4.0844638897788425e-07, 3.707681943070188e-07, 3.3502983790016324e-07, 3.0372434499245564e-07, 2.7355605391381795e-07, 2.473419036889414e-07, 2.3264000930784272e-07, 2.1108311028604015e-07, 1.9088743939300353e-07, 1.7349118137912173e-07, 1.5704509002147218e-07, 1.4155983759612148e-07, 1.3044860479686514e-07, 1.1867898005555166e-07, 1.094701301212734e-07, 9.879108272210285e-08, 8.895033574478917e-08, 8.075758523694838e-08, 7.69987424694458e-08, 7.813677618883234e-08, 7.160337913123368e-08, 6.585316063142475e-08, 7.044298304058224e-08, 6.348875284001085e-08, 5.876289941870284e-08, 5.4524579290997496e-08, 4.922596741032294e-08, 4.4321824758661684e-08, 4.0498963511076065e-08, 3.730753269507244e-08, 3.381986496704896e-08, 3.164864316969172e-08, 3.1228377505173267e-08], "duration": 114730.534283, "accuracy_train": [0.17891208990863788, 0.3674877866602067, 0.6693213016795866, 0.7684265339493356, 0.7918106816975821, 0.8286799715646919, 0.8374901226236618, 0.8717137896825397, 0.8750111751338132, 0.8984712056455334, 0.9036559270025839, 0.9159995832756552, 0.9024701011097268, 0.9299043336447952, 0.9371813284422297, 0.9330422030730897, 0.9488496100959765, 0.9535929136674051, 0.9566613891196014, 0.9604510211909376, 0.9620328433577889, 0.9616604590600776, 0.96703119232189, 0.9701468917266519, 0.9746798504406607, 0.9767728448574198, 0.9773537715716132, 0.9791902786429494, 0.9792600331072352, 0.9775626744762828, 0.9801203381667589, 0.9787950033453304, 0.9812135185954227, 0.9847706357858066, 0.9791212451550388, 0.9847237723214286, 0.9880022321428571, 0.9891651670358066, 0.98753756286914, 0.9879095866786637, 0.9910016741071429, 0.9887230282738095, 0.9915829613095238, 0.9926292782738095, 0.9919317336309523, 0.9925362723214286, 0.992722644714378, 0.9923738723929494, 0.9934198288690477, 0.9939081101190477, 0.9948850331072352, 0.9946986607142857, 0.994140625, 0.9948846726190477, 0.9950706845238095, 0.9946986607142857, 0.9963727678571429, 0.9957449776785714, 0.9962797619047619, 0.9954427083333334, 0.9948149181547619, 0.9964893857858066, 0.9963495163690477, 0.9969075520833334, 0.9970935639880952, 0.9969308035714286, 0.9968145461309523, 0.9973264393572352, 0.9971865699404762, 0.9968381581072352, 0.9970939244762828, 0.9974198057978036, 0.9966521462024732, 0.9969540550595238, 0.9967912946428571, 0.9974655877976191, 0.9973493303571429, 0.9979771205357143, 0.9979306175595238, 0.9974190848214286, 0.9973028273809523, 0.9977217146548542, 0.9978841145833334, 0.9975353422619048, 0.9976283482142857, 0.9977911086309523, 0.9976283482142857, 0.9982328869047619, 0.997745326631137, 0.9980239840000923, 0.9984425107858066, 0.9981402414405685, 0.9974426967977114, 0.997838332583518, 0.9976755721668512, 0.9982332473929494, 0.9978612235834257, 0.9981634929286637, 0.9983026413690477, 0.9981634929286637, 0.9980940989525655, 0.9982332473929494, 0.9981402414405685, 0.99827975036914, 0.9982096354166666, 0.9982564988810447, 0.9984425107858066, 0.99827975036914, 0.9984890137619971, 0.9984192592977114, 0.9986750256667589, 0.9987447801310447, 0.998582019714378, 0.9985584077380952, 0.9985587682262828, 0.9984188988095238, 0.9981634929286637, 0.9986281622023809, 0.9984657622739018, 0.9986052712024732, 0.9986056316906607, 0.9986281622023809, 0.9985122652500923, 0.9986281622023809, 0.9986281622023809, 0.9987215286429494, 0.9986056316906607, 0.9985591287144703, 0.9985355167381875, 0.9986052712024732, 0.9986746651785714, 0.9988377860834257, 0.9988842890596161, 0.9989304315476191, 0.9990005465000923, 0.9990470494762828, 0.9991168039405685, 0.9989772950119971, 0.9990470494762828, 0.9989071800595238, 0.9987909226190477, 0.9991861979166666, 0.9987444196428571, 0.9990237979881875, 0.9989769345238095, 0.9990234375, 0.9989304315476191, 0.9990001860119048, 0.9992327008928571, 0.9990699404761905, 0.9990466889880952, 0.9990931919642857, 0.9990699404761905, 0.9989304315476191, 0.9987909226190477, 0.9988374255952381, 0.9991164434523809, 0.9989071800595238, 0.9989769345238095, 0.9990466889880952, 0.9991861979166666, 0.9993024553571429, 0.9991396949404762, 0.9989071800595238, 0.9993722098214286, 0.9990234375, 0.9991861979166666, 0.9989304315476191, 0.9990931919642857, 0.999070300964378, 0.9991396949404762, 0.9991633069167589, 0.9990234375, 0.9991861979166666, 0.9989071800595238], "end": "2016-01-23 01:31:54.467000", "learning_rate_per_epoch": [0.008715080097317696, 0.004357540048658848, 0.0029050265438854694, 0.002178770024329424, 0.001743015949614346, 0.0014525132719427347, 0.001245011342689395, 0.001089385012164712, 0.0009683421812951565, 0.000871507974807173, 0.0007922799559310079, 0.0007262566359713674, 0.0006703907274641097, 0.0006225056713446975, 0.0005810052971355617, 0.000544692506082356, 0.0005126517498865724, 0.00048417109064757824, 0.00045868841698393226, 0.0004357539874035865, 0.0004150038002990186, 0.00039613997796550393, 0.00037891650572419167, 0.0003631283179856837, 0.000348603178281337, 0.00033519536373205483, 0.0003227807173971087, 0.00031125283567234874, 0.0003005199832841754, 0.00029050264856778085, 0.00028113159351050854, 0.000272346253041178, 0.0002640933380462229, 0.0002563258749432862, 0.0002490022743586451, 0.00024208554532378912, 0.0002355426986468956, 0.00022934420849196613, 0.00022346357582136989, 0.00021787699370179325, 0.00021256292529869825, 0.0002075019001495093, 0.00020267626678105444, 0.00019806998898275197, 0.0001936684420797974, 0.00018945825286209583, 0.00018542722682468593, 0.00018156415899284184, 0.00017785877571441233, 0.0001743015891406685, 0.00017088391177821904, 0.00016759768186602741, 0.00016443546337541193, 0.00016139035869855434, 0.00015845599409658462, 0.00015562641783617437, 0.00015289612929336727, 0.0001502599916420877, 0.00014771321730222553, 0.00014525132428389043, 0.0001428701652912423, 0.00014056579675525427, 0.00013833459524903446, 0.000136173126520589, 0.00013407814549282193, 0.00013204666902311146, 0.00013007581583224237, 0.0001281629374716431, 0.0001263055019080639, 0.00012450113717932254, 0.00012274760229047388, 0.00012104277266189456, 0.0001193846546811983, 0.0001177713493234478, 0.00011620105942711234, 0.00011467210424598306, 0.00011318285396555439, 0.00011173178791068494, 0.00011031746544176713, 0.00010893849685089663, 0.0001075935797416605, 0.00010628146264934912, 0.00010500095959287137, 0.00010375095007475466, 0.00010253034997731447, 0.00010133813339052722, 0.00010017333261203021, 9.903499449137598e-05, 9.792224591365084e-05, 9.68342210398987e-05, 9.577010496286675e-05, 9.472912643104792e-05, 9.371053602080792e-05, 9.271361341234297e-05, 9.173768194159493e-05, 9.078207949642092e-05, 8.984618034446612e-05, 8.892938785720617e-05, 8.80311054061167e-05, 8.715079457033426e-05, 8.62879169289954e-05, 8.544195588910952e-05, 8.461242396151647e-05, 8.379884093301371e-05, 8.300075569422916e-05, 8.221773168770596e-05, 8.14493396319449e-05, 8.069517934927717e-05, 7.995485793799162e-05, 7.922799704829231e-05, 7.851423288229853e-05, 7.781320891808718e-05, 7.712459773756564e-05, 7.644806464668363e-05, 7.578330405522138e-05, 7.512999582104385e-05, 7.44878634577617e-05, 7.385660865111277e-05, 7.32359621906653e-05, 7.262566214194521e-05, 7.2025453846436e-05, 7.143508264562115e-05, 7.085430843289942e-05, 7.028289837762713e-05, 6.972063420107588e-05, 6.916729762451723e-05, 6.862267764518037e-05, 6.80865632602945e-05, 6.755875801900402e-05, 6.703907274641097e-05, 6.652732554357499e-05, 6.602333451155573e-05, 6.552691775141284e-05, 6.503790791612118e-05, 6.455614493461326e-05, 6.408146873582155e-05, 6.361371924867854e-05, 6.315275095403194e-05, 6.269841833272949e-05, 6.225056858966127e-05, 6.180907803354785e-05, 6.137380114523694e-05, 6.09446142334491e-05, 6.052138633094728e-05, 6.010399738443084e-05, 5.969232734059915e-05, 5.928625614615157e-05, 5.88856746617239e-05, 5.8490466471994296e-05, 5.810052971355617e-05, 5.7715758885024115e-05, 5.733605212299153e-05, 5.6961303926073015e-05, 5.659142698277719e-05, 5.622631942969747e-05, 5.586589395534247e-05, 5.551006324822083e-05, 5.515873272088356e-05, 5.481182233779691e-05, 5.446924842544831e-05, 5.413093094830401e-05, 5.379678987083025e-05, 5.346674515749328e-05, 5.314073132467456e-05, 5.281866469886154e-05, 5.2500479796435684e-05, 5.218610749579966e-05, 5.187547503737733e-05, 5.1568520575528964e-05, 5.1265174988657236e-05, 5.096538006910123e-05, 5.066906669526361e-05, 5.037618393544108e-05, 5.0086666306015104e-05, 4.980045559932478e-05], "accuracy_valid": [0.1691453313253012, 0.3631532967808735, 0.6541806875941265, 0.7449539368411144, 0.7674266401543675, 0.7920348385730422, 0.7951983716114458, 0.8312517648719879, 0.8249541133283133, 0.8442221032567772, 0.844761859939759, 0.8538965432040663, 0.8399393472326807, 0.8612207619540663, 0.8638151237763554, 0.8565923851656627, 0.8649240516754518, 0.8683317253388554, 0.8678743293486446, 0.867222797439759, 0.867344867752259, 0.8662874152861446, 0.8692068076995482, 0.8740293204066265, 0.8752500235316265, 0.8725027061370482, 0.8700715949736446, 0.8758603750941265, 0.8735101538968373, 0.8729203925075302, 0.8748632224209337, 0.8732660132718373, 0.8768472326807228, 0.8781797110316265, 0.8752706137048193, 0.8764707266566265, 0.8785665121423193, 0.8785871023155121, 0.8770913733057228, 0.8772134436182228, 0.8787900625941265, 0.8757074195218373, 0.8822595067771084, 0.8810182134789157, 0.8809064382530121, 0.8819138860128012, 0.8833169592432228, 0.8795430746423193, 0.8810182134789157, 0.8821374364646084, 0.8799195806664157, 0.8819844808923193, 0.8798078054405121, 0.8807843679405121, 0.8802960866905121, 0.8833890248493976, 0.8817712255271084, 0.8817609304405121, 0.8835919851280121, 0.8821477315512049, 0.8810593938253012, 0.8812520590173193, 0.8822698018637049, 0.8813844244164157, 0.8833066641566265, 0.8818521154932228, 0.8807431875941265, 0.8829919286521084, 0.8827169027673193, 0.8839479009789157, 0.8842023366905121, 0.8851994893637049, 0.8824727621423193, 0.8848229833396084, 0.8844567724021084, 0.8813741293298193, 0.8834802099021084, 0.8818315253200302, 0.8824624670557228, 0.8817506353539157, 0.8850671239646084, 0.8832154791039157, 0.8839684911521084, 0.8832257741905121, 0.8816285650414157, 0.8839684911521084, 0.8812726491905121, 0.8853112645896084, 0.8830934087914157, 0.8850156485316265, 0.8848023931664157, 0.8843347020896084, 0.8838464208396084, 0.8804990469691265, 0.8812520590173193, 0.8844773625753012, 0.8829713384789157, 0.8828389730798193, 0.8830831137048193, 0.8829610433923193, 0.8822286215173193, 0.8834493246423193, 0.8801534262048193, 0.8818521154932228, 0.8823403967432228, 0.8829919286521084, 0.8839376058923193, 0.8805093420557228, 0.8819947759789157, 0.8836022802146084, 0.8839479009789157, 0.8826463078878012, 0.8832051840173193, 0.8844670674887049, 0.8813947195030121, 0.8823712820030121, 0.8852097844503012, 0.8841214467243976, 0.8818830007530121, 0.8808858480798193, 0.8844670674887049, 0.8850774190512049, 0.8826154226280121, 0.8834905049887049, 0.8854436299887049, 0.8828698583396084, 0.8834493246423193, 0.8827477880271084, 0.8849553487387049, 0.8809976233057228, 0.8814961996423193, 0.8849244634789157, 0.8826051275414157, 0.8839581960655121, 0.8820962561182228, 0.8829713384789157, 0.8840699712914157, 0.8825845373682228, 0.8808652579066265, 0.8843141119164157, 0.8843038168298193, 0.8840802663780121, 0.8845479574548193, 0.8841920416039157, 0.8822080313441265, 0.8817403402673193, 0.8823506918298193, 0.8834596197289157, 0.8831845938441265, 0.8850671239646084, 0.8830728186182228, 0.8822183264307228, 0.8823815770896084, 0.8846700277673193, 0.8811093985316265, 0.8803769766566265, 0.8826051275414157, 0.8847009130271084, 0.8844464773155121, 0.8817197500941265, 0.8822183264307228, 0.8822183264307228, 0.8824624670557228, 0.8817197500941265, 0.8839376058923193, 0.8804681617093373, 0.8815873846950302, 0.8825845373682228, 0.8814859045557228, 0.8821065512048193, 0.8836728750941265, 0.8816182699548193, 0.8805093420557228, 0.8807225974209337, 0.8828286779932228], "accuracy_test": 0.8694620253164557, "start": "2016-01-21 17:39:43.932000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0], "accuracy_train_last": 0.9989071800595238, "batch_size_eval": 1024, "accuracy_train_std": [0.011840494513438438, 0.014822407355668293, 0.019494367041197332, 0.017918718366161268, 0.02048446052132163, 0.020968401617690296, 0.019904281792205738, 0.018852635121943174, 0.019934948165989622, 0.01701662171798117, 0.01693371847103601, 0.016278096001316866, 0.01552344382220845, 0.013748531392564503, 0.012975646095849938, 0.01377655531767787, 0.012519661573450754, 0.011543352597374879, 0.011984994775232415, 0.010909182212617675, 0.009707107947335272, 0.01011646510388414, 0.009629007332175176, 0.00816978679684688, 0.008292086882869061, 0.0074261692031222645, 0.0070640922968449055, 0.007393759602484276, 0.006702120511179907, 0.0072417963553158505, 0.007283356227677592, 0.007575193665231499, 0.006390553355527234, 0.005203697335751161, 0.005360170634988585, 0.006353275002105996, 0.005168738339299294, 0.0042989388801848915, 0.004901269489368124, 0.004892630769028555, 0.004012549391258275, 0.004531654783073652, 0.0037091771954258315, 0.003936374615659311, 0.0034167031285726704, 0.0035838229957214298, 0.003357897335712284, 0.0032201066154963513, 0.0027708321407889772, 0.002951382375859827, 0.00272788881271919, 0.0027135592375177008, 0.002653116038987335, 0.0024373061163278755, 0.0024664119431131175, 0.0022364982619812214, 0.0021552511581578253, 0.0024926853215967684, 0.0021121772598913253, 0.0023793896587548423, 0.0030324255245955784, 0.0019541056585994964, 0.0022659168656445805, 0.001756218619864369, 0.0017891563061021797, 0.0018402491571251282, 0.002055752414152133, 0.0014775165557192565, 0.0015975989012557556, 0.0019159031850897405, 0.0016981994852201066, 0.001292394709752602, 0.0018352289703428683, 0.0019554764136910296, 0.00179925056939668, 0.002011085711139203, 0.0018734409485196882, 0.0015277130106933782, 0.0015396980218697269, 0.0015921752525827745, 0.0016202800772120857, 0.0012548145390056246, 0.0014596676074887934, 0.0016262749674521681, 0.0019663666770708683, 0.0017963937789916946, 0.001553505681593198, 0.0012285941976955827, 0.0013551122696358847, 0.0011958231201584122, 0.0012064996926296502, 0.0012018761049712046, 0.0013301834851232343, 0.0012693669156289764, 0.0013958297201229195, 0.0013348383035257795, 0.0014647068028876282, 0.0011711123467073297, 0.0011683728168155798, 0.0011711123467073295, 0.0013626852117627002, 0.0011717831422176517, 0.0012206224782408221, 0.0011033314441404665, 0.0012592349947964105, 0.0012340234799775825, 0.0012616975455209051, 0.0013102991891836072, 0.0013677767088695657, 0.0011062220723576236, 0.0009485275348530606, 0.001049948132073554, 0.001090664994368792, 0.0012285941976955827, 0.001132260974414923, 0.0012777745223245987, 0.0012821788115260756, 0.0011487741189579266, 0.0013331543816206821, 0.0010683712580318925, 0.0011289170352946363, 0.0011487741189579266, 0.0011521551563987576, 0.0012066155827129019, 0.001187648138869246, 0.0009876028526878814, 0.0011684518808878928, 0.0012814641168406062, 0.0010706694735525245, 0.0011100645343225196, 0.0011830872612731166, 0.0010913237940812026, 0.0009424438151638055, 0.0010177673283221396, 0.0010324651832996836, 0.001010216691850236, 0.000898898890646402, 0.0008236311010231585, 0.0009159069350915449, 0.0009091896761540716, 0.0011034049960662827, 0.0006710783895862078, 0.0010060131906986576, 0.0008249225356480388, 0.00085114421585394, 0.000928896221886076, 0.0010826308339678958, 0.0010967704999135747, 0.0009889402119669243, 0.0009984612422611555, 0.0010105707687918866, 0.0009856546872315218, 0.0009754546578963464, 0.000899324758454879, 0.000948478332141515, 0.0010272843207949462, 0.000948478332141515, 0.0009091896761540716, 0.0008774163999308597, 0.0009162974560657567, 0.0009020258901260613, 0.0008054551746507055, 0.0009091896761540716, 0.0009578371430452931, 0.0007326986261434492, 0.000825345951883317, 0.0007658886296503257, 0.0007919171487130953, 0.0008084699893375052, 0.0008507139343893616, 0.0008838624353026514, 0.000692301679387813, 0.0011475969753769592, 0.0008764916678278815, 0.0010698211560944328], "accuracy_test_std": 0.030880727089075997, "error_valid": [0.8308546686746988, 0.6368467032191265, 0.3458193124058735, 0.25504606315888556, 0.23257335984563254, 0.20796516142695776, 0.2048016283885542, 0.16874823512801207, 0.17504588667168675, 0.15577789674322284, 0.15523814006024095, 0.14610345679593373, 0.1600606527673193, 0.13877923804593373, 0.1361848762236446, 0.14340761483433728, 0.13507594832454817, 0.1316682746611446, 0.1321256706513554, 0.13277720256024095, 0.13265513224774095, 0.1337125847138554, 0.13079319230045183, 0.1259706795933735, 0.12474997646837349, 0.12749729386295183, 0.1299284050263554, 0.12413962490587349, 0.12648984610316272, 0.12707960749246983, 0.12513677757906627, 0.12673398672816272, 0.12315276731927716, 0.12182028896837349, 0.12472938629518071, 0.12352927334337349, 0.12143348785768071, 0.12141289768448793, 0.12290862669427716, 0.12278655638177716, 0.12120993740587349, 0.12429258047816272, 0.1177404932228916, 0.11898178652108427, 0.11909356174698793, 0.11808611398719882, 0.11668304075677716, 0.12045692535768071, 0.11898178652108427, 0.1178625635353916, 0.12008041933358427, 0.11801551910768071, 0.12019219455948793, 0.11921563205948793, 0.11970391330948793, 0.11661097515060237, 0.1182287744728916, 0.11823906955948793, 0.11640801487198793, 0.11785226844879515, 0.11894060617469882, 0.11874794098268071, 0.11773019813629515, 0.11861557558358427, 0.11669333584337349, 0.11814788450677716, 0.11925681240587349, 0.1170080713478916, 0.11728309723268071, 0.11605209902108427, 0.11579766330948793, 0.11480051063629515, 0.11752723785768071, 0.1151770166603916, 0.1155432275978916, 0.11862587067018071, 0.1165197900978916, 0.11816847467996983, 0.11753753294427716, 0.11824936464608427, 0.1149328760353916, 0.11678452089608427, 0.1160315088478916, 0.11677422580948793, 0.11837143495858427, 0.1160315088478916, 0.11872735080948793, 0.1146887354103916, 0.11690659120858427, 0.11498435146837349, 0.11519760683358427, 0.1156652979103916, 0.1161535791603916, 0.11950095303087349, 0.11874794098268071, 0.11552263742469882, 0.11702866152108427, 0.11716102692018071, 0.11691688629518071, 0.11703895660768071, 0.11777137848268071, 0.11655067535768071, 0.11984657379518071, 0.11814788450677716, 0.11765960325677716, 0.1170080713478916, 0.11606239410768071, 0.11949065794427716, 0.11800522402108427, 0.1163977197853916, 0.11605209902108427, 0.11735369211219882, 0.11679481598268071, 0.11553293251129515, 0.11860528049698793, 0.11762871799698793, 0.11479021554969882, 0.11587855327560237, 0.11811699924698793, 0.11911415192018071, 0.11553293251129515, 0.11492258094879515, 0.11738457737198793, 0.11650949501129515, 0.11455637001129515, 0.1171301416603916, 0.11655067535768071, 0.1172522119728916, 0.11504465126129515, 0.11900237669427716, 0.11850380035768071, 0.11507553652108427, 0.11739487245858427, 0.11604180393448793, 0.11790374388177716, 0.11702866152108427, 0.11593002870858427, 0.11741546263177716, 0.11913474209337349, 0.11568588808358427, 0.11569618317018071, 0.11591973362198793, 0.11545204254518071, 0.11580795839608427, 0.11779196865587349, 0.11825965973268071, 0.11764930817018071, 0.11654038027108427, 0.11681540615587349, 0.1149328760353916, 0.11692718138177716, 0.11778167356927716, 0.1176184229103916, 0.11532997223268071, 0.11889060146837349, 0.11962302334337349, 0.11739487245858427, 0.1152990869728916, 0.11555352268448793, 0.11828024990587349, 0.11778167356927716, 0.11778167356927716, 0.11753753294427716, 0.11828024990587349, 0.11606239410768071, 0.11953183829066272, 0.11841261530496983, 0.11741546263177716, 0.11851409544427716, 0.11789344879518071, 0.11632712490587349, 0.11838173004518071, 0.11949065794427716, 0.11927740257906627, 0.11717132200677716], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.7807792176320554, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.008715079682442737, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "rmsprop", "nb_data_augmentation": 2, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 5.887127650228389e-10, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.08604945743078052}, "accuracy_valid_max": 0.8854436299887049, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8828286779932228, "loss_train": [1.678960919380188, 1.1702046394348145, 0.831855297088623, 0.6671946048736572, 0.5785875916481018, 0.5183622241020203, 0.46899986267089844, 0.4287964403629303, 0.3974960744380951, 0.37192410230636597, 0.3453538715839386, 0.32253459095954895, 0.30325454473495483, 0.28614145517349243, 0.2668984830379486, 0.2515621781349182, 0.23888012766838074, 0.22760102152824402, 0.21486398577690125, 0.20306672155857086, 0.19295620918273926, 0.18383415043354034, 0.17453424632549286, 0.16590198874473572, 0.15818892419338226, 0.15179912745952606, 0.14343075454235077, 0.1381363719701767, 0.13161088526248932, 0.12503981590270996, 0.12124565988779068, 0.1147567480802536, 0.11335639655590057, 0.10765049606561661, 0.10291960090398788, 0.09963805973529816, 0.09659867733716965, 0.09247105568647385, 0.0898534506559372, 0.08497940748929977, 0.08216185122728348, 0.08000075072050095, 0.07601434737443924, 0.07392533868551254, 0.0720931887626648, 0.07017552107572556, 0.06665080040693283, 0.06394796073436737, 0.061496637761592865, 0.06203603371977806, 0.05957365781068802, 0.056284379214048386, 0.055125147104263306, 0.05460496246814728, 0.05271412804722786, 0.04962734505534172, 0.04881230369210243, 0.04766743630170822, 0.0468660369515419, 0.04529086500406265, 0.044149503111839294, 0.04358820989727974, 0.0411209799349308, 0.0397740975022316, 0.038769785314798355, 0.03786417469382286, 0.03770780563354492, 0.03670138120651245, 0.03404438495635986, 0.03472528234124184, 0.032893307507038116, 0.033393073827028275, 0.032152995467185974, 0.031403861939907074, 0.03191389888525009, 0.030186781659722328, 0.02973325178027153, 0.02911834977567196, 0.028752706944942474, 0.026667920872569084, 0.026596790179610252, 0.026693200692534447, 0.025814540684223175, 0.025950973853468895, 0.025085177272558212, 0.023434974253177643, 0.024281613528728485, 0.02282778173685074, 0.022151222452521324, 0.023061055690050125, 0.02136901207268238, 0.02118113450706005, 0.02120584435760975, 0.019960150122642517, 0.019451700150966644, 0.0202073622494936, 0.019633399322628975, 0.019057247787714005, 0.019133850932121277, 0.017548805102705956, 0.018337314948439598, 0.01754586584866047, 0.017058275640010834, 0.017453258857131004, 0.016802208498120308, 0.01654745452105999, 0.016903532668948174, 0.015888288617134094, 0.016282977536320686, 0.016036972403526306, 0.015124758705496788, 0.01565008983016014, 0.01582934893667698, 0.01507369615137577, 0.014148307032883167, 0.015101192519068718, 0.014556517824530602, 0.014374895952641964, 0.013990170322358608, 0.014163590036332607, 0.01340849045664072, 0.013414990156888962, 0.013445577584207058, 0.012436182238161564, 0.012911021709442139, 0.012646110728383064, 0.012404918670654297, 0.01229935698211193, 0.012357900850474834, 0.011923537589609623, 0.013124655932188034, 0.011695470660924911, 0.011746124364435673, 0.011421378701925278, 0.011289938353002071, 0.010935993865132332, 0.010381142608821392, 0.010984726250171661, 0.010533413849771023, 0.010238311253488064, 0.010007006116211414, 0.010501108132302761, 0.009779904037714005, 0.010553061030805111, 0.010359000414609909, 0.01010418776422739, 0.009450314566493034, 0.009532785043120384, 0.009525624103844166, 0.00891218800097704, 0.009435603395104408, 0.009655861184000969, 0.009554775431752205, 0.008501681499183178, 0.00877312384545803, 0.009158683009445667, 0.008885047398507595, 0.008753486908972263, 0.009023889899253845, 0.008170823566615582, 0.008352076634764671, 0.00831601582467556, 0.008260441944003105, 0.008814561180770397, 0.007981359958648682, 0.008196130394935608, 0.008329099044203758, 0.008167952299118042, 0.008122380822896957, 0.0077476645819842815, 0.008251960389316082, 0.00793363992124796, 0.008007317781448364, 0.008112236857414246, 0.008162821643054485], "accuracy_train_first": 0.17891208990863788, "model": "residualv4", "loss_std": [0.25715893507003784, 0.16623882949352264, 0.13582924008369446, 0.10178796947002411, 0.09447820484638214, 0.08975780755281448, 0.08424517512321472, 0.079131118953228, 0.0741126611828804, 0.07288192957639694, 0.06855490058660507, 0.06762739270925522, 0.06341235339641571, 0.06242915242910385, 0.05885070934891701, 0.05627511069178581, 0.054275643080472946, 0.05225132033228874, 0.05042218044400215, 0.04851490259170532, 0.04625535383820534, 0.045811183750629425, 0.04278997331857681, 0.04186875745654106, 0.04007398337125778, 0.0389881394803524, 0.03797287866473198, 0.03704725578427315, 0.03636081889271736, 0.033095117658376694, 0.03269408270716667, 0.032014116644859314, 0.03166556730866432, 0.029566869139671326, 0.02945089153945446, 0.028199465945363045, 0.027976347133517265, 0.026503296568989754, 0.025543155148625374, 0.025013703852891922, 0.024425173178315163, 0.02374194748699665, 0.022916771471500397, 0.022950969636440277, 0.022196577861905098, 0.02286517433822155, 0.021019091829657555, 0.019620053470134735, 0.019301943480968475, 0.019692575559020042, 0.019564516842365265, 0.018306471407413483, 0.01914118602871895, 0.018992498517036438, 0.016961870715022087, 0.01600608229637146, 0.017547832801938057, 0.017066769301891327, 0.016392286866903305, 0.016302678734064102, 0.01651262491941452, 0.015755904838442802, 0.014437656849622726, 0.015718620270490646, 0.014479783363640308, 0.014821630902588367, 0.014092419296503067, 0.014007966965436935, 0.01258697360754013, 0.0120542598888278, 0.013027680106461048, 0.013424944132566452, 0.012285228818655014, 0.012782915495336056, 0.012488853186368942, 0.011547964066267014, 0.011849520727992058, 0.011818925850093365, 0.011195976287126541, 0.01095041073858738, 0.011033209972083569, 0.011835659854114056, 0.010686795227229595, 0.010746262967586517, 0.010462317615747452, 0.009801690466701984, 0.010581589303910732, 0.010174033232033253, 0.009835352189838886, 0.0097474604845047, 0.009315086528658867, 0.009031879715621471, 0.00933054555207491, 0.009120170958340168, 0.008573828265070915, 0.009267708286643028, 0.008895679377019405, 0.008898717351257801, 0.008411219343543053, 0.00782705657184124, 0.008407847955822945, 0.007898743264377117, 0.007993312552571297, 0.008506610058248043, 0.008308092132210732, 0.007552418392151594, 0.008247400633990765, 0.007465705741196871, 0.007989888079464436, 0.007644970901310444, 0.007221648469567299, 0.007397142238914967, 0.007940493524074554, 0.007576942443847656, 0.007241425570100546, 0.00801379419863224, 0.007530024275183678, 0.007333449088037014, 0.006945907603949308, 0.007241856772452593, 0.007206742186099291, 0.006840570364147425, 0.00691422913223505, 0.006637373473495245, 0.006788804195821285, 0.006544924806803465, 0.006713122129440308, 0.006886881310492754, 0.006752468179911375, 0.006594788283109665, 0.006944169290363789, 0.006914067547768354, 0.006144467741250992, 0.00615638867020607, 0.005767790134996176, 0.0054474701173603535, 0.005868012551218271, 0.0057023088447749615, 0.0055157942697405815, 0.005773138254880905, 0.005754814948886633, 0.006255191285163164, 0.0049774907529354095, 0.005710775963962078, 0.006320664193481207, 0.006048512179404497, 0.0056361970491707325, 0.005172791890799999, 0.00569660309702158, 0.005286348983645439, 0.005734109319746494, 0.0058053540997207165, 0.005954560823738575, 0.00435984693467617, 0.0045189037919044495, 0.005231907591223717, 0.005474897101521492, 0.005149907898157835, 0.0056205191649496555, 0.00446883961558342, 0.005187958013266325, 0.004919928964227438, 0.004816671833395958, 0.005124997813254595, 0.004669507034122944, 0.005282016005367041, 0.005184175446629524, 0.0050085000693798065, 0.005112297832965851, 0.004573035519570112, 0.004897105973213911, 0.004779135342687368, 0.004949586000293493, 0.004930933937430382, 0.004768896382302046]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:08 2016", "state": "available"}], "summary": "c885d0676b42cf247e696ffa40d8dfb9"}