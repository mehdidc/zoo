{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 16, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.017490370090073766, 0.01409469293428412, 0.011188533586852241, 0.010291292998445651, 0.011819338373554337, 0.012398539396628012, 0.013827810506590932, 0.012764608397467821, 0.014636632598350675, 0.013777816301546663, 0.01597141644566564, 0.015447143315325894, 0.014607825768238108, 0.013388270251114815, 0.0129386649967899, 0.011588800089525656, 0.011639915301170868, 0.011966153750253767, 0.013958069701349411, 0.013807366005334198, 0.014810902684446785, 0.015063550237939616, 0.013870510038263008, 0.014332168108182712, 0.015797992809944428, 0.015285228108820314, 0.015735614477370582, 0.01684452453788141, 0.01666704739113246, 0.015289361952374745, 0.014629362853855393, 0.01675246206599147, 0.01655845311663857, 0.015933544372197284, 0.016071978970652764, 0.015892968952800075, 0.015700008643877245, 0.016313392360088233, 0.016538872848806122, 0.01612227921968679, 0.015582409310615788, 0.014545743228218383, 0.013772809161457419, 0.01294823171258445, 0.01450456122428862, 0.01375010462967669, 0.014508812316524296, 0.013821903829364515, 0.013732216219560568, 0.013110264313811558, 0.01233248913693303, 0.012715549530590821, 0.013030750307686245, 0.01437100261498988, 0.014429754185357049, 0.014360248506007057, 0.014867257879652138, 0.014060304062688218, 0.013872013500644704, 0.014361740837846188, 0.014560125860765783, 0.014517127945897495], "moving_avg_accuracy_train": [0.058569975804032844, 0.1179104392332272, 0.17556214545986754, 0.23024307734706645, 0.2812154408644362, 0.3286433708978246, 0.37250964747429355, 0.41314020900501114, 0.45073059961475365, 0.4854407132182469, 0.5174585961173063, 0.5469441893883276, 0.5741020020155708, 0.5990066298467287, 0.621946026183992, 0.6429377498160542, 0.6622139145896629, 0.6799761230810935, 0.6964038889971905, 0.7114795940205059, 0.725398645767634, 0.7382443737757728, 0.750131049816431, 0.7611243882006516, 0.7714461480297652, 0.7810728784533483, 0.7900578063702873, 0.7985115789586186, 0.8064989374952503, 0.8141385309008101, 0.8214070430169861, 0.8282648160131834, 0.8347484176990558, 0.8408882537103887, 0.8467117251682073, 0.8523201508945113, 0.8576072604243846, 0.8626121247750802, 0.8673606072668875, 0.871848191248809, 0.8761706489384814, 0.8802305967222819, 0.8840705255836454, 0.8876868968267297, 0.8911183422550295, 0.894381101398851, 0.8974059402830522, 0.9002817190514432, 0.9029652510441855, 0.90548974788052, 0.9078524758367924, 0.9101021638843424, 0.9121757112521374, 0.9141418852819624, 0.9159928221171382, 0.9177400815259485, 0.9193521785724585, 0.9208495688905081, 0.9222530237481811, 0.9235696115427059, 0.9248056938315877, 0.9259902475046766], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 480895488, "moving_var_accuracy_train": [0.03087397859116494, 0.05947819613197241, 0.08344384959636075, 0.10200950344519706, 0.11519218968359934, 0.12391764764050729, 0.12884413486260596, 0.13081730414905818, 0.13045291092988953, 0.12825074771420725, 0.12465197637082745, 0.12001138062864158, 0.11464816364604552, 0.10876551166811736, 0.10262490364016806, 0.09632828542555517, 0.09003959163841348, 0.08387509692900955, 0.0779164306730553, 0.0721702795432999, 0.06669691160282297, 0.0615123349950724, 0.05663273910122519, 0.05205714659057249, 0.047810280465244354, 0.04386331786655498, 0.0402035464469528, 0.0368263882410345, 0.03371793048446538, 0.030871407922639305, 0.02825974854582234, 0.02585703514544646, 0.023649665448291255, 0.02162397717967667, 0.019766794840089256, 0.018073205308227553, 0.016517466522032696, 0.015091157874349195, 0.013784974860689274, 0.012587723064573527, 0.01149710352242725, 0.0104957417542492, 0.009578873061767275, 0.008738689024300814, 0.007970793481417323, 0.007269524508350909, 0.006624918909994203, 0.006036857950717337, 0.005497984251250249, 0.005005543584615191, 0.004555231576711836, 0.004145258285842256, 0.0037694288454364366, 0.003427278523732818, 0.0031153843758698336, 0.0028313221772579373, 0.0025715796715184394, 0.0023346013042478894, 0.0021188683436608358, 0.001922582140080978, 0.0017440750208968672, 0.0015822960254470364], "duration": 32761.367142, "accuracy_train": [0.5856997580403286, 0.6519746100959763, 0.6944275014996308, 0.7223714643318567, 0.7399667125207641, 0.7554947411983204, 0.7673061366625139, 0.7788152627814692, 0.7890441151024363, 0.7978317356496861, 0.8056195422088409, 0.8123145288275194, 0.8185223156607604, 0.8231482803271503, 0.8284005932193614, 0.8318632625046143, 0.8356993975521411, 0.8398359995039681, 0.8442537822420635, 0.8471609392303433, 0.8506701114917867, 0.8538559258490217, 0.8571111341823551, 0.8600644336586378, 0.8643419864917867, 0.8677134522655963, 0.8709221576227391, 0.8745955322535992, 0.8783851643249354, 0.882894871550849, 0.8868236520625692, 0.889984772978959, 0.8931008328719084, 0.8961467778123846, 0.8991229682885751, 0.9027959824312477, 0.9051912461932448, 0.90765590393134, 0.9100969496931525, 0.9122364470861019, 0.9150727681455334, 0.9167701267764857, 0.9186298853359173, 0.9202342380144887, 0.9220013511097268, 0.9237459336932448, 0.9246294902408637, 0.9261637279669619, 0.9271170389788667, 0.9282102194075305, 0.9291170274432448, 0.9303493563122923, 0.9308376375622923, 0.9318374515503876, 0.9326512536337209, 0.9334654162052418, 0.9338610519910484, 0.9343260817529531, 0.9348841174672389, 0.9354189016934293, 0.9359304344315246, 0.9366512305624769], "end": "2016-01-24 18:36:26.307000", "learning_rate_per_epoch": [0.006685405503958464, 0.003342702751979232, 0.0022284684237092733, 0.001671351375989616, 0.0013370810775086284, 0.0011142342118546367, 0.0009550579125061631, 0.000835675687994808, 0.0007428228273056448, 0.0006685405387543142, 0.0006077641155570745, 0.0005571171059273183, 0.0005142619484104216, 0.00047752895625308156, 0.00044569370220415294, 0.000417837843997404, 0.00039325913530774415, 0.0003714114136528224, 0.00035186344757676125, 0.0003342702693771571, 0.00031835262780077755, 0.0003038820577785373, 0.0002906697918660939, 0.00027855855296365917, 0.0002674162096809596, 0.0002571309742052108, 0.0002476076188031584, 0.00023876447812654078, 0.00023053122276905924, 0.00022284685110207647, 0.0002156582340830937, 0.000208918921998702, 0.00020258803851902485, 0.00019662956765387207, 0.00019101158250123262, 0.0001857057068264112, 0.00018068663484882563, 0.00017593172378838062, 0.00017142064461950213, 0.00016713513468857855, 0.00016305866301991045, 0.00015917631390038878, 0.0001554745394969359, 0.00015194102888926864, 0.0001485645625507459, 0.00014533489593304694, 0.0001422426721546799, 0.00013927927648182958, 0.00013643683632835746, 0.0001337081048404798, 0.0001310863735852763, 0.0001285654871026054, 0.00012613972648978233, 0.0001238038094015792, 0.00012155282456660643, 0.00011938223906327039, 0.000117287811008282, 0.00011526561138452962, 0.00011331195128150284, 0.00011142342555103824, 0.00010959681094391271, 0.00010782911704154685], "accuracy_valid": [0.579979586314006, 0.6396543204066265, 0.6700910085655121, 0.6899693500564759, 0.6996437900037651, 0.7041603915662651, 0.7079548663403614, 0.7114346056099398, 0.712024366999247, 0.7120140719126506, 0.7135803958019578, 0.7137127612010542, 0.7110375094126506, 0.708972609186747, 0.708118116999247, 0.706287062311747, 0.7055237551769578, 0.7035706301769578, 0.7016072100903614, 0.7005085772778614, 0.6996334949171686, 0.6975582996046686, 0.6929093326430723, 0.6923092761671686, 0.6907017719314759, 0.6886059864457832, 0.6879956348832832, 0.6867543415850903, 0.6872426228350903, 0.6881177051957832, 0.6866425663591867, 0.687842679310994, 0.686988187123494, 0.6867337514118976, 0.6863675404743976, 0.6863675404743976, 0.6861233998493976, 0.6856351185993976, 0.6855027532003012, 0.6856351185993976, 0.6852689076618976, 0.6852689076618976, 0.685035062123494, 0.6861439900225903, 0.6846482610128012, 0.6848924016378012, 0.6845261907003012, 0.6841702748493976, 0.684424710560994, 0.6843232304216867, 0.6843232304216867, 0.6840790897966867, 0.683204007435994, 0.6839261342243976, 0.6840482045368976, 0.684424710560994, 0.6845570759600903, 0.6850556522966867, 0.6851777226091867, 0.6849335819841867, 0.6850556522966867, 0.6852997929216867], "accuracy_test": 0.3407764668367347, "start": "2016-01-24 09:30:24.939000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0], "accuracy_train_last": 0.9366512305624769, "batch_size_eval": 1024, "accuracy_train_std": [0.01892097753786483, 0.02049720901140286, 0.018240937626804253, 0.017481444445253597, 0.020373588139119023, 0.021806286643578848, 0.021169098604839513, 0.020940519101781865, 0.02053246850671655, 0.0210911441651401, 0.020393535537383743, 0.020379707901982947, 0.019567671194592223, 0.019984690055710946, 0.02081276395991692, 0.021330036362900404, 0.021385559837465932, 0.02131750087343958, 0.021986310377290118, 0.021836484501595296, 0.022529382236967713, 0.02186453101128801, 0.021491516188772142, 0.02156382766226457, 0.021082299221264044, 0.021095977775306616, 0.019978137100735024, 0.019314756476584566, 0.01857422497751156, 0.018561575517150916, 0.019378904963482917, 0.018744947427101328, 0.018397587464683503, 0.01774302243679447, 0.01736296560346372, 0.01678516592353717, 0.016492013044221484, 0.01664162732528711, 0.016257165109024002, 0.01561803184920857, 0.01570850376023312, 0.015632489221897288, 0.015162759440373708, 0.014906172183583168, 0.014674241421372884, 0.01436249231046863, 0.01410567383413739, 0.013576832688865312, 0.01298428230964594, 0.012612110438201752, 0.012587582643219715, 0.012143236530944955, 0.011979484508153749, 0.011670026468546047, 0.011229495499071221, 0.011174498527756514, 0.010799180482932285, 0.010601328004454989, 0.010444117485215688, 0.010327833461435626, 0.010577908334532964, 0.01042195412687314], "accuracy_test_std": 0.014304998855292973, "error_valid": [0.42002041368599397, 0.3603456795933735, 0.32990899143448793, 0.31003064994352414, 0.3003562099962349, 0.2958396084337349, 0.2920451336596386, 0.28856539439006024, 0.287975633000753, 0.28798592808734935, 0.28641960419804224, 0.2862872387989458, 0.28896249058734935, 0.291027390813253, 0.291881883000753, 0.293712937688253, 0.29447624482304224, 0.29642936982304224, 0.2983927899096386, 0.2994914227221386, 0.30036650508283136, 0.30244170039533136, 0.3070906673569277, 0.30769072383283136, 0.30929822806852414, 0.3113940135542168, 0.3120043651167168, 0.3132456584149097, 0.3127573771649097, 0.3118822948042168, 0.31335743364081325, 0.31215732068900603, 0.31301181287650603, 0.31326624858810237, 0.31363245952560237, 0.31363245952560237, 0.31387660015060237, 0.31436488140060237, 0.3144972467996988, 0.31436488140060237, 0.31473109233810237, 0.31473109233810237, 0.31496493787650603, 0.3138560099774097, 0.3153517389871988, 0.3151075983621988, 0.3154738092996988, 0.31582972515060237, 0.31557528943900603, 0.31567676957831325, 0.31567676957831325, 0.31592091020331325, 0.31679599256400603, 0.31607386577560237, 0.31595179546310237, 0.31557528943900603, 0.3154429240399097, 0.31494434770331325, 0.31482227739081325, 0.31506641801581325, 0.31494434770331325, 0.31470020707831325], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.6522337968677951, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.006685405331421531, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.0228699186568115e-07, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.026341819562451033}, "accuracy_valid_max": 0.7137127612010542, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6852997929216867, "loss_train": [1.5082077980041504, 1.1760642528533936, 0.9767613410949707, 0.8824002146720886, 0.8167406916618347, 0.7646597623825073, 0.7200262546539307, 0.6806368827819824, 0.6447677612304688, 0.6107944846153259, 0.5786812901496887, 0.5479485392570496, 0.5182129144668579, 0.4892794191837311, 0.46142491698265076, 0.43453484773635864, 0.40843334794044495, 0.38330915570259094, 0.3591907322406769, 0.33597755432128906, 0.31342819333076477, 0.2921314835548401, 0.27170148491859436, 0.25247347354888916, 0.23430562019348145, 0.2173241823911667, 0.20152269303798676, 0.1868634819984436, 0.17318478226661682, 0.1606510728597641, 0.14899291098117828, 0.138318732380867, 0.1284518986940384, 0.11937560141086578, 0.11100691556930542, 0.10329344868659973, 0.09629866480827332, 0.0898241475224495, 0.08386554569005966, 0.07840096205472946, 0.07349412143230438, 0.06902894377708435, 0.06495515257120132, 0.06123962625861168, 0.057854995131492615, 0.05479476973414421, 0.0519937165081501, 0.049452248960733414, 0.047102849930524826, 0.0449504554271698, 0.04299553111195564, 0.041198473423719406, 0.03954822197556496, 0.038029562681913376, 0.03663087263703346, 0.03532559052109718, 0.03410494700074196, 0.03298133984208107, 0.03192621096968651, 0.030943674966692924, 0.030030744150280952, 0.02917306497693062], "accuracy_train_first": 0.5856997580403286, "model": "residualv3", "loss_std": [0.3050488233566284, 0.27171459794044495, 0.2579071819782257, 0.2526033818721771, 0.24707941710948944, 0.24277865886688232, 0.23832859098911285, 0.234071284532547, 0.23013019561767578, 0.22518737614154816, 0.22015328705310822, 0.21484555304050446, 0.20892295241355896, 0.2028132826089859, 0.19620086252689362, 0.18976309895515442, 0.18308383226394653, 0.17609022557735443, 0.16926734149456024, 0.1618129014968872, 0.15410694479942322, 0.14652611315250397, 0.13880674540996552, 0.13151057064533234, 0.12415866553783417, 0.11707977205514908, 0.1102982833981514, 0.10372177511453629, 0.09715814143419266, 0.09113806486129761, 0.08538742363452911, 0.07987542450428009, 0.07464916259050369, 0.06954821199178696, 0.06478740274906158, 0.06032763421535492, 0.05607324466109276, 0.05202628672122955, 0.048264600336551666, 0.04473620280623436, 0.041517097502946854, 0.03854512795805931, 0.03583720326423645, 0.0333222933113575, 0.031001228839159012, 0.028893692418932915, 0.026955971494317055, 0.02520463988184929, 0.023553987964987755, 0.022067775949835777, 0.020718444138765335, 0.01951749064028263, 0.018448280170559883, 0.017496807500720024, 0.016643045470118523, 0.015862971544265747, 0.015127870254218578, 0.014468173496425152, 0.013864600099623203, 0.013302039355039597, 0.01278839074075222, 0.012314911000430584]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:11 2016", "state": "available"}], "summary": "f97cf1409466f3e81f801e1dec63e67e"}