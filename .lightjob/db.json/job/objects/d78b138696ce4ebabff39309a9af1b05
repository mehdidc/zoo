{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6986286640167236, 1.3352715969085693, 1.1905832290649414, 1.0885475873947144, 1.0097618103027344, 0.9471839666366577, 0.8920647501945496, 0.8409658670425415, 0.7977379560470581, 0.7612837553024292, 0.7266760468482971, 0.696549117565155, 0.6734021306037903, 0.6470416784286499, 0.6268285512924194, 0.6074545979499817, 0.5897101759910583, 0.5727581977844238, 0.5562339425086975, 0.5431371331214905, 0.5279873609542847, 0.5184749960899353, 0.5072620511054993, 0.4947458803653717, 0.4807187616825104, 0.47215980291366577, 0.46291583776474, 0.4548317790031433, 0.44489043951034546, 0.4360432028770447, 0.42978936433792114, 0.42063793540000916, 0.41402754187583923, 0.4064972698688507, 0.40014055371284485, 0.3918365240097046, 0.3867175579071045, 0.3767087757587433, 0.3723493218421936, 0.36558738350868225, 0.36059731245040894, 0.3547261655330658, 0.34833332896232605, 0.342104434967041, 0.338692307472229, 0.33295080065727234, 0.3261931836605072, 0.321601003408432, 0.3185887634754181, 0.31364914774894714, 0.3084859549999237, 0.3054158091545105, 0.30007413029670715, 0.29401925206184387, 0.2913416922092438, 0.2859170436859131, 0.2807806730270386, 0.27832460403442383, 0.27598604559898376, 0.2702280879020691, 0.2678146958351135, 0.26480352878570557, 0.26158758997917175, 0.2583121955394745, 0.2542422413825989, 0.25087159872055054, 0.2457502782344818, 0.24581976234912872, 0.24288006126880646, 0.23796063661575317, 0.23759673535823822, 0.23183871805667877, 0.2308812141418457, 0.22849129140377045, 0.22586247324943542, 0.22250021994113922, 0.22100958228111267, 0.2190430462360382, 0.2141251116991043, 0.2138860821723938, 0.21004842221736908, 0.20965318381786346, 0.20512457191944122, 0.20247143507003784, 0.20122535526752472, 0.19798026978969574, 0.1975756287574768, 0.19638091325759888, 0.1935613751411438, 0.18933537602424622, 0.18943160772323608, 0.15902794897556305, 0.14332519471645355, 0.14036795496940613, 0.13568773865699768, 0.1341005265712738, 0.13251101970672607, 0.1296207308769226, 0.12861651182174683, 0.12737859785556793, 0.12614452838897705, 0.1253547966480255, 0.12338914722204208, 0.12370999157428741, 0.12325075268745422, 0.1219160258769989, 0.11994811147451401, 0.12004051357507706, 0.11942591518163681, 0.11998303979635239, 0.11728070676326752, 0.11639472097158432, 0.11654020100831985, 0.11755093187093735, 0.11652199178934097, 0.11490967869758606, 0.11485689133405685, 0.11400078982114792, 0.11236253380775452, 0.11230935156345367, 0.11417459696531296, 0.11327240616083145, 0.11325474083423615, 0.11206918954849243, 0.11307089030742645, 0.11245579272508621, 0.11266878247261047, 0.11241161078214645, 0.11190532147884369, 0.11387845873832703, 0.11273237317800522, 0.1131882295012474, 0.11310227960348129, 0.11296834796667099, 0.11155536770820618, 0.11473646014928818, 0.11263895779848099, 0.11389342695474625, 0.11291315406560898, 0.11363644897937775, 0.1144258975982666, 0.11300083994865417, 0.11329928785562515, 0.11294195055961609, 0.11320041865110397, 0.11254182457923889, 0.11277259886264801, 0.1117856502532959, 0.11257532984018326, 0.11242888122797012, 0.11216369271278381, 0.11343405395746231, 0.11427723616361618, 0.1134771928191185, 0.11241486668586731, 0.11283265799283981, 0.11397385597229004, 0.11262180656194687, 0.11224551498889923, 0.11221582442522049, 0.11199255287647247, 0.11348992586135864, 0.11330347508192062, 0.11213579773902893, 0.11292921006679535, 0.11241701990365982, 0.1120661273598671, 0.11295625567436218, 0.11314595490694046, 0.1140253022313118, 0.11232563853263855, 0.11151189357042313, 0.11271566152572632, 0.11400265991687775, 0.11373747885227203, 0.1124391257762909, 0.111976757645607, 0.1131456196308136, 0.11354612559080124, 0.11321210116147995, 0.11271932721138, 0.11358882486820221, 0.11391938477754593, 0.1136559247970581, 0.11215568333864212, 0.11285315454006195, 0.11241956800222397, 0.1122644916176796, 0.11426761746406555, 0.11309383064508438, 0.1140143945813179, 0.11357110738754272, 0.11151058971881866, 0.1137135699391365, 0.11267701536417007, 0.11263153702020645, 0.11254744976758957, 0.11081711202859879, 0.1134171187877655, 0.11338341236114502, 0.1132599413394928, 0.1112309917807579, 0.11440984904766083, 0.11401747167110443, 0.11289290338754654, 0.11142294108867645, 0.11375471949577332, 0.11320644617080688, 0.11386417597532272, 0.11372169852256775, 0.11332151293754578], "moving_avg_accuracy_train": [0.03893166081233849, 0.08718421054528883, 0.13786441949145206, 0.1902183494795992, 0.24059391526761195, 0.28633635617440556, 0.3302775903486317, 0.37234701892912475, 0.4113789824051215, 0.44670509879177805, 0.4798212166753817, 0.5086096687896818, 0.5349609097710865, 0.5606717519911908, 0.5829464104368668, 0.6018640854000092, 0.6216956183571014, 0.6396162218268546, 0.6578091726531208, 0.6717580226277957, 0.6880340101167696, 0.7019544470353714, 0.7149666875074063, 0.7258478421001504, 0.7369680365203957, 0.7482921375783506, 0.7591114745138063, 0.7688673347509177, 0.7774338034202981, 0.7858062205358172, 0.7929904066160025, 0.7997699970798176, 0.8040955031973249, 0.8098273990696836, 0.8165643406641918, 0.8230371747779128, 0.8289928256671388, 0.8336017442066911, 0.8380777971185247, 0.8419526767712902, 0.8462025009753903, 0.8499110853186043, 0.8539769072442466, 0.8551928121155215, 0.8596740391923137, 0.8630678718340827, 0.8651505171557502, 0.867631545491624, 0.8703362959581501, 0.8719292640459823, 0.8735606090226594, 0.8757399545491956, 0.8786384004933163, 0.8820721773286746, 0.8837096468650947, 0.8854974087324335, 0.8880942942665361, 0.8892621937377692, 0.8918450716438926, 0.891896315102427, 0.8950363781577526, 0.8974069580826105, 0.8987734332495709, 0.8994034806534343, 0.9019953313930005, 0.9022100027860462, 0.9028588641088167, 0.9041776223719383, 0.9046623459170904, 0.9072884186516549, 0.9087661466116094, 0.9097475096982336, 0.9119210498702061, 0.9126891570809521, 0.9147822659610352, 0.9154246868305556, 0.9166188137547352, 0.9166360178639885, 0.9173210002241843, 0.9183744320804572, 0.9186624308309295, 0.9197513833919857, 0.9202250989887413, 0.9219132778530178, 0.9229305508832845, 0.9237044427772373, 0.9247055039270238, 0.925266987235641, 0.9263861254502921, 0.9264822519983323, 0.9275658221450293, 0.9322027841638689, 0.9365063304117954, 0.9406352884039769, 0.9443746381338542, 0.9477447031883629, 0.9508033023255067, 0.9535909548298976, 0.9562369898147928, 0.9585533531833504, 0.9606542841590815, 0.9627172190979629, 0.9645180569715277, 0.9661411362065455, 0.9676274120573287, 0.9689209545932902, 0.9701060692149412, 0.9712074775089325, 0.9722755109330578, 0.9731809734921606, 0.9740516933667818, 0.974874832734884, 0.975571480338795, 0.9762868188370768, 0.9769306234855304, 0.9776704829369958, 0.978373558824267, 0.979015663766868, 0.9795447300902089, 0.980065031559778, 0.9805263274359615, 0.9809228925340505, 0.9813007274616162, 0.9816175274083302, 0.9819561257829917, 0.9821423177797203, 0.9823587187017759, 0.982553479531626, 0.9827217527832437, 0.9828290569311375, 0.9830302263118518, 0.9831950027128279, 0.9833386511760873, 0.9834446833049256, 0.983563399757794, 0.9836167300939378, 0.9837228561167053, 0.9838067437931484, 0.9838403900233758, 0.9839125243091518, 0.9839518685294455, 0.9839431005003288, 0.984032865524124, 0.98411132889673, 0.9841517189975517, 0.9841601683025769, 0.9842375271413852, 0.9842676225665509, 0.9842947084492, 0.9843400120828699, 0.9843505944674678, 0.9843786837552632, 0.9844109395607077, 0.9843748656189412, 0.9844051780892084, 0.9844254838660202, 0.9844298442211126, 0.9844779103192579, 0.9844514513921216, 0.9844601543922137, 0.9844633367946773, 0.984449924915228, 0.9844681172070662, 0.9844519021375685, 0.984479161253592, 0.9844572275306415, 0.98442350023831, 0.9844512743954499, 0.9844855717321139, 0.984509463888683, 0.9845240274319852, 0.9845161722328527, 0.9844696110726904, 0.9844184054333062, 0.984409486689994, 0.9844084352674416, 0.9843958632430969, 0.9844077999092818, 0.984409278362429, 0.984378056886928, 0.9844289765696823, 0.9844655036889229, 0.9844448996736206, 0.9844589081431818, 0.9844761660634058, 0.984498673638036, 0.9844561514373461, 0.9844806604745823, 0.9844632271271516, 0.9844823782977883, 0.9844647371192183, 0.9844790869930292, 0.9844827373330397, 0.984448784209278, 0.9844205875955208, 0.9844137757847967, 0.9844169457503831, 0.9844337856610867, 0.9844396049366633, 0.9843774850668434, 0.9843959098482729, 0.9843869155146545, 0.9843695200191599, 0.9843747904125005, 0.9843702692200877, 0.9844242928183355, 0.9843706435579582, 0.9844013782343237, 0.9844313645918622, 0.9844235111303227, 0.9844582596446898], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.03811079278049698, 0.08588074465832077, 0.13568743087584711, 0.18741159770919613, 0.23673346151696628, 0.28138257512618836, 0.3242015841798346, 0.36476108855364325, 0.4021253117295289, 0.43586091685175676, 0.4677092778209485, 0.4954674233634018, 0.5204578433482213, 0.5445108388985649, 0.5659346902967355, 0.5833870138122427, 0.6014572685680666, 0.6175872500132178, 0.6341948712562484, 0.6466106097989217, 0.6610452284124031, 0.673292785766946, 0.6849178499519231, 0.6941382319842158, 0.7037712602541677, 0.7135325590216726, 0.7228854284018849, 0.7306173580767565, 0.7376177163485237, 0.7442263031003431, 0.749921801555369, 0.7554108725764135, 0.7587701524121456, 0.7632798206234912, 0.7682072802497716, 0.7727142065922643, 0.7773401998072849, 0.7807933797983184, 0.7840325776806853, 0.7862132278098306, 0.7886326206256097, 0.7911549595607896, 0.7940813028366384, 0.7943222865872366, 0.7975879883219616, 0.7996451251072354, 0.8008464870317528, 0.8021453803089992, 0.8036226485657498, 0.8041343189030755, 0.8047046854879185, 0.806339003271958, 0.807926959119461, 0.810176049493283, 0.8107525945816053, 0.811760795919755, 0.8134891367438187, 0.8137130475705663, 0.815178068585347, 0.8148464321899147, 0.8175266221316914, 0.8188248647095614, 0.8196291311094637, 0.8194302369650083, 0.8209837482365043, 0.820544676641393, 0.8203071741033832, 0.8205022838297166, 0.820834662045013, 0.8223493580204816, 0.8234481477455419, 0.823919215642298, 0.825275029159017, 0.8254963437788833, 0.8268196033204226, 0.8270217673765581, 0.8277052328169896, 0.8273111390816008, 0.8278007988933203, 0.8285406385258256, 0.8280630042195533, 0.8286626420035467, 0.8286702071047732, 0.8297503520098832, 0.830808961151892, 0.8309183947147901, 0.8316963606367599, 0.8321443003449213, 0.8326806939173568, 0.8325998656777295, 0.8333114587880439, 0.8365144867138329, 0.8394338329407929, 0.8421202206839877, 0.8447221046302728, 0.8471238058295196, 0.849214153738661, 0.8511565020131383, 0.8530246267553485, 0.854277663420928, 0.8553443612636996, 0.856881208316923, 0.8579459583436645, 0.8587689265153221, 0.8594465036962446, 0.8601417723778251, 0.8607075085436571, 0.8612532921866558, 0.861853331237945, 0.862443224017765, 0.8630107486133529, 0.8636079994767917, 0.8638759410577269, 0.8641303250204783, 0.8646156182432045, 0.8653453508936582, 0.8659044540290665, 0.8664564749759339, 0.8669655008593646, 0.8674358311857022, 0.8678347144169061, 0.8682557739898992, 0.868707969793093, 0.8690772954135577, 0.8694107179806357, 0.8696731476885963, 0.8698249147156704, 0.8700469542587871, 0.8702335533076824, 0.8704014924516882, 0.8706269093774531, 0.8707310988519819, 0.8708492834415578, 0.8709546200635165, 0.871013831438189, 0.8711047722778038, 0.8711611754622975, 0.8711987017884323, 0.8712945401468631, 0.8713696171468606, 0.8714005653531083, 0.8714894538949812, 0.8715816606139168, 0.8717134747859588, 0.8717832794157967, 0.8718216895201507, 0.8718552291054098, 0.8718742372095526, 0.8717672151734618, 0.8717573740683897, 0.8717617536137345, 0.8716792164771352, 0.8716781752416958, 0.8716395875273907, 0.8717035443431757, 0.8717621349860418, 0.8718392806271213, 0.8718710611016833, 0.8718884860061987, 0.8718065121702626, 0.8719524622804201, 0.8718874753709022, 0.8718778152773361, 0.871843677621967, 0.8718506043345443, 0.8717459455859543, 0.871811473627133, 0.871774851631513, 0.8718141045142954, 0.87178736744389, 0.871787718143025, 0.8718378914059063, 0.8718087756463397, 0.8717337433377298, 0.8716651847513213, 0.8717276113533728, 0.8717461446928096, 0.8717516471757124, 0.8717199783165749, 0.8716650032635318, 0.8716043481932026, 0.8717338936073161, 0.8716541424713586, 0.8717166437927468, 0.8717983385531558, 0.8718474497750239, 0.8719038569059553, 0.8718925586588838, 0.8719068042990196, 0.8718463831876416, 0.8717797971561516, 0.8717819343927202, 0.8717340002719722, 0.8716430609469588, 0.8716323987246274, 0.8715516195543483, 0.8715877520736875, 0.871644685403593, 0.8716348902442578, 0.8715772464758561, 0.8716840584905445, 0.8717445977186739, 0.8717370183590806, 0.8718044686316063, 0.8717909021807198, 0.8718285500085815, 0.8717881613574975, 0.8718260832676814, 0.8718835975406873, 0.8718743252301427, 0.8719392223381525], "moving_var_accuracy_train": [0.013641067922462754, 0.0332317381317941, 0.05302491652805559, 0.07239083074208436, 0.08799102632403753, 0.09802326179263733, 0.10559842416016119, 0.11096711313394791, 0.113581849375677, 0.11345507492878174, 0.11197966280903017, 0.10824067130436332, 0.10366609528526752, 0.09924891242574467, 0.09378946486301395, 0.0876314242108125, 0.08240788908458532, 0.07705743243460794, 0.0723305403290496, 0.06684862003668857, 0.06254792795169081, 0.05803714223256467, 0.0537572936282268, 0.04944715999284488, 0.04561537250905687, 0.04220795264108816, 0.03904067984248561, 0.035993203138931514, 0.03305434229420964, 0.030379784379994813, 0.027806318708707894, 0.025439352459550573, 0.02306380724214886, 0.021053118190558034, 0.01935628380993256, 0.01779773366211307, 0.016337188293530817, 0.014894648635115791, 0.013585499218630025, 0.012362081527677767, 0.011288422426801795, 0.010283362564598204, 0.009403804479517691, 0.008476729853469832, 0.0078097894331468345, 0.007132473389835182, 0.006458262754674432, 0.00586783599363767, 0.005346893470049459, 0.004835042049004175, 0.0043754894220001265, 0.003980686402116414, 0.0036582266619236783, 0.0033985214059266996, 0.0030828010236783644, 0.002803285753759325, 0.0025836515086784715, 0.0023375622603847824, 0.0021638473588477683, 0.0019474862559913746, 0.0018414775943150338, 0.0017079066775047805, 0.0015539212991915786, 0.001402101806852457, 0.0013223508384729167, 0.0011905305088885552, 0.0010752666471453862, 0.0009833920926398127, 0.0008871674956128545, 0.0008605170681165762, 0.0007941184806175996, 0.0007233742941259372, 0.0006935553566259477, 0.0006295097191481543, 0.0006059886902882824, 0.0005491041624218115, 0.0005070271981790856, 0.0004563271421935539, 0.00041491723547821364, 0.0003834129800126887, 0.0003458181715338817, 0.00032190871350257076, 0.0002917375003517998, 0.0002882132812167285, 0.00026870555285802737, 0.00024722517554395785, 0.0002315217688200674, 0.00021120696349076308, 0.000201358500233119, 0.00018130581302895025, 0.00017374235009136871, 0.0003498808659596842, 0.0004815773721360973, 0.0005868542818332815, 0.0006540134812709553, 0.0006908281793884421, 0.000705940619585228, 0.0007052856159938428, 0.0006977705646660559, 0.0006762833614962114, 0.0006483802240296655, 0.0006218435066852191, 0.0005888463094384835, 0.0005536711543229481, 0.0005181851820322466, 0.0004814259344600968, 0.0004459238110121472, 0.00041224933198158773, 0.00038129065733886645, 0.00035054035361841384, 0.0003223096961571149, 0.0002961767523152805, 0.000270926938040067, 0.0002484396267401771, 0.00022732602389449405, 0.00020951994957634905, 0.00019301679594807466, 0.00017742580516908073, 0.00016220242522261365, 0.00014841860527347315, 0.00013549188971458115, 0.00012335807563632387, 0.00011230710116508932, 0.00010197965090472219, 9.281352554816145e-05, 8.384418013015705e-05, 7.588122634874021e-05, 6.863448974146127e-05, 6.20258837522053e-05, 5.592692299838169e-05, 5.069845277617604e-05, 4.5872968859426285e-05, 4.147138590245496e-05, 3.7425432623323296e-05, 3.3809731726625995e-05, 3.0454355676742318e-05, 2.7510284703444196e-05, 2.4822590513431096e-05, 2.2350520081364647e-05, 2.0162298269888078e-05, 1.8160000151933986e-05, 1.6344692041751888e-05, 1.4782742673049107e-05, 1.3359876913310656e-05, 1.203857146417903e-05, 1.0835356834559805e-05, 9.805680660579862e-06, 8.83326420606499e-06, 7.956540590808415e-06, 7.1793583047408326e-06, 6.462430356040739e-06, 5.823288393236391e-06, 5.250323486776628e-06, 4.737003101570181e-06, 4.271572404096464e-06, 3.848126084834235e-06, 3.463484590619592e-06, 3.137929279675848e-06, 2.830437025135049e-06, 2.548075002516958e-06, 2.2933586514342344e-06, 2.065641692884067e-06, 1.8620561589366058e-06, 1.6782168993522775e-06, 1.5170827440745005e-06, 1.3697042634892986e-06, 1.2429716093724362e-06, 1.1256170826786479e-06, 1.0236421401309405e-06, 9.264154424274908e-07, 8.356827693263906e-07, 7.526698297744445e-07, 6.969143215179679e-07, 6.508210469088837e-07, 5.864548380584084e-07, 5.278193036570201e-07, 4.764598754564434e-07, 4.3009624390729833e-07, 3.871062919299425e-07, 3.571686875290385e-07, 3.4478714560224977e-07, 3.223165050022488e-07, 2.9390558352126093e-07, 2.6628116014414907e-07, 2.4233356642388514e-07, 2.2265952802312865e-07, 2.1666681318442377e-07, 2.004063680222057e-07, 1.8310102564368843e-07, 1.6809182911009992e-07, 1.5408354683110286e-07, 1.405284620534957e-07, 1.2659554068787554e-07, 1.2431131813768107e-07, 1.1903562757029315e-07, 1.0754967170133221e-07, 9.688514266756992e-08, 8.97488717333811e-08, 8.107876027415416e-08, 1.0770078828469899e-07, 9.998596259270498e-08, 9.071544866857832e-08, 8.436733317324722e-08, 7.618059326960023e-08, 6.8746504570146e-08, 8.813879662186472e-08, 1.0522910521095668e-07, 1.0320777767148696e-07, 1.0097963465016231e-07, 9.143676290851548e-08, 9.31602198741771e-08], "duration": 140764.488088, "accuracy_train": [0.389316608123385, 0.521457158141842, 0.5939863000069214, 0.6614037193729236, 0.6939740073597269, 0.6980183243355482, 0.7257486979166666, 0.7509718761535622, 0.7626666536890919, 0.7646401462716869, 0.7778662776278147, 0.7677057378183831, 0.7721220786037283, 0.79206933197213, 0.7834183364479512, 0.7721231600682908, 0.8001794149709303, 0.8009016530546327, 0.8215457300895165, 0.7972976723998707, 0.8345178975175341, 0.8272383793027871, 0.8320768517557217, 0.8237782334348468, 0.8370497863026025, 0.8502090470999446, 0.8564855069329088, 0.8566700768849206, 0.8545320214447213, 0.861157974575489, 0.8576480813376707, 0.8607863112541528, 0.8430250582548912, 0.8614144619209118, 0.8771968150147655, 0.8812926818014026, 0.8825936836701735, 0.8750820110626615, 0.8783622733250278, 0.8768265936461794, 0.8844509188122923, 0.8832883444075305, 0.8905693045750278, 0.8661359559569952, 0.900005082883444, 0.8936123656100037, 0.8838943250507567, 0.8899608005144887, 0.8946790501568845, 0.886265976836471, 0.8882427138127538, 0.8953540642880213, 0.9047244139904023, 0.9129761688468992, 0.8984468726928755, 0.9015872655384828, 0.9114662640734589, 0.8997732889788667, 0.9150909727990033, 0.8923575062292359, 0.9232969456556847, 0.9187421774063308, 0.911071709752215, 0.9050739072882059, 0.9253219880490956, 0.9041420453234589, 0.9086986160137505, 0.9160464467400333, 0.9090248578234589, 0.9309230732627353, 0.9220656982511997, 0.9185797774778516, 0.9314829114179586, 0.919602121977667, 0.9336202458817828, 0.9212064746562385, 0.9273659560723514, 0.9167908548472684, 0.9234858414659468, 0.927855318786914, 0.9212544195851791, 0.9295519564414912, 0.9244885393595422, 0.9371068876315062, 0.9320860081556847, 0.9306694698228128, 0.9337150542751015, 0.9303203370131967, 0.9364583693821521, 0.9273473909306941, 0.9373179534653008, 0.9739354423334257, 0.9752382466431341, 0.9777959103336102, 0.9780287857027501, 0.9780752886789406, 0.9783306945598007, 0.9786798273694168, 0.9800513046788483, 0.9794006235003692, 0.9795626629406607, 0.9812836335478959, 0.9807255978336102, 0.9807488493217055, 0.981003894714378, 0.9805628374169435, 0.9807721008098007, 0.9811201521548542, 0.9818878117501846, 0.9813301365240864, 0.9818881722383721, 0.9822830870478036, 0.9818413087739941, 0.9827248653216132, 0.9827248653216132, 0.9843292180001846, 0.9847012418097084, 0.9847946082502769, 0.9843063270002769, 0.9847477447858989, 0.9846779903216132, 0.9844919784168512, 0.9847012418097084, 0.984468726928756, 0.9850035111549464, 0.9838180457502769, 0.9843063270002769, 0.9843063270002769, 0.9842362120478036, 0.9837947942621816, 0.9848407507382798, 0.9846779903216132, 0.9846314873454227, 0.9843989724644703, 0.9846318478336102, 0.9840967031192323, 0.9846779903216132, 0.984561732881137, 0.9841432060954227, 0.984561732881137, 0.9843059665120893, 0.9838641882382798, 0.9848407507382798, 0.9848174992501846, 0.9845152299049464, 0.9842362120478036, 0.9849337566906607, 0.9845384813930418, 0.9845384813930418, 0.9847477447858989, 0.9844458359288483, 0.9846314873454227, 0.9847012418097084, 0.9840502001430418, 0.9846779903216132, 0.9846082358573275, 0.9844690874169435, 0.9849105052025655, 0.9842133210478959, 0.9845384813930418, 0.9844919784168512, 0.9843292180001846, 0.9846318478336102, 0.9843059665120893, 0.9847244932978036, 0.9842598240240864, 0.9841199546073275, 0.9847012418097084, 0.9847942477620893, 0.9847244932978036, 0.9846550993217055, 0.9844454754406607, 0.9840505606312293, 0.9839575546788483, 0.9843292180001846, 0.9843989724644703, 0.9842827150239941, 0.9845152299049464, 0.984422584440753, 0.9840970636074198, 0.9848872537144703, 0.9847942477620893, 0.9842594635358989, 0.9845849843692323, 0.9846314873454227, 0.9847012418097084, 0.984073451631137, 0.9847012418097084, 0.9843063270002769, 0.984654738833518, 0.9843059665120893, 0.9846082358573275, 0.9845155903931341, 0.9841432060954227, 0.9841668180717055, 0.9843524694882798, 0.9844454754406607, 0.9845853448574198, 0.9844919784168512, 0.9838184062384644, 0.984561732881137, 0.9843059665120893, 0.9842129605597084, 0.9844222239525655, 0.9843295784883721, 0.9849105052025655, 0.9838878002145626, 0.9846779903216132, 0.9847012418097084, 0.9843528299764673, 0.9847709962739941], "end": "2016-02-01 10:26:20.821000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0], "moving_var_accuracy_valid": [0.01307189273722183, 0.03230241818518605, 0.05139853029440761, 0.07033718217640395, 0.0851972802040136, 0.0946194422984152, 0.1016587058955998, 0.1062984958614809, 0.10823341283716933, 0.10765289103011837, 0.10601646479492198, 0.10234945011103398, 0.09773519491908955, 0.09316859478168421, 0.08798256798209386, 0.0819255635486938, 0.07667181415628782, 0.07134621945344734, 0.06669391525827027, 0.06141187880448347, 0.05714591485468507, 0.052781347319591915, 0.04871949164337626, 0.04461268148243145, 0.04098657043703554, 0.037745459975988424, 0.03475819946917939, 0.03182042415073592, 0.029079426879060212, 0.026564544960861088, 0.024200038788635803, 0.02205120401583885, 0.019947646463387772, 0.01813591578343679, 0.01654084293041072, 0.015069570102879543, 0.013755211411620333, 0.012487010338912571, 0.011332740931311489, 0.010242263953052014, 0.009270718712120199, 0.008400906584243504, 0.0076378872905321004, 0.006874621219991362, 0.006283142368373906, 0.0056929144373164525, 0.005136612427847929, 0.00463813529877422, 0.004193962662418426, 0.003776922654983474, 0.0034021582518550765, 0.003085981378242618, 0.0028000776743809267, 0.002565595574529402, 0.002312027655226279, 0.0020899731191478727, 0.0019078602652702107, 0.001717525463268203, 0.001565089496105125, 0.0014095703907835903, 0.0013332641148212354, 0.0012151066074580633, 0.001099417546690363, 0.0009898318219476144, 0.0009125692151888435, 0.0008230473484606612, 0.0007412502807146448, 0.0006674678628909692, 0.000601715354103905, 0.000562192553776421, 0.000516839348137861, 0.0004671525579942625, 0.0004369813748238976, 0.00039372405879020727, 0.00037011079523966193, 0.000333467548466034, 0.00030432491869380915, 0.00027529021567488195, 0.00024991909468831175, 0.00022985344935591107, 0.00020892131519507416, 0.00019126527292350013, 0.00017213926070795926, 0.00016542575178147968, 0.00015896905644323045, 0.00014317993214110465, 0.00013430901770871084, 0.0001226839657771689, 0.00011300503178040398, 0.00010176332744125474, 9.614427748895156e-05, 0.000178864340780513, 0.0002376811482382567, 0.0002788631453755165, 0.000311905031467383, 0.0003326280461748171, 0.0003386912309886021, 0.0003387765592640253, 0.00033630791380974634, 0.00031680803039635364, 0.00029536782594668176, 0.0002870881331370269, 0.00026858255339833693, 0.00024781978756255703, 0.0002271698063312646, 0.00020880341255441644, 0.00019080358798294694, 0.00017440414724933717, 0.00016020415429205222, 0.00014731550028800047, 0.00013548270775857598, 0.00012514481432762396, 0.00011327646711200851, 0.00010253122120535365, 9.43976846930347e-05, 8.975050389397411e-05, 8.3588820348787e-05, 7.797248244593221e-05, 7.250720035136064e-05, 6.724737585908037e-05, 6.195460876239313e-05, 5.735476836223617e-05, 5.345962092584743e-05, 4.934127155864809e-05, 4.5407679876914946e-05, 4.148673605380565e-05, 3.754536152298729e-05, 3.4234539399055774e-05, 3.1124458304588184e-05, 2.8265844478934156e-05, 2.589657514483246e-05, 2.3404616649772475e-05, 2.1189863359714263e-05, 1.917073925907383e-05, 1.7285219215181846e-05, 1.563112942045231e-05, 1.4096648351396366e-05, 1.2699657542635244e-05, 1.151235670689223e-05, 1.0411850039560512e-05, 9.379285158833996e-06, 8.512467198837207e-06, 7.737739190105312e-06, 7.120340054654953e-06, 6.452160226310643e-06, 5.820222228727987e-06, 5.248324139869311e-06, 4.726743498090294e-06, 4.357152594162444e-06, 3.922308960887558e-06, 3.5302506885556436e-06, 3.2385370299622717e-06, 2.9146930845072073e-06, 2.6366248813141266e-06, 2.4097766617509563e-06, 2.1996947664590924e-06, 2.0332883392513518e-06, 1.8390494923966662e-06, 1.6578771888333355e-06, 1.552566857952778e-06, 1.5890230840522855e-06, 1.4681304613252639e-06, 1.3221572718620942e-06, 1.200429960302812e-06, 1.0808187783966999e-06, 1.0713179834647927e-06, 1.0028315027447284e-06, 9.146188875389262e-07, 8.370240980456126e-07, 7.597555266458435e-07, 6.837810808902094e-07, 6.38059179574524e-07, 5.818828087132971e-07, 5.743631538599157e-07, 5.592293564069316e-07, 5.383801465596213e-07, 4.876334939397797e-07, 4.3914264040865866e-07, 4.042546261194369e-07, 3.91029471621243e-07, 3.8503786246875125e-07, 4.97572205082333e-07, 5.050571777527324e-07, 4.897091965549362e-07, 5.008045818040704e-07, 4.7243133264410637e-07, 4.5382407915883763e-07, 4.09590524724938e-07, 3.704579166183298e-07, 3.6626852125771607e-07, 3.69544965438315e-07, 3.326315789158336e-07, 3.2004754041113934e-07, 3.6247243387521794e-07, 3.2724833735309725e-07, 3.5325097277647335e-07, 3.2967590608304623e-07, 3.258809519617296e-07, 2.941563630831747e-07, 2.946459630947795e-07, 3.678606251216244e-07, 3.640595458918548e-07, 3.2817061152926907e-07, 3.362994037504224e-07, 3.043259006822912e-07, 2.866495410984359e-07, 2.7266577521609996e-07, 2.583418391425087e-07, 2.6227867962288994e-07, 2.3682459334612737e-07, 2.510468456638658e-07], "accuracy_test": 0.8693219866071429, "start": "2016-01-30 19:20:16.332000", "learning_rate_per_epoch": [0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 0.00019318496924825013, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318496924825013e-05, 1.9318497379572364e-06, 1.9318497379572364e-07, 1.9318497024300996e-08, 1.9318497912479415e-09, 1.9318498190035172e-10, 1.931849819003517e-11, 1.931849862371604e-12, 1.931849862371604e-13, 1.931849896252922e-14, 1.9318498539012746e-15, 1.9318498803710542e-16, 1.9318498803710542e-17, 1.931849921730085e-18, 1.931849947579479e-19, 1.931849947579479e-20, 1.9318500283588356e-21, 1.9318500536023846e-22, 1.9318499904935122e-23, 1.931849951050467e-24, 1.9318499757023702e-25, 1.931849944887491e-26, 1.9318498678502933e-27, 1.9318498919244176e-28, 1.931849922017073e-29, 1.931849922017073e-30, 1.931849969036847e-31, 1.9318500278115646e-32, 1.9318500278115646e-33, 1.9318501196470608e-34, 1.9318500622498757e-35, 1.9318500622498757e-36, 1.931850017408325e-37, 1.9318499613563862e-38, 1.9318496810966934e-39, 1.931844075902836e-40, 1.9318300629181928e-41, 1.9323905823039227e-42, 1.9337918807682476e-43, 1.961817850054744e-44, 1.401298464324817e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.389316608123385, "accuracy_train_last": 0.9847709962739941, "batch_size_eval": 1024, "accuracy_train_std": [0.014076421075600283, 0.012658590416983001, 0.015516466428040868, 0.016803651819437136, 0.018543234254192194, 0.016836283518588232, 0.016611019970438838, 0.01523798312615259, 0.016441525920376038, 0.015883007192254908, 0.016256789306021903, 0.0184309368679222, 0.01682623847717403, 0.017031487787973754, 0.02019779388118199, 0.01812996037179216, 0.017751813218920113, 0.017334421607298645, 0.01756130883053973, 0.01833091284546393, 0.01734742195760605, 0.018037328389310183, 0.01756915246046949, 0.015497571181275833, 0.01811734966993714, 0.01703629512966166, 0.016768069917759948, 0.017789063366731177, 0.017093989365106785, 0.015823458860024623, 0.016548862389782436, 0.016384115953743335, 0.016382839587796176, 0.014524289586330676, 0.01601194904000945, 0.013629448296361006, 0.014536467042399545, 0.01654418088205956, 0.015422387711935751, 0.016400578353976406, 0.015768074086726293, 0.015537604910560191, 0.014436356346277872, 0.014007977475617115, 0.014439871090849888, 0.015161272492243572, 0.015072873025452002, 0.01599154713417052, 0.01628072534713754, 0.015396521518877273, 0.014238860371411614, 0.01598864702162648, 0.014680419455918676, 0.014973550386549468, 0.014494228835350446, 0.013984863735685983, 0.013644678193420197, 0.01298880934514878, 0.015515952950379561, 0.01606292401346957, 0.014455404822695455, 0.013737388256527002, 0.0129966648378408, 0.015733196680571528, 0.011941971588364034, 0.016468976787935786, 0.014368110848844751, 0.013693212622823869, 0.014329906488721351, 0.013488667770995909, 0.014477084421399878, 0.013597288351074538, 0.013037840194697834, 0.014152027906176657, 0.013750489825439636, 0.01337229642162743, 0.014381551996741402, 0.012773145403755178, 0.013167100779768088, 0.01313289868849638, 0.012259412899461296, 0.012197516976275675, 0.012002013218654456, 0.013359286972141421, 0.013437172708859647, 0.013233981231282983, 0.01113836232199185, 0.01215671414803155, 0.012720841195165714, 0.012980517509087564, 0.012788774925421352, 0.008655963009978939, 0.008189170033958092, 0.007442891193847761, 0.007394875893331392, 0.007342334150331771, 0.006967511007021797, 0.00705425853491911, 0.006519066319987983, 0.007396326312233664, 0.006789001827059458, 0.006566663621636866, 0.006838260795457641, 0.007198116585315474, 0.006800880952296392, 0.006380507201079511, 0.006671620472636458, 0.006905127622857765, 0.006416691684647846, 0.006324590450971509, 0.005768717717421346, 0.006183710270993744, 0.005668360625085805, 0.0058045279740651596, 0.006023378584715364, 0.00586063982128225, 0.005512045995779927, 0.00543137462588968, 0.005866587635095543, 0.005221308058325206, 0.005352464734082106, 0.0053597572626542375, 0.005781464023891901, 0.005770260819699057, 0.005549591064009704, 0.0056688535170721, 0.005461696728029553, 0.005572816508866145, 0.005808576692763324, 0.005728294860404899, 0.005692677311602134, 0.0056375863678463905, 0.0057869405345950435, 0.005515468294253311, 0.005485140632566454, 0.005448396701890754, 0.0055482693405993905, 0.005822852254113488, 0.005565996147683563, 0.005724533326489089, 0.005535635905200347, 0.005580001556971457, 0.0055839432555773155, 0.0057124471043425015, 0.005788963253616042, 0.005617805282028272, 0.005599787082423178, 0.005521328204650351, 0.005691433367463769, 0.005639447145645832, 0.005653692894673928, 0.005639893388314776, 0.005706354183585356, 0.00566249707728822, 0.005765032355579614, 0.0058986727177144874, 0.00542167599477463, 0.005600031433350521, 0.005316452098437624, 0.005635302201184583, 0.0057678682529315834, 0.0054469765611887865, 0.005680369245740993, 0.005803959000105536, 0.005504436357953162, 0.005555725161147761, 0.0055424923575166886, 0.005454068956475464, 0.005922979628025887, 0.005738708674827932, 0.005494343964055851, 0.005729126523178609, 0.005516655571764881, 0.005485618527497638, 0.005496772726825456, 0.005428325357650117, 0.005446387163326778, 0.005556810048573564, 0.005530090269170758, 0.005572420549284708, 0.005448093626010348, 0.0056038584561391895, 0.0055019430496299464, 0.005499148348661494, 0.005703946566591672, 0.005749958413594678, 0.005665787077710987, 0.005361695702191341, 0.005292788361679336, 0.005624676315623833, 0.005564275003425455, 0.005793805928591921, 0.005622179065746268, 0.005387722285022657, 0.0056943362256175155, 0.005322730190085854, 0.005531514317704875, 0.00565825434585892, 0.005712488188659288, 0.005489952023791519, 0.005563610275589381, 0.005452981168243938, 0.005379729584077986, 0.005651554890669629, 0.005619714132369421, 0.005668542504046001, 0.005347746826452909, 0.005453328546315262, 0.0055079250173642996, 0.0055323031086138485, 0.005603523775455888], "accuracy_test_std": 0.008682803046856367, "error_valid": [0.6188920721950302, 0.4841896884412651, 0.41605239316641573, 0.3470709007906627, 0.31936976421310237, 0.31677540239081325, 0.29042733433734935, 0.27020337208207834, 0.2615966796875, 0.2605186370481928, 0.24565547345632532, 0.2547092667545181, 0.25462837678840367, 0.23901220114834332, 0.24125064711972888, 0.2595420745481928, 0.2359104386295181, 0.23724291698042166, 0.21633653755647586, 0.2416477433170181, 0.2090432040662651, 0.21647919804216864, 0.2104565723832832, 0.22287832972515065, 0.2095314853162651, 0.1986157520707832, 0.19293874717620485, 0.19979527484939763, 0.1993790592055723, 0.1962964161332832, 0.19881871234939763, 0.19518748823418675, 0.2109963290662651, 0.19613316547439763, 0.18744558311370485, 0.18672345632530118, 0.18102586125753017, 0.18812800028237953, 0.18681464137801207, 0.19416092102786142, 0.18959284403237953, 0.1861439900225903, 0.17958160768072284, 0.20350885965737953, 0.17302069606551207, 0.18184064382530118, 0.1883412556475903, 0.1861645801957832, 0.18308193712349397, 0.19126064806099397, 0.19016201524849397, 0.17895213667168675, 0.17778143825301207, 0.1695821371423193, 0.18405849962349397, 0.17916539203689763, 0.1709557958396084, 0.18427175498870485, 0.1716367422816265, 0.18813829536897586, 0.1583516683923193, 0.1694909520896084, 0.17313247129141573, 0.1823598103350903, 0.16503465032003017, 0.1834069677146084, 0.18183034873870485, 0.1777417286332832, 0.1761739340173193, 0.16401837820030118, 0.16666274472891573, 0.17184117328689763, 0.16252264919051207, 0.1725118246423193, 0.16127106080572284, 0.17115875611822284, 0.1661435782191265, 0.17623570453689763, 0.16779226280120485, 0.1648008047816265, 0.17623570453689763, 0.16594061794051207, 0.17126170698418675, 0.1605283438441265, 0.15966355657003017, 0.1680967032191265, 0.16130194606551207, 0.1638242422816265, 0.16249176393072284, 0.16812758847891573, 0.1602842032191265, 0.13465826195406627, 0.13429205101656627, 0.13370228962725905, 0.13186093985316272, 0.13126088337725905, 0.13197271507906627, 0.13136236351656627, 0.13016225056475905, 0.1344450065888554, 0.1350553581513554, 0.12928716820406627, 0.13247129141566272, 0.13382435993975905, 0.13445530167545183, 0.13360080948795183, 0.1342008659638554, 0.1338346550263554, 0.13274631730045183, 0.1322477409638554, 0.1318815300263554, 0.13101674275225905, 0.1337125847138554, 0.13358021931475905, 0.13101674275225905, 0.12808705525225905, 0.12906361775225905, 0.12857533650225905, 0.12845326618975905, 0.12833119587725905, 0.12857533650225905, 0.12795468985316272, 0.12722226797816272, 0.12759877400225905, 0.12758847891566272, 0.12796498493975905, 0.12880918204066272, 0.12795468985316272, 0.12808705525225905, 0.12808705525225905, 0.12734433829066272, 0.12833119587725905, 0.12808705525225905, 0.1280973503388554, 0.12845326618975905, 0.12807676016566272, 0.12833119587725905, 0.1284635612763554, 0.12784291462725905, 0.12795468985316272, 0.12832090079066272, 0.12771054922816272, 0.12758847891566272, 0.12710019766566272, 0.12758847891566272, 0.12783261954066272, 0.12784291462725905, 0.12795468985316272, 0.1291959831513554, 0.12833119587725905, 0.12819883047816272, 0.12906361775225905, 0.12833119587725905, 0.1287077019013554, 0.12772084431475905, 0.12771054922816272, 0.12746640860316272, 0.12784291462725905, 0.12795468985316272, 0.12893125235316272, 0.12673398672816272, 0.12869740681475905, 0.12820912556475905, 0.1284635612763554, 0.12808705525225905, 0.1291959831513554, 0.12759877400225905, 0.12855474632906627, 0.12783261954066272, 0.12845326618975905, 0.12820912556475905, 0.12771054922816272, 0.12845326618975905, 0.12894154743975905, 0.1289518425263554, 0.12771054922816272, 0.12808705525225905, 0.12819883047816272, 0.12856504141566272, 0.1288297722138554, 0.12894154743975905, 0.12710019766566272, 0.12906361775225905, 0.12772084431475905, 0.12746640860316272, 0.12771054922816272, 0.12758847891566272, 0.12820912556475905, 0.12796498493975905, 0.12869740681475905, 0.12881947712725905, 0.12819883047816272, 0.12869740681475905, 0.12917539297816272, 0.1284635612763554, 0.12917539297816272, 0.12808705525225905, 0.12784291462725905, 0.12845326618975905, 0.12894154743975905, 0.12735463337725905, 0.12771054922816272, 0.12833119587725905, 0.12758847891566272, 0.12833119587725905, 0.12783261954066272, 0.12857533650225905, 0.12783261954066272, 0.12759877400225905, 0.12820912556475905, 0.12747670368975905], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.040305739006695866, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.00019318497192047411, "optimization": "rmsprop", "nb_data_augmentation": 3, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.0956131619109302e-06, "rotation_range": [0, 0], "momentum": 0.6824423090579051}, "accuracy_valid_max": 0.8732660132718373, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.872523296310241, "accuracy_valid_std": [0.01225488870850322, 0.016820527105088794, 0.010220416259816616, 0.01385863816577576, 0.010911298664611283, 0.011252454873405749, 0.013436569037749514, 0.010627508576015653, 0.012004546218233353, 0.012381191617476658, 0.009340154605986866, 0.012543187115775437, 0.007555065288432039, 0.01429472280235974, 0.008894783500343948, 0.011017532911277461, 0.007012896006432331, 0.0067771282221289475, 0.012770594460290923, 0.008663639040776721, 0.006963010867714213, 0.010244787571597936, 0.012179306780689344, 0.008755364996433215, 0.007060919313976838, 0.00822623001636966, 0.01572899769584559, 0.01356450804722378, 0.008642836662085096, 0.008272552705943265, 0.013151522958406705, 0.009365184438304315, 0.013446726340796748, 0.01180182440618406, 0.012622155496525019, 0.008903442465359866, 0.016431207539656704, 0.007377378214697265, 0.012636647417323768, 0.009986478647302599, 0.004101594553362351, 0.0058021946733714065, 0.013871727915364765, 0.009212883351517149, 0.009981740989989553, 0.0089992320781111, 0.008747557737146111, 0.00527253844907619, 0.006716896072319372, 0.008765771590633933, 0.00952312079829941, 0.006918798421848615, 0.009090206327508857, 0.010038925374021714, 0.008969314509001802, 0.007436368483990593, 0.009205856876192997, 0.010213517045123409, 0.01112824641999517, 0.008479639549108664, 0.012729970983383186, 0.0055707489584945705, 0.012368948548385127, 0.00635883003760484, 0.00994546373757703, 0.011411761440431704, 0.010165380821253503, 0.007661229449994302, 0.012062011597810857, 0.007165171975488424, 0.005856796230980425, 0.0050946461766925855, 0.005788649898427834, 0.009995481540466483, 0.00723535275535964, 0.010108056726012006, 0.011276783860131201, 0.00792027571261595, 0.007253855270660677, 0.010134053740144196, 0.009544767062476697, 0.008282023012948805, 0.008340582935516518, 0.00935602632567553, 0.009712615481321692, 0.010656125708498219, 0.006205719058916504, 0.010216626904896078, 0.007499649258191966, 0.014167911644138737, 0.008033100785900422, 0.010490292201758178, 0.010428835092097213, 0.008352243993131435, 0.008914732076876899, 0.0070896260638340204, 0.009739940221751986, 0.00985719363674157, 0.007339750048807494, 0.009128850945609047, 0.00925309391258851, 0.011437981718188788, 0.01026716375211462, 0.0099004477154584, 0.007777431593064031, 0.008227503092162684, 0.00800289176833475, 0.00949116409628363, 0.008775598952084632, 0.009573009404489045, 0.007806074972684236, 0.00948560626546412, 0.00865672258746462, 0.009416908771714659, 0.010281651250850482, 0.010186923292549587, 0.01017182615252337, 0.010237764385754914, 0.009676242302704897, 0.010365889207702164, 0.009675026945722594, 0.010587384268288064, 0.010776030873429713, 0.0103798739336988, 0.010531097941718838, 0.00965335683057802, 0.00984355098932074, 0.010336706288956807, 0.009164314709942018, 0.0094461565116768, 0.011380372630465657, 0.01035438267050502, 0.009793140102027427, 0.008727320730676343, 0.010434963064771995, 0.010060996722191646, 0.00999110927450532, 0.009081064237182725, 0.00955996208847166, 0.01040567193307422, 0.01103787361569161, 0.009617431345923156, 0.009570377158234056, 0.010134248824633289, 0.010127160193732497, 0.010108884089780262, 0.008954730700319357, 0.011082488064688537, 0.009522814047136481, 0.008999288682228322, 0.009945243635466004, 0.00971628227622577, 0.009871072826399905, 0.009141988512069571, 0.010446002744184207, 0.010391929240287716, 0.01050943347432168, 0.008874496630267691, 0.010654727453999138, 0.01101554345043102, 0.01011528448229362, 0.009843306633712825, 0.009772136110733427, 0.010252584683604788, 0.00970755700494346, 0.010533260017676044, 0.009454287508250397, 0.010594919559072087, 0.010342045693316372, 0.01091514787185785, 0.009989303208458953, 0.010774897912490863, 0.010548584737023796, 0.009525448650610759, 0.010168679299164549, 0.010253349227736666, 0.00970755700494346, 0.010146969341310912, 0.009740695372366864, 0.010068351930359815, 0.010569532012173474, 0.00956542346275416, 0.010053947557359885, 0.010203537777181557, 0.009829542937660042, 0.010230070049991102, 0.009317926130251135, 0.010072492672831253, 0.010629018777097772, 0.010304805193222036, 0.009405472442773266, 0.010718298175017418, 0.010867839666894887, 0.010491173026193481, 0.009629004443929095, 0.010990611790300848, 0.010021753641996367, 0.01010552874384046, 0.009589616001016142, 0.009323061446209466, 0.009898679518719812, 0.010101075628744087, 0.010331331149907622, 0.010519772111551333, 0.010003033695544956, 0.010782218403816752, 0.008918489234924606, 0.009613239609905103, 0.009185682453009504, 0.009869244793777928, 0.009290189958259018], "accuracy_valid": [0.3811079278049699, 0.5158103115587349, 0.5839476068335843, 0.6529290992093373, 0.6806302357868976, 0.6832245976091867, 0.7095726656626506, 0.7297966279179217, 0.7384033203125, 0.7394813629518072, 0.7543445265436747, 0.7452907332454819, 0.7453716232115963, 0.7609877988516567, 0.7587493528802711, 0.7404579254518072, 0.7640895613704819, 0.7627570830195783, 0.7836634624435241, 0.7583522566829819, 0.7909567959337349, 0.7835208019578314, 0.7895434276167168, 0.7771216702748494, 0.7904685146837349, 0.8013842479292168, 0.8070612528237951, 0.8002047251506024, 0.8006209407944277, 0.8037035838667168, 0.8011812876506024, 0.8048125117658133, 0.7890036709337349, 0.8038668345256024, 0.8125544168862951, 0.8132765436746988, 0.8189741387424698, 0.8118719997176205, 0.8131853586219879, 0.8058390789721386, 0.8104071559676205, 0.8138560099774097, 0.8204183923192772, 0.7964911403426205, 0.8269793039344879, 0.8181593561746988, 0.8116587443524097, 0.8138354198042168, 0.816918062876506, 0.808739351939006, 0.809837984751506, 0.8210478633283133, 0.8222185617469879, 0.8304178628576807, 0.815941500376506, 0.8208346079631024, 0.8290442041603916, 0.8157282450112951, 0.8283632577183735, 0.8118617046310241, 0.8416483316076807, 0.8305090479103916, 0.8268675287085843, 0.8176401896649097, 0.8349653496799698, 0.8165930322853916, 0.8181696512612951, 0.8222582713667168, 0.8238260659826807, 0.8359816217996988, 0.8333372552710843, 0.8281588267131024, 0.8374773508094879, 0.8274881753576807, 0.8387289391942772, 0.8288412438817772, 0.8338564217808735, 0.8237642954631024, 0.8322077371987951, 0.8351991952183735, 0.8237642954631024, 0.8340593820594879, 0.8287382930158133, 0.8394716561558735, 0.8403364434299698, 0.8319032967808735, 0.8386980539344879, 0.8361757577183735, 0.8375082360692772, 0.8318724115210843, 0.8397157967808735, 0.8653417380459337, 0.8657079489834337, 0.866297710372741, 0.8681390601468373, 0.868739116622741, 0.8680272849209337, 0.8686376364834337, 0.869837749435241, 0.8655549934111446, 0.8649446418486446, 0.8707128317959337, 0.8675287085843373, 0.866175640060241, 0.8655446983245482, 0.8663991905120482, 0.8657991340361446, 0.8661653449736446, 0.8672536826995482, 0.8677522590361446, 0.8681184699736446, 0.868983257247741, 0.8662874152861446, 0.866419780685241, 0.868983257247741, 0.871912944747741, 0.870936382247741, 0.871424663497741, 0.871546733810241, 0.871668804122741, 0.871424663497741, 0.8720453101468373, 0.8727777320218373, 0.872401225997741, 0.8724115210843373, 0.872035015060241, 0.8711908179593373, 0.8720453101468373, 0.871912944747741, 0.871912944747741, 0.8726556617093373, 0.871668804122741, 0.871912944747741, 0.8719026496611446, 0.871546733810241, 0.8719232398343373, 0.871668804122741, 0.8715364387236446, 0.872157085372741, 0.8720453101468373, 0.8716790992093373, 0.8722894507718373, 0.8724115210843373, 0.8728998023343373, 0.8724115210843373, 0.8721673804593373, 0.872157085372741, 0.8720453101468373, 0.8708040168486446, 0.871668804122741, 0.8718011695218373, 0.870936382247741, 0.871668804122741, 0.8712922980986446, 0.872279155685241, 0.8722894507718373, 0.8725335913968373, 0.872157085372741, 0.8720453101468373, 0.8710687476468373, 0.8732660132718373, 0.871302593185241, 0.871790874435241, 0.8715364387236446, 0.871912944747741, 0.8708040168486446, 0.872401225997741, 0.8714452536709337, 0.8721673804593373, 0.871546733810241, 0.871790874435241, 0.8722894507718373, 0.871546733810241, 0.871058452560241, 0.8710481574736446, 0.8722894507718373, 0.871912944747741, 0.8718011695218373, 0.8714349585843373, 0.8711702277861446, 0.871058452560241, 0.8728998023343373, 0.870936382247741, 0.872279155685241, 0.8725335913968373, 0.8722894507718373, 0.8724115210843373, 0.871790874435241, 0.872035015060241, 0.871302593185241, 0.871180522872741, 0.8718011695218373, 0.871302593185241, 0.8708246070218373, 0.8715364387236446, 0.8708246070218373, 0.871912944747741, 0.872157085372741, 0.871546733810241, 0.871058452560241, 0.872645366622741, 0.8722894507718373, 0.871668804122741, 0.8724115210843373, 0.871668804122741, 0.8721673804593373, 0.871424663497741, 0.8721673804593373, 0.872401225997741, 0.871790874435241, 0.872523296310241], "seed": 81956777, "model": "residualv3", "loss_std": [0.3502824902534485, 0.17813758552074432, 0.17919600009918213, 0.17936010658740997, 0.1783071905374527, 0.17795543372631073, 0.17399974167346954, 0.17193110287189484, 0.17074066400527954, 0.1706886738538742, 0.16872267425060272, 0.16832396388053894, 0.16835348308086395, 0.16350431740283966, 0.16116420924663544, 0.16222091019153595, 0.1604277640581131, 0.15864352881908417, 0.15680909156799316, 0.1562233567237854, 0.1527099907398224, 0.1536022424697876, 0.15032793581485748, 0.14903417229652405, 0.14642669260501862, 0.14648917317390442, 0.14344635605812073, 0.14381976425647736, 0.14333684742450714, 0.14133507013320923, 0.1395576000213623, 0.13962318003177643, 0.13853156566619873, 0.13680940866470337, 0.13656041026115417, 0.13337449729442596, 0.1333722472190857, 0.13342943787574768, 0.13191358745098114, 0.12954600155353546, 0.12786267697811127, 0.12717461585998535, 0.12579266726970673, 0.1240009218454361, 0.12176813185214996, 0.12145078927278519, 0.12148094922304153, 0.11923666298389435, 0.11961257457733154, 0.1178639829158783, 0.11757059395313263, 0.11794215440750122, 0.11482485383749008, 0.1142955869436264, 0.11480109393596649, 0.11218246072530746, 0.11024255305528641, 0.1082037016749382, 0.10972855985164642, 0.10897983610630035, 0.10623972117900848, 0.10613791644573212, 0.10614297538995743, 0.1039748340845108, 0.10188446193933487, 0.10400853306055069, 0.09853105247020721, 0.10217175632715225, 0.10127097368240356, 0.09813185781240463, 0.09941811114549637, 0.0962207019329071, 0.0970575362443924, 0.09707225859165192, 0.09572920203208923, 0.09287479519844055, 0.09392901510000229, 0.09257402271032333, 0.0907059758901596, 0.09209509938955307, 0.09119857102632523, 0.09166953712701797, 0.08703207969665527, 0.08614622056484222, 0.0878162756562233, 0.085911326110363, 0.08798102289438248, 0.08545713871717453, 0.08407557755708694, 0.08459043502807617, 0.08185099065303802, 0.0785527154803276, 0.06939766556024551, 0.06681911647319794, 0.06567464023828506, 0.06496885418891907, 0.06388971209526062, 0.06210032477974892, 0.06253872811794281, 0.06190209463238716, 0.06172769516706467, 0.06154344230890274, 0.06001913920044899, 0.06180112436413765, 0.06135159730911255, 0.06187496334314346, 0.057326026260852814, 0.05945685878396034, 0.0589446984231472, 0.05845334008336067, 0.05785246193408966, 0.057183925062417984, 0.058843567967414856, 0.05951094999909401, 0.058210063725709915, 0.05872972309589386, 0.058884795755147934, 0.05834799259901047, 0.05684485659003258, 0.05661862716078758, 0.057217903435230255, 0.05903041735291481, 0.0570099763572216, 0.05876779556274414, 0.05682599917054176, 0.05717095360159874, 0.05652367323637009, 0.05677070468664169, 0.05655524879693985, 0.059202536940574646, 0.05686434358358383, 0.05763598531484604, 0.056750863790512085, 0.05635160207748413, 0.05773579329252243, 0.059293266385793686, 0.056706495583057404, 0.05712907388806343, 0.05741538479924202, 0.058283884078264236, 0.05860961601138115, 0.05726776272058487, 0.057720571756362915, 0.057912878692150116, 0.05772187560796738, 0.05732392519712448, 0.057408757507801056, 0.05722518265247345, 0.057908326387405396, 0.05839362367987633, 0.057127222418785095, 0.05767981335520744, 0.05923886224627495, 0.057341840118169785, 0.05603139474987984, 0.056930627673864365, 0.05784434825181961, 0.057863082736730576, 0.05706321820616722, 0.056814663112163544, 0.056156400591135025, 0.0584961399435997, 0.05705150216817856, 0.05928708612918854, 0.05774714797735214, 0.05707886070013046, 0.055699098855257034, 0.05842584744095802, 0.0584530234336853, 0.057834040373563766, 0.05639391392469406, 0.0569007471203804, 0.05619857832789421, 0.056811414659023285, 0.05849357321858406, 0.05777246505022049, 0.05854630097746849, 0.058130402117967606, 0.05921369791030884, 0.057973992079496384, 0.056950826197862625, 0.05732911825180054, 0.05861744284629822, 0.05803878977894783, 0.056371189653873444, 0.05743749067187309, 0.057278990745544434, 0.05723913758993149, 0.05756398290395737, 0.05547834932804108, 0.059377264231443405, 0.05867912620306015, 0.05521328002214432, 0.0567319393157959, 0.0578092560172081, 0.056817468255758286, 0.05687083303928375, 0.0571284145116806, 0.05804147198796272, 0.05981440469622612, 0.05760527774691582, 0.056949205696582794, 0.059793729335069656, 0.05825824290513992, 0.057923682034015656, 0.05696580559015274, 0.05882904306054115, 0.05763973668217659, 0.058227427303791046, 0.05647101625800133, 0.058792173862457275]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:27 2016", "state": "available"}], "summary": "cad69539bc8a17bbf94ddd4b5ca1cec3"}