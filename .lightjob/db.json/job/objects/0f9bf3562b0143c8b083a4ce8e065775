{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 64, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 4, "nbg3": 4, "nbg2": 3, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.020313674824375492, 0.01590163746704215, 0.016775462577777987, 0.01203984486972219, 0.01450751246636471, 0.01167271923075229, 0.012715960763871239, 0.011000437253582367, 0.013519968271771722, 0.013904026649665347, 0.013838552760784105, 0.012438702847427159, 0.014534696807585151, 0.011142080620098515, 0.013016345948575467, 0.01249653306742932, 0.011391005730780366, 0.012273661415924548, 0.012274635599019017, 0.011432753238689307, 0.011457274431016717, 0.009710917438706869, 0.010630900199052967, 0.009068845461013515, 0.011313972008902945, 0.011231793789615713, 0.01184711129081679, 0.011016392586253853, 0.008572210528330097, 0.009131972756453428, 0.00864526706118648, 0.008403144731377505, 0.009147834208470802, 0.009443925009232967, 0.00993099939110329, 0.011390283746513727, 0.009425209583179259, 0.009542093476299127, 0.00779618522605608, 0.008855097050872082, 0.008572981113439447, 0.009648819559795048, 0.010484029556259978, 0.009373273061476967, 0.008618853236962078, 0.009163555257672528, 0.008366247039766394, 0.009638604826823039, 0.011100917663962638, 0.010443572306898734, 0.011069488836020188, 0.01060859513322628, 0.010269955977702765, 0.009914944087923436, 0.011549811018993744, 0.00928260952275003, 0.012151167134473872, 0.01098836716621349, 0.011145906794626391, 0.008377471793057539, 0.009081215726034379, 0.008453415446542219, 0.010000904821252422, 0.010079291842859727, 0.011083492883333653, 0.010756687547750603, 0.011525357960470287, 0.01128819634614427, 0.009848085739454063, 0.010231413023062042, 0.011531395052961673, 0.010357397920741893, 0.00923257945138495, 0.010978452287602775, 0.010974894215558145, 0.0111168910569498, 0.009464945812865829, 0.009992643681537914, 0.009584742208363334, 0.009253943020045586, 0.01129084431085721, 0.009155175861058866, 0.011368206380560367, 0.010905808462331247, 0.010406742101623949, 0.009103331351660766, 0.01047669602113854, 0.009357344604243298, 0.010237731109069463, 0.009999854598564346, 0.009526153480208492, 0.011330705718280225, 0.009505155052516373, 0.00979524622767101, 0.008811741397598865, 0.007640923653063931, 0.008960301099047496, 0.009197585277679146, 0.009305895516000673, 0.00855369313082067, 0.00929668911936975, 0.009422919864599033, 0.009526051755445552, 0.011334610773251914, 0.009622815934477613, 0.010578775515912171, 0.010120680432826828, 0.008876952177420553, 0.009610634587121391, 0.010155953494826632, 0.010219159047865417, 0.009495929657601843, 0.011341451277644517, 0.008660833437188603, 0.011949714724127888, 0.0102782619176295, 0.009149039095868935, 0.008954848697900593, 0.010140573055990872, 0.008855083371726705, 0.00943944953720047, 0.009150393577194278, 0.00977665453728871, 0.008603751946890066, 0.009065383023383819, 0.009349263948137603, 0.010023331805118905, 0.009165122480928996, 0.009795413059665059, 0.008060700075086935, 0.010423138389218623, 0.009615437555067807, 0.01038238845054751, 0.01029028407895675, 0.008816805014566524, 0.009813463153404823, 0.010334488754950868, 0.008891798811454903, 0.009615876319002317, 0.009367065234557739, 0.010342288722540859, 0.009914658622417848, 0.010712540260815615, 0.0077013174237979265, 0.00945092382610717, 0.008922997287767177, 0.010897508426148556, 0.008377681962663992, 0.009832397952528805, 0.010386529054310515, 0.00877230100461727, 0.00922121312382758, 0.00896270128161312, 0.008427215488123797, 0.010376197846652929, 0.009203835971500988, 0.009107079105747998, 0.008413527305175006, 0.010393619351189246, 0.00856208454912178, 0.010594910576119151, 0.009565464618471952, 0.008577848656888714, 0.008918247667785656, 0.009284980435025594, 0.009165905992068445, 0.008439775394944693, 0.008338538187323394], "moving_avg_accuracy_train": [0.056359696555463266, 0.11736793515250091, 0.1819321074572028, 0.24488740126228542, 0.3050483351105401, 0.36167603796553444, 0.41401985587671447, 0.46202651104670267, 0.5062273759996182, 0.5466942175513267, 0.583725780938313, 0.61720993690802, 0.6481498543294826, 0.6762909297123335, 0.7019899934640607, 0.7255119207453309, 0.7472257401199026, 0.7668355708236746, 0.7847260897379849, 0.8014109888679608, 0.8165413664254245, 0.8304284595378655, 0.8430638829235493, 0.8547332748718274, 0.8653775617026587, 0.8750317525146741, 0.8839902415073927, 0.8920972036234954, 0.8996606453481706, 0.9063329924158822, 0.9125984493458517, 0.9183116932470915, 0.9235281256665684, 0.9285344126869363, 0.9330120971219156, 0.9371559093562448, 0.9410550401814177, 0.944445747432425, 0.9477739685202087, 0.9507832462944337, 0.9536822945424359, 0.9562984134120665, 0.9586018031697431, 0.960809640484967, 0.9628385827960587, 0.9647669734724791, 0.9665349329693158, 0.9682353785105164, 0.969835497913064, 0.9711687566765749, 0.9725895426518206, 0.9737311383474174, 0.9748283289377403, 0.9759691521440031, 0.9769797251344293, 0.9780147988615272, 0.978881188951611, 0.9796795051743439, 0.9804235664117084, 0.9811885886753456, 0.9817514785792673, 0.982441802297568, 0.9830584072976007, 0.9835483197286011, 0.984138050440311, 0.9846455205439358, 0.9851603723574364, 0.9856237389895868, 0.9861012228275698, 0.9865425840258021, 0.9868839694839638, 0.987223732430824, 0.9875807444544451, 0.9878555522995137, 0.9881958853124564, 0.9884905592800572, 0.9887162743699548, 0.989012351805606, 0.9893067232834064, 0.9895971982015128, 0.9897517408802079, 0.989923345325548, 0.9901056550632499, 0.9902767453224288, 0.9903842596283181, 0.9905461266702852, 0.9906778200663796, 0.9908707128359506, 0.9910118363428686, 0.991131908101485, 0.9912492011818403, 0.9913896782351218, 0.9915323115271043, 0.9916351048529837, 0.9917462921343889, 0.9918718291781021, 0.9920476636329387, 0.9921222453339674, 0.9921661534256168, 0.9922636552330644, 0.9923630686526335, 0.992452504681427, 0.992542333751398, 0.9926255050631814, 0.9927305861783102, 0.9928065219426313, 0.9928818395769489, 0.9929822135799867, 0.9931097886124919, 0.9932292564393657, 0.9932995751025997, 0.993344260709034, 0.9934286195333872, 0.9935115179217335, 0.9935605498343405, 0.9936163042997345, 0.9936734227161987, 0.9937155647445973, 0.9937791052558795, 0.9938734940969859, 0.9939444571123057, 0.9939641820475313, 0.9940493277558919, 0.9940841422636638, 0.9941596170992206, 0.9942252193024123, 0.9943284391126657, 0.9943539076264176, 0.9943559029495086, 0.9943530123938527, 0.9944410916973337, 0.9945343139633239, 0.9945834449170284, 0.9946020140408202, 0.9946210153522236, 0.9946544646717908, 0.9947240244915256, 0.9947192350626296, 0.9947777396432991, 0.99479548048494, 0.9948183866400266, 0.9948530251700993, 0.9949143907328697, 0.9949650054905628, 0.9950058003284112, 0.9950588277729603, 0.9951275148611589, 0.99517770749649, 0.995194943033755, 0.9951848783803887, 0.9951409429602163, 0.9951362422653851, 0.9951948627555317, 0.9952336342549877, 0.9952731789021172, 0.9952808312500007, 0.9952575635262095, 0.9952970764438451, 0.9952884241935175, 0.9953039247051365, 0.9953131888191559, 0.9953192013729638, 0.9953432138618671, 0.9953229724233087, 0.9954117119738443, 0.9954357739978976, 0.9954620801171647, 0.9954439389947525], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 309679621, "moving_var_accuracy_train": [0.02858773856241509, 0.05922701129659125, 0.09082110127545327, 0.1174093123106664, 0.1382424227330464, 0.15327845103544357, 0.16260948339356818, 0.16709028551970276, 0.1679647051310054, 0.1659063220044446, 0.1616577199859595, 0.15558264629639648, 0.1486398880771792, 0.14090318038279098, 0.13275683924394993, 0.1244606848867832, 0.11625802596458863, 0.10809313251020516, 0.1001644552623943, 0.09265348246695213, 0.08544848914553956, 0.07863930242700805, 0.07221225750152659, 0.06621660412735678, 0.06061465129385418, 0.055392016766581995, 0.05057510581521775, 0.0461091007464633, 0.04201304152831999, 0.038212419314015984, 0.03474448093748612, 0.031563803246612995, 0.028652323426634423, 0.026012657271543716, 0.02359183846548265, 0.02138719523743478, 0.019385304704417624, 0.017550246294934166, 0.015894915165923264, 0.014386925423832938, 0.013023873208147846, 0.011783082588793395, 0.010652524769295985, 0.009631143202860844, 0.008705078344690404, 0.007868038725629509, 0.007109365980108651, 0.006424453017445085, 0.005805051154622257, 0.005240544249534338, 0.004734657519668001, 0.0042729209342910485, 0.003856463285585381, 0.0034825302553183743, 0.0031434685497073473, 0.0028387640933213673, 0.0025616433700829913, 0.002311214812198, 0.0020850759751027347, 0.0018818357091672068, 0.0016965037436459192, 0.0015311422908057632, 0.001381449877259776, 0.0012454650172442365, 0.001124048556330817, 0.0010139614338523927, 0.0009149509419759359, 0.0008253882255004562, 0.0007449013202902254, 0.0006721643856269485, 0.0006059968433436525, 0.0005464361087498192, 0.0004929396161399282, 0.00044432532869133665, 0.0004009352348594908, 0.00036162320609817606, 0.00032591941120462567, 0.00029411642671527957, 0.00026548467514623327, 0.00023969558873405015, 0.00021594098081648963, 0.00019461191550578546, 0.00017544985551935484, 0.00015816831685849247, 0.0001424555191063809, 0.00012844577564921915, 0.00011575728643947115, 0.00010451642638049887, 9.424402634029265e-05, 8.494937875121858e-05, 7.657825987639002e-05, 6.909803811123877e-05, 6.237133260395068e-05, 5.6229297554163864e-05, 5.071763130266404e-05, 4.5787704116495876e-05, 4.148719350441519e-05, 3.7388536025128765e-05, 3.3667033707226526e-05, 3.0385889758603724e-05, 2.7436248034657266e-05, 2.4764612460408702e-05, 2.236077457067453e-05, 2.0186954317541064e-05, 1.826763725259754e-05, 1.6492769690065007e-05, 1.4894547435411205e-05, 1.3495767156242686e-05, 1.2292668940886777e-05, 1.1191855101719506e-05, 1.0117172021138678e-05, 9.123426049826487e-06, 8.275131146060042e-06, 7.509467316567917e-06, 6.780157740996242e-06, 6.130119010598885e-06, 5.5464697310335575e-06, 5.007806312948125e-06, 4.543362250819397e-06, 4.169209305666139e-06, 3.7976101209891195e-06, 3.4213507665171258e-06, 3.1444638147354164e-06, 2.8409258828244263e-06, 2.6081013517629793e-06, 2.386024058159056e-06, 2.243310615401918e-06, 2.0248173605962967e-06, 1.8223714563648039e-06, 1.6402095085363232e-06, 1.5460102309981387e-06, 1.4696227257853578e-06, 1.344385108714067e-06, 1.2130499090682187e-06, 1.094994366676837e-06, 9.95564642824755e-07, 9.395552952362745e-07, 8.458062133749966e-07, 7.920306656713181e-07, 7.156602362633441e-07, 6.48816440104696e-07, 5.947332459845631e-07, 5.691515120332978e-07, 5.352930440969516e-07, 4.967417088430188e-07, 4.723747268373489e-07, 4.6759849892045295e-07, 4.4351235480176835e-07, 4.0183469302490856e-07, 3.625628989488663e-07, 3.4367949936559144e-07, 3.0951041821609267e-07, 3.094866331797436e-07, 2.920670323924129e-07, 2.7693434120254697e-07, 2.497679329354654e-07, 2.296636223757024e-07, 2.207486960788301e-07, 1.993475793925409e-07, 1.8157521419737063e-07, 1.6419010705471246e-07, 1.4809645357888072e-07, 1.3847620483098448e-07, 1.2831602686212018e-07, 1.8635679463908413e-07, 1.7293194418908185e-07, 1.618668569682186e-07, 1.4864207417279842e-07], "duration": 186435.260061, "accuracy_train": [0.5635969655546328, 0.6664420825258398, 0.7630096581995202, 0.8114850455080289, 0.846496739744832, 0.8713253636604835, 0.8851142170773348, 0.8940864075765966, 0.9040351605758582, 0.9108957915167036, 0.9170098514211886, 0.918567340635382, 0.9266091111226468, 0.9295606081579919, 0.9332815672296051, 0.9372092662767626, 0.9426501144910484, 0.9433240471576227, 0.9457407599667773, 0.9515750810377446, 0.9527147644425988, 0.9554122975498339, 0.9567826933947029, 0.9597578024063308, 0.9611761431801403, 0.9619194698228128, 0.9646166424418604, 0.9650598626684201, 0.9677316208702473, 0.9663841160252861, 0.9689875617155776, 0.9697308883582503, 0.9704760174418604, 0.9735909958702473, 0.9733112570367294, 0.9744502194652085, 0.9761472176079733, 0.9749621126914912, 0.9777279583102622, 0.9778667462624585, 0.9797737287744556, 0.9798434832387413, 0.9793323109888336, 0.9806801763219823, 0.9810990635958842, 0.9821224895602622, 0.9824465684408453, 0.9835393883813216, 0.9842365725359912, 0.9831680855481728, 0.985376616429033, 0.9840054996077889, 0.984703044250646, 0.9862365610003692, 0.9860748820482651, 0.987330462405408, 0.9866786997623662, 0.9868643511789406, 0.9871201175479882, 0.9880737890480805, 0.9868174877145626, 0.9886547157622739, 0.9886078522978959, 0.9879575316076044, 0.9894456268456996, 0.9892127514765596, 0.9897940386789406, 0.9897940386789406, 0.9903985773694168, 0.990514834809893, 0.9899564386074198, 0.9902815989525655, 0.9907938526670359, 0.9903288229051311, 0.9912588824289406, 0.9911426249884644, 0.990747710179033, 0.9916770487264673, 0.9919560665836102, 0.9922114724644703, 0.9911426249884644, 0.9914677853336102, 0.9917464427025655, 0.9918165576550388, 0.9913518883813216, 0.9920029300479882, 0.9918630606312293, 0.9926067477620893, 0.9922819479051311, 0.992212553929033, 0.9923048389050388, 0.9926539717146549, 0.9928160111549464, 0.9925602447858989, 0.9927469776670359, 0.9930016625715209, 0.9936301737264673, 0.9927934806432264, 0.9925613262504615, 0.9931411715000923, 0.993257789428756, 0.9932574289405685, 0.993350795381137, 0.9933740468692323, 0.9936763162144703, 0.9934899438215209, 0.9935596982858066, 0.9938855796073275, 0.9942579639050388, 0.9943044668812293, 0.9939324430717055, 0.9937464311669435, 0.9941878489525655, 0.9942576034168512, 0.9940018370478036, 0.9941180944882798, 0.994187488464378, 0.9940948430001846, 0.9943509698574198, 0.9947229936669435, 0.9945831242501846, 0.9941417064645626, 0.994815639131137, 0.9943974728336102, 0.9948388906192323, 0.994815639131137, 0.9952574174049464, 0.9945831242501846, 0.9943738608573275, 0.9943269973929494, 0.9952338054286637, 0.9953733143572352, 0.9950256235003692, 0.9947691361549464, 0.9947920271548542, 0.9949555085478959, 0.99535006286914, 0.9946761302025655, 0.9953042808693245, 0.9949551480597084, 0.9950245420358066, 0.995164771940753, 0.9954666807978036, 0.9954205383098007, 0.9953729538690477, 0.9955360747739018, 0.9957456986549464, 0.9956294412144703, 0.99535006286914, 0.9950942965000923, 0.9947455241786637, 0.9950939360119048, 0.9957224471668512, 0.9955825777500923, 0.9956290807262828, 0.9953497023809523, 0.9950481540120893, 0.9956526927025655, 0.9952105539405685, 0.9954434293097084, 0.9953965658453304, 0.9953733143572352, 0.9955593262619971, 0.9951407994762828, 0.9962103679286637, 0.995652332214378, 0.9956988351905685, 0.9952806688930418], "end": "2016-01-25 16:39:59.712000", "learning_rate_per_epoch": [0.005430521909147501, 0.0027152609545737505, 0.0018101739697158337, 0.0013576304772868752, 0.001086104428395629, 0.0009050869848579168, 0.0007757888524793088, 0.0006788152386434376, 0.0006033913232386112, 0.0005430522141978145, 0.0004936837940476835, 0.0004525434924289584, 0.00041773245902732015, 0.0003878944262396544, 0.0003620347997639328, 0.0003394076193217188, 0.00031944247893989086, 0.0003016956616193056, 0.0002858169609680772, 0.00027152610709890723, 0.00025859629386104643, 0.00024684189702384174, 0.000236109655816108, 0.0002262717462144792, 0.00021722087694797665, 0.00020886622951366007, 0.00020113044593017548, 0.0001939472131198272, 0.00018725938571151346, 0.0001810173998819664, 0.0001751781237544492, 0.0001697038096608594, 0.0001645612792344764, 0.00015972123946994543, 0.00015515777340624481, 0.0001508478308096528, 0.00014677086437586695, 0.0001429084804840386, 0.0001392441481584683, 0.00013576305354945362, 0.0001324517506873235, 0.00012929814693052322, 0.00012629121192730963, 0.00012342094851192087, 0.00012067826901329681, 0.000118054827908054, 0.00011554302182048559, 0.0001131358731072396, 0.00011082697892561555, 0.00010861043847398832, 0.00010648082388797775, 0.00010443311475683004, 0.00010246267629554495, 0.00010056522296508774, 9.873676026472822e-05, 9.69736065599136e-05, 9.527231304673478e-05, 9.362969285575673e-05, 9.204274829244241e-05, 9.05086999409832e-05, 8.902495028451085e-05, 8.75890618772246e-05, 8.619876462034881e-05, 8.48519048304297e-05, 8.354648889508098e-05, 8.22806396172382e-05, 8.105256711132824e-05, 7.986061973497272e-05, 7.870321860536933e-05, 7.757888670312241e-05, 7.648622704437003e-05, 7.54239154048264e-05, 7.439071487169713e-05, 7.338543218793347e-05, 7.240696140797809e-05, 7.14542402420193e-05, 7.052625733194873e-05, 6.962207407923415e-05, 6.874078826513141e-05, 6.788152677472681e-05, 6.704348197672516e-05, 6.622587534366176e-05, 6.542797927977517e-05, 6.464907346526161e-05, 6.388849578797817e-05, 6.314560596365482e-05, 6.241979281185195e-05, 6.171047425596043e-05, 6.1017100961180404e-05, 6.0339134506648406e-05, 5.967606557533145e-05, 5.9027413954027e-05, 5.8392710343468934e-05, 5.7771510910242796e-05, 5.716339001082815e-05, 5.65679365536198e-05, 5.5984761274885386e-05, 5.5413489462807775e-05, 5.485375731950626e-05, 5.430521923699416e-05, 5.376754415920004e-05, 5.3240411943988875e-05, 5.2723513363162056e-05, 5.221655737841502e-05, 5.1719256589422e-05, 5.1231338147772476e-05, 5.0752543756971136e-05, 5.028261148254387e-05, 4.982130121788941e-05, 4.936838013236411e-05, 4.892362267128192e-05, 4.84868032799568e-05, 4.8057718231575564e-05, 4.763615652336739e-05, 4.7221928980434313e-05, 4.6814846427878365e-05, 4.6414719690801576e-05, 4.6021374146221206e-05, 4.563463880913332e-05, 4.52543499704916e-05, 4.488034755922854e-05, 4.4512475142255425e-05, 4.415058356244117e-05, 4.37945309386123e-05, 4.344417538959533e-05, 4.3099382310174406e-05, 4.276001709513366e-05, 4.242595241521485e-05, 4.209706821711734e-05, 4.177324444754049e-05, 4.145436469116248e-05, 4.11403198086191e-05, 4.083099338458851e-05, 4.052628355566412e-05, 4.0226088458439335e-05, 3.993030986748636e-05, 3.963884591939859e-05, 3.9351609302684665e-05, 3.906850179191679e-05, 3.8789443351561204e-05, 3.851433939416893e-05, 3.824311352218501e-05, 3.797567842411809e-05, 3.77119577024132e-05, 3.745187495951541e-05, 3.7195357435848564e-05, 3.6942325095878914e-05, 3.669271609396674e-05, 3.644645767053589e-05, 3.6203480703989044e-05, 3.596372334868647e-05, 3.572712012100965e-05, 3.549360917531885e-05, 3.5263128665974364e-05, 3.5035624023294076e-05, 3.4811037039617077e-05, 3.458931314526126e-05, 3.437039413256571e-05, 3.415422543184832e-05, 3.3940763387363404e-05, 3.3729949791450053e-05, 3.352174098836258e-05, 3.3316086046397686e-05, 3.311293767183088e-05, 3.291225584689528e-05, 3.2713989639887586e-05, 3.251809539506212e-05, 3.2324536732630804e-05], "accuracy_valid": [0.5475059417356928, 0.6517892860504518, 0.7425316500376506, 0.7879241575677711, 0.8141192700489458, 0.835594820689006, 0.844628023814006, 0.8516066217996988, 0.8544348291603916, 0.8605795251317772, 0.8610472162085843, 0.8627664956701807, 0.8644754800451807, 0.8673242775790663, 0.8686567559299698, 0.8712408226656627, 0.8695524284638554, 0.874424945877259, 0.8745779014495482, 0.8749029320406627, 0.8757883094879518, 0.8791047980986446, 0.8758794945406627, 0.8771413780120482, 0.8772531532379518, 0.8767648719879518, 0.8788606574736446, 0.879359233810241, 0.8787179969879518, 0.877039897872741, 0.8784944465361446, 0.8789827277861446, 0.880091655685241, 0.8815256141754518, 0.8819330054593373, 0.8818212302334337, 0.8824418768825302, 0.8815667945218373, 0.880946147872741, 0.8799387001129518, 0.8824212867093373, 0.8845170721950302, 0.8815359092620482, 0.881190288497741, 0.8826448371611446, 0.8816682746611446, 0.881434429122741, 0.8819330054593373, 0.882410991622741, 0.8798269248870482, 0.8820653708584337, 0.882777202560241, 0.8798166298004518, 0.883753765060241, 0.8844964820218373, 0.8840082007718373, 0.884486186935241, 0.882655132247741, 0.8830419333584337, 0.8854833396084337, 0.8815667945218373, 0.8843538215361446, 0.8832757788968373, 0.8826860175075302, 0.8845067771084337, 0.8835199195218373, 0.8821874411709337, 0.8835405096950302, 0.8857377753200302, 0.8856054099209337, 0.8842729315700302, 0.8846288474209337, 0.8854936346950302, 0.8847509177334337, 0.8855039297816265, 0.8861142813441265, 0.8847612128200302, 0.8847406226468373, 0.8845979621611446, 0.883265483810241, 0.8830007530120482, 0.8837434699736446, 0.8838449501129518, 0.8838964255459337, 0.8845067771084337, 0.8852391989834337, 0.884119975997741, 0.8865922675075302, 0.8822683311370482, 0.8831537085843373, 0.8841302710843373, 0.8864599021084337, 0.8846288474209337, 0.8855951148343373, 0.8843744117093373, 0.8853509742093373, 0.8870702536709337, 0.8859613257718373, 0.8846185523343373, 0.8853509742093373, 0.8851171286709337, 0.8831537085843373, 0.884730327560241, 0.8838964255459337, 0.8841405661709337, 0.8841405661709337, 0.884730327560241, 0.8844964820218373, 0.8837743552334337, 0.8849847632718373, 0.884608257247741, 0.8834993293486446, 0.8851171286709337, 0.884730327560241, 0.8847200324736446, 0.8858495505459337, 0.8863275367093373, 0.8877011954066265, 0.885584819747741, 0.8876806052334337, 0.8843538215361446, 0.8865819724209337, 0.885706890060241, 0.8858392554593373, 0.8855745246611446, 0.8849538780120482, 0.8844655967620482, 0.8845979621611446, 0.884364116622741, 0.8847509177334337, 0.8836111045745482, 0.8838552451995482, 0.8862054663968373, 0.8845876670745482, 0.886195171310241, 0.8847200324736446, 0.8854524543486446, 0.8849538780120482, 0.8843332313629518, 0.884608257247741, 0.8865716773343373, 0.8856965949736446, 0.8864290168486446, 0.8867040427334337, 0.8865716773343373, 0.886073100997741, 0.8858495505459337, 0.8860628059111446, 0.8861848762236446, 0.8858392554593373, 0.8867040427334337, 0.8857274802334337, 0.8847200324736446, 0.886439311935241, 0.885462749435241, 0.8856965949736446, 0.886439311935241, 0.885706890060241, 0.8853303840361446, 0.8858392554593373, 0.8859613257718373, 0.8870805487575302, 0.8849847632718373, 0.886561382247741, 0.886561382247741, 0.8860628059111446, 0.885218608810241, 0.8865716773343373], "accuracy_test": 0.8700872927295918, "start": "2016-01-23 12:52:44.452000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0], "accuracy_train_last": 0.9952806688930418, "batch_size_eval": 1024, "accuracy_train_std": [0.016606114318205835, 0.016814087211531128, 0.018735400657441486, 0.016410353279682814, 0.017500257773967222, 0.01642897456102769, 0.014640520544571305, 0.013988005447233307, 0.013825890680535306, 0.012804141082838496, 0.01224261132562622, 0.012045445862471381, 0.010738766740646945, 0.011255971828446776, 0.011393714056018415, 0.010541107477538607, 0.01001941681782436, 0.009714723964652994, 0.010085637912709333, 0.008935171442498767, 0.007996550075852442, 0.008579539063013444, 0.008877561566891973, 0.00878252655776366, 0.008607272973114389, 0.007397848593498786, 0.00780460981260889, 0.00781809520468142, 0.007059459017053128, 0.007747636145335943, 0.007390262309332688, 0.007193050477021602, 0.00703528973044856, 0.006516865841697745, 0.006594476016459093, 0.0069279405850708785, 0.0062966535383601795, 0.006559490758412103, 0.006674901540368417, 0.0061603589576790675, 0.00558160154998607, 0.005608095097813185, 0.0064516683038433244, 0.0061144867558041984, 0.005703885291988196, 0.005854653264417993, 0.005516492197898881, 0.00540006974461667, 0.0062204452189680995, 0.0055949325351961204, 0.005279964457277126, 0.005337023842169726, 0.004769201404871354, 0.0044710567908685505, 0.004859943172385763, 0.004944654568511035, 0.00536625845024106, 0.0051830253377024744, 0.005526580680483213, 0.0046625851998225104, 0.004977686425217126, 0.003964976013200109, 0.004087650507006602, 0.004335229834050123, 0.004204697023671313, 0.004198425192354748, 0.0038082738424088606, 0.004316889894727758, 0.00404914555164001, 0.004242812887644516, 0.004062882881123215, 0.004143956849124139, 0.003704772741418926, 0.004170132199038032, 0.004176029333798709, 0.003754862581547322, 0.0038782503230804137, 0.0033175522054946726, 0.0033076688674194337, 0.003607758617245738, 0.003712290609246822, 0.0036613369286298673, 0.003542525512064434, 0.003315550591906314, 0.0035698889486024442, 0.003233641714978317, 0.0035961871029716714, 0.003184124841071568, 0.003012271478134656, 0.0030162538078870047, 0.003097639090042377, 0.002946769715035449, 0.003013179675685235, 0.0031033085992048914, 0.0032745146663824658, 0.0032036246365703866, 0.0029317291557844154, 0.0030430977764499107, 0.0031447792374794826, 0.002847671600098127, 0.00299029010545951, 0.0029596502591699835, 0.0028774932850453724, 0.002824103119859117, 0.002821631935780832, 0.002973836001506681, 0.002531636967171047, 0.0028680435724950013, 0.0026972351147470292, 0.0027777926754305295, 0.0025620909482884265, 0.0026878026397435468, 0.0029064476089822313, 0.0023369698863082594, 0.0026241877940526986, 0.002683337652557117, 0.002566226739225519, 0.002851259436660448, 0.0027746587809453827, 0.0027787973089602523, 0.0024783371046209553, 0.0026955774857999365, 0.0027000224548300853, 0.002476481651207128, 0.002524407997222777, 0.0023200803054580204, 0.002230964468137878, 0.002441414047923238, 0.002428487264240957, 0.002432529201253643, 0.002422707074740835, 0.002421347023244287, 0.0024726540158811028, 0.0025292419309988624, 0.0025099064289568454, 0.002381414741245727, 0.0025111183827004626, 0.0024413865820898015, 0.002422112942580878, 0.002284815517475769, 0.0026174237289744947, 0.0023432142864635956, 0.0024745869439142163, 0.0024799619537003697, 0.0024115503690163177, 0.0022267301652263627, 0.002223159594493378, 0.0021015640788575388, 0.00221312236188373, 0.002310330833706764, 0.002720763215514688, 0.002507821474968664, 0.0022549186785510186, 0.0025059125229475406, 0.0023077889263777695, 0.0023997518317411953, 0.002715932078108276, 0.0024410981721810394, 0.0022437135198971957, 0.0020626803650418205, 0.0024698667897084343, 0.0026539896451965067, 0.002796790405566243, 0.0025352531506919137, 0.002186067699556387, 0.0022068349378651336, 0.0021320665787562094, 0.0020097693731871125], "accuracy_test_std": 0.009985968092340423, "error_valid": [0.4524940582643072, 0.34821071394954817, 0.25746834996234935, 0.21207584243222888, 0.1858807299510542, 0.16440517931099397, 0.15537197618599397, 0.14839337820030118, 0.1455651708396084, 0.13942047486822284, 0.13895278379141573, 0.1372335043298193, 0.1355245199548193, 0.13267572242093373, 0.13134324407003017, 0.12875917733433728, 0.1304475715361446, 0.12557505412274095, 0.12542209855045183, 0.12509706795933728, 0.12421169051204817, 0.12089520190135539, 0.12412050545933728, 0.12285862198795183, 0.12274684676204817, 0.12323512801204817, 0.12113934252635539, 0.12064076618975905, 0.12128200301204817, 0.12296010212725905, 0.12150555346385539, 0.12101727221385539, 0.11990834431475905, 0.11847438582454817, 0.11806699454066272, 0.11817876976656627, 0.11755812311746983, 0.11843320547816272, 0.11905385212725905, 0.12006129988704817, 0.11757871329066272, 0.11548292780496983, 0.11846409073795183, 0.11880971150225905, 0.11735516283885539, 0.11833172533885539, 0.11856557087725905, 0.11806699454066272, 0.11758900837725905, 0.12017307511295183, 0.11793462914156627, 0.11722279743975905, 0.12018337019954817, 0.11624623493975905, 0.11550351797816272, 0.11599179922816272, 0.11551381306475905, 0.11734486775225905, 0.11695806664156627, 0.11451666039156627, 0.11843320547816272, 0.11564617846385539, 0.11672422110316272, 0.11731398249246983, 0.11549322289156627, 0.11648008047816272, 0.11781255882906627, 0.11645949030496983, 0.11426222467996983, 0.11439459007906627, 0.11572706842996983, 0.11537115257906627, 0.11450636530496983, 0.11524908226656627, 0.11449607021837349, 0.11388571865587349, 0.11523878717996983, 0.11525937735316272, 0.11540203783885539, 0.11673451618975905, 0.11699924698795183, 0.11625653002635539, 0.11615504988704817, 0.11610357445406627, 0.11549322289156627, 0.11476080101656627, 0.11588002400225905, 0.11340773249246983, 0.11773166886295183, 0.11684629141566272, 0.11586972891566272, 0.11354009789156627, 0.11537115257906627, 0.11440488516566272, 0.11562558829066272, 0.11464902579066272, 0.11292974632906627, 0.11403867422816272, 0.11538144766566272, 0.11464902579066272, 0.11488287132906627, 0.11684629141566272, 0.11526967243975905, 0.11610357445406627, 0.11585943382906627, 0.11585943382906627, 0.11526967243975905, 0.11550351797816272, 0.11622564476656627, 0.11501523672816272, 0.11539174275225905, 0.11650067065135539, 0.11488287132906627, 0.11526967243975905, 0.11527996752635539, 0.11415044945406627, 0.11367246329066272, 0.11229880459337349, 0.11441518025225905, 0.11231939476656627, 0.11564617846385539, 0.11341802757906627, 0.11429310993975905, 0.11416074454066272, 0.11442547533885539, 0.11504612198795183, 0.11553440323795183, 0.11540203783885539, 0.11563588337725905, 0.11524908226656627, 0.11638889542545183, 0.11614475480045183, 0.11379453360316272, 0.11541233292545183, 0.11380482868975905, 0.11527996752635539, 0.11454754565135539, 0.11504612198795183, 0.11566676863704817, 0.11539174275225905, 0.11342832266566272, 0.11430340502635539, 0.11357098315135539, 0.11329595726656627, 0.11342832266566272, 0.11392689900225905, 0.11415044945406627, 0.11393719408885539, 0.11381512377635539, 0.11416074454066272, 0.11329595726656627, 0.11427251976656627, 0.11527996752635539, 0.11356068806475905, 0.11453725056475905, 0.11430340502635539, 0.11356068806475905, 0.11429310993975905, 0.11466961596385539, 0.11416074454066272, 0.11403867422816272, 0.11291945124246983, 0.11501523672816272, 0.11343861775225905, 0.11343861775225905, 0.11393719408885539, 0.11478139118975905, 0.11342832266566272], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5958467246946813, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0054305219931021775, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "adam", "nb_data_augmentation": 3, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 3.640764543273196e-10, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.03688046946614988}, "accuracy_valid_max": 0.8877011954066265, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8865716773343373, "loss_train": [1.5139110088348389, 1.0114974975585938, 0.7363462448120117, 0.6036742329597473, 0.5248228311538696, 0.46714821457862854, 0.42524129152297974, 0.3912375569343567, 0.3623048663139343, 0.33880600333213806, 0.3187454640865326, 0.30175575613975525, 0.28640663623809814, 0.27161547541618347, 0.25955599546432495, 0.2494213581085205, 0.2390514463186264, 0.2288089394569397, 0.22069904208183289, 0.21168772876262665, 0.20524176955223083, 0.19661350548267365, 0.19098371267318726, 0.18394434452056885, 0.17851941287517548, 0.17339327931404114, 0.1684473752975464, 0.16372182965278625, 0.1583152860403061, 0.15357069671154022, 0.150618314743042, 0.14576198160648346, 0.14128370583057404, 0.13851192593574524, 0.13445501029491425, 0.13051024079322815, 0.1266864836215973, 0.12514552474021912, 0.12094925343990326, 0.11961273103952408, 0.11741340160369873, 0.11310981214046478, 0.11080709099769592, 0.10836966335773468, 0.10520191490650177, 0.10272911936044693, 0.10145357996225357, 0.09741644561290741, 0.0973086729645729, 0.09405481815338135, 0.09337504953145981, 0.09035579860210419, 0.08960363268852234, 0.08726909011602402, 0.08557005226612091, 0.08362795412540436, 0.08289800584316254, 0.0811886340379715, 0.07900869846343994, 0.07743208855390549, 0.0761728584766388, 0.07554276287555695, 0.07422029972076416, 0.0717640370130539, 0.0706770047545433, 0.06917117536067963, 0.06852052360773087, 0.06779396533966064, 0.06589748710393906, 0.06493309140205383, 0.06426630914211273, 0.062186554074287415, 0.061336591839790344, 0.06093362346291542, 0.06072905287146568, 0.059266019612550735, 0.058039139956235886, 0.05646541714668274, 0.05729266256093979, 0.05557244271039963, 0.05428595840930939, 0.05404190346598625, 0.05306203290820122, 0.05176166817545891, 0.051420800387859344, 0.050890177488327026, 0.05059419199824333, 0.04956863448023796, 0.048369500786066055, 0.04744658246636391, 0.047076161950826645, 0.04743161424994469, 0.04640428349375725, 0.04503637179732323, 0.043987590819597244, 0.04314038157463074, 0.04349895194172859, 0.04231288656592369, 0.04146850109100342, 0.04225844889879227, 0.04070547968149185, 0.041116222739219666, 0.03982563316822052, 0.039467401802539825, 0.03906366229057312, 0.038544077426195145, 0.038463566452264786, 0.037300724536180496, 0.03734691068530083, 0.036927103996276855, 0.036838598549366, 0.03548523038625717, 0.0348941832780838, 0.034850139170885086, 0.035009805113077164, 0.033928509801626205, 0.03386412933468819, 0.032930951565504074, 0.03330957517027855, 0.03290519863367081, 0.03252775967121124, 0.031805619597435, 0.03176518902182579, 0.031234532594680786, 0.030498504638671875, 0.029765037819743156, 0.03078361414372921, 0.03041311912238598, 0.030010981485247612, 0.029316522181034088, 0.02870260179042816, 0.028151066973805428, 0.02836819924414158, 0.027949266135692596, 0.026930592954158783, 0.026629013940691948, 0.02641330473124981, 0.027856167405843735, 0.02654382213950157, 0.02653205208480358, 0.026426415890455246, 0.02599184773862362, 0.02536897361278534, 0.025695066899061203, 0.025831669569015503, 0.025004548951983452, 0.02442849799990654, 0.024418771266937256, 0.02415616065263748, 0.024894198402762413, 0.023948591202497482, 0.0232477318495512, 0.023599809035658836, 0.022706422954797745, 0.02334683947265148, 0.023467136546969414, 0.02261766418814659, 0.02193511091172695, 0.022143559530377388, 0.022038625553250313, 0.020848605781793594, 0.021822353824973106, 0.020959140732884407, 0.02093016915023327, 0.020975390449166298, 0.020539190620183945, 0.020760735496878624, 0.02066946029663086], "accuracy_train_first": 0.5635969655546328, "model": "residualv5", "loss_std": [0.2766004800796509, 0.15919768810272217, 0.13253505527973175, 0.12106966227293015, 0.11309582740068436, 0.10742175579071045, 0.10088692605495453, 0.09816356003284454, 0.0946909487247467, 0.08994362503290176, 0.08741931617259979, 0.08441178500652313, 0.0826844796538353, 0.07852144539356232, 0.07707201689481735, 0.07512133568525314, 0.07318249344825745, 0.06902958452701569, 0.0680399239063263, 0.06781169027090073, 0.06643873453140259, 0.06329769641160965, 0.06396221369504929, 0.06212431937456131, 0.05928737670183182, 0.05817187577486038, 0.05733275040984154, 0.05607524886727333, 0.05347758159041405, 0.053662676364183426, 0.05262532830238342, 0.050381459295749664, 0.05057181790471077, 0.048850834369659424, 0.04845048487186432, 0.046777334064245224, 0.046362899243831635, 0.044708266854286194, 0.04481217637658119, 0.04396127536892891, 0.04454605653882027, 0.04162067174911499, 0.041831593960523605, 0.03936820849776268, 0.039389315992593765, 0.039180491119623184, 0.03856457769870758, 0.038091011345386505, 0.03681756183505058, 0.0362381674349308, 0.036214105784893036, 0.03522700443863869, 0.035739313811063766, 0.0335150882601738, 0.03536847606301308, 0.0334751233458519, 0.03419160470366478, 0.03321751207113266, 0.03229633346199989, 0.031519509851932526, 0.030843565240502357, 0.03188816085457802, 0.030820637941360474, 0.029283490031957626, 0.030200842767953873, 0.02834830991923809, 0.02875092439353466, 0.02944938838481903, 0.028555752709507942, 0.02687603235244751, 0.028311623260378838, 0.02684476226568222, 0.026165256276726723, 0.02717812918126583, 0.02655547298491001, 0.024915382266044617, 0.025440111756324768, 0.024971166625618935, 0.024895938113331795, 0.023796193301677704, 0.02256505750119686, 0.023676540702581406, 0.02420959249138832, 0.023765748366713524, 0.02261761575937271, 0.022659003734588623, 0.02177411876618862, 0.022766390815377235, 0.021996809169650078, 0.022156642749905586, 0.021543240174651146, 0.023355543613433838, 0.02097960188984871, 0.020746251568198204, 0.020397793501615524, 0.020803162828087807, 0.020204978063702583, 0.01996035873889923, 0.019255787134170532, 0.019899090752005577, 0.0192599855363369, 0.019367052242159843, 0.01803293451666832, 0.01852409355342388, 0.01993970386683941, 0.01887446828186512, 0.017505045980215073, 0.018531043082475662, 0.017728695645928383, 0.017553774639964104, 0.018162734806537628, 0.01786469668149948, 0.01708134450018406, 0.01646615006029606, 0.017489567399024963, 0.01744239032268524, 0.016733257099986076, 0.01649784855544567, 0.015881730243563652, 0.016440678387880325, 0.016300322487950325, 0.016558581963181496, 0.01703193224966526, 0.015349292196333408, 0.01604304648935795, 0.01471585314720869, 0.015756186097860336, 0.014775509014725685, 0.014766477979719639, 0.014922574162483215, 0.013922079466283321, 0.014229267835617065, 0.014255846850574017, 0.01427739392966032, 0.014020890928804874, 0.013455823995172977, 0.013614186085760593, 0.013971323147416115, 0.014203405007719994, 0.013611401431262493, 0.013631411828100681, 0.01352311298251152, 0.013228760100901127, 0.01389169879257679, 0.012795032002031803, 0.013442205265164375, 0.01286169420927763, 0.013162786141037941, 0.012104000896215439, 0.013888368383049965, 0.012419400736689568, 0.01279347576200962, 0.012008996680378914, 0.01216090563684702, 0.012708840891718864, 0.012764421291649342, 0.012671125121414661, 0.012000752612948418, 0.011129454709589481, 0.012125625275075436, 0.012013650499284267, 0.011744637973606586, 0.01077613327652216, 0.01192912831902504, 0.011530706658959389, 0.011035415343940258, 0.01232826616615057, 0.011632540263235569]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:13 2016", "state": "available"}], "summary": "b4d7054cda4169af3415326083413654"}