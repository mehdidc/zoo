{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 16, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.7609909772872925, 1.4229941368103027, 1.213188648223877, 1.0872108936309814, 0.9822142124176025, 0.8931230306625366, 0.8247430324554443, 0.7681758403778076, 0.7228203415870667, 0.6865956783294678, 0.6519511938095093, 0.6246980428695679, 0.5974196195602417, 0.5772339105606079, 0.5548854470252991, 0.5346314907073975, 0.5174265503883362, 0.49914050102233887, 0.4857513904571533, 0.47056716680526733, 0.45846953988075256, 0.4464702010154724, 0.43106868863105774, 0.41841936111450195, 0.40820708870887756, 0.3955659866333008, 0.3867068588733673, 0.379423052072525, 0.36948660016059875, 0.36025771498680115, 0.35259994864463806, 0.3444378077983856, 0.33559250831604004, 0.3285467326641083, 0.32102271914482117, 0.31562474370002747, 0.3099871277809143, 0.30229616165161133, 0.29720064997673035, 0.292014479637146, 0.2883016765117645, 0.2826143503189087, 0.27609094977378845, 0.2709185779094696, 0.2681654095649719, 0.264139860868454, 0.26003697514533997, 0.25404441356658936, 0.25038981437683105, 0.24628891050815582, 0.24421590566635132, 0.24001212418079376, 0.23840047419071198, 0.23554782569408417, 0.23055921494960785, 0.23034484684467316, 0.22802141308784485, 0.2246525138616562, 0.2236490547657013, 0.2200801819562912, 0.21716727316379547, 0.21546368300914764, 0.2146582156419754, 0.210447296500206, 0.20884829759597778, 0.20593449473381042, 0.20648080110549927, 0.20366914570331573, 0.19945715367794037, 0.19942723214626312, 0.19739528000354767, 0.1947961449623108, 0.19631052017211914, 0.19478781521320343, 0.19138316810131073, 0.19049793481826782, 0.18957272171974182, 0.1864161193370819, 0.18838383257389069, 0.18512305617332458, 0.1841278076171875, 0.18299603462219238, 0.18043701350688934, 0.18127067387104034, 0.18068434298038483, 0.1785905510187149, 0.1764867901802063, 0.1796892285346985, 0.17377769947052002, 0.17406290769577026, 0.17416790127754211, 0.17480237782001495, 0.17329975962638855, 0.17067210376262665, 0.16927042603492737, 0.16981711983680725, 0.16868174076080322, 0.16820329427719116, 0.16735170781612396, 0.16780686378479004, 0.16604401171207428, 0.16327157616615295, 0.16382017731666565, 0.16287784278392792, 0.16434288024902344, 0.1638544797897339, 0.1615888923406601, 0.15927258133888245, 0.16098441183567047, 0.16092051565647125, 0.1580367237329483, 0.15847398340702057, 0.15920230746269226, 0.15710066258907318, 0.1573031097650528, 0.15544085204601288, 0.1545371115207672, 0.15421903133392334, 0.1529913991689682, 0.15300576388835907, 0.15267814695835114, 0.15256108343601227, 0.15204696357250214, 0.15258900821208954, 0.15219148993492126, 0.14941468834877014, 0.15148411691188812, 0.15038691461086273, 0.1474197655916214, 0.1472107470035553, 0.14676451683044434, 0.1470409482717514, 0.14728444814682007, 0.1472960263490677, 0.14574679732322693, 0.14484210312366486, 0.14556698501110077, 0.14392001926898956, 0.14399011433124542, 0.14392748475074768, 0.1430814266204834, 0.14344817399978638, 0.1414540410041809, 0.1427294909954071, 0.14257271587848663, 0.14196012914180756, 0.13961772620677948, 0.1401468813419342, 0.14094172418117523, 0.14012561738491058, 0.13942186534404755, 0.1382545530796051, 0.13871242105960846, 0.1380067616701126, 0.13756157457828522, 0.13783423602581024, 0.13614122569561005, 0.1364978849887848, 0.13607195019721985, 0.13569621741771698, 0.13737864792346954, 0.1346675604581833, 0.13485188782215118, 0.13500794768333435, 0.13502928614616394, 0.13371510803699493, 0.135078564286232], "moving_avg_accuracy_train": [0.04558018050364525, 0.0914511253720238, 0.14305203953401852, 0.19363301504028813, 0.24132024604940988, 0.2890690072798786, 0.3347121632206338, 0.3787174286492164, 0.4205007959206458, 0.45995148993623164, 0.49681678587154904, 0.5302374758359815, 0.5616179277955727, 0.5907694316949099, 0.6178868724078479, 0.6439480029042355, 0.6671075822568997, 0.6887301645743068, 0.708929849932583, 0.7274837354692713, 0.7443168747903305, 0.7601154527459596, 0.7751316628988738, 0.7887066338079067, 0.801324105318912, 0.8131029707133313, 0.8241572093908908, 0.8341478408304469, 0.8432905077498477, 0.8521165072701748, 0.8597481927027182, 0.8672888217932344, 0.8740079226104042, 0.880368972386324, 0.8864449425572523, 0.8920714979277728, 0.8973538896516992, 0.9016360369948995, 0.9055573268216184, 0.909698037851389, 0.913171164460307, 0.9161389764357417, 0.9195795593719479, 0.9232503236728483, 0.926365710538906, 0.9288999856517283, 0.9312969825472882, 0.9337403091056823, 0.9360578134998853, 0.9381830949844298, 0.9397959762217287, 0.9415009384578984, 0.9439655000430793, 0.9456511824411801, 0.9473472609601665, 0.9488039771629686, 0.9502591809716809, 0.9515549856043024, 0.9528932707855665, 0.953830335335609, 0.9550132372544752, 0.9559382679552458, 0.9568986066728258, 0.9578141008412759, 0.9589589521774141, 0.959775440738281, 0.9605474107263762, 0.9613305393704237, 0.9620795690262662, 0.9626885194522202, 0.96351795389035, 0.9645202112537145, 0.965229255529552, 0.9656186044551867, 0.9663595713906203, 0.9670916539456336, 0.9675551796963268, 0.9684164202457509, 0.9690241620747657, 0.9694525471315933, 0.9699892283553573, 0.970072424007963, 0.9703006157214616, 0.9710546873338394, 0.9709963517099978, 0.9716134564568644, 0.9717898875219106, 0.9721625531221099, 0.9726304856444319, 0.9732935124883497, 0.9737622813657145, 0.9742934553493904, 0.9745762354835175, 0.9747051795685175, 0.9752955596021605, 0.9756246136860105, 0.9761067021686, 0.9762174942648629, 0.9764054907086239, 0.976658501011608, 0.9770116601735517, 0.9775132622728909, 0.9778809267075158, 0.9782187640962879, 0.9784717005211921, 0.9786481900297964, 0.9786140432363498, 0.9790321008913047, 0.97953154961885, 0.9796555686891264, 0.9797973406892614, 0.9800830456084305, 0.9804587626249683, 0.9803854286982042, 0.980454286795069, 0.9805743878024853, 0.9810195531889034, 0.9812923549009747, 0.981600655459696, 0.981859524772069, 0.9819762857615472, 0.9823417152211068, 0.9824776143835199, 0.9825673715463583, 0.9829295320476841, 0.9831647956953058, 0.9832765876281746, 0.9835701156213095, 0.9840133272734642, 0.983998377321127, 0.9838244870961664, 0.9839819170318063, 0.9841305794203107, 0.9842852298116129, 0.9842873395304794, 0.9845100192679076, 0.9847732821470876, 0.9849218270347689, 0.985227578445587, 0.9850749273343709, 0.9852398467283332, 0.9853509997043094, 0.9856440607826972, 0.9853358651949221, 0.98531894188141, 0.9854292328861446, 0.9857052060999295, 0.985904753867336, 0.9860680347675164, 0.9859802196455543, 0.9859801689976748, 0.9859940382586216, 0.9862158200351496, 0.9863736070042722, 0.9865807194431492, 0.9866810540833674, 0.9868108467405068, 0.9868997943950367, 0.9869310191591137, 0.9868475503527446, 0.9866235828543842, 0.9865684964808598, 0.986786310857783, 0.9868986384398711, 0.9869207142530453, 0.9868987298063305, 0.9868068281423734], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04610286850527107, 0.0922422110316265, 0.14445244846573793, 0.19537262036426956, 0.24307256888883658, 0.29055074474656434, 0.3356463256277061, 0.3791470345088361, 0.4199100301355127, 0.4577441871370217, 0.4930187200893587, 0.5250374429599409, 0.5547342293021246, 0.5822547940413398, 0.6076886590460915, 0.6320429517917082, 0.6533392566690133, 0.6731905543172475, 0.6914473472006583, 0.7079802350803666, 0.7228577751547847, 0.7365680325602398, 0.7492225880203303, 0.7604784400993214, 0.7708640251180037, 0.7798651367251792, 0.7884341223937757, 0.7968339212801512, 0.803905459027889, 0.8105831077787146, 0.8159073388871383, 0.8213746516381082, 0.8263370017510293, 0.8307094315646312, 0.83494582321615, 0.8385978252789476, 0.8423962929393058, 0.8454029632971072, 0.8478861515306495, 0.8509196255153858, 0.8531482343117387, 0.8550054388361371, 0.8572119732657764, 0.8590981389851325, 0.8608546642714837, 0.8621527162931306, 0.8633372881785012, 0.8647065196392656, 0.8656204156327938, 0.8670309185442885, 0.8679941658747241, 0.8688529994755048, 0.8698547947839784, 0.8705499205390143, 0.8711693566665887, 0.8722121889780924, 0.8727274629266687, 0.8736856677823451, 0.874487016996204, 0.8745316970266288, 0.8747723690973094, 0.8754619596536929, 0.876219957024167, 0.8764310809094762, 0.8767849313558027, 0.8776038850387465, 0.8779381113221459, 0.8783009796421151, 0.8786306496560662, 0.878888672557553, 0.8795013701549603, 0.8805156356714673, 0.880960489414185, 0.8806232883643328, 0.8812303342925832, 0.8816066596265026, 0.8820299192097861, 0.8826641119650124, 0.8831026671182852, 0.8833030837648904, 0.8839432078996965, 0.8842528239508413, 0.8839211268343716, 0.8841719158357989, 0.8837628603120835, 0.8838921100960108, 0.8837551757712742, 0.8840459444328516, 0.8839637803359519, 0.8846884372232905, 0.8848177850954645, 0.8851040671092614, 0.8853739279529286, 0.8855119399016116, 0.8861438454972939, 0.8863372310906368, 0.8864928940414375, 0.8861477979731371, 0.8861209146837301, 0.8860286250790619, 0.8865122354156587, 0.8868640945171651, 0.8873893167484306, 0.8877958340570212, 0.8877924001712739, 0.888083307932761, 0.888007446077738, 0.8882768492485786, 0.8887034470797449, 0.8889673738326138, 0.8887969283356476, 0.8890535659802454, 0.8892061501295853, 0.8893831854837201, 0.8892830811202126, 0.8892407858093961, 0.8900623597604594, 0.890040822344278, 0.8905738436019435, 0.8903699689838426, 0.8905018056227324, 0.8909459304068447, 0.8907719122437957, 0.8910996061850788, 0.8915724886576853, 0.8917468827700795, 0.8917745605981168, 0.8916103351953081, 0.8917910926678707, 0.891611977518177, 0.8908363042863141, 0.8908970933324567, 0.8913068368888948, 0.8915311807320083, 0.891567192223943, 0.8916403416950728, 0.8917814774239089, 0.8916470043803132, 0.8918810120559867, 0.8919980807487314, 0.8923699382423823, 0.8922906004328278, 0.8923758287931595, 0.8919948441819158, 0.8922592210683176, 0.8920668256463502, 0.8924196016189893, 0.8928326972270452, 0.8931343296127744, 0.8933143195618884, 0.8930356808097658, 0.8928632956636536, 0.892768154679743, 0.8931027144000218, 0.8933936701343419, 0.8935222824601396, 0.8935962649163094, 0.8935916659566815, 0.8939506493045375, 0.8940072386474271, 0.8938963891237989, 0.8936755837486932, 0.8939427851159172, 0.8941609113012382, 0.894158823842048, 0.8943319616009154, 0.8942395269242577], "moving_var_accuracy_train": [0.018697975692703945, 0.03576547037149397, 0.05615281241552651, 0.07356344692246647, 0.08667375024207594, 0.09852587300926718, 0.10742296486642935, 0.1141088388487465, 0.11841060298872427, 0.12057675801465428, 0.12075053261277563, 0.11872796201078648, 0.11571776069640174, 0.11179427624309919, 0.10723304893616661, 0.10262238674729765, 0.09718744311469907, 0.09167652339788658, 0.08618111665525816, 0.08066122500630908, 0.07514529372029788, 0.06987711993704893, 0.06491878704955238, 0.06008542686122694, 0.0555096894610835, 0.051207395544793775, 0.04718642172497846, 0.04336609400153005, 0.039781779826969, 0.03650468625206744, 0.033378401229732325, 0.03055231089048576, 0.027903396643558804, 0.02547722356746848, 0.02326175793238373, 0.021220505267183165, 0.019349587701389896, 0.017579660004070807, 0.015960082628809893, 0.01451838375641748, 0.013175108856749898, 0.011936869142368717, 0.010849720726600065, 0.009886019248914945, 0.008984768041950296, 0.008144094190882498, 0.007381395118850164, 0.006696984209003733, 0.006075623227657707, 0.005508712297388867, 0.004981253540620661, 0.004509290252599478, 0.004113027801603879, 0.0037272987477688898, 0.0033804590140751067, 0.0030615113115271516, 0.0027744187434984554, 0.0025120888559619203, 0.002276999035403248, 0.00205720194160144, 0.001864075059988212, 0.001685368690165705, 0.0015251320752214814, 0.0013801620338515298, 0.0012539419917030933, 0.001134547674663023, 0.001026456346159397, 0.0009293303258016064, 0.000841446702049429, 0.0007606394174359127, 0.0006907671290767223, 0.0006307310945708137, 0.0005721826791796134, 0.0005163287445346882, 0.0004696371580758737, 0.00042749694607447915, 0.00038668095656103246, 0.00035468847846068126, 0.0003225437817912214, 0.00029194102742431796, 0.00026533916530535254, 0.00023886754242432963, 0.0002154494313048817, 0.0001990221041437379, 0.0001791505211344451, 0.00016466283343844813, 0.00014847670138102371, 0.00013487894808906725, 0.00012336170088918068, 0.00011498197216206343, 0.0001054614732893292, 9.745463816880313e-05, 8.842885579023494e-05, 7.973560940471995e-05, 7.489898572136552e-05, 6.838357646011438e-05, 6.363690255951133e-05, 5.738368630090921e-05, 5.196340163661944e-05, 4.73431893937022e-05, 4.3731362997314795e-05, 4.162266869213656e-05, 3.867699605131536e-05, 3.583650335745583e-05, 3.2828644537100346e-05, 2.982611700321682e-05, 2.6853999334419302e-05, 2.574154922677496e-05, 2.54124355871182e-05, 2.300961859653619e-05, 2.0889550437083015e-05, 1.9535241100911475e-05, 1.8852186479465636e-05, 1.7015368614850834e-05, 1.5356504690900203e-05, 1.3950672489651937e-05, 1.4339155232069996e-05, 1.3575026675844147e-05, 1.3072967118830433e-05, 1.2368790294943467e-05, 1.1254609423424532e-05, 1.1330996690308077e-05, 1.0364114262378597e-05, 9.400209970668078e-06, 9.640631032086149e-06, 9.174708783908272e-06, 8.369714831808391e-06, 8.308171493411908e-06, 9.2452834615225e-06, 8.322766625044209e-06, 7.76263025557147e-06, 7.209424891734605e-06, 6.687386954363909e-06, 6.2338989506970255e-06, 5.61054911385058e-06, 5.495770591615543e-06, 5.569959624441689e-06, 5.211553914904111e-06, 5.531753850368733e-06, 5.188299721131536e-06, 4.914255407562487e-06, 4.534024723421537e-06, 4.853585412071844e-06, 5.223087553780868e-06, 4.703356385264854e-06, 4.3424976982666725e-06, 4.593698860981169e-06, 4.492702778175172e-06, 4.2833783716311445e-06, 3.924443995274901e-06, 3.5319996188342803e-06, 3.180530864543748e-06, 3.305162185688696e-06, 3.1987165157437176e-06, 3.264904925207393e-06, 3.029017792935952e-06, 2.8777312182682286e-06, 2.6611632636588725e-06, 2.4038218103179557e-06, 2.226143004016304e-06, 2.454981666511299e-06, 2.2367940767928207e-06, 2.440102594263724e-06, 2.3096497061171095e-06, 2.0830708092511214e-06, 1.8791135714021466e-06, 1.767215456804791e-06], "duration": 306776.473139, "accuracy_train": [0.45580180503645257, 0.5042896291874308, 0.6074602669919712, 0.6488617945967147, 0.6705053251315061, 0.7188078583540974, 0.7455005666874308, 0.77476481750646, 0.7965511013635106, 0.8150077360765043, 0.8286044492894058, 0.8310236855158729, 0.8440419954318937, 0.8531329667889442, 0.8619438388242894, 0.8784981773717239, 0.8755437964308784, 0.8833334054309707, 0.890727018157069, 0.8944687052994648, 0.8958151286798633, 0.9023026543466224, 0.9102775542751015, 0.9108813719892026, 0.9148813489179586, 0.9191127592631044, 0.9236453574889257, 0.9240635237864526, 0.9255745100244556, 0.9315505029531194, 0.9284333615956073, 0.9351544836078812, 0.9344798299649317, 0.9376184203696014, 0.9411286740956073, 0.9427104962624585, 0.9448954151670359, 0.9401753630837025, 0.9408489352620893, 0.9469644371193245, 0.9444293039405685, 0.9428492842146549, 0.9505448057978036, 0.9562872023809523, 0.9544041923334257, 0.9517084616671282, 0.9528699546073275, 0.9557302481312293, 0.9569153530477114, 0.9573106283453304, 0.9543119073574198, 0.9568455985834257, 0.9661465543097084, 0.9608223240240864, 0.9626119676310447, 0.9619144229881875, 0.9633560152500923, 0.9632172272978959, 0.9649378374169435, 0.9622639162859912, 0.965659354524271, 0.9642635442621816, 0.9655416551310447, 0.9660535483573275, 0.9692626142026578, 0.9671238377860835, 0.9674951406192323, 0.9683786971668512, 0.9688208359288483, 0.9681690732858066, 0.970982863833518, 0.9735405275239941, 0.9716106540120893, 0.9691227447858989, 0.9730282738095238, 0.973680396940753, 0.9717269114525655, 0.9761675851905685, 0.9744938385358989, 0.9733080126430418, 0.9748193593692323, 0.9708211848814139, 0.9723543411429494, 0.9778413318452381, 0.9704713310954227, 0.9771673991786637, 0.9733777671073275, 0.9755165435239018, 0.9768418783453304, 0.9792607540836102, 0.9779812012619971, 0.9790740212024732, 0.9771212566906607, 0.975865676333518, 0.9806089799049464, 0.9785861004406607, 0.9804454985119048, 0.9772146231312293, 0.9780974587024732, 0.9789355937384644, 0.9801900926310447, 0.9820276811669435, 0.98118990661914, 0.9812593005952381, 0.9807481283453304, 0.9802365956072352, 0.9783067220953304, 0.9827946197858989, 0.9840265881667589, 0.9807717403216132, 0.9810732886904762, 0.9826543898809523, 0.9838402157738095, 0.9797254233573275, 0.9810740096668512, 0.9816552968692323, 0.9850260416666666, 0.9837475703096161, 0.9843753604881875, 0.9841893485834257, 0.9830271346668512, 0.9856305803571429, 0.9837007068452381, 0.9833751860119048, 0.9861889765596161, 0.9852821685239018, 0.9842827150239941, 0.9862118675595238, 0.9880022321428571, 0.9838638277500923, 0.9822594750715209, 0.9853987864525655, 0.9854685409168512, 0.9856770833333334, 0.9843063270002769, 0.9865141369047619, 0.9871426480597084, 0.9862587310239018, 0.9879793411429494, 0.9837010673334257, 0.9867241212739941, 0.9863513764880952, 0.9882816104881875, 0.9825621049049464, 0.9851666320598007, 0.986421851928756, 0.9881889650239941, 0.9877006837739941, 0.98753756286914, 0.9851898835478959, 0.9859797131667589, 0.9861188616071429, 0.9882118560239018, 0.987793689726375, 0.9884447313930418, 0.9875840658453304, 0.9879789806547619, 0.9877003232858066, 0.9872120420358066, 0.9860963310954227, 0.98460787536914, 0.98607271911914, 0.9887466402500923, 0.9879095866786637, 0.9871193965716132, 0.9867008697858989, 0.9859797131667589], "end": "2016-02-04 22:59:58.403000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0], "moving_var_accuracy_valid": [0.019129270359728844, 0.03637589368263511, 0.057271484350708224, 0.07488011107122149, 0.08786966576731642, 0.0993702938355807, 0.107735767187091, 0.1139929955268293, 0.11754829228629049, 0.11867627398179492, 0.118007280658665, 0.1154333401211668, 0.11182709818052985, 0.1074608217145649, 0.10253667294482774, 0.09762118982659779, 0.09194086425678216, 0.08629344399597304, 0.08066389397386557, 0.07505753201126605, 0.06954384959813271, 0.06428120506143402, 0.05929432452032317, 0.054505139922508, 0.05002536931587971, 0.04575201247577521, 0.04183765886669521, 0.038288902572009675, 0.03491007212986785, 0.031820383852435685, 0.02889347239925528, 0.026273148737782016, 0.02386745813179269, 0.02165277560088729, 0.019649021168824086, 0.017804153123541784, 0.016153593020288695, 0.014619594318024157, 0.013213130900450568, 0.011974635490150157, 0.01082187221563978, 0.009770727871884816, 0.008837474232398987, 0.007985745399246956, 0.007214939289056582, 0.0065086098116090355, 0.005870377725412628, 0.005300213106009686, 0.0047777086483916, 0.0043178434497224555, 0.0038944097135265323, 0.003511607098558347, 0.00316947873326323, 0.002856879658274735, 0.002574645002492562, 0.002326967995312553, 0.0020966607609600304, 0.0018952580937730058, 0.001711511729458675, 0.0015403785232588763, 0.0013868619783434393, 0.0012524555967281755, 0.0011323810771781686, 0.0010195441291148844, 0.000918716607448685, 0.0008328811129170818, 0.0007505983665020086, 0.0006767235906105427, 0.0006100293724123753, 0.0005496256175303625, 0.0004980416408901451, 0.0004574960876429072, 0.0004135275325503063, 0.0003731981202274687, 0.0003391948510357709, 0.0003065499527447397, 0.0002775072955438373, 0.0002533763700464862, 0.00022976970864399722, 0.00020715423926972578, 0.00019012664551440505, 0.00017197673985510313, 0.00015576927266326145, 0.00014075840150606743, 0.00012818849914879963, 0.00011551999879372668, 0.00010413675799797367, 9.448399992917593e-05, 8.50963583856323e-05, 8.131287098637362e-05, 7.333216173605965e-05, 6.67365620852663e-05, 6.0718329751242815e-05, 5.481792245793217e-05, 5.2929872348829363e-05, 4.7973467003359685e-05, 4.339419889129179e-05, 4.0126600669370026e-05, 3.612044500367713e-05, 3.258505684347772e-05, 3.1431461778099636e-05, 2.9402559046105792e-05, 2.894502867143402e-05, 2.7537832703944847e-05, 2.4784155557692296e-05, 2.3067385933163815e-05, 2.081244252927533e-05, 1.9384400892478837e-05, 1.9083832189232527e-05, 1.7802364948228474e-05, 1.6283593460330113e-05, 1.5248000039919896e-05, 1.3932737339595997e-05, 1.2821537255159258e-05, 1.1629571481982598e-05, 1.0482714373637873e-05, 1.550929674986755e-05, 1.3962541817542766e-05, 1.5123292585899069e-05, 1.3985047066461542e-05, 1.2742970453999313e-05, 1.3243894823364559e-05, 1.2192046230666406e-05, 1.1939291479982519e-05, 1.2757922828070921e-05, 1.1755850303203404e-05, 1.058715983236679e-05, 9.771173695479105e-06, 9.08811570091604e-06, 8.468044262472598e-06, 1.3036260499882823e-05, 1.1765892223072983e-05, 1.2100311039148247e-05, 1.1343251374719993e-05, 1.0220597685210266e-05, 9.24669552282836e-06, 8.501299616132382e-06, 7.813916649604179e-06, 7.525361315110777e-06, 6.896170892988279e-06, 7.451055763948784e-06, 6.762600579778066e-06, 6.151715382443701e-06, 6.842887310239677e-06, 6.787654821787282e-06, 6.442033325154303e-06, 6.917887974481886e-06, 7.761931009589286e-06, 7.804576773716615e-06, 7.3156865323839464e-06, 7.282873866805356e-06, 6.822036227525682e-06, 6.221298866148309e-06, 6.606540837430221e-06, 6.707783907690971e-06, 6.185875690045807e-06, 5.6165487554294265e-06, 5.055084233753418e-06, 5.709397206718822e-06, 5.167278669605053e-06, 4.761139354642042e-06, 4.723820542258342e-06, 4.894007623850165e-06, 4.832818155969086e-06, 4.34957555774501e-06, 4.1844081538814085e-06, 3.842864863532787e-06], "accuracy_test": 0.5873844068877551, "start": "2016-02-01 09:47:01.930000", "learning_rate_per_epoch": [0.006804521195590496, 0.004811523016542196, 0.003928591962903738, 0.003402260597795248, 0.003043074393644929, 0.0027779340744018555, 0.002571867313235998, 0.002405761508271098, 0.0022681737318634987, 0.0021517786663025618, 0.0020516403019428253, 0.001964295981451869, 0.0018872346263378859, 0.0018185848603025079, 0.0017569197807461023, 0.001701130298897624, 0.0016503388760611415, 0.00160384108312428, 0.0015610642731189728, 0.0015215371968224645, 0.0014848682330921292, 0.0014507288578897715, 0.001418840722180903, 0.0013889670372009277, 0.0013609043089672923, 0.0013344764010980725, 0.0013095306931063533, 0.001285933656617999, 0.0012635678285732865, 0.0012423299485817552, 0.0012221280485391617, 0.001202880754135549, 0.0011845150729641318, 0.0011669658124446869, 0.0011501740664243698, 0.0011340868659317493, 0.0011186563642695546, 0.0011038391385227442, 0.0010895953746512532, 0.0010758893331512809, 0.0010626877192407846, 0.0010499603813514113, 0.001037679729051888, 0.0010258201509714127, 0.0010143581312149763, 0.0010032719001173973, 0.0009925414342433214, 0.0009821479907259345, 0.000972074456512928, 0.000962304649874568, 0.0009528235532343388, 0.0009436173131689429, 0.0009346728911623359, 0.0009259780636057258, 0.0009175214800052345, 0.0009092924301512539, 0.0009012808441184461, 0.0008934774086810648, 0.0008858732180669904, 0.0008784598903730512, 0.0008712296839803457, 0.0008641750901006162, 0.000857289123814553, 0.000850565149448812, 0.0008439969969913363, 0.000837578671053052, 0.0008313045836985111, 0.0008251694380305707, 0.0008191681117750704, 0.0008132958319038153, 0.0008075481164269149, 0.00080192054156214, 0.0007964089745655656, 0.0007910095155239105, 0.0007857184391468763, 0.0007805321365594864, 0.0007754471735097468, 0.0007704602903686464, 0.0007655684603378177, 0.0007607685984112322, 0.0007560579106211662, 0.0007514336612075567, 0.0007468932308256626, 0.0007424341165460646, 0.0007380539318546653, 0.000733750406652689, 0.0007295212708413601, 0.0007253644289448857, 0.000721277785487473, 0.0007172595360316336, 0.0007133076433092356, 0.0007094203610904515, 0.0007055960013531148, 0.0007018327596597373, 0.0006981291808187962, 0.0006944835186004639, 0.0006908944342285395, 0.0006873604725115001, 0.0006838801200501621, 0.0006804521544836462, 0.0006770751788280904, 0.0006737480289302766, 0.0006704694242216647, 0.0006672382005490363, 0.0006640532519668341, 0.0006609135307371616, 0.0006578178727068007, 0.0006547653465531766, 0.000651754904538393, 0.0006487856735475361, 0.0006458566058427095, 0.0006429668283089995, 0.0006401155260391533, 0.0006373018259182572, 0.0006345248548313975, 0.0006317839142866433, 0.0006290781893767416, 0.0006264069816097617, 0.0006237694178707898, 0.0006211649742908776, 0.0006185928359627724, 0.000616052420809865, 0.0006135430303402245, 0.0006110640242695808, 0.0006086148787289858, 0.0006061949534341693, 0.0006038036081008613, 0.0006014403770677745, 0.0005991046782582998, 0.000596795987803489, 0.0005945137818343937, 0.0005922575364820659, 0.0005900268442928791, 0.0005878211231902242, 0.0005856399657204747, 0.0005834829062223434, 0.000581349479034543, 0.000579239334911108, 0.0005771519499830902, 0.0005750870332121849, 0.000573044060729444, 0.000571022741496563, 0.0005690226680599153, 0.0005670434329658747, 0.0005650847451761365, 0.0005631461972370744, 0.0005612274399027228, 0.0005593281821347773, 0.0005574480746872723, 0.0005555868265219033, 0.0005537440883927047, 0.0005519195692613721, 0.0005501129198819399, 0.0005483239656314254, 0.0005465522990562022, 0.0005447976873256266, 0.000543059897609055, 0.0005413386388681829, 0.0005396336200647056, 0.0005379446665756404, 0.0005362714291550219, 0.0005346136749722064, 0.000532971229404211, 0.0005313438596203923, 0.0005297312745824456, 0.0005281332996673882, 0.0005265496438369155], "accuracy_train_first": 0.45580180503645257, "accuracy_train_last": 0.9859797131667589, "batch_size_eval": 1024, "accuracy_train_std": [0.013285585722265842, 0.016647555129006567, 0.018352957015521226, 0.019192239012861444, 0.018412825094007212, 0.021601809763696533, 0.018063664622692283, 0.020292591759991596, 0.017529806310492144, 0.01932591489315936, 0.01973786890414863, 0.02013145057065333, 0.01916674680225388, 0.01758565911015753, 0.018068878549204827, 0.01632587511871289, 0.018230365753210416, 0.016504879083726484, 0.016213902416597124, 0.016325094246408444, 0.01793818265770084, 0.016957821957192008, 0.015594585076269544, 0.017327797407160686, 0.01576069651479261, 0.014037985894612954, 0.015028922443632006, 0.014261184242029271, 0.015897704805765748, 0.014957561888567478, 0.015883476837519422, 0.013741979176737064, 0.013572826176327648, 0.014380979600930087, 0.01400095899433992, 0.013212431903818229, 0.013390201461780108, 0.01409444172904954, 0.014904900096250888, 0.013658143258431, 0.014291695329168021, 0.013392278900606909, 0.013183625075761832, 0.011855158324798052, 0.012094779811128165, 0.012541029012696358, 0.012346087537855755, 0.012446948834388948, 0.011048246495245875, 0.011863052677173903, 0.011930810439519816, 0.01155070077863817, 0.009127739263994405, 0.010528236606619444, 0.010656978239387875, 0.010512660830982064, 0.009878415265707383, 0.010140096576450435, 0.009900027270551774, 0.009248149644799582, 0.00852112706346292, 0.009416382572852761, 0.009170386866217324, 0.008972549017726508, 0.00831492276842346, 0.008612279097667205, 0.009938925175052927, 0.008269491094620223, 0.008197143569349384, 0.008315222867951413, 0.00810376960464189, 0.00805724267376896, 0.008098408246146835, 0.00835954790351315, 0.007950649076153091, 0.007703999069031232, 0.008094358038040277, 0.0072874106153983835, 0.007730364582244366, 0.00810463011276604, 0.007351949331835863, 0.007430395063185241, 0.009068535640288151, 0.0067270778451651815, 0.008356326699592567, 0.007401422178828258, 0.00776521828853074, 0.007276009091521247, 0.007066627550836396, 0.00602189408025111, 0.006754137853217974, 0.006549053620429345, 0.00678173184439118, 0.00708686106163802, 0.006213211021724613, 0.005361358411703382, 0.006621613956706164, 0.006837511524676149, 0.0067371007005595975, 0.005832132124846686, 0.006395124039791959, 0.005892752564034467, 0.006188543775936572, 0.005663253030749841, 0.006545988913010556, 0.006948257472679898, 0.0068553390153648745, 0.005675816944429414, 0.005365993491512506, 0.006393013782251676, 0.006325088592231884, 0.00566955009454173, 0.005508640178223015, 0.007246549183105459, 0.0057633728323141925, 0.005011305313789475, 0.0055679425398421755, 0.005070108386769357, 0.005790052045463447, 0.006055505821867561, 0.004719014937777102, 0.005425922942809191, 0.005376325303397101, 0.005239432771015919, 0.004613872398663207, 0.005518496306550489, 0.005119740263794034, 0.004827638335335513, 0.004728234554491273, 0.005191434294748976, 0.005003019193287032, 0.004337544167960927, 0.0050228262891294, 0.00450383185000121, 0.005180045271201009, 0.004943990327564088, 0.0040835938830438515, 0.004824710716978818, 0.004135787894896846, 0.00558443152689525, 0.0041176824129578755, 0.004597742929812399, 0.004055437683563876, 0.005433098779638817, 0.005248865498725989, 0.004526213642321102, 0.003812473033410032, 0.004948986633936089, 0.004580367606147795, 0.00443103472895054, 0.004707671488306541, 0.004843291109609361, 0.004370174318998574, 0.004079770196369825, 0.003566920941835798, 0.003669167581614203, 0.004395761315312676, 0.003953765538035225, 0.004093481070003792, 0.004827285904949823, 0.00577751207322002, 0.005111149858336753, 0.004194326969958478, 0.0038601342421641996, 0.004129609435107855, 0.004182754241940848, 0.005259006695833518], "accuracy_test_std": 0.011762264732652778, "error_valid": [0.5389713149472892, 0.4925037062311747, 0.38565541462725905, 0.3463458325489458, 0.32762789439006024, 0.28214567253388556, 0.2584934464420181, 0.22934658556099397, 0.21322300922439763, 0.20174839984939763, 0.1895104833396084, 0.1867940512048193, 0.17799469361822284, 0.17006012330572284, 0.1634065559111446, 0.14876841349774095, 0.15499399943524095, 0.1481477668486446, 0.1442415168486446, 0.14322377400225905, 0.14324436417545183, 0.14003965079066272, 0.1368864128388554, 0.13821889118975905, 0.1356657097138554, 0.13912485881024095, 0.1344450065888554, 0.12756788874246983, 0.13245070124246983, 0.1293180534638554, 0.13617458113704817, 0.12941953360316272, 0.1290018472326807, 0.12993870011295183, 0.1269266519201807, 0.1285341561558735, 0.12341749811746983, 0.1275370034826807, 0.12976515436746983, 0.12177910862198793, 0.12679428652108427, 0.12827972044427716, 0.12292921686746983, 0.12392636954066272, 0.12333660815135539, 0.12616481551204817, 0.12600156485316272, 0.12297039721385539, 0.12615452042545183, 0.12027455525225905, 0.12333660815135539, 0.12341749811746983, 0.12112904743975905, 0.12319394766566272, 0.12325571818524095, 0.11840232021837349, 0.12263507153614461, 0.11769048851656627, 0.11830084007906627, 0.12506618269954817, 0.12306158226656627, 0.11833172533885539, 0.11695806664156627, 0.12166880412274095, 0.12003041462725905, 0.11502553181475905, 0.11905385212725905, 0.11843320547816272, 0.11840232021837349, 0.11878912132906627, 0.11498435146837349, 0.11035597467996983, 0.11503582690135539, 0.12241152108433728, 0.11330625235316272, 0.11500641236822284, 0.11416074454066272, 0.11162815323795183, 0.11295033650225905, 0.11489316641566272, 0.11029567488704817, 0.11296063158885539, 0.11906414721385539, 0.11357098315135539, 0.11991863940135539, 0.11494464184864461, 0.11747723315135539, 0.11333713761295183, 0.11677569653614461, 0.10878965079066272, 0.11401808405496983, 0.11231939476656627, 0.11219732445406627, 0.11324595256024095, 0.10816900414156627, 0.11192229856927716, 0.11210613940135539, 0.11695806664156627, 0.11412103492093373, 0.11480198136295183, 0.10913527155496983, 0.10996917356927716, 0.10788368317018071, 0.10854551016566272, 0.11223850480045183, 0.10929852221385539, 0.11267531061746983, 0.10929852221385539, 0.10745717243975905, 0.10865728539156627, 0.11273708113704817, 0.10863669521837349, 0.10942059252635539, 0.10902349632906627, 0.11161785815135539, 0.11113987198795183, 0.10254347467996983, 0.11015301440135539, 0.10462896507906627, 0.11146490257906627, 0.10831166462725905, 0.10505694653614461, 0.11079425122364461, 0.10595114834337349, 0.10417156908885539, 0.10668357021837349, 0.10797633894954817, 0.10986769342996983, 0.10658209007906627, 0.11000005882906627, 0.11614475480045183, 0.10855580525225905, 0.10500547110316272, 0.10644972467996983, 0.10810870434864461, 0.10770131306475905, 0.10694830101656627, 0.10956325301204817, 0.10601291886295183, 0.10694830101656627, 0.10428334431475905, 0.10842343985316272, 0.10685711596385539, 0.11143401731927716, 0.10536138695406627, 0.10966473315135539, 0.10440541462725905, 0.10344944230045183, 0.10415097891566272, 0.10506577089608427, 0.10947206795933728, 0.10868817065135539, 0.10808811417545183, 0.10388624811746983, 0.10398772825677716, 0.10532020660768071, 0.10573789297816272, 0.10644972467996983, 0.10281850056475905, 0.10548345726656627, 0.10710125658885539, 0.10831166462725905, 0.10365240257906627, 0.10387595303087349, 0.10585996329066272, 0.10410979856927716, 0.10659238516566272], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.09760787539600026, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.006804521267983439, "optimization": "rmsprop", "nb_data_augmentation": 4, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.1428214543249262e-06, "rotation_range": [0, 0], "momentum": 0.5081526897596639}, "accuracy_valid_max": 0.8974565253200302, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8934076148343373, "accuracy_valid_std": [0.019314308844837043, 0.013055250683910372, 0.010786664943375577, 0.02305814534572453, 0.019088558726748223, 0.012736983899982523, 0.013929653887536206, 0.02379489848590709, 0.01983735283533826, 0.017437102954600596, 0.015886261265991936, 0.016389764136065643, 0.016162154501025726, 0.012547787162295189, 0.01446695345184339, 0.014775724800313025, 0.011191816267808302, 0.012796453730446763, 0.00935820112569805, 0.013204151405972805, 0.012663218457340654, 0.013832704853339092, 0.011617630601122681, 0.013100320342796113, 0.013079826302641618, 0.011533324038343605, 0.013215530083316582, 0.012140725529022178, 0.013038554125771804, 0.010945529979056854, 0.014870076989752071, 0.011175601094990228, 0.015685849599242553, 0.006567099365270195, 0.014303331140501281, 0.012836105840855032, 0.00910068162146622, 0.013426987795352471, 0.013207510576801364, 0.012806560428803525, 0.013199651052957201, 0.013358224440254137, 0.011079793623723619, 0.00997616386570895, 0.009845827577862872, 0.008045510936372215, 0.008810203012230117, 0.00696421110342427, 0.008142717283300934, 0.010433354286491666, 0.009348988048390187, 0.009087573240210582, 0.006096283870982603, 0.010096764620771258, 0.011091266439474978, 0.008117660293240498, 0.008086091003337176, 0.005740643106470435, 0.009425035050742858, 0.0064532748970397465, 0.011975943209782024, 0.006572759537214992, 0.011810589626983014, 0.008932810525840957, 0.007511326736543791, 0.008162292324549607, 0.007547569596456206, 0.008609519661062364, 0.008320713235286702, 0.008156272800533907, 0.010865995687357806, 0.008665145908195051, 0.007778427188640636, 0.010941915338953804, 0.011306881397764108, 0.014674166132696585, 0.009952813590602341, 0.007646260221285702, 0.011211761979106198, 0.006192281118228194, 0.008333869193210735, 0.007054828500243587, 0.007339156495561495, 0.009459711911830607, 0.009441703059521876, 0.012573742283109573, 0.009027766241514457, 0.009273102667262525, 0.009442948446559586, 0.009996155500123382, 0.009373763196590506, 0.008080809344963216, 0.008146229358154579, 0.011443435958030898, 0.005030669492061569, 0.00756789922305636, 0.008080905038485936, 0.0052376803223626, 0.01063530749880197, 0.008286166914877416, 0.006952196777272178, 0.008067076079637634, 0.01025232444289874, 0.005993324993201404, 0.009712042670118564, 0.01035402262403679, 0.005366207361654853, 0.0056660331875196, 0.007977507716566686, 0.007597249484594104, 0.007865014389962146, 0.005837108624644277, 0.0071956400015055315, 0.009098685734580374, 0.009148977034803765, 0.006964645920387747, 0.006295182732550821, 0.010342337976046575, 0.005679031354256594, 0.006720367777371241, 0.007890126947559391, 0.011570525818450375, 0.008649305205135144, 0.007782307265710598, 0.00982785948524867, 0.00959085875787807, 0.010232477635261437, 0.007500266303991336, 0.005839631452019681, 0.006498168858946228, 0.0077480134626622755, 0.008443741716588382, 0.008923186366711277, 0.006784700222996851, 0.0106362495254625, 0.005558816808295109, 0.005663820866737706, 0.009312167349154049, 0.008259082885709416, 0.010847632769777672, 0.010837401027693525, 0.006314654090338587, 0.00873583308470959, 0.0059932326001282885, 0.009292516921220805, 0.007748549456465147, 0.01028025424365094, 0.009389015651258405, 0.008486559248217817, 0.008366227002638688, 0.011919977794216219, 0.008516268707339236, 0.009096567783030718, 0.004634950198994956, 0.008417811268372394, 0.007270836922133922, 0.006651396548281403, 0.006423689402041654, 0.008142031503025567, 0.006546051524780579, 0.006853249775008567, 0.008667681726972168, 0.005723369585379303, 0.0069545410576146586, 0.007700652304794845, 0.008585430568313906, 0.008542393673726141], "accuracy_valid": [0.46102868505271083, 0.5074962937688253, 0.614344585372741, 0.6536541674510542, 0.6723721056099398, 0.7178543274661144, 0.7415065535579819, 0.770653414439006, 0.7867769907756024, 0.7982516001506024, 0.8104895166603916, 0.8132059487951807, 0.8220053063817772, 0.8299398766942772, 0.8365934440888554, 0.851231586502259, 0.845006000564759, 0.8518522331513554, 0.8557584831513554, 0.856776225997741, 0.8567556358245482, 0.8599603492093373, 0.8631135871611446, 0.861781108810241, 0.8643342902861446, 0.860875141189759, 0.8655549934111446, 0.8724321112575302, 0.8675492987575302, 0.8706819465361446, 0.8638254188629518, 0.8705804663968373, 0.8709981527673193, 0.8700612998870482, 0.8730733480798193, 0.8714658438441265, 0.8765825018825302, 0.8724629965173193, 0.8702348456325302, 0.8782208913780121, 0.8732057134789157, 0.8717202795557228, 0.8770707831325302, 0.8760736304593373, 0.8766633918486446, 0.8738351844879518, 0.8739984351468373, 0.8770296027861446, 0.8738454795745482, 0.879725444747741, 0.8766633918486446, 0.8765825018825302, 0.878870952560241, 0.8768060523343373, 0.876744281814759, 0.8815976797816265, 0.8773649284638554, 0.8823095114834337, 0.8816991599209337, 0.8749338173004518, 0.8769384177334337, 0.8816682746611446, 0.8830419333584337, 0.878331195877259, 0.879969585372741, 0.884974468185241, 0.880946147872741, 0.8815667945218373, 0.8815976797816265, 0.8812108786709337, 0.8850156485316265, 0.8896440253200302, 0.8849641730986446, 0.8775884789156627, 0.8866937476468373, 0.8849935876317772, 0.8858392554593373, 0.8883718467620482, 0.887049663497741, 0.8851068335843373, 0.8897043251129518, 0.8870393684111446, 0.8809358527861446, 0.8864290168486446, 0.8800813605986446, 0.8850553581513554, 0.8825227668486446, 0.8866628623870482, 0.8832243034638554, 0.8912103492093373, 0.8859819159450302, 0.8876806052334337, 0.8878026755459337, 0.886754047439759, 0.8918309958584337, 0.8880777014307228, 0.8878938605986446, 0.8830419333584337, 0.8858789650790663, 0.8851980186370482, 0.8908647284450302, 0.8900308264307228, 0.8921163168298193, 0.8914544898343373, 0.8877614951995482, 0.8907014777861446, 0.8873246893825302, 0.8907014777861446, 0.892542827560241, 0.8913427146084337, 0.8872629188629518, 0.8913633047816265, 0.8905794074736446, 0.8909765036709337, 0.8883821418486446, 0.8888601280120482, 0.8974565253200302, 0.8898469855986446, 0.8953710349209337, 0.8885350974209337, 0.891688335372741, 0.8949430534638554, 0.8892057487763554, 0.8940488516566265, 0.8958284309111446, 0.8933164297816265, 0.8920236610504518, 0.8901323065700302, 0.8934179099209337, 0.8899999411709337, 0.8838552451995482, 0.891444194747741, 0.8949945288968373, 0.8935502753200302, 0.8918912956513554, 0.892298686935241, 0.8930516989834337, 0.8904367469879518, 0.8939870811370482, 0.8930516989834337, 0.895716655685241, 0.8915765601468373, 0.8931428840361446, 0.8885659826807228, 0.8946386130459337, 0.8903352668486446, 0.895594585372741, 0.8965505576995482, 0.8958490210843373, 0.8949342291039157, 0.8905279320406627, 0.8913118293486446, 0.8919118858245482, 0.8961137518825302, 0.8960122717432228, 0.8946797933923193, 0.8942621070218373, 0.8935502753200302, 0.897181499435241, 0.8945165427334337, 0.8928987434111446, 0.891688335372741, 0.8963475974209337, 0.8961240469691265, 0.8941400367093373, 0.8958902014307228, 0.8934076148343373], "seed": 651865921, "model": "residualv3", "loss_std": [0.335769385099411, 0.275885671377182, 0.27879345417022705, 0.2748810946941376, 0.2701292335987091, 0.26218125224113464, 0.2560746669769287, 0.25199708342552185, 0.24513544142246246, 0.24081836640834808, 0.23644571006298065, 0.22920183837413788, 0.22611811757087708, 0.22277149558067322, 0.21933798491954803, 0.21339143812656403, 0.2102930098772049, 0.20425808429718018, 0.20419998466968536, 0.19585630297660828, 0.19349949061870575, 0.18813171982765198, 0.18508827686309814, 0.17965643107891083, 0.17933039367198944, 0.1730462610721588, 0.17255757749080658, 0.16782179474830627, 0.16369406878948212, 0.16115018725395203, 0.15615803003311157, 0.1534699946641922, 0.1486678272485733, 0.1449768990278244, 0.14213882386684418, 0.13958445191383362, 0.13495054841041565, 0.13211871683597565, 0.1289612054824829, 0.12759272754192352, 0.12491818517446518, 0.12109622359275818, 0.119718536734581, 0.11286938935518265, 0.1117779091000557, 0.10947059094905853, 0.10875696688890457, 0.10166261345148087, 0.1027643010020256, 0.1000654399394989, 0.09578172862529755, 0.09902697056531906, 0.09398745000362396, 0.09179576486349106, 0.0904490202665329, 0.08997403085231781, 0.09072866290807724, 0.08726371824741364, 0.08561860024929047, 0.0844358578324318, 0.08303646743297577, 0.08213747292757034, 0.08242109417915344, 0.078876793384552, 0.0781165063381195, 0.07455690950155258, 0.07695601135492325, 0.07624096423387527, 0.07170409709215164, 0.07484832406044006, 0.06888549029827118, 0.0672958567738533, 0.0700690895318985, 0.07153775542974472, 0.06520285457372665, 0.066856250166893, 0.06688395142555237, 0.0645221695303917, 0.06303336471319199, 0.06433800607919693, 0.0627741739153862, 0.06568737328052521, 0.061422672122716904, 0.06058456376194954, 0.06160510331392288, 0.05879973620176315, 0.0575481578707695, 0.06287941336631775, 0.055422019213438034, 0.057140566408634186, 0.057626254856586456, 0.05852609500288963, 0.05832922086119652, 0.05546189472079277, 0.05405236780643463, 0.0554540753364563, 0.05525146797299385, 0.053586445748806, 0.052653584629297256, 0.0504753477871418, 0.05085146054625511, 0.04885983467102051, 0.05261112377047539, 0.04968954250216484, 0.05276893079280853, 0.0512520857155323, 0.04973089322447777, 0.04826730489730835, 0.04757919907569885, 0.050563469529151917, 0.04691816493868828, 0.05011889711022377, 0.04956168308854103, 0.04748830944299698, 0.047012489289045334, 0.04652494564652443, 0.045607514679431915, 0.044439397752285004, 0.044811949133872986, 0.04610055685043335, 0.04553679749369621, 0.0449879914522171, 0.0451139472424984, 0.04602868854999542, 0.046897049993276596, 0.04328222572803497, 0.046475209295749664, 0.04588456079363823, 0.041571006178855896, 0.04325837269425392, 0.039872776716947556, 0.04127524793148041, 0.04469713568687439, 0.04294717311859131, 0.040927864611148834, 0.04157336801290512, 0.04130547121167183, 0.03947730362415314, 0.04093419760465622, 0.03995306417346001, 0.03966837003827095, 0.04134468734264374, 0.04165187105536461, 0.03959424048662186, 0.042812786996364594, 0.04137010872364044, 0.03878793492913246, 0.04046689718961716, 0.039798036217689514, 0.03737183287739754, 0.03843601793050766, 0.03654634580016136, 0.03987887129187584, 0.03632928058505058, 0.037880294024944305, 0.03869076818227768, 0.03687725588679314, 0.0366847924888134, 0.037589192390441895, 0.035921819508075714, 0.038717903196811676, 0.035697679966688156, 0.034777019172906876, 0.037054356187582016, 0.03802534192800522, 0.03719792515039444, 0.03839430585503578]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:38 2016", "state": "available"}], "summary": "4fccab7fbe72d25f025acc82c468b465"}