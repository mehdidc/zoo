{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 16, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.653316855430603, 1.347299337387085, 1.177027940750122, 1.079803228378296, 1.0148037672042847, 0.9634336829185486, 0.9172654747962952, 0.8815008997917175, 0.8483996987342834, 0.8195474743843079, 0.7952514886856079, 0.7724305987358093, 0.7495747804641724, 0.7332617044448853, 0.7175585031509399, 0.7016663551330566, 0.6864084601402283, 0.6727719902992249, 0.6585800647735596, 0.6504228115081787, 0.6377722024917603, 0.6266026496887207, 0.6149811148643494, 0.6065225601196289, 0.5954309701919556, 0.5870036482810974, 0.5785796642303467, 0.5716466903686523, 0.5638715624809265, 0.5545855760574341, 0.5462958216667175, 0.5412286520004272, 0.5352322459220886, 0.526506781578064, 0.5202742218971252, 0.5142055153846741, 0.5083116292953491, 0.5032090544700623, 0.4968380331993103, 0.4929662346839905, 0.4850086569786072, 0.48052236437797546, 0.47628164291381836, 0.4706569015979767, 0.46498510241508484, 0.4615831673145294, 0.458690881729126, 0.4512968361377716, 0.4488159120082855, 0.44167786836624146, 0.43845221400260925, 0.4383222758769989, 0.4335344731807709, 0.42771056294441223, 0.4241464138031006, 0.4213767349720001, 0.41603681445121765, 0.4130743145942688, 0.4100765883922577, 0.4077155888080597, 0.40175843238830566, 0.3972775936126709, 0.3965717554092407, 0.3918886184692383, 0.3897201120853424, 0.38599973917007446, 0.38191357254981995, 0.3787486255168915, 0.3773675262928009, 0.3749510645866394, 0.37204709649086, 0.36930525302886963, 0.3672545254230499, 0.36191827058792114, 0.36034059524536133, 0.35809314250946045, 0.3527635931968689, 0.3523070514202118, 0.3536056876182556, 0.3480316400527954, 0.34690773487091064, 0.3463132083415985, 0.3415784239768982, 0.34195154905319214, 0.3381573557853699, 0.3331817388534546, 0.333383709192276, 0.32866424322128296, 0.32884418964385986, 0.3273283839225769, 0.32281965017318726, 0.32418715953826904, 0.320670485496521, 0.3219032883644104, 0.3175440728664398, 0.3151169717311859, 0.31500867009162903, 0.3114941716194153, 0.3096707761287689, 0.30775877833366394, 0.30729231238365173, 0.3051162362098694, 0.3062990605831146, 0.3022546172142029, 0.29804283380508423, 0.2972138226032257, 0.2998669743537903, 0.29408445954322815, 0.2934548556804657, 0.29107412695884705, 0.28875645995140076, 0.29093578457832336, 0.28512656688690186, 0.28586092591285706, 0.28373706340789795, 0.28363236784935, 0.28535541892051697, 0.28092968463897705, 0.2833918035030365, 0.2781519293785095, 0.2800791561603546, 0.2774689495563507, 0.2773823142051697, 0.27442264556884766, 0.2745903432369232, 0.27098190784454346, 0.2701282203197479, 0.2694556415081024, 0.26712343096733093, 0.2666066288948059, 0.2658351957798004, 0.26491352915763855, 0.26600879430770874, 0.2601034343242645, 0.260172575712204, 0.2606644928455353, 0.26024702191352844, 0.2603810727596283, 0.2561361491680145, 0.2564675509929657, 0.25312140583992004, 0.25498032569885254, 0.25169476866722107, 0.2528620958328247, 0.2536103427410126, 0.2515491843223572, 0.2494712471961975, 0.24986092746257782, 0.24531885981559753, 0.24761910736560822, 0.24548670649528503, 0.24447934329509735, 0.2463057041168213, 0.24536803364753723, 0.24236373603343964, 0.24355623126029968, 0.2391522377729416, 0.23848998546600342, 0.23834869265556335, 0.2376115918159485, 0.23510068655014038, 0.23523417115211487, 0.2368907630443573, 0.2359790951013565, 0.23309852182865143, 0.23719598352909088, 0.23314790427684784, 0.23214934766292572, 0.23062580823898315, 0.23064756393432617, 0.22997984290122986, 0.23223020136356354, 0.22649353742599487, 0.22579602897167206, 0.22666718065738678, 0.22506830096244812, 0.225943461060524, 0.22488410770893097, 0.22610509395599365, 0.22506847977638245, 0.22240807116031647, 0.22158490121364594, 0.22428104281425476, 0.22261227667331696, 0.22068612277507782, 0.21870838105678558, 0.21878507733345032, 0.21849164366722107, 0.21665456891059875, 0.21778228878974915, 0.21702790260314941, 0.21673241257667542, 0.21666713058948517, 0.21582259237766266, 0.21222564578056335, 0.21477550268173218, 0.21411527693271637, 0.2109089493751526, 0.21004508435726166, 0.21151141822338104, 0.2124367356300354, 0.21057765185832977, 0.2106466293334961, 0.20799916982650757, 0.20909473299980164, 0.20820224285125732, 0.20704185962677002, 0.208362877368927, 0.20615042746067047, 0.20868684351444244, 0.2029472440481186, 0.20333682000637054, 0.20895327627658844], "moving_avg_accuracy_train": [0.04939764705882352, 0.09996376470588234, 0.15470856470588232, 0.2071600611764705, 0.25632405505882344, 0.30271988484705875, 0.3452149551858823, 0.383698165549647, 0.4212248195829176, 0.45509763174227286, 0.4862655156268691, 0.5153613170053586, 0.5423240088342345, 0.5674021961861051, 0.5906172706851416, 0.6119767200872157, 0.6317061069020236, 0.64954961385888, 0.6657758289435802, 0.6813158931080457, 0.6954713626207706, 0.708361873417517, 0.7202198037228241, 0.7313907645270123, 0.742176393956664, 0.7513752251492329, 0.7602894673401921, 0.7672652264885258, 0.7756845861926144, 0.7830643628674706, 0.7898849854042529, 0.7960800162755923, 0.8018626028833272, 0.8076622249479356, 0.8130395318649067, 0.8185520492666514, 0.8232215502223392, 0.82812292461187, 0.8322165145036242, 0.8364136865826736, 0.8404805532185239, 0.8440230861319656, 0.8470913657540632, 0.8507892880021862, 0.8536915356725558, 0.8565553232817709, 0.8594197909535938, 0.8615342824464697, 0.8641502659665286, 0.866511709958111, 0.8681287742564175, 0.8704311909484229, 0.8730492483241689, 0.875344323491752, 0.8778757734955179, 0.8788811373224367, 0.8803647882960753, 0.8825400741723501, 0.8839495961668798, 0.8858346365501919, 0.8878605846598786, 0.8897568791350672, 0.8909976618097957, 0.892262601511169, 0.8934881060659344, 0.8949934131063998, 0.8965623070898775, 0.8983719587338309, 0.9001606452133889, 0.9008786983391088, 0.9016167108581392, 0.9021820985958547, 0.9026815357950927, 0.9041592645685246, 0.9055433381116721, 0.9064548866534461, 0.9077388097528073, 0.908731987601056, 0.9095340829585975, 0.9103994981921495, 0.9112442542552874, 0.9123198288297587, 0.913038434182077, 0.9138216495873988, 0.9146112493345412, 0.9158889479304988, 0.9168388766668607, 0.9174561654707629, 0.9184258430413337, 0.9192985528548473, 0.9199145799223037, 0.9209795925183086, 0.9209898685605954, 0.9217026464104182, 0.9225064994164351, 0.9232746730042034, 0.9243283821743713, 0.9249143674863459, 0.9257735189730054, 0.9262502847227637, 0.9273170209563697, 0.9280841423901445, 0.92911572815113, 0.9295970965124876, 0.9305291515671211, 0.9314527069986442, 0.9322580245340739, 0.9326439867865488, 0.9331937057549528, 0.9335943351794576, 0.9334960781321, 0.933901764436537, 0.9346527644634716, 0.9347733703700656, 0.9354136803918826, 0.9359570182350473, 0.9361683752350719, 0.9366221259468589, 0.9371575604109965, 0.9378912161346028, 0.9388573886387895, 0.9387010615396164, 0.9392074259738901, 0.9399031539647363, 0.9403457797447333, 0.94091120177026, 0.940975375710881, 0.9414307793162635, 0.9418547602081666, 0.9423775194814676, 0.9428738851803797, 0.9433747319564594, 0.943714905819637, 0.944122238767085, 0.9437076619492001, 0.9441392486954565, 0.9444100297082638, 0.9445690267374375, 0.944733300534282, 0.945081146951442, 0.9456436204915919, 0.945811023148315, 0.94575227377466, 0.9458146934560175, 0.9455602829339452, 0.945625431111139, 0.9459970056470839, 0.9462373050823756, 0.9467900451623733, 0.9471769229990771, 0.9470757012874047, 0.947546954688076, 0.9477710827486803, 0.9480551509444005, 0.9483155182029016, 0.9489122016767291, 0.9493150991561149, 0.9495459421816799, 0.9498148773752766, 0.9503651543436312, 0.9506839330269152, 0.9503896573712826, 0.950734221045919, 0.9507031518825035, 0.9507881308119002, 0.9508952000836512, 0.951215091839992, 0.9513806414795222, 0.9518237538021581, 0.9522084372454718, 0.9522770052856304, 0.9523551871100085, 0.9524584919284194, 0.9524879368532244, 0.9523120843443725, 0.9525138170864059, 0.952690670671883, 0.9525839565458711, 0.9527467373618723, 0.9532532400962733, 0.9537137984395871, 0.9538271244779813, 0.9540091179125361, 0.9541423237683413, 0.9544457384503306, 0.9546952822523564, 0.9547598716741796, 0.9549356492126441, 0.9553126725266738, 0.9556072876269477, 0.9556959706289588, 0.9558040206248865, 0.9562142067976919, 0.9563786684708638, 0.9564560957414244, 0.9566528391084584, 0.9567122610799655, 0.9569539761484396, 0.9570962255924191, 0.9570783677390596, 0.9568693544945654, 0.9570906543392266, 0.9568545300817745, 0.9570420182500676, 0.9569354634838845, 0.956961917135496, 0.9568704313042994, 0.9571269175856341, 0.9572354022976589, 0.95738715618554, 0.9577072640963978, 0.9577294788632286, 0.9579541780357292], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04825333333333332, 0.09742799999999999, 0.1506185333333333, 0.20142334666666661, 0.24908101199999994, 0.29426624413333324, 0.33494628638666657, 0.3715583244146666, 0.40681582530653326, 0.4382409094425466, 0.4668701518316253, 0.4936498033151294, 0.5189114896502831, 0.5418070073519214, 0.5631063066167292, 0.5827290092883897, 0.6007627750262173, 0.617059830856929, 0.6317405144379027, 0.6457264629941124, 0.6580738166947012, 0.6694797683585645, 0.6796917915227081, 0.6891226123704374, 0.6985570178000603, 0.7063146493533876, 0.7133098510847155, 0.7192321993095772, 0.7261889793786194, 0.7321434147740907, 0.7375557399633483, 0.7423334993003469, 0.7470468160369788, 0.7515954677666142, 0.7555559209899528, 0.7595203288909576, 0.7628082960018618, 0.7659674664016757, 0.7686773864281747, 0.7713296477853573, 0.7737033496734882, 0.7757730147061394, 0.7773823799021921, 0.7795774752453062, 0.7811797277207756, 0.7829150882820314, 0.7845035794538282, 0.7851465548417788, 0.7866452326909341, 0.7878207094218408, 0.78885197181299, 0.7898467746316911, 0.7912887638351886, 0.7927732207850031, 0.7937892320398362, 0.7939836421691858, 0.7947452779522672, 0.7955507501570406, 0.7962623418080033, 0.7973827742938696, 0.7983111635311493, 0.799066713844701, 0.7996000424602309, 0.8001733715475411, 0.8008627010594536, 0.8014964309535082, 0.8023334545248241, 0.802953442405675, 0.8037514314984409, 0.8038562883485968, 0.8038839928470705, 0.8040022602290301, 0.8042020342061271, 0.804608497452181, 0.8049476477069629, 0.8053728829362666, 0.8058622613093067, 0.8064627018450427, 0.8068964316605385, 0.8074467884944846, 0.8080621096450361, 0.8083892320138658, 0.8083769754791459, 0.8085526112645647, 0.8088973501381083, 0.8095009484576308, 0.8096175202785343, 0.8095091015840142, 0.8096915247589461, 0.8100823722830515, 0.810500801721413, 0.8104507215492717, 0.8099389827276778, 0.8098384177882434, 0.8099879093427523, 0.8104824517418104, 0.8109275399009627, 0.8113681192441997, 0.8115246406531131, 0.8118388432544685, 0.812188292262355, 0.8123161297027862, 0.8125511833991743, 0.8126293983925903, 0.813273125219998, 0.8137458126979982, 0.8144112314281984, 0.8143034416187118, 0.8144730974568407, 0.81475912104449, 0.8150298756067077, 0.8150202213793702, 0.8155581992414332, 0.8153490459839566, 0.815854141385561, 0.8158020605803382, 0.8158218545223044, 0.815799669070074, 0.8157397021630666, 0.8159923986134265, 0.8167531587520839, 0.8170378428768754, 0.8169340585891879, 0.8172139860636024, 0.8172125874572422, 0.8175846620448514, 0.8174928625070329, 0.8177702429229963, 0.8180332186306967, 0.8183365634342937, 0.8183829070908644, 0.8186112830484445, 0.8189501547436, 0.81905513926924, 0.8188962920089826, 0.8191266628080843, 0.8192139965272759, 0.819399263541215, 0.8197660038537601, 0.819589403468384, 0.819803796454879, 0.8196367501427244, 0.820073075128452, 0.8202657676156068, 0.8203325241873795, 0.8199392717686415, 0.8201053445917773, 0.8201481434659329, 0.8204133291193396, 0.8204519962074056, 0.8203934632533317, 0.8204874502613319, 0.820692038568532, 0.8206895013783454, 0.8211672179071775, 0.8212904961164598, 0.8210814465048138, 0.8214933018543324, 0.8218906383355659, 0.8222749078353425, 0.8225807503851417, 0.8220426753466276, 0.8227584078119649, 0.8222825670307684, 0.8223076436610248, 0.8224235459615891, 0.8229145246987635, 0.8228364055622205, 0.8233127650059984, 0.8234614885053986, 0.8233153396548587, 0.8229838056893728, 0.8234054251204356, 0.8234515492750586, 0.8233063943475527, 0.8230157549127974, 0.8231941794215177, 0.8228614281460326, 0.8231352853314293, 0.8234217567982864, 0.8237462477851245, 0.8237049563399454, 0.8237611273726175, 0.8234650146353557, 0.8233718465051535, 0.8235146618546382, 0.8229765290025076, 0.8228922094355903, 0.8230429884920312, 0.8233120229761614, 0.8230741540118786, 0.8233000719440241, 0.8235167314162883, 0.8235517249413261, 0.8234898857805268, 0.8239142305358076, 0.8241761408155602, 0.8243718600673375, 0.8244813407272704, 0.8245798733212101, 0.8241618859890891, 0.824332364056847, 0.8238857943178289, 0.8240438815527127, 0.8242528267307747, 0.8243208773910305, 0.8242621229852608, 0.8247559106867347, 0.8245203196180613, 0.8246416209895885, 0.8248041255572963, 0.8249103796682333, 0.8249526750347433], "moving_var_accuracy_train": [0.02196114781453287, 0.04277742331814532, 0.06547261912969078, 0.08368579255475887, 0.09707109794945931, 0.1067371453501635, 0.11231590984306118, 0.11441293617787096, 0.11564589042647894, 0.11440760801607777, 0.11170978008706306, 0.1081578929990654, 0.10388498445508926, 0.09915672533728008, 0.09409150995951443, 0.08878839367240085, 0.08341279264195559, 0.07793703004243449, 0.07251293754196562, 0.06743508613589042, 0.06249497337643251, 0.05774096345619859, 0.05323236171070842, 0.04903223882723596, 0.045175983164256285, 0.041419951305615064, 0.03799312959960523, 0.03463176758090476, 0.031806561383255764, 0.029116055178866954, 0.026623137687083628, 0.024306229585846897, 0.022176551398145804, 0.020261616803161863, 0.018495693989959427, 0.01691961522390432, 0.015423891854090413, 0.014097713906838508, 0.012838759819971506, 0.011713430119124717, 0.010690941745316377, 0.009734793425770102, 0.008846043141747504, 0.008084510488151229, 0.007351866813197599, 0.00669049164711408, 0.006095289057788939, 0.005525999820471047, 0.005034990166418922, 0.004581678909305457, 0.00414704509087863, 0.003780050685403389, 0.0034637336366673296, 0.0031647666032243067, 0.0029059640949959773, 0.002624464493316671, 0.002381829025889215, 0.00218623294109198, 0.001985490417260349, 0.0018189217707547682, 0.001673969785367579, 0.0015389362014604988, 0.0013988984561276085, 0.0012734092625478427, 0.0011595850890168164, 0.0010640201236898064, 0.0009797709663033577, 0.000911267421325192, 0.0008489352730920582, 0.0007686821484050578, 0.000696715895868761, 0.0006299212759275164, 0.0005691740859786096, 0.0005319098183312059, 0.0004959597726536544, 0.00045384208208438116, 0.0004232940006016042, 0.00038984222068571075, 0.00035664821128044566, 0.0003277238818905761, 0.0003013740089573942, 0.00028164835404889606, 0.0002581310615154303, 0.00023783879270408654, 0.0002196661232798651, 0.00021239213427088783, 0.00019927420228129376, 0.00018277619125997097, 0.00017296104345178618, 0.0001625195408740356, 0.00014968299091718296, 0.000144922958292307, 0.000130431612836482, 0.00012196092192161605, 0.00011558044662699755, 0.00010933321791280051, 0.00010839262325918366, 0.0001006437700059154, 9.722266449858735e-05, 8.954614827001177e-05, 9.083286917180145e-05, 8.704585990203164e-05, 8.791879655224183e-05, 8.12123563908625e-05, 8.09096603755886e-05, 8.049528605389354e-05, 7.828258444433923e-05, 7.179502774292523e-05, 6.733524346664058e-05, 6.20462545419874e-05, 5.592851911398742e-05, 5.181689960105887e-05, 5.1711219005054035e-05, 4.667100916689692e-05, 4.569388056656046e-05, 4.3781436616238024e-05, 3.9805338987749135e-05, 3.7677812464998475e-05, 3.64902418069759e-05, 3.76854741132999e-05, 4.231833047258892e-05, 3.830644088275283e-05, 3.678344125715304e-05, 3.746143406666081e-05, 3.5478548890055906e-05, 3.4808012603606373e-05, 3.1364275995139266e-05, 3.009438038978363e-05, 2.8702780521095514e-05, 2.8291997789385533e-05, 2.7680208173955083e-05, 2.716981479454425e-05, 2.5494297629792336e-05, 2.443814903750363e-05, 2.354119957510194e-05, 2.2863483693489922e-05, 2.1237036536213383e-05, 1.934085338016635e-05, 1.764964096511704e-05, 1.697365103798501e-05, 1.812367428450557e-05, 1.656351970135677e-05, 1.4938231131364819e-05, 1.3479473967815314e-05, 1.2714048994703902e-05, 1.1480842660158475e-05, 1.1575367116006833e-05, 1.0937524771819344e-05, 1.2593466658960565e-05, 1.2681190137858375e-05, 1.1505283638297588e-05, 1.2353473183266106e-05, 1.1570226352891101e-05, 1.1139456375979606e-05, 1.0635630722076184e-05, 1.2776348161318105e-05, 1.2959650755246015e-05, 1.2143282201789388e-05, 1.1579889226804597e-05, 1.314714298123883e-05, 1.2747007323361097e-05, 1.2251690044507513e-05, 1.2095038172967495e-05, 1.0894221991908758e-05, 9.869792558690584e-06, 8.985987763401445e-06, 9.008365609034308e-06, 8.35418919646814e-06, 9.285907051068346e-06, 9.689148509997992e-06, 8.762547844179044e-06, 7.941304638728978e-06, 7.2432211444182894e-06, 6.526702062347476e-06, 6.152348799938126e-06, 5.903378812818891e-06, 5.5945356478018156e-06, 5.137573225235833e-06, 4.862294249234147e-06, 6.68497000391198e-06, 7.925498891884724e-06, 7.248534121499416e-06, 6.821775201338981e-06, 6.299291881392287e-06, 6.4979069164736305e-06, 6.408565206991472e-06, 5.805254826995399e-06, 5.502809031553269e-06, 6.231847342295438e-06, 6.389845123849882e-06, 5.82164268507622e-06, 5.344551631148289e-06, 6.3243707352806815e-06, 5.93536243923539e-06, 5.395781035350135e-06, 5.2045745040620875e-06, 4.715895789935995e-06, 4.770141779889065e-06, 4.47524174071263e-06, 4.0305876929808724e-06, 4.020707751048694e-06, 4.059399567167082e-06, 4.155251595065957e-06, 4.056092754808598e-06, 3.7526687430950137e-06, 3.383700029937783e-06, 3.120656942731714e-06, 3.4006581610749997e-06, 3.1665127396554813e-06, 3.0571246480733877e-06, 3.673633854609507e-06, 3.3107119319366826e-06, 3.4340482018454082e-06], "duration": 123285.894788, "accuracy_train": [0.4939764705882353, 0.5550588235294117, 0.6474117647058824, 0.6792235294117647, 0.6988, 0.7202823529411765, 0.7276705882352941, 0.7300470588235294, 0.758964705882353, 0.7599529411764706, 0.7667764705882353, 0.7772235294117648, 0.7849882352941177, 0.7931058823529412, 0.7995529411764706, 0.8042117647058824, 0.8092705882352941, 0.8101411764705883, 0.8118117647058823, 0.8211764705882353, 0.8228705882352941, 0.8243764705882353, 0.8269411764705883, 0.8319294117647059, 0.8392470588235295, 0.8341647058823529, 0.8405176470588235, 0.8300470588235294, 0.8514588235294117, 0.8494823529411765, 0.8512705882352941, 0.851835294117647, 0.8539058823529412, 0.8598588235294118, 0.8614352941176471, 0.868164705882353, 0.8652470588235294, 0.8722352941176471, 0.8690588235294118, 0.8741882352941176, 0.8770823529411764, 0.8759058823529412, 0.8747058823529412, 0.8840705882352942, 0.8798117647058824, 0.8823294117647059, 0.8852, 0.8805647058823529, 0.8876941176470589, 0.8877647058823529, 0.8826823529411765, 0.8911529411764706, 0.8966117647058823, 0.896, 0.9006588235294117, 0.8879294117647059, 0.8937176470588235, 0.9021176470588236, 0.8966352941176471, 0.9028, 0.9060941176470588, 0.9068235294117647, 0.902164705882353, 0.9036470588235294, 0.9045176470588235, 0.9085411764705882, 0.9106823529411765, 0.9146588235294117, 0.9162588235294118, 0.9073411764705882, 0.9082588235294118, 0.9072705882352942, 0.9071764705882353, 0.9174588235294118, 0.918, 0.9146588235294117, 0.9192941176470588, 0.9176705882352941, 0.9167529411764705, 0.9181882352941176, 0.9188470588235295, 0.922, 0.9195058823529412, 0.9208705882352941, 0.9217176470588235, 0.9273882352941176, 0.9253882352941176, 0.9230117647058823, 0.9271529411764706, 0.9271529411764706, 0.9254588235294118, 0.930564705882353, 0.9210823529411765, 0.9281176470588235, 0.9297411764705882, 0.9301882352941176, 0.9338117647058823, 0.9301882352941176, 0.9335058823529412, 0.9305411764705882, 0.9369176470588235, 0.9349882352941177, 0.9384, 0.9339294117647059, 0.9389176470588235, 0.939764705882353, 0.9395058823529412, 0.9361176470588235, 0.9381411764705883, 0.9372, 0.9326117647058824, 0.9375529411764706, 0.9414117647058824, 0.9358588235294117, 0.9411764705882353, 0.9408470588235294, 0.9380705882352941, 0.9407058823529412, 0.9419764705882353, 0.9444941176470588, 0.9475529411764706, 0.9372941176470588, 0.943764705882353, 0.9461647058823529, 0.9443294117647059, 0.946, 0.9415529411764706, 0.9455294117647058, 0.9456705882352942, 0.9470823529411765, 0.9473411764705882, 0.9478823529411765, 0.9467764705882353, 0.9477882352941176, 0.9399764705882353, 0.9480235294117647, 0.9468470588235294, 0.946, 0.9462117647058823, 0.9482117647058823, 0.9507058823529412, 0.9473176470588235, 0.9452235294117647, 0.9463764705882353, 0.9432705882352941, 0.9462117647058823, 0.9493411764705882, 0.9484, 0.951764705882353, 0.9506588235294118, 0.9461647058823529, 0.9517882352941176, 0.9497882352941176, 0.9506117647058824, 0.9506588235294118, 0.9542823529411765, 0.9529411764705882, 0.9516235294117648, 0.9522352941176471, 0.9553176470588235, 0.9535529411764706, 0.9477411764705882, 0.953835294117647, 0.9504235294117647, 0.9515529411764706, 0.9518588235294118, 0.9540941176470589, 0.9528705882352941, 0.9558117647058824, 0.9556705882352942, 0.9528941176470588, 0.9530588235294117, 0.9533882352941176, 0.9527529411764706, 0.9507294117647059, 0.9543294117647059, 0.9542823529411765, 0.9516235294117648, 0.9542117647058823, 0.9578117647058824, 0.9578588235294118, 0.9548470588235294, 0.9556470588235294, 0.9553411764705882, 0.9571764705882353, 0.9569411764705882, 0.9553411764705882, 0.9565176470588236, 0.9587058823529412, 0.9582588235294117, 0.9564941176470588, 0.9567764705882353, 0.9599058823529412, 0.9578588235294118, 0.9571529411764705, 0.9584235294117647, 0.9572470588235295, 0.9591294117647059, 0.9583764705882353, 0.9569176470588235, 0.9549882352941177, 0.9590823529411765, 0.9547294117647059, 0.9587294117647058, 0.9559764705882353, 0.9572, 0.9560470588235294, 0.959435294117647, 0.9582117647058823, 0.9587529411764706, 0.9605882352941176, 0.9579294117647059, 0.9599764705882353], "end": "2016-02-08 09:32:15.912000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0], "moving_var_accuracy_valid": [0.020955457599999994, 0.040623242415999995, 0.062024013700959994, 0.0790517738513776, 0.09158787405145587, 0.10080443347279926, 0.1056177826651162, 0.10711997635567806, 0.10759580104237007, 0.10572404415473272, 0.10252834141721301, 0.09872985487769322, 0.09460024455838512, 0.08985806267998094, 0.08495519775452945, 0.07992513212034, 0.07485956926848764, 0.06976395860038284, 0.0647272649739866, 0.060014999289740446, 0.05538561365143339, 0.051017913886515505, 0.046854691251808994, 0.04296968556338577, 0.03947378905934168, 0.036068037779262124, 0.03290162962669367, 0.029927134540493016, 0.02736999218680493, 0.02495209067603402, 0.02272052098401907, 0.02065391174415765, 0.018788458761680223, 0.017095824978529843, 0.015527409188285135, 0.014116117039506559, 0.012801801885057395, 0.011611444915087194, 0.010516393422528664, 0.00952806449303703, 0.008625968189616777, 0.007801922990781511, 0.007045041198711753, 0.0063839030709288294, 0.005768617680792276, 0.005218859199211106, 0.004719683017115885, 0.004251435471549887, 0.003846506242054842, 0.0034742913277534832, 0.003136433714052725, 0.002831697036480311, 0.002567241328599312, 0.002330349707662053, 0.002106605246725374, 0.001896284879738381, 0.0017118771933591732, 0.0015465285432772165, 0.0013964329530489722, 0.0012680879783425367, 0.0011490363396913542, 0.0010392704122089919, 0.0009379033256973801, 0.0008470713493088462, 0.0007666407909619042, 0.0006935912340732805, 0.0006305375867963974, 0.0005709432928683759, 0.0005195800429110974, 0.00046772099325120923, 0.0004209558017792094, 0.00037898610616400873, 0.0003414466823249342, 0.00030878892542597527, 0.00027894523894124543, 0.0002526781400492897, 0.00022956574677235426, 0.00020985393162771318, 0.00019056163244059147, 0.00017423150299857344, 0.00016021593376356076, 0.00014515742178490364, 0.00013064303161020334, 0.00011785635981125995, 0.00010714032784852253, 9.97052734456438e-05, 8.985704700593842e-05, 8.097713382523756e-05, 7.317892437548396e-05, 6.723588802182939e-05, 6.208804797363428e-05, 5.5901815389046165e-05, 5.2668523443877745e-05, 4.7492690862881075e-05, 4.294455130041862e-05, 4.085124583057198e-05, 3.854905247227294e-05, 3.644113864423052e-05, 3.301751534284146e-05, 3.0604273280843525e-05, 2.86428774347748e-05, 2.5925671391881242e-05, 2.383035641436425e-05, 2.150237903968327e-05, 2.3081599190633713e-05, 2.2784340338294037e-05, 2.4490945082975652e-05, 2.214641836194031e-05, 2.0190824456446976e-05, 1.890802744502808e-05, 1.7676996997180392e-05, 1.59101361344117e-05, 1.6923904141599182e-05, 1.5625219493457146e-05, 1.6358789826608045e-05, 1.4747322536401106e-05, 1.3276116484008038e-05, 1.1952934584223257e-05, 1.0790005395225231e-05, 1.0285704319923673e-05, 1.4465937785060813e-05, 1.3748749464730046e-05, 1.2470815123594347e-05, 1.1928968129623859e-05, 1.073608892155923e-05, 1.0908435518103673e-05, 9.893436362586472e-06, 9.596551782767987e-06, 9.259302610055799e-06, 9.161534977873891e-06, 8.264711090625526e-06, 7.907640183569163e-06, 8.150382397210583e-06, 7.434539913104216e-06, 6.918177990615134e-06, 6.703996537262607e-06, 6.102241490106822e-06, 5.800932139181245e-06, 6.4313250368746694e-06, 6.0688817982218065e-06, 5.875672792323545e-06, 5.539245746731141e-06, 6.6987366105893845e-06, 6.363036500983568e-06, 5.766840809758745e-06, 6.581983912372036e-06, 6.172007164393647e-06, 5.571292140615167e-06, 5.64707380350833e-06, 5.095822716453059e-06, 4.617075405221307e-06, 4.2348698837545836e-06, 4.188090274366325e-06, 3.769339182936076e-06, 5.446323001917238e-06, 5.038468353680061e-06, 4.927937179476055e-06, 5.9617669218723965e-06, 6.786476743555789e-06, 7.4367965053282795e-06, 7.534973842203881e-06, 9.387199181631685e-06, 1.3058935920908109e-05, 1.3790862370264495e-05, 1.2417435669703235e-05, 1.1296592192217542e-05, 1.2336474056212381e-05, 1.1157750046039173e-05, 1.2084239918523243e-05, 1.1074884040135378e-05, 1.0159631014748946e-05, 1.0132900845711145e-05, 1.0719477262986913e-05, 9.666676475445491e-06, 8.889638404714032e-06, 8.760916093556388e-06, 8.171342232009318e-06, 8.350718710841082e-06, 8.190626661697804e-06, 8.110157107437145e-06, 8.246791001545436e-06, 7.437456752395714e-06, 6.722107741359222e-06, 6.839041745741125e-06, 6.2332602755354435e-06, 5.793500264417695e-06, 7.820432936854643e-06, 7.102377747455462e-06, 6.596748887461004e-06, 6.588489981575811e-06, 6.438875780939006e-06, 6.2543384114291e-06, 6.0513765125826356e-06, 5.457259782475533e-06, 4.945950540503215e-06, 6.0719717284605864e-06, 6.0821475073751605e-06, 5.8186869862840964e-06, 5.344692421749809e-06, 4.897601228191067e-06, 5.980261793694708e-06, 5.643800558602971e-06, 6.8742412890023805e-06, 6.411741324600925e-06, 6.163489979059353e-06, 5.588819012404744e-06, 5.061005832940448e-06, 6.749341896788529e-06, 6.573936071858047e-06, 6.04896866928167e-06, 5.681741413086656e-06, 5.215176696597165e-06, 4.709759109191388e-06], "accuracy_test": 0.7159, "start": "2016-02-06 23:17:30.017000", "learning_rate_per_epoch": [0.008617016486823559, 0.006093150936067104, 0.004975036717951298, 0.004308508243411779, 0.0038536470383405685, 0.003517882199957967, 0.003256926080211997, 0.003046575468033552, 0.0028723387513309717, 0.0027249399572610855, 0.002598128281533718, 0.002487518358975649, 0.002389930421486497, 0.0023029944859445095, 0.0022249040193855762, 0.0021542541217058897, 0.00208993349224329, 0.0020310503896325827, 0.0019768790807574987, 0.0019268235191702843, 0.0018803871935233474, 0.0018371541518718004, 0.001796772121451795, 0.0017589410999789834, 0.0017234033439308405, 0.0016899360343813896, 0.0016583455726504326, 0.0016284630401059985, 0.0016001397743821144, 0.001573244808241725, 0.001547661842778325, 0.001523287734016776, 0.0015000300481915474, 0.0014778061304241419, 0.0014565415913239121, 0.0014361693756654859, 0.0014166288310661912, 0.0013978646602481604, 0.0013798269210383296, 0.0013624699786305428, 0.0013457519235089421, 0.0013296345714479685, 0.0013140826486051083, 0.001299064140766859, 0.0012845490127801895, 0.0012705097906291485, 0.0012569210957735777, 0.0012437591794878244, 0.001231002388522029, 0.0012186301173642278, 0.0012066236231476068, 0.0011949652107432485, 0.0011836382327601314, 0.0011726274387910962, 0.0011619182769209146, 0.0011514972429722548, 0.0011413517640903592, 0.001131469733081758, 0.0011218399740755558, 0.0011124520096927881, 0.0011032959446310997, 0.0010943622328341007, 0.0010856420267373323, 0.0010771270608529449, 0.0010688093025237322, 0.0010606814175844193, 0.001052736071869731, 0.001044966746121645, 0.001037366921082139, 0.0010299304267391562, 0.001022651675157249, 0.0010155251948162913, 0.0010085455141961575, 0.0010017078602686524, 0.0009950073435902596, 0.0009884395403787494, 0.0009820001432672143, 0.0009756850195117295, 0.000969490094576031, 0.0009634117595851421, 0.0009574462892487645, 0.0009515903075225651, 0.000945840438362211, 0.0009401935967616737, 0.0009346466977149248, 0.0009291967726312578, 0.0009238411439582705, 0.0009185770759359002, 0.0009134019492194057, 0.0009083133190870285, 0.0009033087990246713, 0.0008983860607258976, 0.0008935430087149143, 0.000888777372892946, 0.0008840872324071825, 0.0008794705499894917, 0.0008749254629947245, 0.0008704501087777317, 0.0008660427411086857, 0.0008617016719654202, 0.0008574252133257687, 0.0008532117935828865, 0.0008490598411299288, 0.0008449680171906948, 0.0008409346919506788, 0.0008369586430490017, 0.0008330384152941406, 0.0008291727863252163, 0.0008253604755736887, 0.0008216003188863397, 0.000817891035694629, 0.0008142315200529993, 0.0008106207242235541, 0.0008070575422607362, 0.0008035409264266491, 0.0008000698871910572, 0.000796643435023725, 0.0007932606968097389, 0.0007899206248112023, 0.0007866224041208625, 0.0007833651616238058, 0.0007801480242051184, 0.0007769702351652086, 0.0007738309213891625, 0.0007707293843850493, 0.0007676648674532771, 0.0007646365556865931, 0.000761643867008388, 0.0007586859865114093, 0.0007557623321190476, 0.0007528722053393722, 0.0007500150240957737, 0.0007471900898963213, 0.0007443968788720667, 0.000741634692531079, 0.0007389030652120709, 0.0007362014148384333, 0.0007335291593335569, 0.0007308857748284936, 0.0007282707956619561, 0.0007256836979649961, 0.0007231239578686655, 0.0007205911097116768, 0.0007180846878327429, 0.0007156042847782373, 0.0007131493766792119, 0.0007107195560820401, 0.0007083144155330956, 0.0007059334893710911, 0.0007035764283500612, 0.0007012428832240403, 0.0006989323301240802, 0.0006966444780118763, 0.0006943789776414633, 0.0006921354215592146, 0.0006899134605191648, 0.0006877128034830093, 0.0006855330429971218, 0.0006833738880231977, 0.0006812349893152714, 0.0006791160558350384, 0.0006770167383365333, 0.0006749368039891124, 0.0006728759617544711, 0.000670833804178983, 0.0006688102148473263, 0.0006668047863058746, 0.0006648172857239842, 0.0006628474220633507, 0.0006608950207009912, 0.0006589597323909402, 0.0006570413243025541, 0.0006551396218128502, 0.0006532543338835239, 0.0006513852276839316, 0.0006495320703834295, 0.0006476946291513741, 0.0006458726711571217, 0.0006440660799853504, 0.0006422745063900948, 0.0006404977757483721, 0.0006387357716448605, 0.0006369882030412555, 0.0006352548953145742, 0.0006335356738418341, 0.0006318303057923913, 0.0006301386747509241, 0.0006284605478867888, 0.0006267957505770028, 0.0006251441081985831, 0.0006235054461285472, 0.0006218795897439122, 0.0006202664226293564, 0.000618665711954236, 0.0006170773995108902, 0.0006155011942610145, 0.0006139369797892869, 0.0006123846978880465, 0.0006108441157266498, 0.0006093150586821139, 0.000607797468546778, 0.0006062911706976593, 0.000604795990511775, 0.0006033118115738034, 0.0006018385174684227, 0.0006003759917803109, 0.0005989240598864853, 0.0005974826053716242, 0.0005960515118204057, 0.000594630662817508, 0.0005932198837399483, 0.0005918191163800657, 0.0005904282443225384], "accuracy_train_first": 0.4939764705882353, "accuracy_train_last": 0.9599764705882353, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.5174666666666667, 0.45999999999999996, 0.3706666666666667, 0.3413333333333334, 0.32199999999999995, 0.2990666666666667, 0.2989333333333334, 0.2989333333333334, 0.2758666666666667, 0.27893333333333337, 0.27546666666666664, 0.2653333333333333, 0.25373333333333337, 0.2521333333333333, 0.24519999999999997, 0.2406666666666667, 0.23693333333333333, 0.23626666666666662, 0.2361333333333333, 0.22840000000000005, 0.2308, 0.22786666666666666, 0.22840000000000005, 0.22599999999999998, 0.21653333333333336, 0.22386666666666666, 0.22373333333333334, 0.2274666666666667, 0.21120000000000005, 0.21426666666666672, 0.21373333333333333, 0.21466666666666667, 0.21053333333333335, 0.2074666666666667, 0.20879999999999999, 0.20479999999999998, 0.2076, 0.2056, 0.2069333333333333, 0.20479999999999998, 0.2049333333333333, 0.2056, 0.20813333333333328, 0.20066666666666666, 0.20440000000000003, 0.20146666666666668, 0.20120000000000005, 0.20906666666666662, 0.19986666666666664, 0.2016, 0.20186666666666664, 0.20120000000000005, 0.19573333333333331, 0.19386666666666663, 0.1970666666666666, 0.2042666666666667, 0.19840000000000002, 0.19720000000000004, 0.19733333333333336, 0.19253333333333333, 0.19333333333333336, 0.19413333333333338, 0.1956, 0.19466666666666665, 0.1929333333333333, 0.19279999999999997, 0.19013333333333338, 0.19146666666666667, 0.18906666666666672, 0.19520000000000004, 0.19586666666666663, 0.1949333333333333, 0.19399999999999995, 0.1917333333333333, 0.19199999999999995, 0.19079999999999997, 0.1897333333333333, 0.18813333333333337, 0.18920000000000003, 0.1876, 0.1864, 0.18866666666666665, 0.1917333333333333, 0.18986666666666663, 0.18799999999999994, 0.1850666666666667, 0.18933333333333335, 0.19146666666666667, 0.18866666666666665, 0.1864, 0.1857333333333333, 0.18999999999999995, 0.19466666666666665, 0.19106666666666672, 0.18866666666666665, 0.1850666666666667, 0.1850666666666667, 0.18466666666666665, 0.18706666666666671, 0.18533333333333335, 0.18466666666666665, 0.18653333333333333, 0.18533333333333335, 0.18666666666666665, 0.18093333333333328, 0.18200000000000005, 0.17959999999999998, 0.18666666666666665, 0.18400000000000005, 0.18266666666666664, 0.18253333333333333, 0.1850666666666667, 0.17959999999999998, 0.18653333333333333, 0.17959999999999998, 0.18466666666666665, 0.18400000000000005, 0.1844, 0.18479999999999996, 0.1817333333333333, 0.1764, 0.1804, 0.18400000000000005, 0.1802666666666667, 0.18279999999999996, 0.1790666666666667, 0.18333333333333335, 0.1797333333333333, 0.17959999999999998, 0.1789333333333334, 0.18120000000000003, 0.17933333333333334, 0.17800000000000005, 0.18000000000000005, 0.18253333333333333, 0.17879999999999996, 0.18000000000000005, 0.1789333333333334, 0.1769333333333334, 0.18200000000000005, 0.17826666666666668, 0.18186666666666662, 0.17600000000000005, 0.17800000000000005, 0.1790666666666667, 0.18359999999999999, 0.1784, 0.17946666666666666, 0.17720000000000002, 0.17920000000000003, 0.18013333333333337, 0.17866666666666664, 0.17746666666666666, 0.17933333333333334, 0.17453333333333332, 0.17759999999999998, 0.18079999999999996, 0.17479999999999996, 0.17453333333333332, 0.17426666666666668, 0.17466666666666664, 0.18279999999999996, 0.17079999999999995, 0.18200000000000005, 0.17746666666666666, 0.17653333333333332, 0.17266666666666663, 0.17786666666666662, 0.1724, 0.17520000000000002, 0.17800000000000005, 0.18000000000000005, 0.17279999999999995, 0.17613333333333336, 0.17800000000000005, 0.17959999999999998, 0.17520000000000002, 0.18013333333333337, 0.1744, 0.17400000000000004, 0.17333333333333334, 0.17666666666666664, 0.1757333333333333, 0.17920000000000003, 0.17746666666666666, 0.17520000000000002, 0.18186666666666662, 0.17786666666666662, 0.17559999999999998, 0.17426666666666668, 0.1790666666666667, 0.17466666666666664, 0.17453333333333332, 0.17613333333333336, 0.1770666666666667, 0.17226666666666668, 0.17346666666666666, 0.1738666666666666, 0.17453333333333332, 0.17453333333333332, 0.17959999999999998, 0.17413333333333336, 0.18013333333333337, 0.17453333333333332, 0.1738666666666666, 0.1750666666666667, 0.17626666666666668, 0.17079999999999995, 0.17759999999999998, 0.17426666666666668, 0.1737333333333333, 0.17413333333333336, 0.17466666666666664], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.018278697376938248, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.008617016542549049, "optimization": "nesterov_momentum", "nb_data_augmentation": 1, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.131123476882714e-08, "rotation_range": [0, 0], "momentum": 0.5513243827213952}, "accuracy_valid_max": 0.8292, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8253333333333334, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.4825333333333333, 0.54, 0.6293333333333333, 0.6586666666666666, 0.678, 0.7009333333333333, 0.7010666666666666, 0.7010666666666666, 0.7241333333333333, 0.7210666666666666, 0.7245333333333334, 0.7346666666666667, 0.7462666666666666, 0.7478666666666667, 0.7548, 0.7593333333333333, 0.7630666666666667, 0.7637333333333334, 0.7638666666666667, 0.7716, 0.7692, 0.7721333333333333, 0.7716, 0.774, 0.7834666666666666, 0.7761333333333333, 0.7762666666666667, 0.7725333333333333, 0.7888, 0.7857333333333333, 0.7862666666666667, 0.7853333333333333, 0.7894666666666666, 0.7925333333333333, 0.7912, 0.7952, 0.7924, 0.7944, 0.7930666666666667, 0.7952, 0.7950666666666667, 0.7944, 0.7918666666666667, 0.7993333333333333, 0.7956, 0.7985333333333333, 0.7988, 0.7909333333333334, 0.8001333333333334, 0.7984, 0.7981333333333334, 0.7988, 0.8042666666666667, 0.8061333333333334, 0.8029333333333334, 0.7957333333333333, 0.8016, 0.8028, 0.8026666666666666, 0.8074666666666667, 0.8066666666666666, 0.8058666666666666, 0.8044, 0.8053333333333333, 0.8070666666666667, 0.8072, 0.8098666666666666, 0.8085333333333333, 0.8109333333333333, 0.8048, 0.8041333333333334, 0.8050666666666667, 0.806, 0.8082666666666667, 0.808, 0.8092, 0.8102666666666667, 0.8118666666666666, 0.8108, 0.8124, 0.8136, 0.8113333333333334, 0.8082666666666667, 0.8101333333333334, 0.812, 0.8149333333333333, 0.8106666666666666, 0.8085333333333333, 0.8113333333333334, 0.8136, 0.8142666666666667, 0.81, 0.8053333333333333, 0.8089333333333333, 0.8113333333333334, 0.8149333333333333, 0.8149333333333333, 0.8153333333333334, 0.8129333333333333, 0.8146666666666667, 0.8153333333333334, 0.8134666666666667, 0.8146666666666667, 0.8133333333333334, 0.8190666666666667, 0.818, 0.8204, 0.8133333333333334, 0.816, 0.8173333333333334, 0.8174666666666667, 0.8149333333333333, 0.8204, 0.8134666666666667, 0.8204, 0.8153333333333334, 0.816, 0.8156, 0.8152, 0.8182666666666667, 0.8236, 0.8196, 0.816, 0.8197333333333333, 0.8172, 0.8209333333333333, 0.8166666666666667, 0.8202666666666667, 0.8204, 0.8210666666666666, 0.8188, 0.8206666666666667, 0.822, 0.82, 0.8174666666666667, 0.8212, 0.82, 0.8210666666666666, 0.8230666666666666, 0.818, 0.8217333333333333, 0.8181333333333334, 0.824, 0.822, 0.8209333333333333, 0.8164, 0.8216, 0.8205333333333333, 0.8228, 0.8208, 0.8198666666666666, 0.8213333333333334, 0.8225333333333333, 0.8206666666666667, 0.8254666666666667, 0.8224, 0.8192, 0.8252, 0.8254666666666667, 0.8257333333333333, 0.8253333333333334, 0.8172, 0.8292, 0.818, 0.8225333333333333, 0.8234666666666667, 0.8273333333333334, 0.8221333333333334, 0.8276, 0.8248, 0.822, 0.82, 0.8272, 0.8238666666666666, 0.822, 0.8204, 0.8248, 0.8198666666666666, 0.8256, 0.826, 0.8266666666666667, 0.8233333333333334, 0.8242666666666667, 0.8208, 0.8225333333333333, 0.8248, 0.8181333333333334, 0.8221333333333334, 0.8244, 0.8257333333333333, 0.8209333333333333, 0.8253333333333334, 0.8254666666666667, 0.8238666666666666, 0.8229333333333333, 0.8277333333333333, 0.8265333333333333, 0.8261333333333334, 0.8254666666666667, 0.8254666666666667, 0.8204, 0.8258666666666666, 0.8198666666666666, 0.8254666666666667, 0.8261333333333334, 0.8249333333333333, 0.8237333333333333, 0.8292, 0.8224, 0.8257333333333333, 0.8262666666666667, 0.8258666666666666, 0.8253333333333334], "seed": 257214246, "model": "residualv3", "loss_std": [0.26521822810173035, 0.2531423568725586, 0.25175824761390686, 0.25477704405784607, 0.2525511085987091, 0.25321194529533386, 0.2503584027290344, 0.24866227805614471, 0.24504166841506958, 0.244956374168396, 0.24151135981082916, 0.23916296660900116, 0.23779696226119995, 0.23575150966644287, 0.2354366034269333, 0.23327116668224335, 0.2335905134677887, 0.23124626278877258, 0.22790366411209106, 0.23065759241580963, 0.22507959604263306, 0.22617991268634796, 0.22506527602672577, 0.22156491875648499, 0.21810093522071838, 0.22068510949611664, 0.21844245493412018, 0.21742160618305206, 0.2165173590183258, 0.21506240963935852, 0.2123945951461792, 0.2119719535112381, 0.21080836653709412, 0.20701389014720917, 0.20526400208473206, 0.20783095061779022, 0.20289219915866852, 0.20233356952667236, 0.20224477350711823, 0.20066475868225098, 0.1965891569852829, 0.1978573352098465, 0.19672846794128418, 0.19704636931419373, 0.192812979221344, 0.19144068658351898, 0.19204913079738617, 0.18859121203422546, 0.189467653632164, 0.18803679943084717, 0.18521837890148163, 0.18558114767074585, 0.18718163669109344, 0.18419769406318665, 0.1826699674129486, 0.18022170662879944, 0.1775522083044052, 0.1783323436975479, 0.17994460463523865, 0.17804916203022003, 0.17734216153621674, 0.17313361167907715, 0.17453423142433167, 0.1740596741437912, 0.17166976630687714, 0.17507615685462952, 0.1711718589067459, 0.16951529681682587, 0.16678428649902344, 0.16679447889328003, 0.16668134927749634, 0.16635586321353912, 0.16352201998233795, 0.16390860080718994, 0.16194351017475128, 0.16288000345230103, 0.1601552665233612, 0.1619960516691208, 0.1612710803747177, 0.15765313804149628, 0.15578238666057587, 0.16268552839756012, 0.15486347675323486, 0.1569911539554596, 0.15757893025875092, 0.1555347591638565, 0.15457332134246826, 0.15303783118724823, 0.1504317820072174, 0.15296043455600739, 0.15317590534687042, 0.15174567699432373, 0.1504664123058319, 0.1507280021905899, 0.1477172076702118, 0.14931243658065796, 0.14653165638446808, 0.14613384008407593, 0.14629846811294556, 0.14734193682670593, 0.14530347287654877, 0.14715687930583954, 0.14288757741451263, 0.14611020684242249, 0.1451510190963745, 0.14248095452785492, 0.14443400502204895, 0.14328943192958832, 0.14211077988147736, 0.14036953449249268, 0.13975705206394196, 0.1406988501548767, 0.1392705887556076, 0.13938705623149872, 0.1368618607521057, 0.13689567148685455, 0.14047297835350037, 0.13690727949142456, 0.14145183563232422, 0.138346865773201, 0.13701021671295166, 0.13627949357032776, 0.1353124976158142, 0.1361415833234787, 0.13406772911548615, 0.13264204561710358, 0.13355842232704163, 0.13439440727233887, 0.13151872158050537, 0.13219942152500153, 0.1337180733680725, 0.13421393930912018, 0.1321803778409958, 0.13334695994853973, 0.13240844011306763, 0.13003095984458923, 0.13297469913959503, 0.13053379952907562, 0.1284981667995453, 0.13089396059513092, 0.1286991536617279, 0.1285368651151657, 0.12898477911949158, 0.12739165127277374, 0.12848129868507385, 0.12840618193149567, 0.12644270062446594, 0.12794730067253113, 0.12827378511428833, 0.12777884304523468, 0.12579576671123505, 0.12338808178901672, 0.12698687613010406, 0.12525129318237305, 0.12509649991989136, 0.12507349252700806, 0.1251496970653534, 0.12474767863750458, 0.12203662097454071, 0.12145543098449707, 0.12228067219257355, 0.12231024354696274, 0.12330909818410873, 0.12340645492076874, 0.12178222090005875, 0.12333886325359344, 0.12265145778656006, 0.11987783014774323, 0.1208849623799324, 0.12247538566589355, 0.1209692656993866, 0.125107541680336, 0.12188255786895752, 0.12135719507932663, 0.12167248129844666, 0.12104986608028412, 0.12061270326375961, 0.1185530498623848, 0.1207519993185997, 0.11782151460647583, 0.11984191089868546, 0.11868447065353394, 0.12143633514642715, 0.11904981732368469, 0.11804288625717163, 0.11711250990629196, 0.11728516221046448, 0.1186898872256279, 0.1147623062133789, 0.11861781775951385, 0.1141255721449852, 0.11543496698141098, 0.11678053438663483, 0.11619023978710175, 0.1141170933842659, 0.11913624405860901, 0.117276631295681, 0.11573274433612823, 0.11575574427843094, 0.11569487303495407, 0.11642903089523315, 0.11431325227022171, 0.11756587773561478, 0.11405535042285919, 0.11365888267755508, 0.11462382227182388, 0.11320554465055466, 0.11643808335065842, 0.11253629624843597, 0.11666731536388397, 0.11589861661195755, 0.11453994363546371, 0.11495624482631683]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:46 2016", "state": "available"}], "summary": "716fe1235bf58ccd235b5628d0762b10"}