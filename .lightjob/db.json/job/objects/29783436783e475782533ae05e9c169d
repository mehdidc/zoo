{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 16, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.5640634298324585, 1.1518985033035278, 0.996406078338623, 0.8900538682937622, 0.8106955885887146, 0.7426751255989075, 0.6820443868637085, 0.621597170829773, 0.5647069811820984, 0.5086765885353088, 0.45766323804855347, 0.4103899896144867, 0.3697996735572815, 0.3335530161857605, 0.3043593764305115, 0.27400392293930054, 0.24695153534412384, 0.2183433324098587, 0.19086667895317078, 0.1673310548067093, 0.16075201332569122, 0.1431012600660324, 0.12882088124752045, 0.11480781435966492, 0.10525546967983246, 0.10737238824367523, 0.10409902036190033, 0.09717965126037598, 0.08876869082450867, 0.10081160813570023, 0.07825042307376862, 0.07803982496261597, 0.06954008340835571, 0.07013124972581863, 0.07110230624675751, 0.06436973065137863, 0.06777630746364594, 0.06323426961898804, 0.060485050082206726, 0.059764616191387177, 0.05871310085058212, 0.064076729118824, 0.05637277290225029, 0.04655974358320236, 0.057528119534254074, 0.061906348913908005, 0.05491885170340538, 0.04396704211831093, 0.042205940932035446, 0.052935853600502014, 0.04436083510518074, 0.05013349652290344, 0.05946510657668114, 0.0463312566280365, 0.041262879967689514, 0.048653148114681244, 0.0408671610057354, 0.04226863756775856, 0.04519558697938919, 0.04004209116101265, 0.0391785092651844, 0.03656603395938873, 0.04586101323366165, 0.048823609948158264, 0.030876843258738518, 0.027160558849573135, 0.03721623867750168, 0.05661653354763985, 0.03530457615852356, 0.03255850076675415, 0.03814709931612015, 0.033453233540058136, 0.027538955211639404, 0.037428274750709534, 0.047121964395046234, 0.03219000995159149, 0.026102129369974136, 0.028923291712999344, 0.03893359750509262, 0.03886125236749649, 0.03263188898563385, 0.026950355619192123, 0.03747381642460823, 0.02771754376590252, 0.04015909880399704, 0.03177377209067345, 0.029028652235865593, 0.029881956055760384, 0.03554154187440872, 0.03234051913022995, 0.13036367297172546, 0.056645121425390244, 0.036204975098371506, 0.02486022189259529, 0.01362200453877449, 0.004959708079695702, 0.003337086411193013, 0.0031001719180494547, 0.0030243482906371355, 0.002974890870973468, 0.0029373690485954285, 0.0029072356410324574, 0.0028821094892919064, 0.002860692096874118, 0.002842092653736472, 0.0028257782105356455, 0.002811310812830925, 0.0027983426116406918, 0.002786624478176236, 0.0027759154327213764, 0.0027660084888339043, 0.0027567222714424133, 0.0027479014825075865, 0.0027393540367484093, 0.0027309097349643707, 0.002722403034567833, 0.00271615874953568, 0.002715174574404955, 0.0027150714304298162, 0.002715064911171794, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.002715064911171794, 0.002715064911171794, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503, 0.0027150646783411503], "moving_avg_accuracy_train": [0.055423526756875216, 0.11388459006725266, 0.17061659251424646, 0.22441101340078784, 0.27559946214261194, 0.32369362174461136, 0.3684128740554308, 0.4083976756126046, 0.44313553629862173, 0.4750785904172368, 0.5057051581416169, 0.5332970790257294, 0.5590365077106888, 0.5842198982544501, 0.6102857772805186, 0.6320507558982125, 0.6494909433494118, 0.6685977448708753, 0.6882599118578097, 0.7064862484163532, 0.7218301160477246, 0.7391080700610307, 0.7541581053860461, 0.7692792996808672, 0.7820609820582031, 0.7947477446001218, 0.8066565814819331, 0.8179998555100503, 0.8265791251616108, 0.8365531044586187, 0.8464271572175834, 0.8527960650769436, 0.8587672480383799, 0.8637161709597172, 0.8710344965317041, 0.8776512164810161, 0.8834178913330067, 0.8874918633200456, 0.8940226970023452, 0.8960183500249789, 0.8983325493928668, 0.9033422225405126, 0.9079090931424506, 0.9099478935342779, 0.9142518695809054, 0.917909137085947, 0.922207359128552, 0.9257713086681238, 0.9293533564073468, 0.9311309928619426, 0.9351348532472138, 0.9350370151285647, 0.9373225772681169, 0.937127126827204, 0.9395268374325972, 0.9409241805096586, 0.9436535003289863, 0.9469119203103918, 0.9479590549508643, 0.9499546243427288, 0.9519923441251411, 0.9531080732889006, 0.9546978425969153, 0.9560171359777275, 0.9575578505418688, 0.9595956074139, 0.9578838448106606, 0.9594216473450984, 0.9610497021046362, 0.962856784312039, 0.9635322084844251, 0.9637496454836663, 0.9609557381462169, 0.9586065954985388, 0.9604138116927324, 0.9616358385211059, 0.9638237602118617, 0.9635237247395404, 0.9641255515203851, 0.9646904831600595, 0.9659801716357663, 0.965769269759196, 0.96617211867498, 0.9656559927420981, 0.9667723643000681, 0.9683466880629369, 0.9699960222328337, 0.9702249868238729, 0.9701404474034365, 0.9691714687333679, 0.967202494473311, 0.9694802140355314, 0.9711557766343777, 0.9735960955483209, 0.9759156154577746, 0.978265925191759, 0.9804091057380593, 0.9823449436761581, 0.9840895229692566, 0.9856642946306643, 0.9870815891259312, 0.9883618044692906, 0.989518648575933, 0.9905621334207205, 0.9915012697810294, 0.9923464925053074, 0.9931071929571575, 0.9937918233638227, 0.9944079907298213, 0.9949625413592201, 0.995461636925679, 0.995910822935492, 0.9963150903443238, 0.9966789310122722, 0.9970063876134259, 0.9973010985544641, 0.9975663384013986, 0.9978050542636396, 0.9980198985396566, 0.9982132583880718, 0.9983872822516455, 0.9985439037288619, 0.9986848630583567, 0.9988117264549019, 0.9989259035117927, 0.9990286628629943, 0.9991211462790758, 0.999204381353549, 0.999279292920575, 0.9993467133308984, 0.9994073917001894, 0.9994620022325514, 0.9995111517116771, 0.9995553862428903, 0.9995951973209822, 0.999631027291265, 0.9996632742645194, 0.9996922965404483, 0.9997184165887844, 0.9997419246322868, 0.999763081871439, 0.999782123386676, 0.9997992607503894, 0.9998146843777314, 0.9998285656423391, 0.9998410587804861, 0.9998523026048184, 0.9998624220467175], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05449733504329818, 0.11108726409544425, 0.1661107913097703, 0.2174915525873023, 0.26543410360681, 0.309226283654638, 0.3488060261005447, 0.3834543203278396, 0.41233256817080866, 0.4373727547366344, 0.46050927326711255, 0.48009408224499467, 0.4973643474015193, 0.5142005018462318, 0.5311923316032502, 0.5451157318672475, 0.5548756489384293, 0.5659739097693605, 0.5772136387566563, 0.5877688479702227, 0.5957211847469955, 0.6052964566149164, 0.6122508094474006, 0.618956386682254, 0.624844921818622, 0.6307621615644706, 0.636614491624138, 0.6413210947489982, 0.6445916525839629, 0.6497416858383679, 0.6546696845173323, 0.656331828217331, 0.6588916812107335, 0.6599901413370246, 0.6627223019021173, 0.6653949429939087, 0.6672133529678611, 0.6685650421910297, 0.6726321248977551, 0.6731021990702537, 0.6740635517818278, 0.6758320895347444, 0.6773443542729115, 0.6776210257733312, 0.6799462549448686, 0.6819433639665715, 0.6847265901640408, 0.6862324291239168, 0.6878380018647631, 0.6887722339636783, 0.6912489321128676, 0.6917007343552706, 0.6935783771753911, 0.6928867080384242, 0.6945826593166602, 0.6948272771858225, 0.6966953824868186, 0.6981345956500343, 0.6988785120733593, 0.6992174175019421, 0.700481787385784, 0.700670513297055, 0.7025952376638857, 0.7025279084569851, 0.7029423568808649, 0.7040641074032453, 0.7026930176343064, 0.7036545375952733, 0.7049167076121015, 0.7058389640440389, 0.7067829761486711, 0.7066599955048131, 0.7045442712743619, 0.7033969554044558, 0.7053745073696578, 0.7065938102094993, 0.7080370976749951, 0.7073198372203722, 0.7078267642193441, 0.7074955714664609, 0.708307455396622, 0.7074003497289477, 0.7083427966367006, 0.7079825027599281, 0.7078645812207576, 0.7091075493976275, 0.709892512878422, 0.7095154956832004, 0.7089463056310701, 0.7077097017057643, 0.706791188236995, 0.7091607092852835, 0.7106870447008816, 0.7131564379341067, 0.7162019104810575, 0.7188745940564458, 0.7214519372204549, 0.723795960130563, 0.7257346823121603, 0.7274429111818479, 0.729031204306886, 0.7304728751506703, 0.7317581718788261, 0.7328650813005068, 0.7338124716550194, 0.7347139510990808, 0.7354876319963263, 0.7361717377725973, 0.7368362610962411, 0.7374587461500206, 0.7380200122070818, 0.7385861868146869, 0.7389970582028718, 0.7393790494834882, 0.739722841636043, 0.7399956334795923, 0.7402655602012866, 0.7405207012820616, 0.7407503282547591, 0.7409569925301868, 0.7411429903780717, 0.7413103884411681, 0.741461046697955, 0.7415966391290632, 0.7417186723170605, 0.7418285021862581, 0.7419273490685359, 0.7420163112625859, 0.742096377237231, 0.7421684366144115, 0.742233290053874, 0.7422916581493901, 0.7423441894353547, 0.7423914675927229, 0.7424340179343543, 0.7424723132418225, 0.7425067790185439, 0.7425377982175931, 0.7425657154967374, 0.7425908410479674, 0.7426134540440742, 0.7426338057405704, 0.742652122267417, 0.7426686071415789, 0.7426834435283246, 0.7426967962763957, 0.7427088137496598, 0.7427196294755974], "moving_var_accuracy_train": [0.027645905863530575, 0.055640578587597155, 0.07904320164364885, 0.0971834389459492, 0.1110474106127035, 0.12076010324183648, 0.12668239666280137, 0.12840321619662035, 0.12642336526232853, 0.12296425709391864, 0.11910971123971112, 0.11405056699841618, 0.10860817399982754, 0.10345518503336125, 0.09922453697463986, 0.09356551192523388, 0.08694640197770725, 0.08153739055936189, 0.07686305879902448, 0.07216654701822973, 0.06706880078140816, 0.06304866995724065, 0.058782335031074495, 0.054961956180082594, 0.050936103201630244, 0.0472910783756225, 0.043838354100958564, 0.040612547481955315, 0.037213727543547466, 0.03438767715634702, 0.03182638170169367, 0.029008810417413478, 0.026428824609302657, 0.024006368691104434, 0.02208775282459237, 0.02027300638812175, 0.01854499659894681, 0.016839872168812733, 0.015539751049204194, 0.014021619623164498, 0.012667657329277042, 0.011626763021765525, 0.010651793483442586, 0.009624024498437763, 0.008828339936883483, 0.008065886393626036, 0.0074255701688112526, 0.00679732877881566, 0.006233075494988751, 0.005638207867772243, 0.005218665162857717, 0.004696884797249092, 0.004274210466167972, 0.003847133227424852, 0.003514247403589101, 0.0031803957723052943, 0.002929398875160335, 0.0027320146946213076, 0.002468681643756673, 0.0022576541541607232, 0.0020692594559493585, 0.001873537174456192, 0.001708929755084923, 0.0015537015947983286, 0.001419695647631908, 0.001315098160494314, 0.0012099595253335226, 0.001110247102514481, 0.0010230774529635153, 0.0009501596226059679, 0.0008592494406591607, 0.0007737500062309956, 0.0007666282695001737, 0.00073963168316242, 0.0006950627881991831, 0.0006389966555026459, 0.000618180001876298, 0.0005571721932505277, 0.0005047147331927529, 0.0004571155896910235, 0.00042637369800126125, 0.00038413664461500323, 0.00034718356539403794, 0.00031486268266197313, 0.00029459298349477443, 0.0002874401429382971, 0.0002831788574803695, 0.00025533279478788057, 0.000229863837531562, 0.0002153277307458378, 0.00022868669440215638, 0.0002525100825990342, 0.00025252666454300297, 0.0002808704057044421, 0.0003012049186271632, 0.0003208000293754052, 0.00033005903212422456, 0.000330780345615047, 0.00032509432324271433, 0.0003149040429885972, 0.00030149215186656234, 0.0002860934986082582, 0.00026952874333109107, 0.00025237561458969575, 0.00023507584706001448, 0.0002179978754367366, 0.00020140607449006906, 0.00018548393618463723, 0.00017035250257246937, 0.00015608498992032208, 0.00014271835838842065, 0.00013026243519228452, 0.00011870708091364792, 0.00010802779310716257, 9.819006422719873e-05, 8.915274865338838e-05, 8.087064337566626e-05, 7.329644640406922e-05, 6.63822243300977e-05, 6.0080494175900614e-05, 5.4345003504148844e-05, 4.913127573786299e-05, 4.439697395722121e-05, 4.010212545394615e-05, 3.620924051143367e-05, 3.2683351818624837e-05, 2.949199527701333e-05, 2.6605148447915303e-05, 2.399513928899246e-05, 2.163653496564686e-05, 1.9506018249580635e-05, 1.758225721682633e-05, 1.584577253682875e-05, 1.4278805526910773e-05, 1.2865189271669267e-05, 1.1590224425436493e-05, 1.0440560788449506e-05, 9.404085342105456e-06, 8.469817120220646e-06, 7.627809061182429e-06, 6.869056813981105e-06, 6.1854143463057e-06, 5.5695161147905236e-06, 5.01470549783494e-06, 4.514969153615458e-06, 4.064876944760762e-06, 3.6595270625552356e-06, 3.2944959842388584e-06], "duration": 46156.354545, "accuracy_train": [0.5542352675687523, 0.6400341598606497, 0.6812046145371908, 0.7085608013796604, 0.7362955008190292, 0.7565410581626061, 0.7708861448528055, 0.7682608896271688, 0.755776282472776, 0.7625660774847729, 0.7813442676610374, 0.7816243669827427, 0.7906913658753231, 0.8108704131483019, 0.8448786885151348, 0.8279355634574567, 0.8064526304102067, 0.8405589585640458, 0.8652194147402179, 0.8705232774432448, 0.8599249247300664, 0.8946096561807864, 0.889608423311185, 0.9053700483342562, 0.8970961234542267, 0.9089286074773901, 0.9138361134182356, 0.9200893217631044, 0.9037925520256552, 0.9263189181316908, 0.9352936320482651, 0.910116235811185, 0.9125078946913067, 0.9082564772517534, 0.9368994266795865, 0.9372016960248246, 0.9353179650009228, 0.924157611203396, 0.9528002001430418, 0.9139792272286821, 0.9191603437038575, 0.9484292808693245, 0.949010928559893, 0.9282970970607235, 0.9529876540005537, 0.9508245446313216, 0.9608913575119971, 0.957846854524271, 0.9615917860603543, 0.9471297209533037, 0.9711695967146549, 0.9341564720607235, 0.9578926365240864, 0.9353680728589886, 0.961124232881137, 0.9535002682032114, 0.9682173787029347, 0.9762377001430418, 0.9573832667151162, 0.9679147488695091, 0.9703318221668512, 0.9631496357627353, 0.9690057663690477, 0.9678907764050388, 0.97142428161914, 0.9779354192621816, 0.9424779813815062, 0.9732618701550388, 0.9757021949404762, 0.9791205241786637, 0.9696110260358989, 0.9657065784768365, 0.9358105721091732, 0.9374643116694352, 0.9766787574404762, 0.9726340799764673, 0.9835150554286637, 0.960823405488649, 0.9695419925479882, 0.9697748679171282, 0.9775873679171282, 0.9638711528700628, 0.9697977589170359, 0.9610108593461609, 0.9768197083217978, 0.982515601928756, 0.9848400297619048, 0.9722856681432264, 0.9693795926195091, 0.9604506607027501, 0.9494817261327981, 0.989979690095515, 0.9862358400239941, 0.9955589657738095, 0.9967912946428571, 0.9994187127976191, 0.9996977306547619, 0.9997674851190477, 0.9997907366071429, 0.9998372395833334, 0.9998372395833334, 0.9998837425595238, 0.9999302455357143, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095, 0.9999534970238095], "end": "2016-02-02 18:57:24.347000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0], "moving_var_accuracy_valid": [0.026729635741393466, 0.05287845279839646, 0.07483890444250793, 0.09111485766338571, 0.10268976568137006, 0.10968058441330553, 0.11281153008073375, 0.11233491570841116, 0.10860700292388957, 0.10338940112076286, 0.097868147416087, 0.09153341535877949, 0.08506443235009155, 0.07910909398345832, 0.07379668509153595, 0.06816176625658567, 0.06220289346205423, 0.05709114665709124, 0.05251901556075285, 0.04826982597855695, 0.04401200032260219, 0.04043597277244336, 0.036827642705067165, 0.0335495613290337, 0.030506678810600495, 0.027771134465429104, 0.025302268923031772, 0.0229714110475031, 0.020770538879719427, 0.018932190574440774, 0.017257538055815574, 0.015556648745349025, 0.014059959496944604, 0.012664823079091607, 0.011465523083363476, 0.01038325786867691, 0.009374691615309549, 0.008453666027582869, 0.007757169880514681, 0.006983441620012065, 0.006293415249335315, 0.005692223256453206, 0.005143583432552619, 0.0046299140133696575, 0.004215582828334207, 0.0038299205455018903, 0.0035166456235482192, 0.0031853890199511226, 0.0028900508923913482, 0.002608900909684006, 0.0024032171222153897, 0.002164732537390014, 0.0019799891666905643, 0.0017862959057767997, 0.0016335525718424696, 0.0014707358557754441, 0.0013550706269383863, 0.0012382055750071084, 0.00111936572231043, 0.0010084628620850927, 0.0009220042567050815, 0.000830124388260839, 0.0007804530244291996, 0.0007024485209851964, 0.000633749576351185, 0.0005816995368262121, 0.0005404485675339937, 0.0004947243964986353, 0.00045958961521119014, 0.000421285666026321, 0.0003871775291069188, 0.0003485958943451007, 0.00035402290608445514, 0.0003304676188240546, 0.000332617262917318, 0.000312735831362795, 0.0003002099565990323, 0.0002748191239770231, 0.0002496499864198998, 0.00022567218553397103, 0.00020903736662505906, 0.0001955391961934938, 0.0001839791321395399, 0.00016674952402434356, 0.00015019972102651234, 0.00014908447792226536, 0.00013972153912566762, 0.00012702866290253655, 0.00011724159245127947, 0.00011928013661888699, 0.00011494512588779208, 0.00015398228328355066, 0.00015955135316337844, 0.0001984773443097216, 0.0002621037371868269, 0.00030018250091549384, 0.0003299485308895215, 0.0003464036684285756, 0.00034559109486247526, 0.000337294398217335, 0.0003262690338549912, 0.00031234786386585145, 0.0002959809665939398, 0.00027741010614479425, 0.000257747031884726, 0.00023928631538883922, 0.00022074492302681935, 0.00020288243714228349, 0.0001865685146570557, 0.00017139905197096006, 0.00015709432305514525, 0.00014426987392630153, 0.00013136222421233253, 0.0001195392578373022, 0.00010864906945099637, 9.845390101506008e-05, 8.926425482931622e-05, 8.092370208627572e-05, 7.330588879695997e-05, 6.635969102190655e-05, 6.003507871447637e-05, 5.428376984678471e-05, 4.905967405514859e-05, 4.431917441599803e-05, 4.00212858651533e-05, 3.612772068014958e-05, 3.260288496735902e-05, 2.941382491835488e-05, 2.6530137469182114e-05, 2.392385670682071e-05, 2.1569324753629654e-05, 1.9443053789434414e-05, 1.7523584234536832e-05, 1.5791342828560294e-05, 1.4228503329860751e-05, 1.2818851772041427e-05, 1.154765760272235e-05, 1.0401551558837017e-05, 9.368410773226707e-06, 8.437251335825481e-06, 7.598128330579304e-06, 6.842043221473835e-06, 6.1608583557279464e-06, 5.5472182798403625e-06, 4.994477517201347e-06, 4.49663442841068e-06, 4.048270762542481e-06, 3.6444965056362563e-06], "accuracy_test": 0.7295041454081632, "start": "2016-02-02 06:08:07.992000", "learning_rate_per_epoch": [0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 0.0002831572783179581, 2.831572783179581e-05, 2.8315728286543163e-06, 2.8315727718108974e-07, 2.831572842865171e-08, 2.831572754047329e-09, 2.831572754047329e-10, 2.831572823436268e-11, 2.831572866804355e-12, 2.831572866804355e-13, 2.831572866804355e-14, 2.8315729091560023e-15, 2.8315729091560023e-16, 2.831572909156002e-17, 2.831572826437941e-18, 2.8315728781367293e-19, 2.8315730073837e-20, 2.8315729266043433e-21, 2.8315729266043433e-22, 2.831572989713216e-23, 2.831573029156261e-24, 2.8315730784600676e-25, 2.8315730784600676e-26, 2.8315730784600676e-27, 2.8315729821635704e-28, 2.8315729219782596e-29, 2.8315729219782596e-30, 2.8315728749584856e-31, 2.831572816183768e-32, 2.8315728529179665e-33, 2.8315728070002184e-34, 2.8315727496030333e-35, 2.8315728213495147e-36, 2.8315728661910656e-37], "accuracy_train_first": 0.5542352675687523, "accuracy_train_last": 0.9999534970238095, "batch_size_eval": 1024, "accuracy_train_std": [0.013018854067072478, 0.01625141711736966, 0.017790970938000302, 0.01919336528411194, 0.018812279772390795, 0.021254928116614103, 0.021241418121597383, 0.020846322658793888, 0.022651704225407483, 0.02352680715152941, 0.025745533565453158, 0.026827580629682244, 0.027246396278882964, 0.027585273369321782, 0.02992334002352729, 0.028153333500088796, 0.028799187914004706, 0.026949084588781105, 0.0317211884856543, 0.02648227263052635, 0.026069369900557995, 0.02818187589029132, 0.027265989472057866, 0.0269504159602425, 0.025108494779283657, 0.02377763867045688, 0.02294317132184117, 0.022788308137659242, 0.022400637165983443, 0.01895372890175539, 0.018485706855542027, 0.020436418641419103, 0.02086353009324492, 0.02108698534963869, 0.015565646753573593, 0.014805270722167495, 0.015446675211571283, 0.01666868434164235, 0.014870040385719185, 0.017425250150393052, 0.016590841135707926, 0.01533814108394602, 0.014948075602304373, 0.01545362962983132, 0.013813864909435454, 0.012914224980364411, 0.01148890732302215, 0.01257759832685783, 0.011791481489150376, 0.015617552847598708, 0.009720353543563686, 0.015503199921554679, 0.011452062773667908, 0.013468415449787424, 0.011628006446963357, 0.012363845397064959, 0.009475599409653014, 0.007778413440258957, 0.01092123055233034, 0.009538243447829256, 0.009414663701269392, 0.00915060968722779, 0.010997167321239726, 0.00948598343402295, 0.008938874664742915, 0.00804682660984131, 0.0127918451410461, 0.009371579226765254, 0.007648608728289104, 0.007862484774459036, 0.009235135006531814, 0.009375339367418169, 0.01095469850132296, 0.010011190296872238, 0.008273250643289907, 0.008439846121379128, 0.005876529678880886, 0.009065846619097297, 0.009450549069883405, 0.008852335926085413, 0.005662135403853048, 0.008842051368255038, 0.008552901145932752, 0.010055442545418352, 0.006846019241392072, 0.0055170375467365005, 0.0060920673706422995, 0.0077019068215344565, 0.010194453571062953, 0.009114419029494726, 0.00891920168791905, 0.0036368880239479734, 0.0045810939573269404, 0.0021331893979387112, 0.0016546174548602301, 0.0007701123122060637, 0.0005428114550748116, 0.0004673491267262319, 0.00045385094159093966, 0.0004217437952756942, 0.0004217437952756942, 0.00031625442961159415, 0.00025150329767466447, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793, 0.00020796763183591793], "accuracy_test_std": 0.016123192468724415, "error_valid": [0.4550266495670181, 0.37960337443524095, 0.33867746376129515, 0.3200815959149097, 0.3030829372176205, 0.2966440959149097, 0.29497629188629515, 0.30471103162650603, 0.3277632012424698, 0.33726556617093373, 0.33126205995858427, 0.34364263695406627, 0.34720326618975905, 0.3342741081513554, 0.31588120058358427, 0.32957366575677716, 0.35728509742093373, 0.33414174275225905, 0.3216288003576807, 0.3172342691076807, 0.33270778426204817, 0.30852609657379515, 0.32516001506024095, 0.32069341820406627, 0.32215826195406627, 0.3159826807228916, 0.3107145378388554, 0.31631947712725905, 0.3259733269013554, 0.30390801487198793, 0.30097832737198793, 0.3287088784826807, 0.3180696418486446, 0.3301237175263554, 0.31268825301204817, 0.3105512871799698, 0.31642095726656627, 0.31926975480045183, 0.2907641307417168, 0.32266713337725905, 0.31728427381400603, 0.30825107068900603, 0.30904526308358427, 0.3198889307228916, 0.29912668251129515, 0.30008265483810237, 0.2902243740587349, 0.3002150202371988, 0.2977118434676205, 0.30281967714608427, 0.2864607845444277, 0.30423304546310237, 0.28952283744352414, 0.31333831419427716, 0.2901537791792168, 0.3029711619917168, 0.2864916698042168, 0.28891248588102414, 0.2944262401167168, 0.29773243364081325, 0.2881388836596386, 0.29763095350150603, 0.2800822430346386, 0.2980780544051205, 0.2933276073042168, 0.28584013789533136, 0.3096467902861446, 0.28769178275602414, 0.2837237622364458, 0.28586072806852414, 0.2847209149096386, 0.2944468302899097, 0.3144972467996988, 0.3069288874246988, 0.27682752494352414, 0.2824324642319277, 0.27897331513554224, 0.2991355068712349, 0.2876108927899097, 0.29548516330948793, 0.2843855892319277, 0.3007636012801205, 0.28317518119352414, 0.29526014213102414, 0.29319671263177716, 0.27970573701054224, 0.2830428157944277, 0.29387765907379515, 0.29617640483810237, 0.30341973362198793, 0.3014754329819277, 0.2695136012801205, 0.2755759365587349, 0.26461902296686746, 0.25638883659638556, 0.25707125376506024, 0.2553519743034638, 0.2551078336784638, 0.2568168180534638, 0.2571830289909638, 0.2566741575677711, 0.2565520872552711, 0.2566741575677711, 0.25717273390436746, 0.25766101515436746, 0.25717273390436746, 0.2575492399284638, 0.2576713102409638, 0.2571830289909638, 0.2569388883659638, 0.25692859327936746, 0.25631824171686746, 0.2573050993034638, 0.2571830289909638, 0.2571830289909638, 0.2575492399284638, 0.2573050993034638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638, 0.2571830289909638], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.01218848956238463, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.0002831572645849102, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 5.505044978803649e-07, "rotation_range": [0, 0], "momentum": 0.5693937981472119}, "accuracy_valid_max": 0.7448921663215362, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7428169710090362, "accuracy_valid_std": [0.020560093718426188, 0.015978415988538382, 0.011156524790054967, 0.010046313104637685, 0.009366562740508285, 0.014268567610666048, 0.021374539553478317, 0.015507390966005874, 0.020484967440443234, 0.02498647080200287, 0.016879606084541744, 0.014734671189404828, 0.01534760705905358, 0.019226981370464448, 0.017676534833276282, 0.014243200273610262, 0.01565223889755882, 0.015695559530772552, 0.014776057397501277, 0.017647664785444415, 0.0211720734010314, 0.01578068703788304, 0.025239825715704604, 0.019579767077296903, 0.019305816852341725, 0.013075647677223022, 0.023916985468915616, 0.022277327660235435, 0.020442089351072227, 0.01749726538297197, 0.018707912009837464, 0.013830712149568878, 0.025438410638586855, 0.019294057065966912, 0.02587309825067095, 0.021265596701714382, 0.021537291230872644, 0.02379972751111474, 0.01480216809162738, 0.019274229599437562, 0.012988633102497675, 0.01506473455549776, 0.019436067548988578, 0.015942142928580864, 0.01868344917870595, 0.016176646449174964, 0.008268307018587625, 0.015180048528057441, 0.012635843141319063, 0.020256085185781807, 0.014812449131950676, 0.015306305524944048, 0.01501347327961651, 0.02236587929515138, 0.01335999773040505, 0.012175178672327674, 0.01727655736294971, 0.01621438539918093, 0.0133495186756254, 0.021782177721814445, 0.011174291169014939, 0.01407394243901642, 0.014025229103967566, 0.010078582772677591, 0.01309591041543893, 0.011769226957802947, 0.02774904384661584, 0.014492355102549637, 0.01362014952054612, 0.017313726461407303, 0.013533751678295234, 0.014172112465824553, 0.012893640080026789, 0.017522680549875956, 0.016889440141646304, 0.015369515492655453, 0.018150409696319224, 0.015929709141354758, 0.02171715513927481, 0.02215078828708602, 0.014197262574391287, 0.009388416760601342, 0.016134917857526857, 0.012194298748616992, 0.025909780740431272, 0.012153186319277412, 0.012066280244991946, 0.018728591279803856, 0.01566835280365201, 0.016665429030212325, 0.009441588626408327, 0.02117176118364124, 0.014306842784230766, 0.009788704084766814, 0.009653966284341134, 0.015087958188702066, 0.014753987862570527, 0.014424786805106914, 0.0133743763764464, 0.013230961738774309, 0.012502171445065408, 0.012087890759558998, 0.011694185194167261, 0.012788802988876517, 0.012435493970968768, 0.011910390051969531, 0.012621094339616749, 0.0120579395739601, 0.012219212003641679, 0.012767492887056048, 0.012152542277772044, 0.012721996070517333, 0.013683466850981521, 0.01332075577149905, 0.01334757618675503, 0.014085193264636527, 0.013666031900435, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364, 0.01359534884716364], "accuracy_valid": [0.5449733504329819, 0.620396625564759, 0.6613225362387049, 0.6799184040850903, 0.6969170627823795, 0.7033559040850903, 0.7050237081137049, 0.695288968373494, 0.6722367987575302, 0.6627344338290663, 0.6687379400414157, 0.6563573630459337, 0.652796733810241, 0.6657258918486446, 0.6841187994164157, 0.6704263342432228, 0.6427149025790663, 0.665858257247741, 0.6783711996423193, 0.6827657308923193, 0.6672922157379518, 0.6914739034262049, 0.674839984939759, 0.6793065817959337, 0.6778417380459337, 0.6840173192771084, 0.6892854621611446, 0.683680522872741, 0.6740266730986446, 0.6960919851280121, 0.6990216726280121, 0.6712911215173193, 0.6819303581513554, 0.6698762824736446, 0.6873117469879518, 0.6894487128200302, 0.6835790427334337, 0.6807302451995482, 0.7092358692582832, 0.677332866622741, 0.682715726185994, 0.691748929310994, 0.6909547369164157, 0.6801110692771084, 0.7008733174887049, 0.6999173451618976, 0.7097756259412651, 0.6997849797628012, 0.7022881565323795, 0.6971803228539157, 0.7135392154555723, 0.6957669545368976, 0.7104771625564759, 0.6866616858057228, 0.7098462208207832, 0.6970288380082832, 0.7135083301957832, 0.7110875141189759, 0.7055737598832832, 0.7022675663591867, 0.7118611163403614, 0.702369046498494, 0.7199177569653614, 0.7019219455948795, 0.7066723926957832, 0.7141598621046686, 0.6903532097138554, 0.7123082172439759, 0.7162762377635542, 0.7141392719314759, 0.7152790850903614, 0.7055531697100903, 0.6855027532003012, 0.6930711125753012, 0.7231724750564759, 0.7175675357680723, 0.7210266848644578, 0.7008644931287651, 0.7123891072100903, 0.7045148366905121, 0.7156144107680723, 0.6992363987198795, 0.7168248188064759, 0.7047398578689759, 0.7068032873682228, 0.7202942629894578, 0.7169571842055723, 0.7061223409262049, 0.7038235951618976, 0.6965802663780121, 0.6985245670180723, 0.7304863987198795, 0.7244240634412651, 0.7353809770331325, 0.7436111634036144, 0.7429287462349398, 0.7446480256965362, 0.7448921663215362, 0.7431831819465362, 0.7428169710090362, 0.7433258424322289, 0.7434479127447289, 0.7433258424322289, 0.7428272660956325, 0.7423389848456325, 0.7428272660956325, 0.7424507600715362, 0.7423286897590362, 0.7428169710090362, 0.7430611116340362, 0.7430714067206325, 0.7436817582831325, 0.7426949006965362, 0.7428169710090362, 0.7428169710090362, 0.7424507600715362, 0.7426949006965362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362, 0.7428169710090362], "seed": 759538699, "model": "residualv3", "loss_std": [0.3328207731246948, 0.10610748827457428, 0.09791526943445206, 0.09084901213645935, 0.08819438517093658, 0.08575215190649033, 0.08579680323600769, 0.08432752639055252, 0.0822504311800003, 0.07979079335927963, 0.07798926532268524, 0.07470113784074783, 0.07057150453329086, 0.06342285126447678, 0.06183197349309921, 0.05671626701951027, 0.055644769221544266, 0.053191445767879486, 0.047126833349466324, 0.04146204888820648, 0.046325188130140305, 0.039279233664274216, 0.035770170390605927, 0.03712388873100281, 0.034719232469797134, 0.033135004341602325, 0.035116713494062424, 0.03821317479014397, 0.03251044079661369, 0.04887210577726364, 0.03236176818609238, 0.029396681115031242, 0.02849089913070202, 0.029142605140805244, 0.028713583946228027, 0.028622567653656006, 0.028356952592730522, 0.026932593435049057, 0.027077894657850266, 0.025273863226175308, 0.025802921503782272, 0.030196888372302055, 0.02558613382279873, 0.021679319441318512, 0.027536706998944283, 0.029919706284999847, 0.02757982164621353, 0.02241319976747036, 0.02132437564432621, 0.027132369577884674, 0.022467684000730515, 0.0284079909324646, 0.02862882800400257, 0.026126915588974953, 0.02372533269226551, 0.02661963179707527, 0.02161259390413761, 0.02253633365035057, 0.023795992136001587, 0.022853508591651917, 0.022190429270267487, 0.018787242472171783, 0.02628221921622753, 0.02578892558813095, 0.018970930948853493, 0.01681397669017315, 0.022413481026887894, 0.030460601672530174, 0.021662121638655663, 0.019369425252079964, 0.027239015325903893, 0.020953964442014694, 0.017748592421412468, 0.021874072030186653, 0.029010741040110588, 0.019916901364922523, 0.016669584438204765, 0.017449572682380676, 0.024089625105261803, 0.025094449520111084, 0.02104332111775875, 0.019080601632595062, 0.0250664371997118, 0.018605681136250496, 0.02434670925140381, 0.019376952201128006, 0.020429600030183792, 0.020296610891819, 0.023673448711633682, 0.03351535648107529, 0.1872619390487671, 0.10484140366315842, 0.055259667336940765, 0.028225859627127647, 0.014381232671439648, 0.002557925647124648, 0.0002811614831443876, 0.00011970326886512339, 8.964243170339614e-05, 7.251839997479692e-05, 6.019873762852512e-05, 5.069089820608497e-05, 4.302589877624996e-05, 3.6689703847514465e-05, 3.141306297038682e-05, 2.6972234991262667e-05, 2.3170699932961725e-05, 1.9913208234356716e-05, 1.7117754396167584e-05, 1.470503138989443e-05, 1.264456568605965e-05, 1.0874110557779204e-05, 9.347969353257213e-06, 8.027019248402212e-06, 6.8975728026998695e-06, 5.943567884969525e-06, 5.4621946219413076e-06, 5.224224423727719e-06, 5.202402917348081e-06, 5.200450686970726e-06, 5.20038111062604e-06, 5.20042203788762e-06, 5.200385658099549e-06, 5.200408395467093e-06, 5.2003751989104785e-06, 5.200403847993584e-06, 5.20039657203597e-06, 5.200433406571392e-06, 5.2004120334459e-06, 5.200340638111811e-06, 5.2003861128469e-06, 5.200409304961795e-06, 5.20037610840518e-06, 5.200373834668426e-06, 5.200382020120742e-06, 5.200394298299216e-06, 5.20034018336446e-06, 5.20036564921611e-06, 5.200349278311478e-06, 5.200379746383987e-06, 5.2003633754793555e-06, 5.200350642553531e-06, 5.200390205573058e-06, 5.2003993005200755e-06, 5.200362011237303e-06, 5.200394753046567e-06, 5.20034518558532e-06, 5.200373379921075e-06]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:31 2016", "state": "available"}], "summary": "25cbe88ad9d7af862ef909931075f3ce"}