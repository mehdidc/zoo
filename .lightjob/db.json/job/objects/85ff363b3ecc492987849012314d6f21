{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 64, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.011424871149574207, 0.01432704067860744, 0.012964467407477183, 0.013169486323887907, 0.011790264781684809, 0.01793626736679857, 0.010021742418613146, 0.015368111380509322, 0.012328649441291811, 0.012308054996913806, 0.010274412528694422, 0.014256876473974651, 0.0147282876410842, 0.01405027773619916, 0.018227325844977167, 0.008583394503576185, 0.019600500124879412, 0.012356185168744042, 0.014149350427883324, 0.01498117373592938, 0.014765344136539663, 0.01851927553896194, 0.01769841580876912, 0.011407721152296879, 0.01375497009291297, 0.0129049249693529, 0.013425114437889633, 0.01568672439555901, 0.015785595729519836, 0.016769094802004912, 0.016686053432340364, 0.013214639913016952, 0.013643101952438253, 0.015424647354838885, 0.01248441489226856, 0.014335267405418461, 0.019421982054158307, 0.01540363575067141, 0.009534650815232408, 0.017361274533643875, 0.015214646079773237, 0.013946815739645305, 0.018255531285437603, 0.016296578099242412, 0.014017214840510352, 0.017517651721965014, 0.014354073969045082, 0.013791278568804247, 0.015079025179102947, 0.01532323840842226, 0.013553938230844767, 0.011684084970398488, 0.01092288122856837, 0.013212553093249476, 0.01584954509912861, 0.017063903835124426, 0.013569342661381295, 0.018639904934635784, 0.01055441805819263, 0.012418990061498712, 0.01218272091595323, 0.014502195860583184, 0.017099667709315737, 0.013082885244069207, 0.0131501699548882, 0.012928781777184093, 0.009727828493403316, 0.01284570877657801, 0.010999866219718157, 0.013008862441068974, 0.014187236986418696, 0.016846768000678074, 0.016051581611905417, 0.011188037210773246, 0.013330625444893518, 0.013036038493220773, 0.014550021535095168, 0.006544550356965638, 0.01100097089962009, 0.009059983715174061, 0.009861295384883666, 0.01354273951034786, 0.011101033795194969, 0.016637671991961353, 0.010240732019876915, 0.010032244706186213, 0.009742321506638025, 0.010659432333508894, 0.008711939198837138, 0.010072530468220368, 0.008603695631708742, 0.010737389954931977, 0.009267215551213624, 0.008182704409593169, 0.0033019500311822665, 0.0097298481617876, 0.009650774739686866, 0.011894786225594136, 0.012429048150199124, 0.010092931734594118, 0.008227499805872021, 0.008180986928015051, 0.009134602517244522, 0.008787183898761335, 0.007777987390274802, 0.007220284586896361, 0.006048931925321521, 0.005424163532216639, 0.010534276568815963, 0.004291220247542166, 0.009882335869133415, 0.0077365991589680196, 0.005018249526410063, 0.009090274857557125, 0.00773012811447609, 0.006986089556293178, 0.012602210036777174, 0.008293622442024254, 0.005091251877868801, 0.016222564901369422, 0.0059208245885342395, 0.013343892500165414, 0.008720229808589285, 0.005628632127608056, 0.008199494611190235, 0.009971109258077886, 0.009579915609798116, 0.008025364832908142, 0.008734424469483495, 0.010060520287958114, 0.009995388920208206, 0.010906247970617134, 0.0046954882760681165, 0.009257098936427406, 0.009636959620320473, 0.007754544477191515, 0.01518934564125959, 0.01039439016945004, 0.007154860112911857, 0.009962589798016916, 0.004431669889323631, 0.011434711633124274, 0.010932592464873268, 0.007803238096314945, 0.008284126359997875, 0.009353473735198674, 0.005521653781676627, 0.008144877186789565, 0.011304074256141201, 0.00779162153351001, 0.006795232735591005, 0.006121467352302491, 0.006820779138598033, 0.007899113777452836, 0.0131134975983292, 0.004642973644338912, 0.007013051302763393, 0.010402734717763602, 0.0071256752046872695, 0.008121518126196913, 0.006574399522835147, 0.006487600505370124, 0.009466691288680803, 0.010981743206658356, 0.006227603212772715, 0.008479180764618939, 0.006887051601484001, 0.0075143787426774624, 0.00898682875224285, 0.008935890686689145, 0.010368324503009894, 0.005997331172911397, 0.007779659548338353, 0.00801267310433798, 0.01123113706713627, 0.008621085915469158, 0.011158107230551216, 0.01114044833385652, 0.00828712175119677, 0.013229751827746973, 0.004869579069448055, 0.00937223420875793, 0.016584479248967013, 0.008616751268040547, 0.008859640655601889, 0.00898514761830546, 0.009020676740893479, 0.009111227963328172, 0.009920432219648459, 0.009956480127963376, 0.010983323651617076, 0.009569632074880216, 0.013698634516394559, 0.011034388023790663, 0.012818516057280356, 0.007893628780888453, 0.009803001959940561, 0.007172364479198002, 0.010644305408072902, 0.008523362200299492, 0.014471263945822644, 0.005848316394829171, 0.012626445828749059, 0.012003212055750221, 0.009095051176086468, 0.010895730314758252, 0.01334458237735641, 0.012198663225273078, 0.01640273259647212, 0.012732753323037199, 0.011741974729884029, 0.011459790274527676], "moving_avg_accuracy_train": [0.05719428078280729, 0.11061041645037373, 0.16103984566058738, 0.20589344028657364, 0.24788190763319312, 0.29022326839372947, 0.32801205583776366, 0.363351517070617, 0.3947938403312556, 0.4208072112065372, 0.4440912536633106, 0.4693216869729208, 0.48900034532204806, 0.5093939630254468, 0.5281359780774149, 0.5488255431930307, 0.5656003801793293, 0.5787800264314222, 0.5942423902461519, 0.609074482115086, 0.6217119774078685, 0.6322999310202102, 0.6441075909093759, 0.6521192331311921, 0.6612105041760001, 0.6709963519161614, 0.6795385479180207, 0.6860106517364069, 0.6927257527276334, 0.6989787512078693, 0.7051669909472706, 0.7091788454009009, 0.716218063487472, 0.7209867502000482, 0.7248229111723377, 0.7293728181390371, 0.734172290547171, 0.7380291471502152, 0.7420535953631652, 0.7472167398297096, 0.7512358878174843, 0.7557806390885691, 0.7598780709230677, 0.7634960051098306, 0.7659387403315903, 0.7683530082846347, 0.771309172249453, 0.7734423075750337, 0.7766220355834494, 0.77681048248999, 0.7789863276403081, 0.7801148345899631, 0.7818040990718582, 0.7827550640377806, 0.7844246264439878, 0.7850902871846019, 0.7851435126131978, 0.7860580831749677, 0.787132348800808, 0.7891965499557032, 0.7890036939403839, 0.7884770613957365, 0.7899929869006812, 0.7905249886789594, 0.7922663821317999, 0.7916483207977727, 0.79159647869354, 0.7933185020185842, 0.7941734279564453, 0.7952755017755572, 0.7957744727139575, 0.7952078170169434, 0.7934983024456478, 0.7909949107219856, 0.7915617950419982, 0.7925766203193136, 0.7913765830451343, 0.7927022133445799, 0.7950301334772224, 0.7971067325037619, 0.7981946379205599, 0.7966977216552665, 0.7964595929986454, 0.7961125634814498, 0.7963045058170903, 0.7983628054084156, 0.8001850120572658, 0.8002630027246935, 0.8002520304099576, 0.7995932585647445, 0.8009366721301047, 0.8019158069485173, 0.8018253323756201, 0.8026273176123568, 0.8032118844968391, 0.8034195214036058, 0.8032851996446129, 0.8032160401164289, 0.8038348128005224, 0.8056514702363247, 0.8059174539631997, 0.8057057604483396, 0.8073096383359992, 0.8083906575623384, 0.8083291720363557, 0.8086319079796378, 0.8079536007584089, 0.8076617417438453, 0.8092467328114782, 0.8100734445259468, 0.8094090577203159, 0.8101337948284983, 0.8089378533128412, 0.8085542560987129, 0.8080210059917947, 0.8082452765454853, 0.809685991773458, 0.8101897957834046, 0.8097507587865056, 0.8113852280582796, 0.8101477043815123, 0.8116167408139775, 0.8107465827151841, 0.8105541363703452, 0.8087464631933513, 0.8079658033542672, 0.8094582581217382, 0.8097737155898339, 0.8093830097169894, 0.809798673538572, 0.8106190914029687, 0.8120410972797445, 0.8113285384296309, 0.8110777162692536, 0.8112007125975238, 0.8096936464538197, 0.8100875111481387, 0.811169724901588, 0.8109998882606817, 0.8122976397504589, 0.8125495795579434, 0.8114975656370789, 0.8134733209665973, 0.8140493988286401, 0.8149652170091759, 0.8166474332823723, 0.8168691138247977, 0.8157711932772662, 0.8167338646356784, 0.8184023010022601, 0.8190368295191437, 0.8177687845736432, 0.8182502816988279, 0.8184022861055416, 0.8191434485179663, 0.8168813118726111, 0.8181071400857284, 0.8182870490584216, 0.818295651507692, 0.8199472018227312, 0.8209315452075031, 0.8226589057811142, 0.8215099229641692, 0.822402954206189, 0.8225579658061497, 0.8239787774354369, 0.8233766427589844, 0.8235505429441417, 0.8240395493905451, 0.8236800563435633, 0.8236353502143289, 0.8236950600480085, 0.8248834354685491, 0.8258645816434551, 0.8267011462734987, 0.8263683541417652, 0.8268828235506323, 0.827336040739912, 0.8266143825234789, 0.8279687358166736, 0.828515902067509, 0.827941324682602, 0.828114521890883, 0.8262966724784189, 0.8267808553309369, 0.8272632670696687, 0.8279113873738221, 0.8280436888761501, 0.8277602210936477, 0.825768682563325, 0.826556636283506, 0.826268486036477, 0.8262392323998374, 0.8249926335876869, 0.8245961410853229, 0.8252739890534332, 0.8261325187079828, 0.8254403516470774, 0.8251336575791766, 0.8249924555001994, 0.8250884076707405, 0.8252121472492737, 0.8254166990664283, 0.8261584348791469, 0.8259540663070222, 0.827580992928904, 0.8274416672840387, 0.8275625597333757, 0.8283967372687129, 0.8291357992088313, 0.8287084234050116, 0.829255881463643], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 564406332, "moving_var_accuracy_train": [0.0294406717883634, 0.052176156556429834, 0.06984668687499838, 0.0809686227453493, 0.08873904298187754, 0.09600025616317477, 0.09925216265527087, 0.1005668440717989, 0.09940773689285745, 0.09555722238222625, 0.09088081984216305, 0.08752191074286296, 0.08225496601837175, 0.07777256620382599, 0.07315667773731716, 0.06969353290564521, 0.06525673601833273, 0.06029439009447222, 0.05641671333767656, 0.05275496054678541, 0.04891682107758281, 0.0450340818250984, 0.041785461131112485, 0.03818459271781473, 0.03510999432892475, 0.032460860239974826, 0.029871496228784974, 0.0272613397564302, 0.024941039012688508, 0.022798835021364153, 0.02086360031887849, 0.018922095072404655, 0.017475840886596972, 0.01593292015460158, 0.01447207331818927, 0.013211180867020923, 0.012097377198886783, 0.011021517564706106, 0.010065131459003947, 0.009298540860145218, 0.0085140687290594, 0.007848554733197694, 0.007214799788623187, 0.0066111248397786055, 0.006003714954073372, 0.005455801666407909, 0.004988871648249123, 0.004530936880279375, 0.004168839224118971, 0.003752274911836337, 0.003419656139716168, 0.003089152277163329, 0.0028059195798551292, 0.0025334666311673214, 0.002305206915704572, 0.0020786741621284695, 0.0018708322424318654, 0.0016912769720007844, 0.0015325356945144635, 0.0014176304627338534, 0.0012762021574442715, 0.0011510780182335814, 0.0010566524876390977, 0.0009535344719040078, 0.0008854730851319688, 0.0008003637749323468, 0.0007203515858730537, 0.0006750047062737184, 0.0006140823208793946, 0.0005636051891164008, 0.0005094854181810739, 0.00046142676447359296, 0.0004415860486514836, 0.0004538301748852397, 0.0004113393778872009, 0.0003794742731897872, 0.0003544876510055883, 0.0003348545471223033, 0.00035014200170572964, 0.00035393817318837926, 0.0003291961996326282, 0.0003164434044170634, 0.0002853094112892948, 0.0002578623355326109, 0.00023240767872125014, 0.00024729628571797406, 0.00025245059078620163, 0.00022726027460543384, 0.00020453533067010644, 0.00018798762069950617, 0.0001854316986978992, 0.0001755168737617597, 0.000158038857220652, 0.00014802359437807825, 0.00013629670092217048, 0.00012305504859541868, 0.0001109119247503274, 9.986377963834245e-05, 9.332331838573126e-05, 0.0001136931846986608, 0.00010296059231545609, 9.306786038201454e-05, 0.00010691289285052319, 0.00010673902667490517, 9.609914823656312e-05, 8.731407487510104e-05, 8.272357356493144e-05, 7.521785136787704e-05, 9.03058363913718e-05, 8.74263230817918e-05, 8.265637922107998e-05, 7.911793618276284e-05, 8.407862754433745e-05, 7.699508619408693e-05, 7.185477866343239e-05, 6.512197632836335e-05, 7.729072200853851e-05, 7.18460161316287e-05, 6.639619588028032e-05, 8.379998449561165e-05, 8.920316970109009e-05, 9.970546509017268e-05, 9.654949463321809e-05, 8.722786553067266e-05, 0.00010791421981101455, 0.00010260766588914248, 0.00011239369039675155, 0.0001020499420846723, 9.321880758788245e-05, 8.545191454224894e-05, 8.29644923380162e-05, 9.286694952648003e-05, 8.814991560770912e-05, 7.99011298521651e-05, 7.204716973786033e-05, 8.528368801756454e-05, 7.815148379268726e-05, 8.087701488681241e-05, 7.304891375948021e-05, 8.090145274650185e-05, 7.338257047120989e-05, 7.600491303132277e-05, 0.00010353690382727377, 9.617000477276813e-05, 9.410151075368987e-05, 0.00011016002398658444, 9.958630195393608e-05, 0.00010047653751676831, 9.876950906385565e-05, 0.00011394567734145966, 0.0001061747475559605, 0.00011002871465465014, 0.00010111239852323425, 9.120910672785438e-05, 8.703209154939131e-05, 0.00012438424221477917, 0.00012546971126596915, 0.0001132140452854723, 0.00010189330677612613, 0.00011625254208646889, 0.00011334767497012002, 0.00012886687843450197, 0.00012786164421376437, 0.0001222530229853983, 0.00011024397805195971, 0.00011738793142002347, 0.00010891223379530029, 9.829318188534996e-05, 9.061600943843243e-05, 8.271752575204349e-05, 7.446376091875934e-05, 6.704947220502595e-05, 7.3054650245829e-05, 7.441301557003815e-05, 7.327027743519372e-05, 6.694000511816732e-05, 6.262811356029056e-05, 5.82139545901889e-05, 5.7079674363278124e-05, 6.788016251203757e-05, 6.37866644153134e-05, 6.037925051500166e-05, 5.461130092010839e-05, 7.889135920566146e-05, 7.31121205971478e-05, 6.789539830842829e-05, 6.488639783548934e-05, 5.8555291239604655e-05, 5.3422947969095696e-05, 8.377668463202453e-05, 8.098685575514551e-05, 7.363544526339717e-05, 6.627960271436722e-05, 7.363771982902797e-05, 6.768880458600377e-05, 6.505522493824589e-05, 6.518336095409137e-05, 6.297688202050304e-05, 5.7525745080022604e-05, 5.195261281598737e-05, 4.684021290567267e-05, 4.229399496476554e-05, 3.844116848140013e-05, 3.954859977608348e-05, 3.596963841792583e-05, 5.619468667302165e-05, 5.074992272357366e-05, 4.58064653099766e-05, 4.7488488223130486e-05, 4.76555523628013e-05, 4.453384782573551e-05, 4.277785597680575e-05], "duration": 264101.38989, "accuracy_train": [0.571942807828073, 0.5913556374584718, 0.6149047085525101, 0.6095757919204503, 0.6257781137527685, 0.6712955152385567, 0.6681111428340716, 0.6814066681662975, 0.6777747496770026, 0.6549275490840716, 0.6536476357742709, 0.696395586759413, 0.6661082704641934, 0.6929365223560355, 0.6968141135451273, 0.7350316292335732, 0.716573913056017, 0.6973968427002584, 0.7334036645787191, 0.7425633089354928, 0.7354494350429125, 0.7275915135312846, 0.7503765299118678, 0.7242240131275379, 0.7430319435792727, 0.7590689815776117, 0.7564183119347545, 0.7442595861018826, 0.753161661648671, 0.7552557375299926, 0.7608611486018826, 0.7452855354835732, 0.7795710262666113, 0.7639049306132337, 0.759348359922942, 0.7703219808393319, 0.7773675422203765, 0.7727408565776117, 0.7782736292797158, 0.7936850400286084, 0.7874082197074567, 0.7966834005283315, 0.7967549574335548, 0.7960574127906977, 0.7879233573274271, 0.7900814198620341, 0.7979146479328165, 0.7926405255052602, 0.8052395876591916, 0.7785065046488556, 0.798568933993171, 0.7902713971368586, 0.7970074794089147, 0.7913137487310816, 0.7994506880998523, 0.7910812338501293, 0.7856225414705611, 0.794289218230897, 0.7968007394333703, 0.80777436034976, 0.7872679898025102, 0.7837373684939092, 0.8036363164451827, 0.7953130046834626, 0.8079389232073644, 0.7860857687915282, 0.7911298997554448, 0.8088167119439831, 0.8018677613971945, 0.8051941661475637, 0.8002652111595607, 0.7901079157438169, 0.7781126713039868, 0.7684643852090256, 0.7966637539221114, 0.8017100478151532, 0.7805762475775194, 0.8046328860395902, 0.815981414671004, 0.8157961237426172, 0.8079857866717424, 0.7832254752676264, 0.794316435089055, 0.7929892978266887, 0.7980319868378553, 0.8168875017303433, 0.8165848718969176, 0.800964918731543, 0.8001532795773348, 0.7936643119578257, 0.8130273942183462, 0.8107280203142304, 0.8010110612195459, 0.8098451847429864, 0.8084729864571798, 0.8052882535645073, 0.8020763038136766, 0.8025936043627722, 0.8094037669573644, 0.8220013871585455, 0.8083113075050757, 0.8038005188145996, 0.8217445393249354, 0.8181198305993909, 0.8077758023025102, 0.8113565314691769, 0.8018488357673496, 0.8050350106127722, 0.8235116524201735, 0.8175138499561646, 0.8034295764696382, 0.8166564288021411, 0.7981743796719268, 0.8051018811715578, 0.8032217550295312, 0.8102637115287007, 0.8226524288252122, 0.8147240318729235, 0.805799425814415, 0.8260954515042451, 0.7990099912906055, 0.8248380687061646, 0.8029151598260429, 0.8088221192667959, 0.7924774046004062, 0.8009398648025102, 0.8228903510289776, 0.8126128328026948, 0.8058666568613879, 0.8135396479328165, 0.8180028521825397, 0.8248391501707272, 0.8049155087786084, 0.8088203168258582, 0.8123076795519564, 0.7961300511604835, 0.8136322933970099, 0.8209096486826319, 0.8094713584925249, 0.8239774031584534, 0.8148170378253045, 0.8020294403492986, 0.8312551189322629, 0.8192340995870249, 0.8232075806339978, 0.8317873797411407, 0.8188642387066261, 0.8058899083494832, 0.8253979068613879, 0.8334182283014949, 0.8247475861710963, 0.8063563800641381, 0.822583755825489, 0.8197703257659652, 0.8258139102297897, 0.796522082064415, 0.8291395940037836, 0.8199062298126615, 0.818373073551126, 0.8348111546580842, 0.8297906356704503, 0.8382051509436139, 0.8111690776116648, 0.830440235384367, 0.8239530702057956, 0.8367660820990217, 0.8179574306709118, 0.8251156446105574, 0.8284406074081765, 0.8204446189207272, 0.8232329950512183, 0.824232448551126, 0.8355788142534146, 0.834694897217608, 0.8342302279438908, 0.8233732249561646, 0.8315130482304356, 0.8314149954434293, 0.8201194585755813, 0.8401579154554264, 0.8334403983250278, 0.8227701282184385, 0.8296732967654117, 0.8099360277662422, 0.8311385010035992, 0.831604972718254, 0.8337444701112033, 0.8292344023971022, 0.825209011051126, 0.8078448357904209, 0.8336482197651348, 0.8236751338132153, 0.8259759496700813, 0.8137732442783315, 0.8210277085640458, 0.8313746207664268, 0.8338592855989295, 0.8192108480989295, 0.8223734109680694, 0.8237216367894058, 0.825951977205611, 0.8263258034560724, 0.8272576654208195, 0.8328340571936139, 0.8241147491578996, 0.8422233325258398, 0.826187736480251, 0.8286505917774087, 0.8359043350867479, 0.8357873566698967, 0.8248620411706349, 0.8341830039913253], "end": "2016-01-26 14:02:27.166000", "learning_rate_per_epoch": [0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549, 0.003057512454688549], "accuracy_valid": [0.5561038097703314, 0.5747099727033133, 0.5953207360692772, 0.5877523766942772, 0.606654155685241, 0.6553013813064759, 0.6490743246423193, 0.6647610951618976, 0.6565720891378012, 0.6287282920745482, 0.6352494940700302, 0.6849556428840362, 0.6468373493975903, 0.6738560688064759, 0.6780991152108433, 0.7130627000188253, 0.6935623352786144, 0.6826760165662651, 0.7111801698983433, 0.7290450865963856, 0.7134289109563253, 0.7067356339420181, 0.7292583419615963, 0.7049854692206325, 0.7215061417545181, 0.7381797698606928, 0.7390239669615963, 0.7260433334902108, 0.7337440582643072, 0.7337955336972892, 0.7480174604668675, 0.7244873046875, 0.7584743269954819, 0.7506927122552711, 0.7448421616152108, 0.7607127729668675, 0.7657382459525602, 0.7567962278802711, 0.7567344573606928, 0.7762877682605422, 0.7687797086784638, 0.7780776426016567, 0.7767039839043675, 0.7758289015436747, 0.7697253859186747, 0.767223679875753, 0.7756259412650602, 0.7728080289909638, 0.7862166439194277, 0.7619128859186747, 0.7768466443900602, 0.7718314664909638, 0.7769584196159638, 0.7774981762989458, 0.7801734280873494, 0.7698783414909638, 0.7743449383471386, 0.7807028896837349, 0.7753715055534638, 0.7911803463855422, 0.771252000188253, 0.7637645307793675, 0.7864504894578314, 0.7767657544239458, 0.783459031438253, 0.772350633000753, 0.7731948301016567, 0.7855136365775602, 0.7797057370105422, 0.7869284756212349, 0.7847312099962349, 0.7746096691453314, 0.7638365963855422, 0.7599597609186747, 0.774792039250753, 0.780651414250753, 0.7677222562123494, 0.7885962796498494, 0.7974162274096386, 0.7948630459337349, 0.7965720303087349, 0.7720050122364458, 0.7830222256212349, 0.7796351421310241, 0.7766127988516567, 0.7950160015060241, 0.7978030285203314, 0.7792380459337349, 0.7843444088855422, 0.7783526684864458, 0.7945777249623494, 0.7920760189194277, 0.7811602856739458, 0.7919333584337349, 0.789318406438253, 0.7849341702748494, 0.7819235928087349, 0.7870799604668675, 0.7901228939194277, 0.8015166133283133, 0.7897552122552711, 0.786266648625753, 0.8030829372176205, 0.7973647519766567, 0.7902243740587349, 0.7947409756212349, 0.7803763883659638, 0.7889419004141567, 0.8042021602033133, 0.7958087231739458, 0.7771319653614458, 0.7985148602221386, 0.7813529508659638, 0.7861857586596386, 0.7844561841114458, 0.7897963926016567, 0.7979662791792168, 0.7941203289721386, 0.7869784803275602, 0.8091467432228916, 0.7832045957266567, 0.803734469126506, 0.7842223385730422, 0.7906508847891567, 0.7695224256400602, 0.7841193877070783, 0.8054522778614458, 0.792126023625753, 0.7854224515248494, 0.7943335843373494, 0.8013533626694277, 0.8059214396649097, 0.7918921780873494, 0.7894507718373494, 0.7937938276543675, 0.7803660932793675, 0.798119234751506, 0.7983927899096386, 0.792003953313253, 0.7958499035203314, 0.7989825512989458, 0.7861651684864458, 0.8162165262612951, 0.799694383000753, 0.8001429546310241, 0.8142119258283133, 0.7983824948230422, 0.7906199995293675, 0.8076304240399097, 0.8143339961408133, 0.8079554546310241, 0.786754929875753, 0.8032653073230422, 0.7988913662462349, 0.8091158579631024, 0.779186570500753, 0.8156458843185241, 0.8023093349962349, 0.7984030849962349, 0.8129191570971386, 0.8132147731551205, 0.8190535579819277, 0.7881182934864458, 0.8078833890248494, 0.8010680416980422, 0.8161547557417168, 0.7958896131400602, 0.8044051204819277, 0.8105998211596386, 0.8039374294051205, 0.8059214396649097, 0.8041403896837349, 0.8144045910203314, 0.8152193735881024, 0.8131441782756024, 0.8036829936935241, 0.8058905544051205, 0.8089114269578314, 0.8019225338855422, 0.8144957760730422, 0.8130927028426205, 0.8028490916792168, 0.8146810876317772, 0.7890433805534638, 0.8107836619917168, 0.8068465267319277, 0.8100615352033133, 0.810814547251506, 0.8042418698230422, 0.7874976468373494, 0.8132353633283133, 0.7976706631212349, 0.8060641001506024, 0.7945071300828314, 0.7971514966114458, 0.8086878765060241, 0.8123396907944277, 0.7980265789721386, 0.8049948818712349, 0.8026652508471386, 0.8059817394578314, 0.8074465832078314, 0.8100718302899097, 0.811180758189006, 0.8033373729292168, 0.817162203501506, 0.8022284450301205, 0.8107630718185241, 0.8083510801016567, 0.8104880459337349, 0.8014548428087349, 0.8118102291980422], "accuracy_test": 0.15961017219387755, "start": "2016-01-23 12:40:45.777000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0], "accuracy_train_last": 0.8341830039913253, "batch_size_eval": 1024, "accuracy_train_std": [0.017377073019271473, 0.01814501891856869, 0.017905428537949124, 0.01745982260357119, 0.018773556113104474, 0.01807430369031441, 0.017660597450719188, 0.018433086893711374, 0.022326621195093786, 0.019148405247775457, 0.019636230819725726, 0.02282367334300107, 0.019982290575144165, 0.01955892779812273, 0.02210211042647682, 0.023364424542993086, 0.02229124062736494, 0.021762674230479485, 0.02471457735296611, 0.02528992902383302, 0.0244717418001696, 0.02465760059301462, 0.022049621451217527, 0.021008289263547367, 0.02252508279822155, 0.02343042076906573, 0.024166956612369724, 0.025119127411871674, 0.026512186890358596, 0.02469381387739956, 0.02320681011008505, 0.02367598665503491, 0.02543012547646161, 0.023629877678316567, 0.02430050861999247, 0.023601682377005353, 0.021602152419050964, 0.02248230994838105, 0.021852377109206574, 0.02343662501691401, 0.02286321509151465, 0.023426400686601262, 0.021674569552931795, 0.02118997262640796, 0.020188216354626486, 0.022692625397466793, 0.024074255531085217, 0.02267336432679377, 0.02161971679877701, 0.02185842323256187, 0.021621065515736535, 0.019912491530803867, 0.0240869854816786, 0.022405393490139924, 0.02299437753749451, 0.02365062584230879, 0.020244628749795705, 0.022679143460116572, 0.02308023891229689, 0.022675197950153624, 0.02281681247631764, 0.022311794122079703, 0.021898352225678406, 0.022781567963939736, 0.02020787013616607, 0.021767619215106764, 0.023052573632750914, 0.023965632959254406, 0.023633759498558234, 0.020891361093332075, 0.022702846642691815, 0.021097054969714632, 0.02231268208505202, 0.02191316291629061, 0.021477780395346268, 0.019799705484858675, 0.020433124704961524, 0.023877265721102717, 0.02085071454241955, 0.020794747637153155, 0.019084427928516428, 0.021919270610325704, 0.020169038122467755, 0.023005286116058257, 0.023479003921519, 0.020817176958322376, 0.022105559911139813, 0.020685627721770423, 0.019101270041695508, 0.022411599656275042, 0.021673835758796833, 0.021024823994132833, 0.021633158067550923, 0.020754680930663807, 0.019721191349195612, 0.020041836346729194, 0.02238939796954776, 0.01991025699822125, 0.019821569149947014, 0.021875280721982637, 0.019647672297691515, 0.019410297630111734, 0.022169048367961276, 0.021137512745267234, 0.019877601014279555, 0.02018103065908926, 0.019995968571970977, 0.01971472074284544, 0.020847063648443666, 0.02156777907820869, 0.020407469593597533, 0.02089965700723799, 0.02029163201758626, 0.02035365904323031, 0.01844816907226637, 0.02030349036640997, 0.021896282754894608, 0.021796134102183326, 0.020908966169681003, 0.02066274542548744, 0.02225301649262317, 0.02077845730898252, 0.023024194174759833, 0.020265722591403957, 0.021755220146562452, 0.021900261790346154, 0.01888047857020522, 0.019612467965159867, 0.022504023223401837, 0.020950104203974498, 0.02125282113621409, 0.019914073867125195, 0.021276297604197185, 0.0217303471003689, 0.021428670302583126, 0.020267787981360846, 0.022091099887992422, 0.020960503928896156, 0.022316145490912397, 0.02194595265456099, 0.022265807630473287, 0.0237020021990352, 0.019111683614883464, 0.02123731325876164, 0.022964739281030454, 0.022641727041025905, 0.02104462831848466, 0.022532540284075784, 0.020193610864603685, 0.020678338640682404, 0.02133444010699627, 0.02150021223770593, 0.02172780042523757, 0.02108566229170755, 0.022207610678551544, 0.022770584254882435, 0.020076768836527967, 0.023529020997277664, 0.023045708077518236, 0.02069661057094242, 0.020255322240595484, 0.02053228888096763, 0.02116782450262399, 0.021021915696081046, 0.020948635661800975, 0.019868108420489975, 0.021705955739806045, 0.022340573965070706, 0.021524481030575436, 0.021161540452292748, 0.021988884973435627, 0.02332138190081575, 0.020815800882569988, 0.021708049137652768, 0.021625432463911357, 0.021671950454775102, 0.01884597610587622, 0.023077084008125515, 0.020458486810916145, 0.020200581214310388, 0.020602035539533336, 0.021580679033089436, 0.021120151041874806, 0.02183807500560415, 0.021489627056627963, 0.01912245914168745, 0.018851406607406782, 0.020361759732529745, 0.021067997881773923, 0.021128198355909268, 0.02236613675072744, 0.021429029061847526, 0.02243815785741355, 0.020823159976345103, 0.021246696349754617, 0.019464300469408526, 0.022207442383723856, 0.022535586213910126, 0.020828077940104824, 0.021871684501626843, 0.0225986967103139, 0.021477446785250744, 0.01956804376257645, 0.021713311880216923, 0.022811462737520695, 0.02337376386585647, 0.020463057127793553, 0.021363928255959654, 0.0214336509674048, 0.021796352795581855, 0.02174414749521158, 0.021757995780322024], "accuracy_test_std": 0.010137313070063911, "error_valid": [0.44389619022966864, 0.42529002729668675, 0.40467926393072284, 0.41224762330572284, 0.39334584431475905, 0.34469861869352414, 0.3509256753576807, 0.33523890483810237, 0.3434279108621988, 0.37127170792545183, 0.3647505059299698, 0.3150443571159638, 0.3531626506024097, 0.32614393119352414, 0.3219008847891567, 0.2869372999811747, 0.30643766472138556, 0.3173239834337349, 0.2888198301016567, 0.27095491340361444, 0.2865710890436747, 0.2932643660579819, 0.27074165803840367, 0.29501453077936746, 0.2784938582454819, 0.2618202301393072, 0.26097603303840367, 0.2739566665097892, 0.2662559417356928, 0.2662044663027108, 0.25198253953313254, 0.2755126953125, 0.2415256730045181, 0.24930728774472888, 0.2551578383847892, 0.23928722703313254, 0.23426175404743976, 0.24320377211972888, 0.24326554263930722, 0.22371223173945776, 0.2312202913215362, 0.22192235739834332, 0.22329601609563254, 0.22417109845632532, 0.23027461408132532, 0.23277632012424698, 0.22437405873493976, 0.2271919710090362, 0.2137833560805723, 0.23808711408132532, 0.22315335560993976, 0.2281685335090362, 0.2230415803840362, 0.2225018237010542, 0.21982657191265065, 0.2301216585090362, 0.22565506165286142, 0.2192971103162651, 0.2246284944465362, 0.20881965361445776, 0.22874799981174698, 0.23623546922063254, 0.21354951054216864, 0.2232342455760542, 0.21654096856174698, 0.22764936699924698, 0.22680516989834332, 0.21448636342243976, 0.22029426298945776, 0.2130715243787651, 0.2152687900037651, 0.22539033085466864, 0.23616340361445776, 0.24004023908132532, 0.22520796074924698, 0.21934858574924698, 0.23227774378765065, 0.21140372035015065, 0.20258377259036142, 0.2051369540662651, 0.2034279696912651, 0.2279949877635542, 0.2169777743787651, 0.22036485786897586, 0.22338720114834332, 0.20498399849397586, 0.20219697147966864, 0.2207619540662651, 0.21565559111445776, 0.2216473315135542, 0.20542227503765065, 0.2079239810805723, 0.2188397143260542, 0.2080666415662651, 0.21068159356174698, 0.21506582972515065, 0.2180764071912651, 0.21292003953313254, 0.2098771060805723, 0.19848338667168675, 0.21024478774472888, 0.21373335137424698, 0.19691706278237953, 0.20263524802334332, 0.2097756259412651, 0.2052590243787651, 0.2196236116340362, 0.21105809958584332, 0.19579783979668675, 0.2041912768260542, 0.2228680346385542, 0.20148513977786142, 0.2186470491340362, 0.21381424134036142, 0.2155438158885542, 0.21020360739834332, 0.2020337208207832, 0.20587967102786142, 0.21302151967243976, 0.1908532567771084, 0.21679540427334332, 0.19626553087349397, 0.21577766142695776, 0.20934911521084332, 0.23047757435993976, 0.21588061229292166, 0.1945477221385542, 0.20787397637424698, 0.21457754847515065, 0.20566641566265065, 0.1986466373305723, 0.1940785603350903, 0.20810782191265065, 0.21054922816265065, 0.20620617234563254, 0.21963390672063254, 0.20188076524849397, 0.20160721009036142, 0.20799604668674698, 0.20415009647966864, 0.2010174487010542, 0.2138348315135542, 0.18378347373870485, 0.20030561699924698, 0.19985704536897586, 0.18578807417168675, 0.20161750517695776, 0.20938000047063254, 0.1923695759600903, 0.18566600385918675, 0.19204454536897586, 0.21324507012424698, 0.19673469267695776, 0.2011086337537651, 0.19088414203689763, 0.22081342949924698, 0.18435411568147586, 0.1976906650037651, 0.2015969150037651, 0.18708084290286142, 0.18678522684487953, 0.1809464420180723, 0.2118817065135542, 0.19211661097515065, 0.19893195830195776, 0.1838452442582832, 0.20411038685993976, 0.1955948795180723, 0.18940017884036142, 0.19606257059487953, 0.1940785603350903, 0.1958596103162651, 0.18559540897966864, 0.18478062641189763, 0.18685582172439763, 0.19631700630647586, 0.19410944559487953, 0.19108857304216864, 0.19807746611445776, 0.18550422392695776, 0.18690729715737953, 0.1971509083207832, 0.18531891236822284, 0.2109566194465362, 0.1892163380082832, 0.1931534732680723, 0.18993846479668675, 0.18918545274849397, 0.19575813017695776, 0.21250235316265065, 0.18676463667168675, 0.2023293368787651, 0.19393589984939763, 0.20549286991716864, 0.2028485033885542, 0.19131212349397586, 0.1876603092055723, 0.20197342102786142, 0.1950051181287651, 0.19733474915286142, 0.19401826054216864, 0.19255341679216864, 0.1899281697100903, 0.18881924181099397, 0.1966626270707832, 0.18283779649849397, 0.19777155496987953, 0.18923692818147586, 0.19164891989834332, 0.1895119540662651, 0.1985451571912651, 0.18818977080195776], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.8220169086141571, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0030575125561567613, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "nesterov_momentum", "nb_data_augmentation": 4, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.00032874772679257675, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.02009781975463979}, "accuracy_valid_max": 0.8190535579819277, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8118102291980422, "loss_train": [10.5757474899292, 8.08327579498291, 6.253754615783691, 4.913029193878174, 3.9634830951690674, 3.310128688812256, 2.8714711666107178, 2.576512575149536, 2.3739583492279053, 2.2330596446990967, 2.1279051303863525, 2.045698642730713, 1.98145592212677, 1.9301965236663818, 1.8855451345443726, 1.8462731838226318, 1.8113977909088135, 1.7799731492996216, 1.75416898727417, 1.728296160697937, 1.7017544507980347, 1.6798778772354126, 1.6565628051757812, 1.6375844478607178, 1.6179157495498657, 1.5992289781570435, 1.5822103023529053, 1.5654494762420654, 1.545363426208496, 1.5307579040527344, 1.512608289718628, 1.4989932775497437, 1.4836183786392212, 1.4668015241622925, 1.4527043104171753, 1.4379048347473145, 1.4237008094787598, 1.4061682224273682, 1.3946330547332764, 1.3789069652557373, 1.3630670309066772, 1.3472635746002197, 1.335610032081604, 1.3251606225967407, 1.3135013580322266, 1.3027020692825317, 1.2930512428283691, 1.282562017440796, 1.2741252183914185, 1.2664631605148315, 1.2574609518051147, 1.2491339445114136, 1.2414952516555786, 1.2353848218917847, 1.2284327745437622, 1.218441128730774, 1.215427041053772, 1.2103416919708252, 1.2032678127288818, 1.1981534957885742, 1.1936341524124146, 1.1893060207366943, 1.186032772064209, 1.1834474802017212, 1.1790794134140015, 1.1752020120620728, 1.1736961603164673, 1.1707775592803955, 1.1686322689056396, 1.165437936782837, 1.1622377634048462, 1.161170482635498, 1.1581958532333374, 1.1583595275878906, 1.1547846794128418, 1.1539114713668823, 1.1527804136276245, 1.1481951475143433, 1.1485925912857056, 1.1457164287567139, 1.1445544958114624, 1.1424630880355835, 1.1416478157043457, 1.1416367292404175, 1.1385265588760376, 1.1389846801757812, 1.137117862701416, 1.1369819641113281, 1.1360431909561157, 1.1348471641540527, 1.1345012187957764, 1.1319109201431274, 1.131708025932312, 1.1290256977081299, 1.128335952758789, 1.1287325620651245, 1.126468539237976, 1.1244665384292603, 1.124123454093933, 1.122045636177063, 1.1217834949493408, 1.1219016313552856, 1.120898962020874, 1.1180099248886108, 1.1188206672668457, 1.1189860105514526, 1.1177781820297241, 1.1157060861587524, 1.1162766218185425, 1.1134635210037231, 1.1142586469650269, 1.1117745637893677, 1.1122286319732666, 1.110984444618225, 1.1110236644744873, 1.1101716756820679, 1.109920620918274, 1.106029748916626, 1.1060258150100708, 1.1065034866333008, 1.107258677482605, 1.1054531335830688, 1.1028790473937988, 1.1053266525268555, 1.1015398502349854, 1.1013309955596924, 1.1010475158691406, 1.1030704975128174, 1.098696231842041, 1.1006845235824585, 1.0980044603347778, 1.0979235172271729, 1.0988084077835083, 1.0967448949813843, 1.095346212387085, 1.096364140510559, 1.0975453853607178, 1.0950047969818115, 1.096108317375183, 1.094548225402832, 1.0916905403137207, 1.0939946174621582, 1.0926718711853027, 1.0927523374557495, 1.0913785696029663, 1.0911688804626465, 1.0920581817626953, 1.0892527103424072, 1.089901328086853, 1.0880982875823975, 1.0884400606155396, 1.0875327587127686, 1.0875478982925415, 1.089746356010437, 1.0843807458877563, 1.085024118423462, 1.0866978168487549, 1.0871468782424927, 1.0837459564208984, 1.0839197635650635, 1.0830531120300293, 1.0823860168457031, 1.0809558629989624, 1.0841264724731445, 1.0833767652511597, 1.0790760517120361, 1.0815708637237549, 1.0808477401733398, 1.0799068212509155, 1.079411268234253, 1.080391764640808, 1.0793507099151611, 1.078290581703186, 1.077162265777588, 1.0774785280227661, 1.0778765678405762, 1.0776201486587524, 1.0765016078948975, 1.0752277374267578, 1.0759388208389282, 1.0744810104370117, 1.0730527639389038, 1.0733318328857422, 1.0726068019866943, 1.0738494396209717, 1.0739082098007202, 1.0747621059417725, 1.073403000831604, 1.0716352462768555, 1.0728262662887573, 1.0710117816925049, 1.072658658027649, 1.0709254741668701, 1.0721279382705688, 1.0708222389221191, 1.0702812671661377, 1.069892168045044, 1.0719467401504517, 1.068764090538025, 1.0694875717163086, 1.070015788078308, 1.0705801248550415, 1.0683742761611938, 1.0692323446273804, 1.068361759185791, 1.0672471523284912, 1.0668928623199463, 1.0681891441345215, 1.0687099695205688, 1.0677142143249512, 1.065139889717102, 1.066393256187439], "accuracy_train_first": 0.571942807828073, "model": "residualv3", "loss_std": [0.9062358140945435, 0.6640193462371826, 0.5222828388214111, 0.41687893867492676, 0.34320318698883057, 0.2998737096786499, 0.2777470052242279, 0.26764997839927673, 0.2604660093784332, 0.25589585304260254, 0.2539663314819336, 0.2530200481414795, 0.25167036056518555, 0.24975553154945374, 0.24967867136001587, 0.24642160534858704, 0.24842970073223114, 0.2470698356628418, 0.24498051404953003, 0.24636180698871613, 0.24584706127643585, 0.24409940838813782, 0.24226143956184387, 0.24023695290088654, 0.2425902932882309, 0.24061769247055054, 0.24065428972244263, 0.23872986435890198, 0.2393721491098404, 0.238983616232872, 0.23867195844650269, 0.2363566756248474, 0.2372487485408783, 0.2366829812526703, 0.23528815805912018, 0.23506906628608704, 0.23705755174160004, 0.23617342114448547, 0.23597726225852966, 0.2336367964744568, 0.23277395963668823, 0.23213674128055573, 0.2334236353635788, 0.23379449546337128, 0.23207037150859833, 0.2331138402223587, 0.23265241086483002, 0.23160412907600403, 0.23194082081317902, 0.2318078875541687, 0.23401376605033875, 0.2298058718442917, 0.23310701549053192, 0.23549877107143402, 0.23234926164150238, 0.23189879953861237, 0.23069505393505096, 0.23136405646800995, 0.2309812307357788, 0.23217850923538208, 0.23093649744987488, 0.2301998734474182, 0.23006406426429749, 0.23102058470249176, 0.2311881184577942, 0.23037919402122498, 0.23005539178848267, 0.22922171652317047, 0.23102298378944397, 0.2292868196964264, 0.2298053354024887, 0.23039862513542175, 0.23093281686306, 0.23103322088718414, 0.22746019065380096, 0.2297745645046234, 0.229375422000885, 0.23009224236011505, 0.23043057322502136, 0.2303508222103119, 0.22848576307296753, 0.23093590140342712, 0.23085126280784607, 0.23160091042518616, 0.22810743749141693, 0.23135033249855042, 0.22914642095565796, 0.22830301523208618, 0.22916279733181, 0.2271001636981964, 0.22890032827854156, 0.2277798354625702, 0.22663545608520508, 0.22701705992221832, 0.22831909358501434, 0.22738993167877197, 0.22776015102863312, 0.22761541604995728, 0.22633801400661469, 0.2269381731748581, 0.22684751451015472, 0.2292286604642868, 0.22754253447055817, 0.2257266342639923, 0.22414511442184448, 0.2249433696269989, 0.22448164224624634, 0.22539125382900238, 0.22654397785663605, 0.22754116356372833, 0.22793728113174438, 0.2253057062625885, 0.22840255498886108, 0.22599777579307556, 0.22609193623065948, 0.22510160505771637, 0.23029842972755432, 0.22635582089424133, 0.22754748165607452, 0.22495310008525848, 0.22678019106388092, 0.22740504145622253, 0.22507551312446594, 0.2260252684354782, 0.22642211616039276, 0.2253638356924057, 0.22385463118553162, 0.22677524387836456, 0.22396376729011536, 0.22518813610076904, 0.22725507616996765, 0.22575941681861877, 0.22455242276191711, 0.22336453199386597, 0.2244768887758255, 0.22721894085407257, 0.22244639694690704, 0.22517463564872742, 0.2249501645565033, 0.2240898460149765, 0.2228335738182068, 0.2241453379392624, 0.226811483502388, 0.22450771927833557, 0.22355753183364868, 0.22567446529865265, 0.22590243816375732, 0.22577176988124847, 0.22822844982147217, 0.22587424516677856, 0.22414688766002655, 0.2242642641067505, 0.22339381277561188, 0.22648276388645172, 0.22527344524860382, 0.2241777777671814, 0.22348898649215698, 0.22414878010749817, 0.2245510369539261, 0.22273148596286774, 0.22490450739860535, 0.22485660016536713, 0.22448237240314484, 0.2236594259738922, 0.22362685203552246, 0.22342070937156677, 0.2230835258960724, 0.2239980697631836, 0.2226942479610443, 0.2237522006034851, 0.22453893721103668, 0.22373008728027344, 0.22664059698581696, 0.2241467982530594, 0.22519873082637787, 0.22264327108860016, 0.22200694680213928, 0.22418692708015442, 0.22183306515216827, 0.22419799864292145, 0.22303427755832672, 0.22433729469776154, 0.22184114158153534, 0.22417323291301727, 0.22217907011508942, 0.222291961312294, 0.223691925406456, 0.22269301116466522, 0.22352485358715057, 0.22185936570167542, 0.22253088653087616, 0.22430741786956787, 0.22617769241333008, 0.22403232753276825, 0.22589239478111267, 0.22172798216342926, 0.22315844893455505, 0.22273294627666473, 0.2235318124294281, 0.22223198413848877, 0.22371245920658112, 0.2245779186487198, 0.2207850217819214, 0.22367945313453674, 0.22195139527320862, 0.22435297071933746, 0.22431176900863647, 0.22294238209724426, 0.2211337685585022, 0.22144588828086853, 0.22092702984809875, 0.22432783246040344]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:15 2016", "state": "available"}], "summary": "1861b5e9d9be6ab87d23c76994255dc9"}