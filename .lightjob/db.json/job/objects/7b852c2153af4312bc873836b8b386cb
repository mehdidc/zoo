{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 64, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.856462001800537, 1.526032567024231, 1.4085278511047363, 1.3313714265823364, 1.2715978622436523, 1.2222627401351929, 1.1801027059555054, 1.1432888507843018, 1.1105791330337524, 1.0812894105911255, 1.0549771785736084, 1.0311774015426636, 1.0096136331558228, 0.9900927543640137, 0.9723347425460815, 0.9561910033226013, 0.9413924813270569, 0.927839457988739, 0.9154081344604492, 0.9040009379386902, 0.8934979438781738, 0.8838199973106384, 0.8749522566795349, 0.8667963743209839, 0.8593077659606934, 0.8523536920547485, 0.8459693193435669, 0.8400834798812866, 0.834649384021759, 0.8296658992767334, 0.8250518441200256, 0.8208138346672058, 0.8169106841087341, 0.8133183717727661, 0.8100075125694275, 0.8069692850112915, 0.804171085357666, 0.8015912175178528, 0.7992190718650818, 0.7970332503318787, 0.7950302362442017, 0.7931860685348511, 0.7914908528327942, 0.7899367213249207, 0.7885040640830994, 0.7871890068054199, 0.7859798073768616, 0.7848691344261169, 0.7838526964187622, 0.7829180359840393, 0.7820613980293274, 0.7812722325325012, 0.780549168586731, 0.7798848152160645, 0.7792715430259705, 0.7787070870399475, 0.7781906127929688, 0.7777161002159119, 0.7772800326347351, 0.7768782377243042, 0.776508092880249, 0.7761685848236084, 0.7758558392524719, 0.775568425655365, 0.7753037214279175, 0.7750601172447205, 0.7748352885246277, 0.7746288776397705, 0.7744389772415161, 0.7742639780044556, 0.7741023898124695, 0.7739537954330444, 0.7738171219825745, 0.7736912965774536, 0.7735757231712341, 0.7734698057174683, 0.7733726501464844, 0.7732831239700317, 0.7732011675834656, 0.773125946521759, 0.7730568647384644, 0.7729937434196472, 0.7729361653327942, 0.7728834748268127, 0.7728352546691895, 0.77279132604599, 0.7727512121200562, 0.7727146148681641, 0.7726813554763794, 0.7726510763168335, 0.7726237177848816, 0.7725986242294312, 0.7725759148597717, 0.7725553512573242, 0.7725368142127991, 0.7725198864936829, 0.7725045680999756, 0.7724907994270325, 0.7724784016609192, 0.7724673748016357, 0.7724572420120239, 0.7724484205245972, 0.7724403738975525, 0.7724332809448242, 0.772426962852478, 0.7724214196205139, 0.7724164128303528, 0.7724120616912842, 0.7724081873893738, 0.7724049091339111, 0.7724018096923828, 0.7723994255065918, 0.7723971009254456, 0.7723951935768127, 0.772393524646759, 0.7723920345306396, 0.7723907828330994, 0.7723897695541382, 0.7723888158798218, 0.7723879814147949, 0.7723873853683472, 0.7723867893218994, 0.7723862528800964, 0.7723858952522278, 0.7723855376243591, 0.7723851799964905, 0.7723850011825562, 0.7723848223686218, 0.7723845839500427, 0.7723844051361084, 0.7723843455314636, 0.7723841667175293, 0.7723841667175293, 0.772383987903595, 0.772383987903595, 0.7723838686943054, 0.7723838090896606, 0.7723838090896606, 0.7723838090896606, 0.7723838090896606, 0.7723838090896606, 0.7723836898803711, 0.7723836898803711, 0.7723836898803711, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815, 0.7723835706710815], "moving_avg_accuracy_train": [0.04259813209440752, 0.08461847732673494, 0.12517298350117662, 0.16495711383784373, 0.20331269026206045, 0.24002728903185844, 0.2747511500257748, 0.30767401037734093, 0.3387062168400683, 0.3678302570469806, 0.39521827433554574, 0.4208208730047966, 0.4446118015773328, 0.466765143469941, 0.48737730013277525, 0.5065557429173475, 0.5243672935448634, 0.5408743085667613, 0.5561329088793359, 0.5702562381118342, 0.5833764245627402, 0.5955007405090131, 0.6067450129939645, 0.6171901988196604, 0.6268372957877775, 0.6358241694066741, 0.6441633996374722, 0.6518547187499524, 0.6589605566583181, 0.6655093066460946, 0.671528703621989, 0.6771135716145797, 0.6822399702555397, 0.6869722755728707, 0.6913220311620398, 0.6953484904327868, 0.6990652736800212, 0.7024987342572941, 0.7056702289851731, 0.7086013041509784, 0.7113113874621172, 0.7138015796671329, 0.7160961950254473, 0.718214863319368, 0.7201425911231825, 0.7219193627763681, 0.7235394557011585, 0.7250370668632317, 0.72642211929005, 0.7276896288622908, 0.7288466635189743, 0.7299251970909418, 0.7309098281985698, 0.7317820813513964, 0.73256943433775, 0.733284991423078, 0.7339406545927396, 0.7345516777847207, 0.7351108632039232, 0.7356164552300148, 0.7360737771534881, 0.7364900171822331, 0.736862308059294, 0.7371973698486488, 0.7375035757566872, 0.737776871973931, 0.7380321031158699, 0.7382664614412338, 0.7384843233316711, 0.7386920247771124, 0.7388836063756286, 0.7390583549631026, 0.7392202789894483, 0.7393567100179212, 0.7394818230923565, 0.7395967500081578, 0.7396978590835694, 0.7397935075490589, 0.7398912169120471, 0.7399861307851651, 0.7400762035685902, 0.7401595942224823, 0.7402299955133662, 0.7402933566751617, 0.7403480565719682, 0.7404042619255227, 0.7404548106949028, 0.7405072800337735, 0.7405498521411382, 0.7405904921865759, 0.7406340436738984, 0.7406685897148696, 0.7407043314493628, 0.7407388241592161, 0.7407652173004651, 0.7407866459787795, 0.7408059317892626, 0.7408232890186973, 0.7408365853763791, 0.7408532023959118, 0.7408681577134912, 0.7408816174993127, 0.740898381604171, 0.740911144149734, 0.7409226304407407, 0.7409329681026467, 0.7409422719983622, 0.7409506455045061, 0.7409581816600356, 0.7409672893488216, 0.740975486268729, 0.7409875137942648, 0.7409960134184375, 0.7410059882290024, 0.7410149655585109, 0.7410230451550684, 0.7410303167919703, 0.7410391864139915, 0.7410471690738105, 0.7410543534676476, 0.741060819422101, 0.741066638781109, 0.7410718762042162, 0.7410765898850128, 0.7410831573465392, 0.7410890680619129, 0.7410943877057492, 0.741099175385202, 0.7411034842967095, 0.7411073623170662, 0.7411108525353872, 0.7411139937318761, 0.7411168208087162, 0.7411193651778722, 0.7411216551101126, 0.741123716049129, 0.7411255708942437, 0.741127240254847, 0.74112874267939, 0.7411300948614786, 0.7411313118253584, 0.7411324070928502, 0.7411333928335928, 0.7411342800002612, 0.7411350784502627, 0.7411357970552641, 0.7411364437997654, 0.7411370258698166, 0.7411375497328625, 0.7411380212096039, 0.7411384455386711, 0.7411388274348317, 0.7411391711413762, 0.7411394804772662, 0.7411397588795672, 0.7411400094416382, 0.741140234947502, 0.7411404379027795, 0.7411406205625292, 0.7411407849563039, 0.7411409329107012, 0.7411410660696587, 0.7411411859127205, 0.7411412937714761, 0.7411413908443562, 0.7411414782099481], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04282005953501505, 0.08539878753294425, 0.12610619087678837, 0.1650754263636577, 0.2026635633206654, 0.23817951591179162, 0.2714847347366516, 0.30256924201411595, 0.33178335626338207, 0.3590668876362908, 0.38439213834931835, 0.40827437829827207, 0.43043169197446896, 0.45071507115804615, 0.46941971356219636, 0.4866333392033412, 0.5026291790876004, 0.5174659176170934, 0.5310020877623871, 0.5433667168532418, 0.5546413674100109, 0.5649726878885128, 0.5746503237965742, 0.5834955029662391, 0.5916647132588471, 0.5991268658034443, 0.6059414888522414, 0.6122709916048186, 0.6179787216047283, 0.623091264542147, 0.6276050449497546, 0.6317030389016918, 0.6355143332795948, 0.638919054648548, 0.6421450838128349, 0.6451594028506026, 0.6480065673283435, 0.65059445892947, 0.6529968035579838, 0.6551090560899867, 0.6569480187038796, 0.6586274991188833, 0.6603119889472058, 0.6619012719801961, 0.6634313419772065, 0.664857233099516, 0.6661527421408445, 0.6672566356131305, 0.6682867608319379, 0.6692504946226147, 0.6701300620654738, 0.6708972587015468, 0.6715877356740125, 0.6721725438554819, 0.6726988712188042, 0.6731359447520443, 0.6735415179632103, 0.6739197703931694, 0.6742978481825423, 0.6746381181929779, 0.6749565682336199, 0.6752431732701977, 0.6755255318656176, 0.6757796546014956, 0.6760083650637858, 0.676189790417347, 0.676377487298052, 0.6765342074594366, 0.6766630485734327, 0.6767667985447792, 0.676860173518991, 0.6769442109957817, 0.6770076376936434, 0.6770769287529688, 0.6771392907063616, 0.6772076234956652, 0.6772813300372885, 0.6773110448309995, 0.6773377881453393, 0.6773740641594952, 0.6774067125722354, 0.6774360961437016, 0.6774625413580213, 0.677486342050909, 0.6774955556432578, 0.6775038478763719, 0.6775113108861744, 0.6775180275949968, 0.6775240726329369, 0.6775417201983329, 0.6776074606408491, 0.6776788340703636, 0.6777430701569266, 0.6778008826348333, 0.6778529138649493, 0.6778997419720537, 0.6779418872684477, 0.6779798180352024, 0.6780139557252814, 0.6780568866776027, 0.6780955245346918, 0.678130298606072, 0.6781615952703142, 0.6782019692993821, 0.6782505129567933, 0.6782942022484634, 0.6783335226109665, 0.6783689109372192, 0.6784007604308466, 0.6784294249751114, 0.6784552230649497, 0.678478441345804, 0.6784993377985731, 0.6785181446060652, 0.678535070732808, 0.6785625112781267, 0.6785872077689135, 0.6786094346106215, 0.6786294387681588, 0.6786474425099424, 0.6786636458775476, 0.6786782289083922, 0.6786913536361524, 0.6787031658911366, 0.6787137969206223, 0.6787233648471594, 0.678731975981043, 0.6787397260015381, 0.6787467010199837, 0.6787529785365848, 0.6787586283015258, 0.6787637130899726, 0.6787682893995748, 0.6787724080782167, 0.6787761148889945, 0.6787794510186944, 0.6787824535354244, 0.6787851558004814, 0.6787875878390327, 0.6787897766737289, 0.6787917466249554, 0.6787935195810594, 0.6787951152415529, 0.678796551335997, 0.6787978438209967, 0.6787990070574965, 0.6788000539703463, 0.678800996191911, 0.6788018441913194, 0.6788026073907869, 0.6788032942703076, 0.6788039124618763, 0.6788044688342881, 0.6788049695694588, 0.6788054202311123, 0.6788058258266005, 0.6788061908625399, 0.6788065193948853, 0.6788068150739962, 0.6788070811851961, 0.6788073206852759, 0.6788075362353477, 0.6788077302304124, 0.6788079048259706, 0.678808061961973, 0.6788082033843751], "moving_var_accuracy_train": [0.016331407721393333, 0.030589651670249837, 0.0423326982427003, 0.052344421658234525, 0.06035033167791644, 0.06644695437557008, 0.07065397763893654, 0.07334381247860139, 0.07467641177224928, 0.07484265805678897, 0.07410932367009877, 0.07259782883065745, 0.07043212048868339, 0.06780584345291214, 0.06484900812825993, 0.061674421324203935, 0.0583622412135926, 0.054978350996631814, 0.05157593984845872, 0.04821356172109872, 0.04494145918154766, 0.04177030459787835, 0.03873117711153304, 0.03583997656277959, 0.0330935772257119, 0.0305110945801189, 0.02808586996968732, 0.025809690479928614, 0.023683157823355467, 0.02170081717864155, 0.01985683372035806, 0.01815186710277423, 0.016573200059731123, 0.015117432476305951, 0.013775972591844935, 0.012544286700991296, 0.01141418833025447, 0.010378867361050207, 0.009431506034225861, 0.008565676245451681, 0.0077752095848863285, 0.007053498141358984, 0.006395535664006596, 0.005796380895662923, 0.0052501880164670224, 0.004753581472388396, 0.004301845634914156, 0.0038918466241576353, 0.0035199272937671867, 0.0031823937890319677, 0.0028762029728996697, 0.0025990517876024505, 0.0023478720946051817, 0.002119932315208208, 0.0019135184062134658, 0.001726774763073387, 0.0015579663344945042, 0.001405529845115303, 0.0012677910556012093, 0.0011433125597127155, 0.0010308635938166472, 0.000929336536288749, 0.0008376502871341593, 0.0007548956560449144, 0.0006802499489634815, 0.0006128971714683713, 0.0005521937407438725, 0.0004974686810914916, 0.0004481489872120869, 0.00040372234750482366, 0.00036368044433435134, 0.0003275872335203338, 0.0002950644846810725, 0.00026572555704273715, 0.00023929388087101525, 0.00021548336654769404, 0.00019402703729909997, 0.00017470667122974443, 0.00015732192818330998, 0.0001416708131547712, 0.0001275767497961197, 0.00011488166082691684, 0.00010343810182004828, 9.313042336946017e-05, 8.38443097409099e-05, 7.548831014273247e-05, 6.796247573123188e-05, 6.11910054418025e-05, 5.508821635655144e-05, 4.959425924053488e-05, 4.46519039049134e-05, 4.01974543749431e-05, 3.618920618170998e-05, 3.258099328683621e-05, 2.9329163339297464e-05, 2.6400379699656483e-05, 2.3763689212064738e-05, 2.1390031751581125e-05, 1.9252619714571426e-05, 1.7329842871157617e-05, 1.5598871537756954e-05, 1.4040614876490485e-05, 1.2639082705746745e-05, 1.1376640378295292e-05, 1.0240163754395572e-05, 9.21710918423916e-06, 8.296177328094591e-06, 7.467190635731402e-06, 6.72098271491974e-06, 6.0496309933829755e-06, 5.445272599508398e-06, 4.902047291892174e-06, 4.412492755202641e-06, 3.972138951294626e-06, 3.575650388171084e-06, 3.218672868278771e-06, 2.8972814717799776e-06, 2.6082613563551597e-06, 2.348008726439719e-06, 2.113672393429008e-06, 1.9026814311890487e-06, 1.7127180725235268e-06, 1.5416931406784148e-06, 1.3877237956904385e-06, 1.2493396000795024e-06, 1.12472006907762e-06, 1.0125027496647739e-06, 9.114587715691785e-07, 8.20479994877675e-07, 7.385673467668932e-07, 6.648202467055624e-07, 5.984270260734469e-07, 5.386562547372393e-07, 4.848488935931365e-07, 4.364111983408163e-07, 3.928083057333996e-07, 3.5355843921365855e-07, 3.1822767617570793e-07, 2.8642522407370345e-07, 2.577991572339419e-07, 2.3203257052031092e-07, 2.0884010996618812e-07, 1.8796484413287514e-07, 1.6917544330186547e-07, 1.52263636673324e-07, 1.3704192054432417e-07, 1.2334149299594113e-07, 1.1101039294624701e-07, 9.991182354404133e-08, 8.992264180249665e-08, 8.093199811866318e-08, 7.2840110908894e-08, 6.55571630257033e-08, 5.900230792136895e-08, 5.310277469980324e-08, 4.7793062261985586e-08, 4.301421371183879e-08, 3.871316305825684e-08, 3.4842147033688736e-08, 3.1358175558138525e-08, 2.82225550168578e-08, 2.540045909694386e-08, 2.286054244848467e-08, 2.0574592905236717e-08, 1.8517218423009468e-08, 1.666556527542863e-08], "duration": 77039.333568, "accuracy_train": [0.4259813209440753, 0.4628015844176818, 0.4901635390711517, 0.5230142868678479, 0.548512878080011, 0.5704586779600407, 0.5872658989710225, 0.603979753541436, 0.6179960750046143, 0.6299466189091916, 0.641710429932632, 0.6512442610280547, 0.6587301587301587, 0.6661452205034145, 0.6728867100982835, 0.6791617279784976, 0.6846712491925064, 0.6894374437638427, 0.6934603116925064, 0.6973662012043189, 0.7014581026208934, 0.7046195840254706, 0.7079434653585271, 0.7111968712509229, 0.7136611685008306, 0.7167060319767442, 0.7192164717146549, 0.7210765907622739, 0.7229130978336102, 0.7244480565360835, 0.7257032764050388, 0.7273773835478959, 0.7283775580241787, 0.7295630234288483, 0.7304698314645626, 0.731586623869509, 0.7325163229051311, 0.7333998794527501, 0.7342136815360835, 0.7349809806432264, 0.7357021372623661, 0.7362133095122739, 0.7367477332502769, 0.7372828779646549, 0.7374921413575121, 0.7379103076550388, 0.7381202920242709, 0.7385155673218899, 0.7388875911314138, 0.7390972150124584, 0.7392599754291251, 0.739631999238649, 0.7397715081672204, 0.7396323597268365, 0.7396556112149317, 0.7397250051910299, 0.7398416231196936, 0.7400508865125508, 0.7401435319767442, 0.7401667834648394, 0.7401896744647471, 0.7402361774409376, 0.7402129259528424, 0.7402129259528424, 0.7402594289290328, 0.7402365379291251, 0.7403291833933185, 0.740375686369509, 0.7404450803456073, 0.7405613377860835, 0.7406078407622739, 0.7406310922503692, 0.7406775952265596, 0.7405845892741787, 0.7406078407622739, 0.7406310922503692, 0.7406078407622739, 0.7406543437384644, 0.7407706011789406, 0.7408403556432264, 0.7408868586194168, 0.7409101101075121, 0.7408636071313216, 0.7408636071313216, 0.7408403556432264, 0.7409101101075121, 0.7409097496193245, 0.7409795040836102, 0.7409330011074198, 0.740956252595515, 0.7410260070598007, 0.7409795040836102, 0.7410260070598007, 0.7410492585478959, 0.7410027555717055, 0.7409795040836102, 0.7409795040836102, 0.7409795040836102, 0.740956252595515, 0.7410027555717055, 0.7410027555717055, 0.7410027555717055, 0.7410492585478959, 0.7410260070598007, 0.7410260070598007, 0.7410260070598007, 0.7410260070598007, 0.7410260070598007, 0.7410260070598007, 0.7410492585478959, 0.7410492585478959, 0.7410957615240864, 0.7410725100359912, 0.7410957615240864, 0.7410957615240864, 0.7410957615240864, 0.7410957615240864, 0.7411190130121816, 0.7411190130121816, 0.7411190130121816, 0.7411190130121816, 0.7411190130121816, 0.7411190130121816, 0.7411190130121816, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769, 0.7411422645002769], "end": "2016-01-31 22:28:02.426000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0], "moving_var_accuracy_valid": [0.016502017487240103, 0.031168348439810863, 0.042965347778815485, 0.052336224830813634, 0.0598184147068212, 0.0651890192322352, 0.06865325571775796, 0.07048414948012678, 0.07111691477443667, 0.07070474305458148, 0.06940658366222265, 0.06759917776081498, 0.06525777892884192, 0.06243474027590069, 0.05934003907551482, 0.056072815337384646, 0.05276833584607191, 0.04947266155319759, 0.0461744465176989, 0.0429329583389187, 0.03978372221162214, 0.03676597563592554, 0.033932287803253976, 0.031243193773819837, 0.02871949836768153, 0.026348702016302926, 0.024131783600347408, 0.022079168686166592, 0.020164455453316757, 0.018383252765567636, 0.016728295411123792, 0.015206607859882437, 0.013816680757409517, 0.012539341830070423, 0.01137907302458284, 0.010322940795477596, 0.009363603825999626, 0.008487518089852293, 0.007690707618294408, 0.006961791353295538, 0.006296048269423646, 0.005691829332660708, 0.005148183953230135, 0.004656097942937678, 0.004211558176405676, 0.0038087008481992373, 0.0034429358564647887, 0.0031096094980017116, 0.002808198969899348, 0.0025357381182830414, 0.002289127056433575, 0.0020655116668958345, 0.001863251326251801, 0.001680004199108641, 0.001514496963638213, 0.0013647665667355223, 0.00122977031672851, 0.0011080809591625885, 0.000998559348579683, 0.0008997454668417307, 0.0008106836140130215, 0.000730354534634645, 0.0006580366185588485, 0.0005928141619869748, 0.0005340035220683263, 0.0004808994062917269, 0.00043312653673379195, 0.0003900349339412724, 0.000351180840841047, 0.0003161596332659319, 0.00028462213991162027, 0.00025622348659800633, 0.00023063734445222034, 0.00020761682126512027, 0.00018689014025768705, 0.00016824315056276445, 0.00015146772939499052, 0.000136328903176179, 0.000122702449702318, 0.00011044404827491351, 9.940923671711228e-05, 8.947608359385006e-05, 8.053476937870874e-05, 7.248639069767526e-05, 6.523851564046347e-05, 5.871528292658728e-05, 5.284425590256637e-05, 4.7560236339906364e-05, 4.2804541588269e-05, 3.8526890358521784e-05, 3.471309757470952e-05, 3.1287635315204505e-05, 2.8196008257036465e-05, 2.5406487974748273e-05, 2.2890204417439972e-05, 2.0620919820230866e-05, 1.857481387228104e-05, 1.673028117265228e-05, 1.5067741492342522e-05, 1.3577554943113158e-05, 1.2233235404805803e-05, 1.1020794988688432e-05, 9.927530820553789e-06, 8.949448298507049e-06, 8.075711848730033e-06, 7.28531945171672e-06, 6.570702324711395e-06, 5.924903094954996e-06, 5.3415422976584386e-06, 4.814782972773738e-06, 4.33929454845009e-06, 3.910216890697599e-06, 3.5231251572727788e-06, 3.1739959056179007e-06, 2.8591747589547546e-06, 2.580034134805712e-06, 2.327519971239749e-06, 2.0992142665466073e-06, 1.8928943367609195e-06, 1.706522115548694e-06, 1.5382328460895554e-06, 1.3863235445781407e-06, 1.2492415164293346e-06, 1.1255731290966978e-06, 1.0140329852783685e-06, 9.134535937145175e-07, 8.227755989838939e-07, 7.410386044445746e-07, 6.673726019409637e-07, 6.009900066789526e-07, 5.411782846060455e-07, 4.872931518073809e-07, 4.387523201128139e-07, 3.95029759725331e-07, 3.5565044776807457e-07, 3.201855708436411e-07, 2.882481497196997e-07, 2.5948905487567204e-07, 2.3359338269173798e-07, 2.1027716339850683e-07, 1.8928437342916946e-07, 1.7038422644636804e-07, 1.5336871899342463e-07, 1.380504083993537e-07, 1.242604022166882e-07, 1.1184654006740792e-07, 1.0067175029930179e-07, 9.061256530266567e-08, 8.155778069936724e-08, 7.340724489027462e-08, 6.607076663253085e-08, 5.9467129416617507e-08, 5.3523202427300885e-08, 4.8173138805970294e-08, 4.3357652788706806e-08, 3.902336807913627e-08, 3.512223053235574e-08, 3.1610978880637964e-08, 2.8450667827803573e-08, 2.5606238381559012e-08, 2.304613078599707e-08, 2.0741935863898452e-08, 1.866808098427448e-08, 1.680154723832737e-08, 1.5121614740003698e-08, 1.3609633268665667e-08], "accuracy_test": 0.678563456632653, "start": "2016-01-31 01:04:03.093000", "learning_rate_per_epoch": [0.0006893877871334553, 0.00063178944401443, 0.0005790034774690866, 0.0005306277889758348, 0.0004862938658334315, 0.00044566404540091753, 0.0004084288375452161, 0.00037430462543852627, 0.000343031482771039, 0.0003143712237942964, 0.000288105511572212, 0.00026403431547805667, 0.0002419742668280378, 0.00022175733465701342, 0.00020322951604612172, 0.00018624970107339323, 0.00017068855231627822, 0.00015642752987332642, 0.0001433580182492733, 0.0001313804677920416, 0.00012040363799314946, 0.0001103439208236523, 0.00010112469317391515, 9.267572750104591e-05, 8.493267523590475e-05, 7.78365574660711e-05, 7.133331382647157e-05, 6.537341687362641e-05, 5.991147190798074e-05, 5.4905871365917847e-05, 5.031848922953941e-05, 4.611438271240331e-05, 4.226152668707073e-05, 3.873057721648365e-05, 3.549463872332126e-05, 3.25290639011655e-05, 2.981126272061374e-05, 2.7320533263264224e-05, 2.5037903469637968e-05, 2.294598743901588e-05, 2.10288508242229e-05, 1.927189077832736e-05, 1.7661724996287376e-05, 1.6186088032554835e-05, 1.4833741261099931e-05, 1.3594382835435681e-05, 1.2458573110052384e-05, 1.1417660061852075e-05, 1.0463714716024697e-05, 9.589472028892487e-06, 8.788271770754363e-06, 8.05401214165613e-06, 7.381099749181885e-06, 6.764409135939786e-06, 6.199243216542527e-06, 5.681296897819266e-06, 5.206624791753711e-06, 4.771611656906316e-06, 4.372943749331171e-06, 4.007584720966406e-06, 3.672751290650922e-06, 3.365893007867271e-06, 3.0846729259792482e-06, 2.8269487302168272e-06, 2.590757276266231e-06, 2.3742995836073533e-06, 2.1759269657195546e-06, 1.9941282971558394e-06, 1.8275189859195962e-06, 1.6748298321545008e-06, 1.5348979331974988e-06, 1.4066572475712746e-06, 1.289131091652962e-06, 1.1814242952823406e-06, 1.0827163805515738e-06, 9.922555364028085e-07, 9.093526500691951e-07, 8.333763048540277e-07, 7.637477779098845e-07, 6.999366632953752e-07, 6.414570066226588e-07, 5.878633260181232e-07, 5.3874737204751e-07, 4.937350581712963e-07, 4.524835048869136e-07, 4.1467853861831827e-07, 3.8003216218385205e-07, 3.482804800114536e-07, 3.1918165177557967e-07, 2.925140449860919e-07, 2.680745012639818e-07, 2.4567688683418965e-07, 2.2515060038585943e-07, 2.063392798845598e-07, 1.8909963728219736e-07, 1.7330037849205837e-07, 1.5882113757470506e-07, 1.4555163829754747e-07, 1.3339081306185108e-07, 1.2224602130572748e-07, 1.1203237448853542e-07, 1.0267207528613653e-07, 9.409382784042464e-08, 8.623229064141924e-08, 7.902758625277784e-08, 7.242483235359032e-08, 6.637374383444694e-08, 6.082822068265159e-08, 5.574602468527701e-08, 5.108844547407898e-08, 4.682000920297469e-08, 4.290820143637575e-08, 3.932322201194438e-08, 3.6037768325059005e-08, 3.302681506056615e-08, 3.026742589895548e-08, 2.773858476246005e-08, 2.5421027061156565e-08, 2.3297101137131904e-08, 2.135062970864965e-08, 1.9566785525171326e-08, 1.7931981233232364e-08, 1.6433764571388565e-08, 1.5060724223303623e-08, 1.380240188808557e-08, 1.264921145605058e-08, 1.159237061898466e-08, 1.0623828927691648e-08, 9.736208284039094e-09, 8.922748762074661e-09, 8.177253540964102e-09, 7.494044496070273e-09, 6.867917345942942e-09, 6.2941030165575285e-09, 5.768231225999898e-09, 5.286295845507993e-09, 4.844626033673194e-09, 4.439857814730885e-09, 4.068907877297079e-09, 3.728950925818708e-09, 3.4173972540685327e-09, 3.1318738713537186e-09, 2.8702060728136303e-09, 2.6304005640298556e-09, 2.410630806082281e-09, 2.2092228046943774e-09, 2.024642453690717e-09, 1.8554836556106125e-09, 1.7004581076562886e-09, 1.5583848655964516e-09, 1.428181906071302e-09, 1.3088573558306393e-09, 1.1995023863065057e-09, 1.0992839971635249e-09, 1.007438910072267e-09, 9.232674624826132e-10, 8.461285005978425e-10, 7.754344943933233e-10, 7.106469857021125e-10, 6.51272480389764e-10, 5.968586735960457e-10, 5.469911745770162e-10, 5.012900650136487e-10, 4.594073177432989e-10, 4.210238546686895e-10, 3.8584732631186114e-10], "accuracy_train_first": 0.4259813209440753, "accuracy_train_last": 0.7411422645002769, "batch_size_eval": 1024, "accuracy_train_std": [0.014629967383292228, 0.013831328349510777, 0.01454053513536464, 0.01364639620293517, 0.013376696869517011, 0.014284320387042825, 0.016087094387436687, 0.017057873974109117, 0.017265073219408976, 0.017592690260093732, 0.017625418995945486, 0.016481231027967596, 0.01540906989487462, 0.01608497190035242, 0.015795915280253744, 0.015908729800868004, 0.015463848187387491, 0.015429127121252856, 0.014735925830070095, 0.01443724435794369, 0.01437041598587248, 0.014341716414593166, 0.014286607499657914, 0.014074720171004498, 0.014195352592788549, 0.014148315680234195, 0.01410625261649296, 0.013640773933658137, 0.013493117591280102, 0.012867714534989935, 0.012997703637094607, 0.01301626856635277, 0.013019293497631087, 0.013115970793209171, 0.0128811310544076, 0.012356802492095033, 0.01241210855138577, 0.01269094765408086, 0.012893064420634499, 0.012690682806114349, 0.012889117569086716, 0.013049141012202099, 0.013101087438187258, 0.013002454348708066, 0.012705214197228873, 0.012790199970852113, 0.012682943046843621, 0.012782561763349775, 0.012805834667340721, 0.012904363132832913, 0.012609008676400319, 0.012389041422461661, 0.012460811710203436, 0.012539811459385554, 0.012405283275163244, 0.012668643646867555, 0.012541773960137144, 0.012525750670548364, 0.012383190533174947, 0.01229777964533692, 0.012215787420289502, 0.01233977845776514, 0.012274262171248444, 0.012364576495786466, 0.012359059231768455, 0.012458474670988099, 0.012469381478436736, 0.01240870110504858, 0.012403365919151893, 0.012265525593962165, 0.012323301671629476, 0.012334546651429688, 0.012331121945756527, 0.012282458907617111, 0.012299324952007234, 0.01230321173639766, 0.012306707379547616, 0.012286741587306827, 0.012266492694602242, 0.012301143718647045, 0.012337474975438599, 0.012274407036907128, 0.012286122996354142, 0.012193365620174009, 0.01220291950577611, 0.012218783745819176, 0.01223748634406621, 0.012232509105455425, 0.012233397110600033, 0.012188345624717815, 0.012216583958266948, 0.012180423361272856, 0.01221100669156017, 0.01216009392980255, 0.012159386693000013, 0.012178559036156762, 0.012186014625294833, 0.012208354074162975, 0.012199518328347803, 0.012196677662528679, 0.012196677662528679, 0.012196677662528679, 0.012230845463977104, 0.012238867609448902, 0.012238867609448902, 0.012238867609448902, 0.012238867609448902, 0.012238867609448902, 0.012238867609448902, 0.012247542542398591, 0.012247542542398591, 0.012277694486709033, 0.01226727815804419, 0.012272144992661525, 0.012272144992661525, 0.012272144992661525, 0.012272144992661525, 0.012282513177670415, 0.012282513177670415, 0.012282513177670415, 0.012282513177670415, 0.012282513177670415, 0.012282513177670415, 0.012282513177670415, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496, 0.012309441640430496], "accuracy_test_std": 0.013301763336102296, "error_valid": [0.5717994046498494, 0.5313926604856928, 0.5075271790286144, 0.4842014542545181, 0.4590432040662651, 0.4421769107680723, 0.4287682958396084, 0.41767019248870485, 0.40528961549322284, 0.3953813300075302, 0.38768060523343373, 0.3767854621611446, 0.37015248493975905, 0.36673451618975905, 0.36223850480045183, 0.3584440300263554, 0.35340826195406627, 0.3490034356174698, 0.3471723809299698, 0.34535162132906627, 0.34388677757906627, 0.3420454278049698, 0.3382509530308735, 0.33689788450677716, 0.3348123941076807, 0.3337137612951807, 0.33272690370858427, 0.33076348362198793, 0.33065170839608427, 0.33089584902108427, 0.33177093138177716, 0.3314150155308735, 0.33018401731927716, 0.3304384530308735, 0.32882065370858427, 0.32771172580948793, 0.32636895237198793, 0.3261145166603916, 0.3253820947853916, 0.32588067112198793, 0.32650131777108427, 0.32625717714608427, 0.3245276025978916, 0.3237951807228916, 0.3227980280496988, 0.3223097467996988, 0.3221876764871988, 0.32280832313629515, 0.32244211219879515, 0.32207590126129515, 0.32195383094879515, 0.32219797157379515, 0.32219797157379515, 0.32256418251129515, 0.32256418251129515, 0.32293039344879515, 0.32280832313629515, 0.3226759577371988, 0.32229945171310237, 0.32229945171310237, 0.32217738140060237, 0.32217738140060237, 0.32193324077560237, 0.32193324077560237, 0.32193324077560237, 0.32217738140060237, 0.32193324077560237, 0.32205531108810237, 0.32217738140060237, 0.32229945171310237, 0.32229945171310237, 0.32229945171310237, 0.32242152202560237, 0.32229945171310237, 0.32229945171310237, 0.32217738140060237, 0.32205531108810237, 0.32242152202560237, 0.32242152202560237, 0.32229945171310237, 0.32229945171310237, 0.32229945171310237, 0.32229945171310237, 0.32229945171310237, 0.32242152202560237, 0.32242152202560237, 0.32242152202560237, 0.32242152202560237, 0.32242152202560237, 0.32229945171310237, 0.32180087537650603, 0.32167880506400603, 0.32167880506400603, 0.32167880506400603, 0.32167880506400603, 0.32167880506400603, 0.32167880506400603, 0.32167880506400603, 0.32167880506400603, 0.32155673475150603, 0.32155673475150603, 0.32155673475150603, 0.32155673475150603, 0.32143466443900603, 0.32131259412650603, 0.32131259412650603, 0.32131259412650603, 0.32131259412650603, 0.32131259412650603, 0.32131259412650603, 0.32131259412650603, 0.32131259412650603, 0.32131259412650603, 0.32131259412650603, 0.32131259412650603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603, 0.32119052381400603], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.08354995012406091, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0007522371734334079, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.0585271291226562e-08, "rotation_range": [0, 0], "momentum": 0.5398230669936845}, "accuracy_valid_max": 0.678809476185994, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.678809476185994, "accuracy_valid_std": [0.014652524176778236, 0.014452162599215352, 0.01289527060400751, 0.009030443235139286, 0.009867083107977076, 0.010264729743555504, 0.013954075248858116, 0.009417050608622968, 0.009442246454696646, 0.008563430454409056, 0.008021583718520837, 0.005680273843609316, 0.0038808104168055785, 0.005419287476253837, 0.006846785784949955, 0.00657899795460131, 0.0051309686276185444, 0.005310515159287797, 0.00709968718749612, 0.008935903636980762, 0.010399329936489156, 0.010258044101516128, 0.009730026896919364, 0.009428851306873166, 0.009381837967969864, 0.011094266636549938, 0.010166310222074378, 0.010227208984482924, 0.011241555361011976, 0.011033646625140496, 0.012438637027731802, 0.013511826254774614, 0.012811345842488877, 0.01376488639550494, 0.011913389132605643, 0.010468928528554477, 0.010907067142197711, 0.009754001889654166, 0.009795437791631475, 0.010239700057589441, 0.01091573284916646, 0.011232480514633006, 0.010279710222097749, 0.010111901954508524, 0.009228564489709897, 0.009696957134199916, 0.009591678190113976, 0.010614371835821607, 0.010244083199827123, 0.010261287497577634, 0.010147306252062152, 0.009974175857227789, 0.009926253558845827, 0.009758447586330257, 0.009684874176805058, 0.00938507268420429, 0.009181126213923705, 0.008431819325406768, 0.00741363985853405, 0.00741363985853405, 0.007209381459387477, 0.007058995241183114, 0.006949547467327835, 0.006949547467327835, 0.007185685082722774, 0.007142934040753397, 0.007051717625887475, 0.007031027018700076, 0.007126225377536982, 0.007267481595444924, 0.007267481595444924, 0.00741363985853405, 0.007306789688489209, 0.007267481595444924, 0.007267481595444924, 0.007356694110731666, 0.007378491112607189, 0.007547546239562609, 0.007547546239562609, 0.007728546275599054, 0.007728546275599054, 0.007728546275599054, 0.007805287978645235, 0.007805287978645235, 0.007857087616946049, 0.007857087616946049, 0.007857087616946049, 0.007857087616946049, 0.007857087616946049, 0.008045944176739534, 0.007693634157687464, 0.007770131486188101, 0.007770131486188101, 0.007770131486188101, 0.007770131486188101, 0.007770131486188101, 0.007770131486188101, 0.007770131486188101, 0.007770131486188101, 0.0077829559573579865, 0.0077829559573579865, 0.0077829559573579865, 0.0077829559573579865, 0.007885085459772666, 0.007818077467584828, 0.007818077467584828, 0.007818077467584828, 0.007818077467584828, 0.007818077467584828, 0.007818077467584828, 0.007818077467584828, 0.007818077467584828, 0.007818077467584828, 0.007818077467584828, 0.007818077467584828, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437, 0.007840332073491437], "accuracy_valid": [0.4282005953501506, 0.4686073395143072, 0.49247282097138556, 0.5157985457454819, 0.5409567959337349, 0.5578230892319277, 0.5712317041603916, 0.5823298075112951, 0.5947103845067772, 0.6046186699924698, 0.6123193947665663, 0.6232145378388554, 0.629847515060241, 0.633265483810241, 0.6377614951995482, 0.6415559699736446, 0.6465917380459337, 0.6509965643825302, 0.6528276190700302, 0.6546483786709337, 0.6561132224209337, 0.6579545721950302, 0.6617490469691265, 0.6631021154932228, 0.6651876058923193, 0.6662862387048193, 0.6672730962914157, 0.6692365163780121, 0.6693482916039157, 0.6691041509789157, 0.6682290686182228, 0.6685849844691265, 0.6698159826807228, 0.6695615469691265, 0.6711793462914157, 0.6722882741905121, 0.6736310476280121, 0.6738854833396084, 0.6746179052146084, 0.6741193288780121, 0.6734986822289157, 0.6737428228539157, 0.6754723974021084, 0.6762048192771084, 0.6772019719503012, 0.6776902532003012, 0.6778123235128012, 0.6771916768637049, 0.6775578878012049, 0.6779240987387049, 0.6780461690512049, 0.6778020284262049, 0.6778020284262049, 0.6774358174887049, 0.6774358174887049, 0.6770696065512049, 0.6771916768637049, 0.6773240422628012, 0.6777005482868976, 0.6777005482868976, 0.6778226185993976, 0.6778226185993976, 0.6780667592243976, 0.6780667592243976, 0.6780667592243976, 0.6778226185993976, 0.6780667592243976, 0.6779446889118976, 0.6778226185993976, 0.6777005482868976, 0.6777005482868976, 0.6777005482868976, 0.6775784779743976, 0.6777005482868976, 0.6777005482868976, 0.6778226185993976, 0.6779446889118976, 0.6775784779743976, 0.6775784779743976, 0.6777005482868976, 0.6777005482868976, 0.6777005482868976, 0.6777005482868976, 0.6777005482868976, 0.6775784779743976, 0.6775784779743976, 0.6775784779743976, 0.6775784779743976, 0.6775784779743976, 0.6777005482868976, 0.678199124623494, 0.678321194935994, 0.678321194935994, 0.678321194935994, 0.678321194935994, 0.678321194935994, 0.678321194935994, 0.678321194935994, 0.678321194935994, 0.678443265248494, 0.678443265248494, 0.678443265248494, 0.678443265248494, 0.678565335560994, 0.678687405873494, 0.678687405873494, 0.678687405873494, 0.678687405873494, 0.678687405873494, 0.678687405873494, 0.678687405873494, 0.678687405873494, 0.678687405873494, 0.678687405873494, 0.678687405873494, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994, 0.678809476185994], "seed": 228824341, "model": "residualv3", "loss_std": [0.2885063588619232, 0.1664859503507614, 0.17342214286327362, 0.17642827332019806, 0.17764370143413544, 0.17833618819713593, 0.17856396734714508, 0.17862015962600708, 0.1785866767168045, 0.178421750664711, 0.17816811800003052, 0.1779557466506958, 0.17755024135112762, 0.17715352773666382, 0.1767517775297165, 0.17634634673595428, 0.1759219914674759, 0.1754455864429474, 0.17496059834957123, 0.17448319494724274, 0.17403872311115265, 0.17358307540416718, 0.17311327159404755, 0.17264296114444733, 0.17221754789352417, 0.17179028689861298, 0.17141762375831604, 0.17104531824588776, 0.1706915646791458, 0.17036448419094086, 0.17005318403244019, 0.16977614164352417, 0.16951093077659607, 0.16924139857292175, 0.16901051998138428, 0.16879533231258392, 0.16859063506126404, 0.1683845818042755, 0.16820038855075836, 0.16803328692913055, 0.16788233816623688, 0.16773612797260284, 0.16760481894016266, 0.16748186945915222, 0.16736014187335968, 0.16725225746631622, 0.16714969277381897, 0.16705389320850372, 0.166963592171669, 0.16688235104084015, 0.1668115109205246, 0.16674359142780304, 0.1666809618473053, 0.16662485897541046, 0.16657203435897827, 0.16652610898017883, 0.16648423671722412, 0.16644416749477386, 0.1664106249809265, 0.16638082265853882, 0.16635453701019287, 0.1663309931755066, 0.16631032526493073, 0.16629363596439362, 0.16627928614616394, 0.1662660390138626, 0.1662541776895523, 0.16624324023723602, 0.1662340760231018, 0.16622593998908997, 0.16621829569339752, 0.1662108600139618, 0.16620399057865143, 0.1661975085735321, 0.16619178652763367, 0.16618657112121582, 0.16618117690086365, 0.1661762148141861, 0.16617125272750854, 0.1661664992570877, 0.16616174578666687, 0.1661570966243744, 0.16615256667137146, 0.16614802181720734, 0.1661434918642044, 0.16613885760307312, 0.1661343276500702, 0.16612988710403442, 0.1661253571510315, 0.16612088680267334, 0.16611655056476593, 0.1661122590303421, 0.16610805690288544, 0.16610383987426758, 0.1660996526479721, 0.16609551012516022, 0.1660914123058319, 0.16608743369579315, 0.16608355939388275, 0.1660798192024231, 0.16607627272605896, 0.1660727858543396, 0.16606944799423218, 0.1660662442445755, 0.16606318950653076, 0.16606028378009796, 0.16605746746063232, 0.1660548597574234, 0.16605247557163239, 0.16605030000209808, 0.1660483330488205, 0.16604654490947723, 0.16604498028755188, 0.16604357957839966, 0.166042298078537, 0.16604118049144745, 0.16604016721248627, 0.16603928804397583, 0.16603848338127136, 0.16603775322437286, 0.16603714227676392, 0.16603662073612213, 0.16603617370128632, 0.16603578627109528, 0.16603544354438782, 0.16603514552116394, 0.16603492200374603, 0.16603469848632812, 0.166034534573555, 0.16603435575962067, 0.16603423655033112, 0.16603413224220276, 0.1660340428352356, 0.16603395342826843, 0.16603389382362366, 0.16603383421897888, 0.1660337895154953, 0.1660337746143341, 0.16603371500968933, 0.16603370010852814, 0.16603367030620575, 0.16603365540504456, 0.16603362560272217, 0.16603361070156097, 0.16603361070156097, 0.16603359580039978, 0.1660335808992386, 0.1660335808992386, 0.1660335659980774, 0.1660335659980774, 0.1660335659980774, 0.1660335659980774, 0.1660335659980774, 0.1660335510969162, 0.1660335510969162, 0.1660335659980774, 0.1660335510969162, 0.1660335659980774, 0.1660335659980774, 0.1660335510969162, 0.1660335510969162, 0.1660335659980774, 0.1660335510969162, 0.1660335510969162, 0.1660335659980774, 0.1660335510969162]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:26 2016", "state": "available"}], "summary": "e87b6b7d71acdb68af8910a5f7d60e2f"}