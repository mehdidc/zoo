{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 64, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 2, "nbg3": 2, "nbg2": 2, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.08553652015418926, 0.08882673248527449, 0.08405370634810148, 0.0827844969121277, 0.08011573977113755, 0.08372843634135178, 0.08362740973184284, 0.07663143331391865, 0.07545784159647743, 0.07518779602857208, 0.07293525325868513, 0.07424685232994309, 0.07535566725988242, 0.07509522970808807, 0.07601124544967539, 0.08061251871954918, 0.07709769547126118, 0.07954975053250263, 0.07993667011994696, 0.08108855151117517, 0.07812089075320566, 0.07570796286918444, 0.0795451543642995, 0.07846122586926586, 0.07848168085315507, 0.07829830474734188, 0.07792430756874459, 0.07971548355841135, 0.07035544894272842, 0.0812879041095182, 0.0758086037253513, 0.07522775418960022, 0.07363031689089769, 0.0755768704056927, 0.07722389689980966, 0.07432943848642187, 0.07238408019327607, 0.07220880700901168, 0.07326912736661838, 0.07466472199296972, 0.07126406740515585, 0.07249524048501228, 0.07599470197006135, 0.07093043582157285, 0.0765842900099711, 0.07744252733358455, 0.07608582239702491, 0.07964699253088174, 0.0768679922442038, 0.07752815058809882, 0.07632348823084263, 0.07872117567534033, 0.08231480549337099, 0.07428971736216909, 0.08022652443771024, 0.07485128150505319, 0.07599517133826604, 0.08413641719004038, 0.07791091732266894, 0.0769111355952696, 0.0726824609758183, 0.07387117498733717, 0.07440282471414501, 0.07598249737903652, 0.07725622303329455, 0.07567721425958039, 0.07888141467704708, 0.08388198062520069, 0.0835574295162726, 0.07908058026350016, 0.07750709880115446, 0.07786157099464901, 0.07856742013183861, 0.07446416416526554, 0.07643941925205205, 0.08403460766984243, 0.07917490704342238, 0.07585246720107269, 0.07535803397284165, 0.07697650036032655, 0.0754194242040088, 0.08351803987689348, 0.07586093122511232, 0.07837423262933224, 0.07562770749284854, 0.0812668386997291, 0.07913344866521092, 0.07419651136166233, 0.06940231299209128, 0.07813732644374978, 0.07663236424757817, 0.0751247921193795, 0.07407093062325998, 0.07434731206591841, 0.07604678415468225, 0.0766973852574222, 0.07896005702051254, 0.07846213509289618, 0.06921498085507256, 0.07259849260083107, 0.07665132955887112, 0.07359591348208355, 0.07780795299788178, 0.08139939403748313, 0.07539399886273096, 0.07340045376451668, 0.07533755944578947, 0.07633470375169339, 0.07798642194872374, 0.08057655901796014, 0.07414156589286142, 0.07735369678509728, 0.0769783538702876, 0.07812317369477594, 0.07544354079161823, 0.07714348468381566, 0.07873884510870427, 0.07933827600852408, 0.07834612398692228, 0.07645061776549751, 0.07734020774676147, 0.07555232426069078, 0.07862063360855417, 0.07997537062544251, 0.08348525438786403, 0.07274831560233183, 0.07868582490524158, 0.07861745769555732, 0.07381719540086124, 0.07523095466697996], "moving_avg_accuracy_train": [0.04763742469879517, 0.09895896084337347, 0.14906871234939756, 0.19763369728915658, 0.2443730572289156, 0.2889116551204819, 0.3315684, 0.3720067218975904, 0.4098601724487952, 0.44498955430030124, 0.47766727432207834, 0.5081384986970994, 0.5364709214177509, 0.562770177167542, 0.5875596127640408, 0.6102089602225765, 0.630798098085861, 0.6493353816507689, 0.666101297551957, 0.6814118191521831, 0.695826642508049, 0.7088282214801358, 0.7199225265911584, 0.7305427551067414, 0.7401809683008865, 0.7501543059587497, 0.7595397601520314, 0.7682313978416475, 0.7751149598647116, 0.7845645896312525, 0.793349282776561, 0.8023685525410735, 0.8106506167146771, 0.8195046062480287, 0.826976679508768, 0.8334497570398189, 0.8412074733539093, 0.8474034617112896, 0.8534387179497992, 0.8583456932933735, 0.8644185976086145, 0.8702701301670303, 0.8749458656443031, 0.8792410945919209, 0.8841233669098373, 0.8892939556706608, 0.8944934192903418, 0.8988811443793798, 0.9003051534354177, 0.9039493368870566, 0.9064666772947364, 0.9098500359206845, 0.9126573892563269, 0.9149392783427424, 0.9172777112012391, 0.9199658851112358, 0.920359168588064, 0.921141399319619, 0.9236738143575366, 0.9258400360844335, 0.9257306183193637, 0.9260886558850177, 0.9281945869832628, 0.9297063594596353, 0.9320293982124669, 0.9332071059815816, 0.9347070843894475, 0.9360100017035148, 0.9380556506295489, 0.9383295283376782, 0.9406915114978863, 0.9425372699866519, 0.9451656024759385, 0.9469333984030435, 0.9489268055506909, 0.9493607439715254, 0.9497559948755776, 0.950612944333803, 0.9515818645088564, 0.953729306823031, 0.953692407767234, 0.9542521956049684, 0.9538123864360378, 0.9542119271599039, 0.9539691041728291, 0.9534258270386786, 0.9543417157203529, 0.9548436322507272, 0.9560789602907147, 0.9556470808279083, 0.9564867402149969, 0.9570824186031358, 0.9575502874355933, 0.9595526946860099, 0.9608277527776499, 0.9607940174095234, 0.9621732000059204, 0.9621155185595452, 0.9630448740831088, 0.9638954130302195, 0.9632113498898482, 0.9618450341779718, 0.9629238025975241, 0.9613062152594584, 0.9632860041250787, 0.9637194519053418, 0.9642860421063739, 0.9645677165101944, 0.965364804045922, 0.9654491820750647, 0.9666899378133413, 0.967707785146465, 0.9679178989510957, 0.9668974757728537, 0.9675110037979779, 0.9686397038699874, 0.9673800256516635, 0.9682276782069791, 0.969400015807968, 0.9696315127211471, 0.9689621302743335, 0.9700469036926833, 0.9711337984137765, 0.9714225270061337, 0.9715764904199782, 0.9704467028237636, 0.9712700671799415, 0.9708815770282124, 0.9698777566747888, 0.9711062837181533], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.020423918087400426, 0.04208662692881371, 0.0604768489998917, 0.07565618395969372, 0.08775167547192952, 0.09682968824405626, 0.10352310037309725, 0.10788811123682575, 0.10999525358083576, 0.11010238944617247, 0.10870265097395009, 0.10618884551077098, 0.1027944965546891, 0.0987399045761565, 0.09439655917327751, 0.08957383971862705, 0.08443166912834438, 0.07908118015320217, 0.07370292556193339, 0.06844234165077892, 0.06346819167712842, 0.058642742011322226, 0.05388622026325815, 0.04951270152044115, 0.045397487750579175, 0.04175294615186102, 0.038370432290402615, 0.03521329015291036, 0.03211841197274767, 0.02971023029999516, 0.027433744772910714, 0.0254224953393851, 0.02349757908822777, 0.021853359355315287, 0.020170510329108455, 0.018530565890704575, 0.017219148763323255, 0.015842746332514042, 0.01458629056004286, 0.013344367167240589, 0.01234185195191518, 0.011415830656263448, 0.010471010110917958, 0.009589950025238254, 0.008845484269591058, 0.00820155073583394, 0.0076247054596420135, 0.007035504096790583, 0.006350203903236627, 0.005834704170175758, 0.005308266777711423, 0.0048804641402662744, 0.004463348820999929, 0.004063877099124256, 0.0037067038033151106, 0.0034010699337170755, 0.003062354987383683, 0.002761626452901817, 0.0025431819409300767, 0.0023310963959677924, 0.002098094506596829, 0.0018894387740229213, 0.0017404094087356359, 0.0015869375720449295, 0.0014768123962648554, 0.0013416141169432692, 0.0012277021222655185, 0.0011202102517846337, 0.001045851342363429, 0.0009419412891181784, 0.0008979578402483196, 0.0008388234758131395, 0.000817114313299982, 0.0007635288039289831, 0.000722938972042713, 0.0006523397978161291, 0.0005885118275289035, 0.000536269906141588, 0.0004910921722780596, 0.0004834865314846215, 0.0004351501321990278, 0.0003944553807886036, 0.00035675073165542193, 0.0003225123536001268, 0.00029079178526758115, 0.00026436895714123884, 0.00024548173012208684, 0.00022320083894104538, 0.00021461507334435544, 0.00019483224484346544, 0.00018169427133605428, 0.00016671833888131027, 0.0001520166161926456, 0.00017290166774206871, 0.00017024345920137087, 0.00015322935595679744, 0.00015502572206895843, 0.000139553094205366, 0.00013337109998743207, 0.00012654473849366027, 0.0001181017460644267, 0.00012309293907866652, 0.00012125731689800843, 0.0001326808843746407, 0.00015468887150908825, 0.0001409108771621155, 0.00012970900954905408, 0.0001174521728220571, 0.00011142509239636126, 0.00010034666002294326, 0.00010416726723924725, 0.00010307465925724577, 9.316452362958832e-05, 9.321944243087297e-05, 8.728524792630162e-05, 9.002239780665836e-05, 9.5301260949473e-05, 9.2237768545323e-05, 9.538337074702099e-05, 8.632735105962198e-05, 8.172727169457805e-05, 8.41451448475447e-05, 8.636269157545088e-05, 7.847670021830738e-05, 7.08423727917004e-05, 7.524591562557538e-05, 7.382268383023541e-05, 6.779873682912625e-05, 7.008776086374203e-05, 7.666249304386888e-05], "duration": 5372.631638, "accuracy_train": [0.47637424698795183, 0.5608527861445783, 0.6000564759036144, 0.6347185617469879, 0.665027296686747, 0.6897590361445783, 0.7154791039156626, 0.7359516189759037, 0.7505412274096386, 0.7611539909638554, 0.7717667545180723, 0.7823795180722891, 0.7914627259036144, 0.7994634789156626, 0.8106645331325302, 0.8140530873493976, 0.8161003388554217, 0.8161709337349398, 0.8169945406626506, 0.8192065135542169, 0.8255600527108434, 0.8258424322289156, 0.8197712725903614, 0.8261248117469879, 0.8269248870481928, 0.8399143448795181, 0.8440088478915663, 0.8464561370481928, 0.8370670180722891, 0.8696112575301205, 0.8724115210843374, 0.8835419804216867, 0.8851891942771084, 0.8991905120481928, 0.8942253388554217, 0.8917074548192772, 0.9110269201807228, 0.9031673569277109, 0.9077560240963856, 0.9025084713855421, 0.9190747364457831, 0.9229339231927711, 0.917027484939759, 0.9178981551204819, 0.9280638177710844, 0.9358292545180723, 0.9412885918674698, 0.9383706701807228, 0.913121234939759, 0.9367469879518072, 0.9291227409638554, 0.9403002635542169, 0.9379235692771084, 0.9354762801204819, 0.9383236069277109, 0.9441594503012049, 0.9238987198795181, 0.9281814759036144, 0.9464655496987951, 0.945336031626506, 0.9247458584337349, 0.9293109939759037, 0.9471479668674698, 0.9433123117469879, 0.9529367469879518, 0.9438064759036144, 0.948206890060241, 0.9477362575301205, 0.9564664909638554, 0.9407944277108434, 0.961949359939759, 0.9591490963855421, 0.9688205948795181, 0.9628435617469879, 0.9668674698795181, 0.9532661897590361, 0.9533132530120482, 0.9583254894578314, 0.9603021460843374, 0.9730562876506024, 0.9533603162650602, 0.9592902861445783, 0.9498541039156626, 0.9578077936746988, 0.9517836972891566, 0.9485363328313253, 0.9625847138554217, 0.9593608810240963, 0.9671969126506024, 0.9517601656626506, 0.9640436746987951, 0.9624435240963856, 0.9617611069277109, 0.977574359939759, 0.9723032756024096, 0.9604903990963856, 0.974585843373494, 0.9615963855421686, 0.9714090737951807, 0.9715502635542169, 0.957054781626506, 0.9495481927710844, 0.972632718373494, 0.9467479292168675, 0.9811041039156626, 0.9676204819277109, 0.9693853539156626, 0.9671027861445783, 0.9725385918674698, 0.9662085843373494, 0.9778567394578314, 0.9768684111445783, 0.9698089231927711, 0.9577136671686747, 0.9730327560240963, 0.9787980045180723, 0.956042921686747, 0.9758565512048193, 0.9799510542168675, 0.971714984939759, 0.9629376882530121, 0.9798098644578314, 0.9809158509036144, 0.9740210843373494, 0.9729621611445783, 0.9602786144578314, 0.9786803463855421, 0.9673851656626506, 0.9608433734939759, 0.9821630271084337], "end": "2016-01-18 01:45:31.250000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0], "accuracy_valid": [0.4736912393162393, 0.5544871794871795, 0.5896100427350427, 0.6185897435897436, 0.6420940170940171, 0.6651976495726496, 0.6852297008547008, 0.6951121794871795, 0.702323717948718, 0.7112713675213675, 0.7150106837606838, 0.7208867521367521, 0.7279647435897436, 0.7321047008547008, 0.733840811965812, 0.7319711538461539, 0.7353098290598291, 0.7293002136752137, 0.7263621794871795, 0.7250267094017094, 0.7247596153846154, 0.7226228632478633, 0.7123397435897436, 0.7215544871794872, 0.7151442307692307, 0.7186164529914529, 0.7186164529914529, 0.7175480769230769, 0.7044604700854701, 0.7254273504273504, 0.7232905982905983, 0.7311698717948718, 0.7280982905982906, 0.7303685897435898, 0.7369123931623932, 0.7299679487179487, 0.7405181623931624, 0.7275641025641025, 0.735176282051282, 0.7276976495726496, 0.7382478632478633, 0.7433226495726496, 0.7238247863247863, 0.7363782051282052, 0.7411858974358975, 0.7406517094017094, 0.7450587606837606, 0.7375801282051282, 0.7242254273504274, 0.7374465811965812, 0.7377136752136753, 0.7382478632478633, 0.7417200854700855, 0.733840811965812, 0.7278311965811965, 0.7447916666666666, 0.7283653846153846, 0.7284989316239316, 0.7414529914529915, 0.7365117521367521, 0.7238247863247863, 0.7270299145299145, 0.7443910256410257, 0.7379807692307693, 0.7459935897435898, 0.7334401709401709, 0.7369123931623932, 0.7358440170940171, 0.7399839743589743, 0.7345085470085471, 0.7525373931623932, 0.7407852564102564, 0.7534722222222222, 0.7462606837606838, 0.7426549145299145, 0.7421207264957265, 0.7426549145299145, 0.7383814102564102, 0.7469284188034188, 0.7589476495726496, 0.7369123931623932, 0.7411858974358975, 0.7287660256410257, 0.7387820512820513, 0.7342414529914529, 0.7342414529914529, 0.734375, 0.7367788461538461, 0.7459935897435898, 0.7391826923076923, 0.7381143162393162, 0.7394497863247863, 0.7383814102564102, 0.7514690170940171, 0.7510683760683761, 0.7427884615384616, 0.749465811965812, 0.7381143162393162, 0.750534188034188, 0.7478632478632479, 0.7299679487179487, 0.7299679487179487, 0.7487980769230769, 0.7290331196581197, 0.7520032051282052, 0.7391826923076923, 0.75, 0.7409188034188035, 0.7433226495726496, 0.7441239316239316, 0.7481303418803419, 0.7453258547008547, 0.7431891025641025, 0.7314369658119658, 0.7449252136752137, 0.749732905982906, 0.7270299145299145, 0.7482638888888888, 0.7490651709401709, 0.7485309829059829, 0.7390491452991453, 0.7546741452991453, 0.7471955128205128, 0.7425213675213675, 0.7426549145299145, 0.7386485042735043, 0.7513354700854701, 0.7429220085470085, 0.7290331196581197, 0.7564102564102564], "accuracy_test": 0.749198717948718, "start": "2016-01-18 00:15:58.618000", "learning_rate_per_epoch": [0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347, 0.00032456140615977347], "accuracy_train_last": 0.9821630271084337, "error_valid": [0.5263087606837606, 0.4455128205128205, 0.4103899572649573, 0.3814102564102564, 0.35790598290598286, 0.3348023504273504, 0.3147702991452992, 0.3048878205128205, 0.29767628205128205, 0.28872863247863245, 0.2849893162393162, 0.27911324786324787, 0.2720352564102564, 0.2678952991452992, 0.26615918803418803, 0.26802884615384615, 0.2646901709401709, 0.2706997863247863, 0.2736378205128205, 0.27497329059829057, 0.2752403846153846, 0.2773771367521367, 0.2876602564102564, 0.2784455128205128, 0.2848557692307693, 0.28138354700854706, 0.28138354700854706, 0.28245192307692313, 0.2955395299145299, 0.2745726495726496, 0.2767094017094017, 0.2688301282051282, 0.27190170940170943, 0.26963141025641024, 0.2630876068376068, 0.2700320512820513, 0.25948183760683763, 0.27243589743589747, 0.26482371794871795, 0.2723023504273504, 0.2617521367521367, 0.2566773504273504, 0.2761752136752137, 0.2636217948717948, 0.25881410256410253, 0.25934829059829057, 0.25494123931623935, 0.2624198717948718, 0.2757745726495726, 0.26255341880341876, 0.26228632478632474, 0.2617521367521367, 0.2582799145299145, 0.26615918803418803, 0.27216880341880345, 0.25520833333333337, 0.2716346153846154, 0.27150106837606836, 0.2585470085470085, 0.26348824786324787, 0.2761752136752137, 0.2729700854700855, 0.25560897435897434, 0.2620192307692307, 0.25400641025641024, 0.2665598290598291, 0.2630876068376068, 0.26415598290598286, 0.26001602564102566, 0.26549145299145294, 0.2474626068376068, 0.2592147435897436, 0.2465277777777778, 0.2537393162393162, 0.2573450854700855, 0.25787927350427353, 0.2573450854700855, 0.26161858974358976, 0.25307158119658124, 0.2410523504273504, 0.2630876068376068, 0.25881410256410253, 0.27123397435897434, 0.2612179487179487, 0.26575854700854706, 0.26575854700854706, 0.265625, 0.26322115384615385, 0.25400641025641024, 0.2608173076923077, 0.2618856837606838, 0.2605502136752137, 0.26161858974358976, 0.24853098290598286, 0.24893162393162394, 0.25721153846153844, 0.25053418803418803, 0.2618856837606838, 0.24946581196581197, 0.25213675213675213, 0.2700320512820513, 0.2700320512820513, 0.25120192307692313, 0.2709668803418803, 0.24799679487179482, 0.2608173076923077, 0.25, 0.25908119658119655, 0.2566773504273504, 0.25587606837606836, 0.2518696581196581, 0.25467414529914534, 0.25681089743589747, 0.2685630341880342, 0.2550747863247863, 0.250267094017094, 0.2729700854700855, 0.25173611111111116, 0.2509348290598291, 0.25146901709401714, 0.26095085470085466, 0.24532585470085466, 0.2528044871794872, 0.25747863247863245, 0.2573450854700855, 0.26135149572649574, 0.24866452991452992, 0.2570779914529915, 0.2709668803418803, 0.2435897435897436], "accuracy_train_std": [0.08885696718267427, 0.08687074660078825, 0.08474370918076028, 0.0847465025309577, 0.08323037165035703, 0.08247963657304502, 0.08111943509712795, 0.07889595006564942, 0.07876526819525197, 0.0762424553538213, 0.07457436042780956, 0.07509301510185233, 0.07377859566576761, 0.07289985473632898, 0.07074217502446241, 0.06965957660802453, 0.07098061892030723, 0.072499226590402, 0.07242745265631663, 0.07155053690008958, 0.07120652683622862, 0.07188418140123617, 0.07183396988423588, 0.07123495204100189, 0.07177733579426156, 0.06953422826222079, 0.06776634107789277, 0.0685868038822295, 0.07059569527641125, 0.06613306057642246, 0.06331179629021381, 0.06102225360109905, 0.06152655494526032, 0.05794684886395379, 0.05736104628499444, 0.06002982031420543, 0.05424851460339123, 0.05729501664671458, 0.05579185379745009, 0.05668783477602634, 0.05225563662099505, 0.05219324837946693, 0.05277868425697408, 0.052460387651782896, 0.04818936416349413, 0.04609354313073426, 0.0431798549731481, 0.045262562117554014, 0.0516502423860253, 0.04559649619570737, 0.04839228500400456, 0.0451191003169329, 0.044983780061101604, 0.045927538181591364, 0.04447678482219225, 0.042016435744416714, 0.04771793795080582, 0.04621074644309192, 0.040925923120708814, 0.04299226874657716, 0.05018432591893399, 0.047442653711409176, 0.04299157966475751, 0.042316113357691615, 0.03865517874665676, 0.042149445816177286, 0.041668597319832724, 0.04018997209132834, 0.037249100507130095, 0.043803425888302425, 0.03684507957001919, 0.037503241365091367, 0.03262002073010167, 0.035373216539964746, 0.03289152402878042, 0.03929685807936526, 0.039221736876823544, 0.0374892340939796, 0.037002844814761016, 0.029653575004386083, 0.03827255382458032, 0.03668709624160807, 0.041459326899965926, 0.03644445793510082, 0.0395354786196966, 0.03977481853306208, 0.0350259071467716, 0.03606862586847552, 0.03324221851990177, 0.040088803793340975, 0.03459888582747445, 0.03476888397911319, 0.03415715086663283, 0.02750941746546295, 0.030581981013157133, 0.0364449441401668, 0.030046560741474732, 0.0345832619197204, 0.03016500285029673, 0.030127825473753075, 0.0370529726233051, 0.03978623973796639, 0.0302758952799917, 0.040228256697272084, 0.024726880404905078, 0.033126379238069055, 0.03172236942384184, 0.03220388847250354, 0.030865018198498913, 0.03370898720858325, 0.02752430890520644, 0.02664394324904008, 0.03122027896005252, 0.03822874054287748, 0.029169460997836953, 0.026537905902836622, 0.035713164299246175, 0.027108392881390737, 0.02531373090899888, 0.0293828142233946, 0.03333433266678007, 0.02589210431958141, 0.024641635699943584, 0.028392211682357164, 0.028913915726926222, 0.03634575313322365, 0.02665122664733704, 0.03233101617254961, 0.03458940987305979, 0.024725323962558755], "accuracy_test_std": 0.07759828196384866, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.6475932244735235, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00032456139386887053, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 2.4166262911317855e-07, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.08887562633086504}, "accuracy_valid_max": 0.7589476495726496, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7564102564102564, "loss_train": [1.6825095415115356, 1.3306838274002075, 1.1507409811019897, 1.037409782409668, 0.958080530166626, 0.893109917640686, 0.8348211646080017, 0.7807043194770813, 0.7325787544250488, 0.6900829076766968, 0.6520916819572449, 0.6173046827316284, 0.5847839117050171, 0.5533928275108337, 0.5242424011230469, 0.49674248695373535, 0.4696132242679596, 0.4450457692146301, 0.4209097921848297, 0.39992475509643555, 0.3784874975681305, 0.3602539598941803, 0.34242379665374756, 0.3228810429573059, 0.30675366520881653, 0.2881965637207031, 0.2778623104095459, 0.2649620771408081, 0.25450024008750916, 0.23696580529212952, 0.22819767892360687, 0.2138025313615799, 0.2156183272600174, 0.19955112040042877, 0.193850576877594, 0.18093426525592804, 0.1759590059518814, 0.17462556064128876, 0.1725766807794571, 0.15765748918056488, 0.15898044407367706, 0.15184809267520905, 0.143589586019516, 0.1349496990442276, 0.1361728012561798, 0.13743433356285095, 0.1345759928226471, 0.12575481832027435, 0.11951245367527008, 0.12333915382623672, 0.11511576920747757, 0.11594776064157486, 0.11367467045783997, 0.11203032732009888, 0.10746081173419952, 0.11451171338558197, 0.10297433286905289, 0.1072891503572464, 0.10031745582818985, 0.09698081016540527, 0.10162244737148285, 0.09957472234964371, 0.09660392999649048, 0.09714432060718536, 0.09275209903717041, 0.08988278359174728, 0.09378203004598618, 0.08949743956327438, 0.0926043912768364, 0.08234548568725586, 0.08135802298784256, 0.09453324228525162, 0.08673587441444397, 0.0837637260556221, 0.08595354855060577, 0.07980789989233017, 0.08480145782232285, 0.0833960697054863, 0.07522889226675034, 0.08171798288822174, 0.07627975195646286, 0.08354663848876953, 0.07506462186574936, 0.07716760039329529, 0.07677776366472244, 0.07119113951921463, 0.07308395951986313, 0.08065958321094513, 0.07401251792907715, 0.07011067867279053, 0.07676514983177185, 0.07456061244010925, 0.06841596961021423, 0.07063352316617966, 0.07295462489128113, 0.06783509999513626, 0.06059371307492256, 0.06993968784809113, 0.07561369985342026, 0.06483330577611923, 0.072577565908432, 0.06654947996139526, 0.06770172715187073, 0.061951201409101486, 0.06614230573177338, 0.0662752166390419, 0.06402607262134552, 0.06718665361404419, 0.0702752098441124, 0.06006273999810219, 0.0626446008682251, 0.06915155798196793, 0.059315275400877, 0.0651177242398262, 0.06126229837536812, 0.06928867846727371, 0.05603236332535744, 0.06794285029172897, 0.06436978280544281, 0.06091533601284027, 0.0614917017519474, 0.06673667579889297, 0.057145312428474426, 0.05974011495709419, 0.06389158964157104, 0.054805513471364975, 0.06602176278829575, 0.04749655723571777, 0.07178156077861786, 0.05807239189743996], "accuracy_train_first": 0.47637424698795183, "model": "residual", "loss_std": [0.24523912370204926, 0.1968560814857483, 0.1945558488368988, 0.19185186922550201, 0.1908077448606491, 0.19087134301662445, 0.19021302461624146, 0.18947875499725342, 0.187742218375206, 0.18604892492294312, 0.18350249528884888, 0.17988330125808716, 0.17603246867656708, 0.17273478209972382, 0.1685263067483902, 0.16594381630420685, 0.16316772997379303, 0.1606035679578781, 0.15752460062503815, 0.1529006063938141, 0.1492302566766739, 0.1466190665960312, 0.14199064671993256, 0.14077094197273254, 0.1363123059272766, 0.13117147982120514, 0.12811292707920074, 0.1262587606906891, 0.12275375425815582, 0.11720714718103409, 0.11782612651586533, 0.11340858787298203, 0.11264349520206451, 0.10957662016153336, 0.10573413968086243, 0.10287657380104065, 0.10462029278278351, 0.10231970250606537, 0.10587753355503082, 0.09884409606456757, 0.09992410987615585, 0.09696546196937561, 0.09762709587812424, 0.09638465940952301, 0.09644314646720886, 0.09865526109933853, 0.09342798590660095, 0.0940609946846962, 0.09474679827690125, 0.09092224389314651, 0.09008744359016418, 0.09228207916021347, 0.0901658833026886, 0.09093015640974045, 0.0871722400188446, 0.09446128457784653, 0.08583324402570724, 0.09451520442962646, 0.09062398225069046, 0.0814509242773056, 0.08708986639976501, 0.085782989859581, 0.08418959379196167, 0.08847597241401672, 0.08644766360521317, 0.08318116515874863, 0.08776498585939407, 0.08333875238895416, 0.08290427178144455, 0.08125221729278564, 0.07701060175895691, 0.08819607645273209, 0.08685502409934998, 0.08259056508541107, 0.08787719905376434, 0.08003613352775574, 0.08443430811166763, 0.08805928379297256, 0.0740659236907959, 0.08357550948858261, 0.0801505520939827, 0.08321128040552139, 0.07698267698287964, 0.07920066267251968, 0.0819912850856781, 0.07222723215818405, 0.07764270901679993, 0.08647774904966354, 0.07989735156297684, 0.07550820708274841, 0.08365651965141296, 0.07774446904659271, 0.07246442139148712, 0.078028604388237, 0.07811849564313889, 0.07455664873123169, 0.06649661809206009, 0.07874873280525208, 0.08141450583934784, 0.07416412979364395, 0.08155037462711334, 0.07274959981441498, 0.07659156620502472, 0.06924820691347122, 0.07527559995651245, 0.0760829895734787, 0.07400129735469818, 0.07319542020559311, 0.08410212397575378, 0.06881902366876602, 0.0722961500287056, 0.08306300640106201, 0.06871552020311356, 0.07561548799276352, 0.07387030124664307, 0.07964643836021423, 0.06743645668029785, 0.07941212505102158, 0.0733439028263092, 0.07066746056079865, 0.07624252140522003, 0.08208859711885452, 0.07252491265535355, 0.07308103889226913, 0.08015522360801697, 0.0669475719332695, 0.0880318433046341, 0.06186063215136528, 0.08901382237672806, 0.07520831376314163]}, "state": "available", "life": [{"dt": "Sun May 15 22:04:59 2016", "state": "available"}], "summary": "9b503931822f398ae7b542f8f1b7939d"}