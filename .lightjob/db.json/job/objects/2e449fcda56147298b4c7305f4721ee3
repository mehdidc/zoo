{"content": {"hp_model": {"f1": 11, "f2": 15, "f3": 120, "nonlin": "very_leaky_rectify", "ds2": 2747, "ds1": 3395, "do2": 0.45910739051666793, "do3": 0.37438931951650944, "do1": 0.10202661475394559, "do4": 0.8587224387989508, "do5": 0.14768422675141857}, "accuracy_valid_std": [0.0313774711413921, 0.03166784825937484, 0.0349512827039365, 0.03474406189173051, 0.036241847114694005, 0.04013850414898947, 0.039101728593029184, 0.03959785677385099, 0.03686404144235994, 0.039142308483646616, 0.04165031000171192, 0.04529869172320682, 0.04672244205642072, 0.04681302879363321, 0.050049790347441316, 0.04701953909694479, 0.0475536424507881, 0.04629801359220075, 0.0484037462463008, 0.048296611066453055, 0.04904201259083452, 0.046497251714361615, 0.04830036762815206, 0.047990596919182074, 0.04805860066467485, 0.04750401633232858, 0.048062375829302166, 0.04703651449602637, 0.0472604790077336, 0.04661514035620223, 0.048009307551618675, 0.047905835223136996, 0.04795561303891515, 0.04747822846586539, 0.04647305248662065, 0.04534693027741236, 0.04579127272467556, 0.04573418080162575, 0.04571592807816477, 0.04546041866151233, 0.044851896539380036, 0.04435645106872291, 0.04491576570474602, 0.04414673059814091, 0.04433783577814405, 0.04389366070819348, 0.04334941271089618, 0.04467111493140976, 0.04446614305644649, 0.04493939043900689, 0.0452369675727043, 0.04514562975042217, 0.0450434337478609, 0.045228343538941715, 0.04476706874574525, 0.04499910361533313, 0.044931315004222774, 0.04539791526152002, 0.04518821012357635, 0.04502550545109554, 0.04500232909741239, 0.044973896573384214, 0.044835105690307825, 0.04481162842054201, 0.044668677904197894, 0.04435910975824213, 0.04416316692161521, 0.04444471619480478, 0.044424504259589, 0.04462987063878425, 0.04458675705435787, 0.044384461561545144, 0.043984298357134015, 0.04400821698875935, 0.04418924702897204, 0.04424197627532262, 0.044213054786429985, 0.04446267464607275, 0.04425509752310465, 0.04409306449141469, 0.04436790275008659, 0.044123709459275594, 0.04431409505935245, 0.04446614305644649, 0.04398677330147807, 0.044402240073317674, 0.044482257414979076, 0.04448225741497908, 0.04446614305644649, 0.04409306449141469, 0.04448225741497908, 0.04421325996998389, 0.04371057635637099, 0.04379848607688066, 0.04387050677058271, 0.043685456466863115, 0.04371866977392413, 0.04379848607688066, 0.04379848607688066, 0.04387050677058271, 0.044045306391525596, 0.0439027535570253, 0.04410972647740169, 0.044045306391525596, 0.043980379410420024, 0.043980379410420024, 0.04409409319139759, 0.04404530639152559, 0.04399749645153318, 0.04385375389451405, 0.043980379410420024, 0.04385375389451405, 0.043980379410420024, 0.043932498813108824, 0.04385375389451405, 0.04404530639152559, 0.043980379410420024, 0.04380573489304701, 0.04380573489304701, 0.04398264831686719, 0.04387050677058271, 0.04387050677058271, 0.04387050677058271, 0.04398264831686719, 0.04387050677058271, 0.04398264831686719, 0.04398264831686719, 0.044109726477401696, 0.04399749645153318, 0.04399749645153318, 0.044109726477401696, 0.04399749645153318, 0.04387050677058271, 0.04387050677058271, 0.04387050677058271, 0.04387050677058271, 0.04387050677058271, 0.043982648316867184, 0.043982648316867184, 0.043982648316867184, 0.043982648316867184, 0.043982648316867184], "moving_avg_accuracy_train": [0.01889354292168674, 0.03787203501506023, 0.05498562217620481, 0.07234803510918673, 0.08957435735128011, 0.10678409029085088, 0.12397653969550074, 0.14067103557534824, 0.15689148849371704, 0.17260529521663448, 0.1878419418997903, 0.20329861743872693, 0.21833208400810725, 0.23253285527597123, 0.24595360965801266, 0.2583476123970307, 0.27014227510311073, 0.2810516168699081, 0.29147008093592935, 0.30111731230016775, 0.3102116239918377, 0.31870947514687076, 0.3266634523309789, 0.3341138239653509, 0.3410850658158038, 0.34747213528844023, 0.3534793457053793, 0.3591658614360462, 0.36444374065388735, 0.36935149384753474, 0.3739331931073596, 0.3781884995496357, 0.38218299673322637, 0.3859309997707471, 0.3893583252454796, 0.392607639558281, 0.39560261731932034, 0.39835222004521964, 0.40090922319130007, 0.4033211246673508, 0.40554360557410973, 0.40757442950465056, 0.4094257026686433, 0.4111812686969597, 0.41277775026099867, 0.41426400008429637, 0.4156557476662282, 0.4169671495562319, 0.4181497644198858, 0.4192682405381382, 0.42027486904456535, 0.42124201692926544, 0.42214068797730275, 0.42298714252294595, 0.42375601110197664, 0.42445975863635726, 0.4251190162064564, 0.42572411383279873, 0.42631105862421764, 0.42687460637625374, 0.4273841525157368, 0.4278615693424764, 0.4283288950889516, 0.4287565477487312, 0.429157907281087, 0.4295285435108096, 0.4298550566296082, 0.4301700969003823, 0.4304677521199826, 0.4307779987453338, 0.4310642801961016, 0.4313219335017926, 0.43154676198896275, 0.43175146079006643, 0.4319098049219032, 0.43208055259236344, 0.4322342254957777, 0.4323607652955975, 0.4325028890672426, 0.43263080046172314, 0.43274121439145447, 0.43284999957881504, 0.4329643783859938, 0.4330555534992016, 0.4331470237516911, 0.43323405330423287, 0.4333053204135686, 0.4333647544866696, 0.43342765780306286, 0.43349133027576864, 0.4335486355012038, 0.4336002102040955, 0.4336489805993486, 0.4337022866056788, 0.43376202782462897, 0.43381108859638295, 0.4338528901283109, 0.4338999241576485, 0.4339375484587511, 0.43395964451649044, 0.43398188413110644, 0.4340089592722126, 0.43404744587511185, 0.43406796484181753, 0.4340840787492021, 0.43409858126584816, 0.4341163398561308, 0.43413702891268635, 0.4341556490635864, 0.43417476036204705, 0.43419902001861344, 0.43421850054687255, 0.43423838618495636, 0.4342562832592318, 0.4342700374634291, 0.4342847694098573, 0.43429802816164265, 0.4343099610382495, 0.4343277601151474, 0.43433671979640376, 0.43434243034688386, 0.43434992300496655, 0.434356666397241, 0.4343603822876374, 0.43436843291429533, 0.4343733253156369, 0.4343777284768443, 0.4343863976472322, 0.43439184673793063, 0.43439675091955926, 0.4343988115203744, 0.4344030192237586, 0.43440915931945506, 0.43441939173088306, 0.4344262477385176, 0.43443241814538874, 0.43443797151157276, 0.43444296954113837, 0.4344474677677474, 0.43445151617169553, 0.43445280657259827, 0.4344563210960613], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.003212693677202573, 0.006133072768726477, 0.008155639281552662, 0.010053155799095798, 0.011718555821082185, 0.013212274409636094, 0.014551269817455489, 0.015604498569850151, 0.016411976548758304, 0.016993092389409326, 0.017383181769794306, 0.017795042961258535, 0.01804958471896676, 0.018059583388489803, 0.01787467488328858, 0.017469709130012796, 0.01697476483216328, 0.016348411989010055, 0.01569047033156383, 0.014959044955363751, 0.014207499006134583, 0.013436670373799002, 0.012662395113826913, 0.011895727939856516, 0.011143539062308413, 0.010396337064112138, 0.009681482550641359, 0.009004362445973321, 0.008354630282719067, 0.007735941627134968, 0.007151275177388785, 0.0065991163559089945, 0.006082808790065518, 0.005600955651982348, 0.005146579125971868, 0.004726943604905067, 0.004334978270516644, 0.003969523279817436, 0.0036314153376372795, 0.003320629222445132, 0.0030330210926287895, 0.0027668371958976265, 0.0025209983872573425, 0.0022966366572496153, 0.002089911771983501, 0.0019008010416204234, 0.0017281535894446984, 0.001570816204754176, 0.001426321785520375, 0.0012949485064122457, 0.0011745733643205869, 0.001065534403166447, 0.0009662494497230267, 0.0008760728724312846, 0.0007937860152145425, 0.0007188647590224099, 0.000650889868013767, 0.0005890961694490362, 0.0005332870901976968, 0.0004828166557973512, 0.0004368717256319749, 0.00039523589450686416, 0.00035767784523604564, 0.00032355604188918967, 0.00029265024296818646, 0.0002646215596044149, 0.00023911890099470142, 0.00021610026424511615, 0.0001952876254884024, 0.00017662513965643808, 0.00015970023931227787, 0.0001443276824144518, 0.00013034984481079555, 0.00011769197472227572, 0.0001061484330268323, 9.579598262685761e-05, 8.642892261536576e-05, 7.793014124227514e-05, 7.03189196162472e-05, 6.343427957816413e-05, 5.7200572743256096e-05, 5.1587023421832175e-05, 4.654606368343363e-05, 4.196627342650646e-05, 3.784494734767021e-05, 3.412861990004372e-05, 3.076146891789701e-05, 2.7717113707515648e-05, 2.4981013781683545e-05, 2.2519400057539382e-05, 2.029701505154504e-05, 1.829125309619581e-05, 1.6483534749654525e-05, 1.4860755047486951e-05, 1.3406800661913151e-05, 1.2087783229647694e-05, 1.0894731219326651e-05, 9.825167896635546e-06, 8.855391399273136e-06, 7.974246381254407e-06, 7.181273147253386e-06, 6.469743401921347e-06, 5.83610002915371e-06, 5.25627927819036e-06, 4.732988272472094e-06, 4.261582352126507e-06, 3.838262424673301e-06, 3.4582885157564173e-06, 3.115580054356638e-06, 2.807309224480638e-06, 2.5318750804630417e-06, 2.2821029912480325e-06, 2.0574516395412386e-06, 1.8545892229957023e-06, 1.6708329038940405e-06, 1.5057028857146993e-06, 1.3567147476333802e-06, 1.222324814767064e-06, 1.102943597536129e-06, 9.933717207764535e-07, 8.943280421798805e-07, 8.054004972881904e-07, 7.252697076136724e-07, 6.528670074252461e-07, 5.881636199889923e-07, 5.295626783080743e-07, 4.7678090093483145e-07, 4.2977920147827174e-07, 3.8706851463540584e-07, 3.4857812214888397e-07, 3.137585246154704e-07, 2.8254201506385166e-07, 2.5462712053391804e-07, 2.301067286732101e-07, 2.075190993720589e-07, 1.871098547234508e-07, 1.6867642813486997e-07, 1.520336080172321e-07, 1.3701235359914682e-07, 1.2345862440997907e-07, 1.1112774817938922e-07, 1.0012614023800241e-07], "duration": 38843.334526, "accuracy_train": [0.18893542921686746, 0.2086784638554217, 0.20900790662650603, 0.22860975150602408, 0.24461125753012047, 0.26167168674698793, 0.2787085843373494, 0.2909214984939759, 0.30287556475903615, 0.31402955572289154, 0.3249717620481928, 0.3424086972891566, 0.3536332831325301, 0.360339796686747, 0.36674039909638556, 0.3698936370481928, 0.3762942394578313, 0.3792356927710843, 0.3852362575301205, 0.38794239457831325, 0.39206042921686746, 0.3951901355421687, 0.39824924698795183, 0.4011671686746988, 0.4038262424698795, 0.4049557605421687, 0.4075442394578313, 0.41034450301204817, 0.4119446536144578, 0.4135212725903614, 0.41516848644578314, 0.4164862575301205, 0.4181334713855422, 0.41966302710843373, 0.4202042545180723, 0.42185146837349397, 0.4225574171686747, 0.42309864457831325, 0.4239222515060241, 0.4250282379518072, 0.42554593373493976, 0.42585184487951805, 0.4260871611445783, 0.4269813629518072, 0.4271460843373494, 0.4276402484939759, 0.42818147590361444, 0.4287697665662651, 0.42879329819277107, 0.42933452560240964, 0.42933452560240964, 0.42994634789156627, 0.4302287274096386, 0.4306052334337349, 0.430675828313253, 0.43079348644578314, 0.4310523343373494, 0.4311699924698795, 0.43159356174698793, 0.4319465361445783, 0.4319700677710843, 0.43215832078313254, 0.43253482680722893, 0.432605421686747, 0.43277014307228917, 0.43286426957831325, 0.4327936746987952, 0.4330054593373494, 0.43314664909638556, 0.43357021837349397, 0.43364081325301207, 0.43364081325301207, 0.43357021837349397, 0.43359375, 0.43333490210843373, 0.43361728162650603, 0.43361728162650603, 0.4334996234939759, 0.43378200301204817, 0.43378200301204817, 0.43373493975903615, 0.43382906626506024, 0.4339937876506024, 0.4338761295180723, 0.4339702560240964, 0.43401731927710846, 0.43394672439759036, 0.4338996611445783, 0.4339937876506024, 0.4340643825301205, 0.4340643825301205, 0.4340643825301205, 0.4340879141566265, 0.4341820406626506, 0.4342996987951807, 0.4342526355421687, 0.43422910391566266, 0.43432323042168675, 0.4342761671686747, 0.43415850903614456, 0.4341820406626506, 0.4342526355421687, 0.4343938253012048, 0.4342526355421687, 0.43422910391566266, 0.43422910391566266, 0.4342761671686747, 0.43432323042168675, 0.43432323042168675, 0.4343467620481928, 0.43441735692771083, 0.4343938253012048, 0.43441735692771083, 0.43441735692771083, 0.4343938253012048, 0.43441735692771083, 0.43441735692771083, 0.43441735692771083, 0.43448795180722893, 0.43441735692771083, 0.4343938253012048, 0.43441735692771083, 0.43441735692771083, 0.4343938253012048, 0.43444088855421686, 0.43441735692771083, 0.43441735692771083, 0.4344644201807229, 0.43444088855421686, 0.43444088855421686, 0.43441735692771083, 0.43444088855421686, 0.4344644201807229, 0.4345114834337349, 0.43448795180722893, 0.43448795180722893, 0.43448795180722893, 0.43448795180722893, 0.43448795180722893, 0.43448795180722893, 0.4344644201807229, 0.43448795180722893], "end": "2016-01-18 11:03:17.253000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0], "accuracy_valid": [0.1896551724137931, 0.20299030172413793, 0.20352909482758622, 0.21672952586206898, 0.23343211206896552, 0.25767780172413796, 0.27491918103448276, 0.2867726293103448, 0.29606681034482757, 0.30495689655172414, 0.31627155172413796, 0.33620689655172414, 0.35573814655172414, 0.36341594827586204, 0.37257543103448276, 0.3760775862068966, 0.3817349137931034, 0.38429418103448276, 0.38523706896551724, 0.38995150862068967, 0.3922413793103448, 0.39439655172413796, 0.39695581896551724, 0.39911099137931033, 0.40099676724137934, 0.40382543103448276, 0.40827047413793105, 0.40867456896551724, 0.41029094827586204, 0.41258081896551724, 0.41271551724137934, 0.4136584051724138, 0.41487068965517243, 0.4158135775862069, 0.4175646551724138, 0.4201239224137931, 0.42120150862068967, 0.4230872844827586, 0.4241648706896552, 0.42497306034482757, 0.4267241379310345, 0.4267241379310345, 0.4269935344827586, 0.4273976293103448, 0.4279364224137931, 0.4292834051724138, 0.4302262931034483, 0.43103448275862066, 0.43116918103448276, 0.43170797413793105, 0.43157327586206895, 0.43224676724137934, 0.4319773706896552, 0.43278556034482757, 0.43332435344827586, 0.4330549568965517, 0.43332435344827586, 0.43345905172413796, 0.43386314655172414, 0.43372844827586204, 0.43359375, 0.43426724137931033, 0.4341325431034483, 0.4341325431034483, 0.43426724137931033, 0.43440193965517243, 0.4346713362068966, 0.4348060344827586, 0.4346713362068966, 0.43494073275862066, 0.43547952586206895, 0.4352101293103448, 0.43507543103448276, 0.43507543103448276, 0.4353448275862069, 0.4352101293103448, 0.4353448275862069, 0.43561422413793105, 0.43547952586206895, 0.4358836206896552, 0.43561422413793105, 0.4357489224137931, 0.4357489224137931, 0.43601831896551724, 0.43561422413793105, 0.4358836206896552, 0.43615301724137934, 0.43615301724137934, 0.43601831896551724, 0.4358836206896552, 0.43615301724137934, 0.4362877155172414, 0.4362877155172414, 0.43601831896551724, 0.43601831896551724, 0.4358836206896552, 0.43615301724137934, 0.43601831896551724, 0.43601831896551724, 0.43601831896551724, 0.4358836206896552, 0.4362877155172414, 0.43601831896551724, 0.4358836206896552, 0.4357489224137931, 0.4357489224137931, 0.4362877155172414, 0.4358836206896552, 0.4358836206896552, 0.4358836206896552, 0.4357489224137931, 0.4358836206896552, 0.4357489224137931, 0.4357489224137931, 0.4358836206896552, 0.4358836206896552, 0.4357489224137931, 0.4358836206896552, 0.4358836206896552, 0.43615301724137934, 0.43601831896551724, 0.43601831896551724, 0.43601831896551724, 0.43615301724137934, 0.43601831896551724, 0.43615301724137934, 0.43615301724137934, 0.43601831896551724, 0.4358836206896552, 0.4358836206896552, 0.43601831896551724, 0.4358836206896552, 0.43601831896551724, 0.43601831896551724, 0.43601831896551724, 0.43601831896551724, 0.43601831896551724, 0.43615301724137934, 0.43615301724137934, 0.43615301724137934, 0.43615301724137934, 0.43615301724137934], "accuracy_test": 0.4344951923076923, "start": "2016-01-18 00:15:53.919000", "learning_rate_per_epoch": [0.006740798242390156, 0.00633701216429472, 0.0059574139304459095, 0.0056005544029176235, 0.005265071056783199, 0.004949683789163828, 0.004653188865631819, 0.004374454729259014, 0.0041124168783426285, 0.0038660757709294558, 0.003634491004049778, 0.0034167785197496414, 0.0032121073454618454, 0.003019696334376931, 0.0028388111386448145, 0.002668761182576418, 0.002508897567167878, 0.0023586100433021784, 0.002217325149103999, 0.0020845034159719944, 0.0019596379715949297, 0.0018422520952299237, 0.0017318979371339083, 0.0016281541902571917, 0.0015306248096749187, 0.0014389376156032085, 0.0013527426635846496, 0.0012717109639197588, 0.001195533201098442, 0.0011239185696467757, 0.001056593842804432, 0.0009933019755408168, 0.0009338014060631394, 0.0008778650080785155, 0.0008252793340943754, 0.0007758436258882284, 0.0007293692324310541, 0.0006856787367723882, 0.0006446053739637136, 0.0006059923325665295, 0.0005696922889910638, 0.0005355667090043426, 0.0005034853238612413, 0.00047332566464319825, 0.00044497259659692645, 0.00041831794078461826, 0.00039325995021499693, 0.00036970298970118165, 0.0003475571284070611, 0.00032673784880898893, 0.0003071656683459878, 0.0002887659065891057, 0.00027146830689162016, 0.0002552068617660552, 0.00023991950729396194, 0.00022554790484718978, 0.00021203717915341258, 0.00019933577277697623, 0.0001873951987363398, 0.0001761698949849233, 0.00016561700613237917, 0.0001556962524773553, 0.00014636976993642747, 0.0001376019645249471, 0.00012935936683788896, 0.00012161050835857168, 0.00011432581959525123, 0.00010747749911388382, 0.00010103940439876169, 9.498696454102173e-05, 8.929707837523893e-05, 8.394802716793492e-05, 7.891938730608672e-05, 7.419197208946571e-05, 6.974773714318871e-05, 6.556972221005708e-05, 6.164197839098051e-05, 5.7949513575294986e-05, 5.447823423310183e-05, 5.1214890845585614e-05, 4.814702697331086e-05, 4.526293560047634e-05, 4.255160456523299e-05, 4.000268745585345e-05, 3.760645631700754e-05, 3.535376163199544e-05, 3.323600685689598e-05, 3.124511204077862e-05, 2.9373473807936534e-05, 2.76139508059714e-05, 2.595982550701592e-05, 2.4404786017839797e-05, 2.2942895157029852e-05, 2.1568574084085412e-05, 2.0276578652556054e-05, 1.9061975763179362e-05, 1.7920128811965697e-05, 1.6846681319293566e-05, 1.583753510203678e-05, 1.4888838450133335e-05, 1.3996970665175468e-05, 1.3158527508494444e-05, 1.2370308468234725e-05, 1.1629304935922846e-05, 1.0932688383036293e-05, 1.0277800356561784e-05, 9.662141565058846e-06, 9.083361874218099e-06, 8.539252121408936e-06, 8.027735930227209e-06, 7.546860160800861e-06, 7.094789907569066e-06, 6.669799404335208e-06, 6.270266567298677e-06, 5.894666173844598e-06, 5.541565315070329e-06, 5.2096156650804915e-06, 4.897550297755515e-06, 4.604178229783429e-06, 4.32837987318635e-06, 4.069102033099625e-06, 3.825355634035077e-06, 3.5962100355391158e-06, 3.3807905310823116e-06, 3.1782751648279373e-06, 2.9878908662794856e-06, 2.80891094917024e-06, 2.640652155605494e-06, 2.4824723823257955e-06, 2.3337679522228427e-06, 2.193971113229054e-06, 2.062548219328164e-06, 1.9389979115658207e-06, 1.8228485032523167e-06, 1.713656615720538e-06, 1.611005473023397e-06, 1.514503310318105e-06, 1.4237818959372817e-06, 1.3384948260863894e-06, 1.258316615349031e-06, 1.1829412187580601e-06, 1.1120808949272032e-06], "accuracy_train_last": 0.43448795180722893, "error_valid": [0.8103448275862069, 0.7970096982758621, 0.7964709051724138, 0.783270474137931, 0.7665678879310345, 0.7423221982758621, 0.7250808189655172, 0.7132273706896552, 0.7039331896551724, 0.6950431034482758, 0.6837284482758621, 0.6637931034482758, 0.6442618534482758, 0.6365840517241379, 0.6274245689655172, 0.6239224137931034, 0.6182650862068966, 0.6157058189655172, 0.6147629310344828, 0.6100484913793103, 0.6077586206896552, 0.6056034482758621, 0.6030441810344828, 0.6008890086206897, 0.5990032327586207, 0.5961745689655172, 0.591729525862069, 0.5913254310344828, 0.5897090517241379, 0.5874191810344828, 0.5872844827586207, 0.5863415948275862, 0.5851293103448276, 0.5841864224137931, 0.5824353448275862, 0.5798760775862069, 0.5787984913793103, 0.5769127155172413, 0.5758351293103448, 0.5750269396551724, 0.5732758620689655, 0.5732758620689655, 0.5730064655172413, 0.5726023706896552, 0.5720635775862069, 0.5707165948275862, 0.5697737068965517, 0.5689655172413793, 0.5688308189655172, 0.568292025862069, 0.568426724137931, 0.5677532327586207, 0.5680226293103448, 0.5672144396551724, 0.5666756465517242, 0.5669450431034483, 0.5666756465517242, 0.5665409482758621, 0.5661368534482758, 0.5662715517241379, 0.56640625, 0.5657327586206897, 0.5658674568965517, 0.5658674568965517, 0.5657327586206897, 0.5655980603448276, 0.5653286637931034, 0.5651939655172413, 0.5653286637931034, 0.5650592672413793, 0.564520474137931, 0.5647898706896552, 0.5649245689655172, 0.5649245689655172, 0.5646551724137931, 0.5647898706896552, 0.5646551724137931, 0.564385775862069, 0.564520474137931, 0.5641163793103448, 0.564385775862069, 0.5642510775862069, 0.5642510775862069, 0.5639816810344828, 0.564385775862069, 0.5641163793103448, 0.5638469827586207, 0.5638469827586207, 0.5639816810344828, 0.5641163793103448, 0.5638469827586207, 0.5637122844827587, 0.5637122844827587, 0.5639816810344828, 0.5639816810344828, 0.5641163793103448, 0.5638469827586207, 0.5639816810344828, 0.5639816810344828, 0.5639816810344828, 0.5641163793103448, 0.5637122844827587, 0.5639816810344828, 0.5641163793103448, 0.5642510775862069, 0.5642510775862069, 0.5637122844827587, 0.5641163793103448, 0.5641163793103448, 0.5641163793103448, 0.5642510775862069, 0.5641163793103448, 0.5642510775862069, 0.5642510775862069, 0.5641163793103448, 0.5641163793103448, 0.5642510775862069, 0.5641163793103448, 0.5641163793103448, 0.5638469827586207, 0.5639816810344828, 0.5639816810344828, 0.5639816810344828, 0.5638469827586207, 0.5639816810344828, 0.5638469827586207, 0.5638469827586207, 0.5639816810344828, 0.5641163793103448, 0.5641163793103448, 0.5639816810344828, 0.5641163793103448, 0.5639816810344828, 0.5639816810344828, 0.5639816810344828, 0.5639816810344828, 0.5639816810344828, 0.5638469827586207, 0.5638469827586207, 0.5638469827586207, 0.5638469827586207, 0.5638469827586207], "accuracy_train_std": [0.03741291070687366, 0.03797972442609514, 0.039038365539347974, 0.0387273883511884, 0.039574592285162476, 0.0402378427995826, 0.03903158478700752, 0.04096057292025552, 0.042590885360328914, 0.04304841434312053, 0.04340066429014548, 0.04468483627094574, 0.04461372978431334, 0.04543295317375852, 0.04453379433727726, 0.04447441925298298, 0.04468210993322696, 0.044534820138331106, 0.04427946228418883, 0.04413767912041226, 0.04457893209670674, 0.04421213768859387, 0.04391417640472107, 0.04355616981849636, 0.0429474489848164, 0.044500707352379106, 0.0435232747555769, 0.04393301101442588, 0.043745970884098186, 0.0437791666089206, 0.04399895046010678, 0.04390420743394424, 0.043622348855922075, 0.04325102709520235, 0.04339198110436855, 0.042857358756192405, 0.042821890295189326, 0.042949183108371064, 0.04305680028853275, 0.04298781850016021, 0.04304077939708528, 0.04330537315846452, 0.04318531764160258, 0.043322759725657266, 0.043436552524021795, 0.043316470684950176, 0.043008655335618595, 0.043091407188625905, 0.04323458507987039, 0.04295446240665428, 0.042825872912527735, 0.04304491542896038, 0.04301238249340856, 0.043452529442476344, 0.04320701402620743, 0.04335495758243764, 0.04342173659985925, 0.043241801640660334, 0.043322913105488824, 0.043536688869539576, 0.04346995905912015, 0.04363275660692591, 0.043740293392972196, 0.043888798682832283, 0.04388801012396698, 0.04405273208586824, 0.0440181073208976, 0.04409238477320724, 0.04416486352254436, 0.04384544477258129, 0.04373627376474892, 0.04386638547240838, 0.04370685910001206, 0.043811894820124694, 0.04378594563914043, 0.04394595985122221, 0.04374469873442628, 0.04388307026885419, 0.04389114535019677, 0.0439706560307107, 0.04411275635233973, 0.04374407213964609, 0.04368820583804315, 0.04362596011911429, 0.04370944990506891, 0.043460155759869966, 0.04367177626210258, 0.04358364069506852, 0.043587096363573795, 0.043632762952358205, 0.04351462876160496, 0.04356107682973436, 0.04356503635938905, 0.04353009363637493, 0.04357899670900873, 0.043647177392519186, 0.043584301358760125, 0.04361234492082304, 0.043638390987424155, 0.04351773362818834, 0.04354698357691605, 0.0436429652139939, 0.04359845875370362, 0.04354175068517712, 0.04356320599554671, 0.04352943214995095, 0.0435920252989647, 0.04351105916376985, 0.04351105916376985, 0.04352333200809023, 0.04358115041113525, 0.04358158876224878, 0.04358115041113525, 0.04349247470782394, 0.04358158876224878, 0.04349670145970682, 0.04356427371713395, 0.04352627723024286, 0.04346993995152563, 0.04352627723024286, 0.04350559284835629, 0.04359802057220741, 0.04362752765701624, 0.04362375150859146, 0.04360600240545178, 0.043593803643986656, 0.04355583291791186, 0.04352958480155637, 0.04350892744608472, 0.04354271719612727, 0.04353894641027075, 0.04354271719612727, 0.043622399631466094, 0.04360457378370889, 0.04362613781854909, 0.04362613781854909, 0.04362613781854909, 0.04362613781854909, 0.04362613781854909, 0.04362613781854909, 0.04356757840382338, 0.04362613781854909], "accuracy_test_std": 0.0420200627558013, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.7332198129933515, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.007170312486159517, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adadelta", "nb_data_augmentation": 4, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 7.012782565090995e-10, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.059901789964742415}, "accuracy_valid_max": 0.4362877155172414, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.43615301724137934, "loss_train": [2.3029725551605225, 2.298013925552368, 2.273646593093872, 2.1606013774871826, 2.0757291316986084, 2.038508176803589, 2.007732391357422, 1.9798426628112793, 1.952622413635254, 1.9203999042510986, 1.889977216720581, 1.8616855144500732, 1.8337023258209229, 1.8108335733413696, 1.792112946510315, 1.7750012874603271, 1.7606472969055176, 1.749682903289795, 1.7403150796890259, 1.7310171127319336, 1.722793698310852, 1.7157363891601562, 1.7080354690551758, 1.7004834413528442, 1.6946678161621094, 1.6876029968261719, 1.681593894958496, 1.6786980628967285, 1.674178957939148, 1.6705045700073242, 1.6668521165847778, 1.6624506711959839, 1.6597577333450317, 1.6566158533096313, 1.6541147232055664, 1.6516481637954712, 1.6499687433242798, 1.6474523544311523, 1.644812822341919, 1.6431632041931152, 1.6408722400665283, 1.6389130353927612, 1.6392736434936523, 1.6377795934677124, 1.6350268125534058, 1.6334360837936401, 1.6334316730499268, 1.632401943206787, 1.6315218210220337, 1.6300147771835327, 1.6290572881698608, 1.6281764507293701, 1.6276265382766724, 1.6283327341079712, 1.6270359754562378, 1.6260628700256348, 1.6276341676712036, 1.6261115074157715, 1.624982237815857, 1.623624563217163, 1.623108148574829, 1.6242661476135254, 1.6236326694488525, 1.622643232345581, 1.6221723556518555, 1.6204502582550049, 1.6209460496902466, 1.621437430381775, 1.621200442314148, 1.6202542781829834, 1.6209412813186646, 1.6209425926208496, 1.6184276342391968, 1.6170377731323242, 1.619566559791565, 1.6177204847335815, 1.6196765899658203, 1.6185046434402466, 1.6180979013442993, 1.618539810180664, 1.6176280975341797, 1.6179890632629395, 1.6194192171096802, 1.6189626455307007, 1.6194086074829102, 1.6178642511367798, 1.6185429096221924, 1.6192530393600464, 1.618143916130066, 1.6189604997634888, 1.6163616180419922, 1.616700291633606, 1.616522192955017, 1.618746042251587, 1.6169546842575073, 1.6164473295211792, 1.616632103919983, 1.6182608604431152, 1.6168726682662964, 1.6160411834716797, 1.618004560470581, 1.6171233654022217, 1.6176280975341797, 1.6173374652862549, 1.6169257164001465, 1.6174644231796265, 1.6173477172851562, 1.6171070337295532, 1.6161949634552002, 1.6159327030181885, 1.6156255006790161, 1.6179059743881226, 1.6168626546859741, 1.615675926208496, 1.6159554719924927, 1.6160895824432373, 1.6164485216140747, 1.6158682107925415, 1.6160427331924438, 1.614555835723877, 1.6161167621612549, 1.6157222986221313, 1.6175880432128906, 1.6154481172561646, 1.6157082319259644, 1.6166797876358032, 1.6178582906723022, 1.6162426471710205, 1.6164510250091553, 1.6163374185562134, 1.616767406463623, 1.6166391372680664, 1.6171408891677856, 1.6153568029403687, 1.616774320602417, 1.6172369718551636, 1.6173770427703857, 1.6157236099243164, 1.6170445680618286, 1.6171778440475464, 1.6160258054733276, 1.6156213283538818], "accuracy_train_first": 0.18893542921686746, "model": "vgg", "loss_std": [0.003602165961638093, 0.0035613675136119127, 0.018983840942382812, 0.05377640575170517, 0.05106815695762634, 0.05440317839384079, 0.05781961604952812, 0.059424988925457, 0.06076512113213539, 0.06346476823091507, 0.06282810866832733, 0.06461864709854126, 0.06555786728858948, 0.06475310772657394, 0.0657937154173851, 0.06602875888347626, 0.06981018930673599, 0.0682123526930809, 0.06990731507539749, 0.06876469403505325, 0.0691414475440979, 0.07019784301519394, 0.06960432976484299, 0.07248219102621078, 0.0715695470571518, 0.06918108463287354, 0.0749828964471817, 0.07089919596910477, 0.07318032532930374, 0.07170980423688889, 0.07445395737886429, 0.070580393075943, 0.07201851159334183, 0.07357941567897797, 0.07287879288196564, 0.07514586299657822, 0.07236145436763763, 0.07306491583585739, 0.07384050637483597, 0.07428460568189621, 0.0748869776725769, 0.07595451176166534, 0.07336331903934479, 0.07435209304094315, 0.07267843931913376, 0.07262997329235077, 0.07670731842517853, 0.0759885162115097, 0.0741087794303894, 0.07677048444747925, 0.07564577460289001, 0.07673369348049164, 0.07543178647756577, 0.07590457797050476, 0.07471020519733429, 0.0747947245836258, 0.07433106750249863, 0.07611405849456787, 0.07502724975347519, 0.07480402290821075, 0.07609643042087555, 0.07479628175497055, 0.07605446875095367, 0.07686468958854675, 0.07803280651569366, 0.07854249328374863, 0.07458305358886719, 0.0727442279458046, 0.07558628171682358, 0.07647509127855301, 0.07385937124490738, 0.07643725723028183, 0.07816428691148758, 0.07551363110542297, 0.07651031017303467, 0.07893601059913635, 0.07891014963388443, 0.07424147427082062, 0.07516874372959137, 0.0756896361708641, 0.07668397575616837, 0.07484584301710129, 0.07504649460315704, 0.076661616563797, 0.07645561546087265, 0.0762518122792244, 0.0755339115858078, 0.07603858411312103, 0.07459700852632523, 0.07886294275522232, 0.07558818906545639, 0.07431351393461227, 0.0755520761013031, 0.0787355825304985, 0.07733538746833801, 0.07645434886217117, 0.07771774381399155, 0.07589418441057205, 0.0785774290561676, 0.076084665954113, 0.07568101584911346, 0.07619111984968185, 0.07655663788318634, 0.07638593018054962, 0.07669755071401596, 0.07531193643808365, 0.07773075997829437, 0.07683498412370682, 0.07792351394891739, 0.07807896286249161, 0.07784426957368851, 0.07559932768344879, 0.07613496482372284, 0.07655158638954163, 0.07686088979244232, 0.07673235237598419, 0.07613730430603027, 0.07835625857114792, 0.07888985425233841, 0.07775281369686127, 0.07781776040792465, 0.07873882353305817, 0.0751226469874382, 0.07715649157762527, 0.07685108482837677, 0.07680539041757584, 0.0734812542796135, 0.07818123698234558, 0.07827490568161011, 0.07705113291740417, 0.07626789808273315, 0.07864926755428314, 0.0755268782377243, 0.07803277671337128, 0.0779157429933548, 0.0754779577255249, 0.07505914568901062, 0.0762309581041336, 0.07505863904953003, 0.07439510524272919, 0.07751678675413132, 0.07581912726163864]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:00 2016", "state": "available"}], "summary": "02f04ee421738fc860af900ba15e74b6"}