{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 16, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.011607494231625487, 0.007284807693838294, 0.010785338965498574, 0.012588376797810767, 0.009435960219014881, 0.016779091345228474, 0.016806057793934764, 0.015908614072854547, 0.013654415030518955, 0.013357481100658092, 0.01482873263307412, 0.014779909314178203, 0.014855494726726634, 0.014097618203781387, 0.013555436992242528, 0.015329522082180153, 0.019417917494598817, 0.023225021710501323, 0.020787951889964223, 0.019196107260792784, 0.017993864980376178, 0.018807556740252702, 0.021934311790568952, 0.01972417592048837, 0.020156831544619305, 0.01838150813808418, 0.014914859682975318, 0.014805480847340128, 0.015059936977099128, 0.01607428531182143, 0.014773512370027822, 0.015665349071060673, 0.01326569714463685, 0.013340560536358938, 0.014616050361582885, 0.017454848412049133, 0.016647186145597844, 0.01808543832820098, 0.018280934035250756, 0.017518791135836473, 0.015971496688776662, 0.012744052447628917, 0.013128651876117652, 0.013790669698889863, 0.015550021734623094, 0.013654374476729793, 0.015574398311645055, 0.014953537972767722, 0.014468283108756084, 0.01436605557287484, 0.015608839121131453, 0.014734299562723017, 0.013941190976374212, 0.011858442310105197, 0.013455790627535344, 0.013333900113078754, 0.013410967207252657, 0.011207339732509581, 0.011169522070355434, 0.008653742525019315, 0.011089873117408439, 0.012926149901043887, 0.01202639428615256, 0.014237668065289796, 0.012929740142624437, 0.015191686881812128, 0.015201251932362216, 0.009997242027790397, 0.010940389807332182, 0.010686837783790018, 0.009309859005718964, 0.00911934022605574, 0.008527136557561321, 0.009656915765750824, 0.009451683645355797, 0.011381685159671834, 0.00851584707548829, 0.009131377734341608, 0.00708972994923802, 0.012004550542668047, 0.00685115372390977, 0.010688987885906743, 0.007089150856418252, 0.011112230953862679, 0.01082901690059068, 0.014741114335850005, 0.014091335265273395, 0.007836741424238229, 0.011227407368440282, 0.013406356784931793, 0.014802168676146307, 0.016131756964239838, 0.016271556033650568, 0.013475812988301748, 0.01066493538426149, 0.010183937429634136], "moving_avg_accuracy_train": [0.03740385780038759, 0.07669336081695274, 0.11565448757325117, 0.15349068249786588, 0.1898075923823078, 0.22450613780562184, 0.25738303475318813, 0.28845775975355226, 0.3177385050383484, 0.3452884111386571, 0.37089483961227754, 0.3950633116253484, 0.41771691393111365, 0.4390979224503315, 0.45927986579730645, 0.47827587388811843, 0.49614415847697363, 0.512932243552126, 0.5286831169959166, 0.5435494001941193, 0.5573778808903774, 0.570369743243154, 0.5826110102844255, 0.5941721633453609, 0.6052421576109078, 0.6156490395796066, 0.6255196464478272, 0.6349772880922655, 0.6438472384889267, 0.6522719721197313, 0.6603122867029316, 0.6680926906980593, 0.675466933907923, 0.6825477841217819, 0.6893087049701703, 0.6958212529682161, 0.7019475770819241, 0.7077867535687761, 0.7133626666497446, 0.71875301223214, 0.7239854674169641, 0.7290758572927926, 0.7340313768952778, 0.7389190277231918, 0.7434945887290196, 0.7479032011866363, 0.7523312237163208, 0.7565512119251612, 0.7608256044773387, 0.7648143558028606, 0.7686204347862973, 0.7724759142058772, 0.7759922765620522, 0.7793892652218307, 0.7827976164370508, 0.7860743598747872, 0.7893535280020649, 0.7923977131713582, 0.7954350628225225, 0.7984614660144765, 0.8012828851372352, 0.8039245049441558, 0.8065134431655947, 0.808715748575641, 0.8107348455815412, 0.8125172998499836, 0.8141746626748321, 0.815643109826738, 0.817306400991997, 0.8189032723419021, 0.8206426898532361, 0.8217175952634459, 0.8222968184279005, 0.8232830769401769, 0.8242450422654929, 0.8250502850915922, 0.8253171295636991, 0.8263011569635492, 0.8271727946817384, 0.8273297307912685, 0.8276962243338193, 0.8276587671078478, 0.8290988755103429, 0.8283006268251263, 0.8286539884631655, 0.8268289116625762, 0.8285407392001761, 0.8307227285184383, 0.8319381814763951, 0.8337990277575116, 0.8328911258642355, 0.8342243081744528, 0.8360102539489234, 0.8373340451376412, 0.8399016208633566, 0.8431144165105019], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 548864658, "moving_var_accuracy_train": [0.012591437205164542, 0.02522527891024623, 0.0363644756023048, 0.04561222685943528, 0.052921265665483816, 0.05846504058937954, 0.06234654970654962, 0.06480264154052894, 0.06603863578637403, 0.0662657481429591, 0.06554037594123495, 0.06424337370213076, 0.06243770760876765, 0.0603082645755807, 0.05794323565336719, 0.05539654699850622, 0.05273037264599048, 0.04999389358580643, 0.04722731435540659, 0.04449364030502841, 0.04176531817982661, 0.03910788274838539, 0.03654573204252834, 0.0340941011791609, 0.031787594018598, 0.02958356334753206, 0.027502066932301523, 0.02555688310834291, 0.023709278977861698, 0.021977136310825623, 0.020361242607114482, 0.018869930523349617, 0.01747235263727864, 0.016176363331310725, 0.014970117454643214, 0.013854825242020535, 0.012807129342134294, 0.0118332802463223, 0.010929769481868707, 0.010098294963160686, 0.009334872752195342, 0.008634594098767245, 0.00799214925966606, 0.007407936509239938, 0.006855564684978405, 0.006344930990693641, 0.005886904343334816, 0.005458488613346104, 0.005077073637222488, 0.0047125575077319164, 0.004371677892012153, 0.004068292596804178, 0.0037727465751030767, 0.0034993277051847457, 0.0032539466567228985, 0.0030251854190613465, 0.002819443369617794, 0.0026209026027605234, 0.0024418417786153212, 0.002280089647276219, 0.002123724335345013, 0.00197415529864935, 0.0018370631788142636, 0.0016970082030049098, 0.0015639981571775384, 0.0014361926304315815, 0.00131729503118713, 0.001204972561409881, 0.0011093741427727513, 0.001021386711468807, 0.0009464781998305438, 0.0008622291746155741, 0.0007790257524221839, 0.0007098775298573044, 0.0006472181724555672, 0.0005883320992908697, 0.000530139743112429, 0.00048584055811408774, 0.0004440942731086094, 0.00039990650628001856, 0.0003611247133026002, 0.0003250248693663375, 0.00031118759232813477, 0.0002858036417663722, 0.000258347057614875, 0.00026249049980583245, 0.0002626146314916168, 0.0002792028648075482, 0.00026457851136384755, 0.00026928540016496846, 0.0002497754327788002, 0.00024079426515140684, 0.0002454212594204092, 0.00023665094148031144, 0.0002723178532978268, 0.000337984570800888], "duration": 18718.997877, "accuracy_train": [0.37403857800387597, 0.43029888796603916, 0.4663046283799372, 0.49401643681939833, 0.516659781342285, 0.5367930466154485, 0.5532751072812846, 0.5681302847568291, 0.5812652126015135, 0.593237566041436, 0.6013526958748615, 0.6125795597429863, 0.6215993346830011, 0.6315269991232927, 0.6409173559200811, 0.6492399467054264, 0.6569587197766703, 0.6640250092284976, 0.6704409779900332, 0.6773459489779439, 0.6818342071566998, 0.6872965044181433, 0.6927824136558692, 0.69822254089378, 0.7048721060008306, 0.7093109772978959, 0.7143551082618125, 0.7200960628922112, 0.7236767920588778, 0.728094574796973, 0.732675117951735, 0.7381163266542082, 0.7418351227966962, 0.7462754360465117, 0.7501569926056663, 0.7544341849506275, 0.7570844941052971, 0.760339341950443, 0.7635458843784607, 0.7672661224736989, 0.7710775640803802, 0.7748893661752492, 0.7786310533176449, 0.7829078851744187, 0.7846746377814692, 0.7875807133051864, 0.792183426483481, 0.794531105804725, 0.7992951374469361, 0.8007131177325582, 0.8028751456372278, 0.8071752289820967, 0.8076395377676264, 0.8099621631598376, 0.8134727773740311, 0.815565050814415, 0.8188660411475637, 0.8197953796949982, 0.8227712096830011, 0.8256990947420635, 0.8266756572420635, 0.8276990832064415, 0.8298138871585455, 0.8285364972660576, 0.8289067186346438, 0.8285593882659652, 0.8290909280984681, 0.8288591341938908, 0.8322760214793282, 0.8332751144910484, 0.8362974474552418, 0.8313917439553341, 0.8275098269079919, 0.8321594035506644, 0.832902730193337, 0.8322974705264857, 0.8277187298126615, 0.8351574035622, 0.8350175341454411, 0.8287421557770396, 0.8309946662167773, 0.8273216520741048, 0.8420598511327981, 0.8211163886581765, 0.8318342432055187, 0.8104032204572721, 0.8439471870385751, 0.8503606323827981, 0.8428772580980066, 0.85054664428756, 0.8247200088247508, 0.8462229489664084, 0.8520837659191584, 0.8492481658361019, 0.8630098023947952, 0.8720295773348099], "end": "2016-01-27 17:25:10.144000", "learning_rate_per_epoch": [0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399, 0.002018597675487399], "accuracy_valid": [0.3676713690700301, 0.4253312076430723, 0.46266560382153615, 0.4924419357115964, 0.5139469008847892, 0.5343326430722892, 0.5491046216114458, 0.5605895260730422, 0.5761439311935241, 0.588768648814006, 0.5951471903237951, 0.6007624246987951, 0.6092058664344879, 0.6154417474585843, 0.6247499764683735, 0.6297548592808735, 0.6337522943335843, 0.6376482492469879, 0.6413515389683735, 0.6468652932040663, 0.652501117752259, 0.6531011742281627, 0.6561220467808735, 0.6613004753388554, 0.662266742752259, 0.6654611610504518, 0.6670892554593373, 0.6681981833584337, 0.667567241622741, 0.668910015060241, 0.6710058005459337, 0.6716058570218373, 0.6732133612575302, 0.6744443594691265, 0.6760209784450302, 0.6782079489834337, 0.6777402579066265, 0.6782079489834337, 0.6767431052334337, 0.6757768378200302, 0.6771402014307228, 0.6755841726280121, 0.6761842291039157, 0.6786256353539157, 0.6786153402673193, 0.6802331395896084, 0.6761533438441265, 0.6760415686182228, 0.6737119375941265, 0.6739457831325302, 0.6721044333584337, 0.6696630271084337, 0.6698056875941265, 0.6705484045557228, 0.6698159826807228, 0.6677201971950302, 0.6663671286709337, 0.6633359610316265, 0.6641801581325302, 0.6653199712914157, 0.6616063864834337, 0.6612298804593373, 0.6605077536709337, 0.657923686935241, 0.6584325583584337, 0.6526437782379518, 0.6537630012236446, 0.6520746070218373, 0.6514745505459337, 0.6514745505459337, 0.6527158438441265, 0.6484433829066265, 0.6477006659450302, 0.6485345679593373, 0.6487787085843373, 0.6433664344879518, 0.6405794074736446, 0.6420442512236446, 0.6417898155120482, 0.637608539627259, 0.6399690559111446, 0.6334375588290663, 0.6363981315888554, 0.6291445077183735, 0.6325727715549698, 0.6206701807228916, 0.6381674157567772, 0.6396940300263554, 0.6367643425263554, 0.6396528496799698, 0.6244955407567772, 0.6370790780308735, 0.6370790780308735, 0.6374555840549698, 0.639927875564759, 0.6492361045745482], "accuracy_test": 0.3346619897959184, "start": "2016-01-27 12:13:11.146000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0], "accuracy_train_last": 0.8720295773348099, "batch_size_eval": 1024, "accuracy_train_std": [0.01740092000692444, 0.016322702669655374, 0.017323915897681213, 0.015968776039296646, 0.016771970150521395, 0.01563704739220498, 0.015513551401474212, 0.016966296306994606, 0.01750510256731779, 0.017001304471282994, 0.01550816491109654, 0.015719079398859127, 0.015287015623234155, 0.015406705596133413, 0.016377216531185308, 0.015262420209406175, 0.01556588205776482, 0.015740500446819867, 0.015739591179499912, 0.015978245388408693, 0.016133130161860007, 0.01540482205686511, 0.015825009093981376, 0.01609406236555884, 0.016081165142022864, 0.01615982500044708, 0.01641540025557019, 0.016912029017065587, 0.016967519924612942, 0.01656074715855887, 0.015830902488873123, 0.01553708413344818, 0.015372182579314664, 0.015046188940107381, 0.015595981442486744, 0.015186351377622706, 0.015590018368022507, 0.013920214874737867, 0.01562281114033362, 0.015312891308865775, 0.016243654963988684, 0.016384656973933407, 0.016150069402249966, 0.016443871771726633, 0.017065856303572848, 0.01699633926388061, 0.017215021849000357, 0.018204074505971946, 0.01872997633711446, 0.018688907258564546, 0.01937624594526151, 0.018747839749090323, 0.01967098166999317, 0.02069349649324153, 0.021302175837723012, 0.02102892910121993, 0.020914097501331122, 0.020895257485336292, 0.021130735043327024, 0.02104997196474334, 0.020734060291279492, 0.02032627756658917, 0.021396627716745973, 0.020764206829429963, 0.022249435926882964, 0.021480261887773636, 0.023376017487542353, 0.023309566399585195, 0.02332741825976424, 0.02351211055418639, 0.023396491380985086, 0.02335665751932315, 0.02390639718959366, 0.023633832657523068, 0.024183960233691532, 0.025042412749307446, 0.02434368075782173, 0.023690120483167713, 0.0252256094971265, 0.02422478672606094, 0.025582460402887315, 0.025170971643567987, 0.02599237455587933, 0.025638948433152217, 0.02508255724243818, 0.02283857499643892, 0.022393467577298112, 0.025876027328296684, 0.024543784424394723, 0.024152291602053573, 0.025404395135721885, 0.024754922747986005, 0.023694883185715724, 0.0221514367076497, 0.025305827821936827, 0.0234099233690575], "accuracy_test_std": 0.0162139906394931, "error_valid": [0.6323286309299698, 0.5746687923569277, 0.5373343961784638, 0.5075580642884037, 0.4860530991152108, 0.4656673569277108, 0.4508953783885542, 0.43941047392695776, 0.42385606880647586, 0.41123135118599397, 0.40485280967620485, 0.39923757530120485, 0.39079413356551207, 0.38455825254141573, 0.3752500235316265, 0.3702451407191265, 0.36624770566641573, 0.36235175075301207, 0.3586484610316265, 0.35313470679593373, 0.34749888224774095, 0.3468988257718373, 0.3438779532191265, 0.3386995246611446, 0.33773325724774095, 0.33453883894954817, 0.3329107445406627, 0.33180181664156627, 0.33243275837725905, 0.33108998493975905, 0.32899419945406627, 0.3283941429781627, 0.3267866387424698, 0.3255556405308735, 0.3239790215549698, 0.32179205101656627, 0.3222597420933735, 0.32179205101656627, 0.32325689476656627, 0.3242231621799698, 0.32285979856927716, 0.32441582737198793, 0.32381577089608427, 0.32137436464608427, 0.3213846597326807, 0.3197668604103916, 0.3238466561558735, 0.32395843138177716, 0.3262880624058735, 0.3260542168674698, 0.32789556664156627, 0.33033697289156627, 0.3301943124058735, 0.32945159544427716, 0.33018401731927716, 0.3322798028049698, 0.33363287132906627, 0.3366640389683735, 0.3358198418674698, 0.33468002870858427, 0.33839361351656627, 0.3387701195406627, 0.33949224632906627, 0.34207631306475905, 0.34156744164156627, 0.34735622176204817, 0.3462369987763554, 0.3479253929781627, 0.34852544945406627, 0.34852544945406627, 0.3472841561558735, 0.3515566170933735, 0.3522993340549698, 0.3514654320406627, 0.3512212914156627, 0.35663356551204817, 0.3594205925263554, 0.3579557487763554, 0.35821018448795183, 0.36239146037274095, 0.3600309440888554, 0.36656244117093373, 0.3636018684111446, 0.3708554922816265, 0.3674272284450302, 0.3793298192771084, 0.36183258424322284, 0.3603059699736446, 0.3632356574736446, 0.3603471503200302, 0.37550445924322284, 0.3629209219691265, 0.3629209219691265, 0.3625444159450302, 0.36007212443524095, 0.35076389542545183], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.5615003551657753, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00201859762662529, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "l2_decay": 3.560521749064998e-05, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.02949944555653965}, "accuracy_valid_max": 0.6802331395896084, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop', 'santa_sss'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6492361045745482, "loss_train": [1.9448106288909912, 1.6882885694503784, 1.590397596359253, 1.5133743286132812, 1.4484071731567383, 1.3903977870941162, 1.3389122486114502, 1.2943196296691895, 1.2549760341644287, 1.2200486660003662, 1.1885907649993896, 1.159473180770874, 1.1327309608459473, 1.1080886125564575, 1.0851161479949951, 1.063683032989502, 1.0435978174209595, 1.0246344804763794, 1.0067702531814575, 0.9893292188644409, 0.9724537134170532, 0.9565327167510986, 0.9412984848022461, 0.9264028072357178, 0.9120888113975525, 0.8980531096458435, 0.884272575378418, 0.8707598447799683, 0.8577588796615601, 0.8450257778167725, 0.8325467109680176, 0.820111870765686, 0.8078281283378601, 0.7956952452659607, 0.783454418182373, 0.7712289690971375, 0.7590559720993042, 0.7469078898429871, 0.7348792552947998, 0.7230768203735352, 0.7113542556762695, 0.6994248032569885, 0.687511146068573, 0.6756313443183899, 0.6639173626899719, 0.6519445180892944, 0.6398594975471497, 0.6279183030128479, 0.6160295605659485, 0.6039822697639465, 0.5916358232498169, 0.5794565677642822, 0.5676279664039612, 0.5551780462265015, 0.5426557064056396, 0.5305495262145996, 0.5183539986610413, 0.5059586763381958, 0.49377167224884033, 0.48155584931373596, 0.4692988991737366, 0.45717933773994446, 0.4446781873703003, 0.43251168727874756, 0.4210146367549896, 0.4088596999645233, 0.39641544222831726, 0.38502389192581177, 0.37266162037849426, 0.36037102341651917, 0.34882524609565735, 0.33641549944877625, 0.3258209526538849, 0.3149923086166382, 0.30392026901245117, 0.2928483784198761, 0.2835576832294464, 0.2747616171836853, 0.26584380865097046, 0.2616487741470337, 0.25225192308425903, 0.24522925913333893, 0.24258312582969666, 0.23585209250450134, 0.23522256314754486, 0.2268306463956833, 0.21905101835727692, 0.2127884477376938, 0.21506302058696747, 0.2122519165277481, 0.2103048861026764, 0.21592099964618683, 0.20675726234912872, 0.20053811371326447, 0.1910184770822525, 0.18697801232337952], "accuracy_train_first": 0.37403857800387597, "model": "residualv3", "loss_std": [0.2722211480140686, 0.11530300974845886, 0.11982348561286926, 0.1236642599105835, 0.12656661868095398, 0.12905046343803406, 0.13141097128391266, 0.13319802284240723, 0.13425445556640625, 0.1347094476222992, 0.13506370782852173, 0.13525515794754028, 0.1353645622730255, 0.13568653166294098, 0.13563765585422516, 0.13554854691028595, 0.13535989820957184, 0.135197713971138, 0.13509124517440796, 0.1346430480480194, 0.13381439447402954, 0.13340237736701965, 0.13304056227207184, 0.13255256414413452, 0.13209916651248932, 0.13154751062393188, 0.13097982108592987, 0.13024519383907318, 0.12975893914699554, 0.1295097917318344, 0.1287534385919571, 0.12823764979839325, 0.12750838696956635, 0.12691375613212585, 0.12652306258678436, 0.1258125603199005, 0.1248491182923317, 0.12414984405040741, 0.12327590584754944, 0.12219521403312683, 0.12109946459531784, 0.1200796514749527, 0.11885292828083038, 0.11753816157579422, 0.11636386811733246, 0.115363709628582, 0.11389312148094177, 0.11268042027950287, 0.11123889684677124, 0.10981491953134537, 0.1082082986831665, 0.10647808760404587, 0.10493733733892441, 0.10337984561920166, 0.10177326947450638, 0.10055622458457947, 0.09900456666946411, 0.09764870256185532, 0.09566199034452438, 0.09384418278932571, 0.09180138260126114, 0.08987029641866684, 0.0881441980600357, 0.08607685565948486, 0.08467195183038712, 0.08267449587583542, 0.08035987615585327, 0.07800591737031937, 0.07582446932792664, 0.0733838751912117, 0.07090014219284058, 0.06800306588411331, 0.06596186012029648, 0.06399447470903397, 0.06153172627091408, 0.05942956358194351, 0.05748593434691429, 0.05501916632056236, 0.053288962692022324, 0.05318151041865349, 0.05046757683157921, 0.0484125055372715, 0.05142742767930031, 0.0477592870593071, 0.048074860125780106, 0.04655119404196739, 0.04421791061758995, 0.04184051603078842, 0.0448155552148819, 0.0430246964097023, 0.044734787195920944, 0.04726560041308403, 0.04314728453755379, 0.04152989387512207, 0.03831711411476135, 0.036267057061195374]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:18 2016", "state": "available"}], "summary": "208636fbfc48a24d2ba3a96b313e5428"}