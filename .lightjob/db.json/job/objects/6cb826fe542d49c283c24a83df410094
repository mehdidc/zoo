{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6500405073165894, 1.2320448160171509, 0.9737425446510315, 0.8455169796943665, 0.7643060684204102, 0.7101548314094543, 0.6675683259963989, 0.6329177021980286, 0.6067671179771423, 0.5828282833099365, 0.5622557401657104, 0.5449521541595459, 0.5297841429710388, 0.5151818990707397, 0.5011061429977417, 0.4911578297615051, 0.4788707494735718, 0.4683228135108948, 0.45954838395118713, 0.45090949535369873, 0.442360520362854, 0.4342597723007202, 0.42714887857437134, 0.41899704933166504, 0.4122413098812103, 0.40802904963493347, 0.40151742100715637, 0.3959105908870697, 0.3900652229785919, 0.38622623682022095, 0.3795306980609894, 0.3751753568649292, 0.37106993794441223, 0.36732715368270874, 0.3629063665866852, 0.3588576018810272, 0.3561843931674957, 0.35093528032302856, 0.3486742675304413, 0.34445950388908386, 0.3418458104133606, 0.3384178578853607, 0.334279865026474, 0.33061346411705017, 0.3280262351036072, 0.32597285509109497, 0.32097479701042175, 0.3205009698867798, 0.31765472888946533, 0.3140799403190613, 0.3129371404647827, 0.3092482089996338, 0.30846402049064636, 0.30559271574020386, 0.3017750680446625, 0.3004542887210846, 0.2989858090877533, 0.2968433201313019, 0.29407092928886414, 0.29250583052635193, 0.29065898060798645, 0.28925952315330505, 0.2873518168926239, 0.28499650955200195, 0.28427284955978394, 0.2806239724159241, 0.27928799390792847, 0.2775752544403076, 0.27583858370780945, 0.2736364006996155, 0.2747112214565277, 0.2715238034725189, 0.26962870359420776, 0.2686735689640045, 0.26687148213386536, 0.2669118344783783, 0.2645585238933563, 0.2645447552204132, 0.2622859477996826, 0.2607497572898865, 0.2600845694541931, 0.25836312770843506, 0.25485336780548096, 0.256318598985672, 0.2522154450416565, 0.2539294362068176, 0.25250425934791565, 0.25145798921585083, 0.24812567234039307, 0.24889442324638367, 0.24664494395256042, 0.2454887330532074, 0.24535149335861206, 0.243135467171669, 0.24314454197883606, 0.24169209599494934, 0.241227388381958, 0.23997047543525696, 0.23805898427963257, 0.23766666650772095, 0.23721876740455627, 0.236050084233284, 0.234847292304039, 0.23402515053749084, 0.23260711133480072, 0.2324715554714203, 0.23223057389259338], "moving_avg_accuracy_train": [0.05754981225775193, 0.11926091630329456, 0.18173363852263286, 0.24335285697703252, 0.30116953074892133, 0.35500649142218343, 0.40472474512695655, 0.4504405802707301, 0.4923867919464994, 0.5306032680213216, 0.5658605104040825, 0.5980267953271293, 0.6275948692435673, 0.6545501856945336, 0.6792259918932142, 0.7018713454482173, 0.7225800456786816, 0.7414503186694145, 0.7585661699408544, 0.7743258233648198, 0.7889350857761691, 0.8022369178166309, 0.8142758517732476, 0.8254297261116574, 0.8356006383519129, 0.8449914803514391, 0.8536501763950604, 0.861578005660547, 0.8688245509958857, 0.8755278394536522, 0.8816467574739569, 0.8873513852922219, 0.8925809535274885, 0.8972944322392006, 0.901699359545227, 0.9057171643968135, 0.9094633249989372, 0.9129536684230468, 0.9161808638154229, 0.9192551115804752, 0.9220706545475662, 0.9247464412465102, 0.9271686722660544, 0.929525247297893, 0.9316858726004034, 0.9337164298297964, 0.935650780035032, 0.9374453538864568, 0.9390766742967682, 0.9405960159398581, 0.9420866563055438, 0.9434049450977469, 0.9446914224583579, 0.9458724675221843, 0.946984272253447, 0.9481523072258689, 0.9492895331581822, 0.9502781953139402, 0.951126138575551, 0.9519846546610099, 0.9528246763557616, 0.9536087058132088, 0.9543281750713121, 0.9549757334524238, 0.9556794797823384, 0.9563895453411568, 0.9570355076928847, 0.9576587985856486, 0.9582638661188796, 0.9588502795773589, 0.9593338738626094, 0.9598970640014961, 0.9603807557360364, 0.9608625091756756, 0.9612704385368087, 0.9616468755570665, 0.9620415084955459, 0.9625199470759009, 0.9629412051541636, 0.9632853880948196, 0.9636556426592764, 0.9641004068125072, 0.9644566248694901, 0.9647330432933938, 0.9651026915641837, 0.9654261104614752, 0.9657660516428563, 0.9660974671965479, 0.9664027887389364, 0.9667263702032672, 0.9670874200830881, 0.9673052999832326, 0.9674758513052766, 0.9676968128594112, 0.9678723907212183, 0.968128175193301, 0.9684210160407575, 0.9686287692320398, 0.9688134940530219, 0.9690006727311915, 0.9691899877831923, 0.9694463657871266, 0.9696422648073434, 0.9698603905552912, 0.9700334882891678, 0.9701846259520376, 0.9702787611212302], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05790147896272589, 0.11976652214326053, 0.1804521499082266, 0.23991555337561177, 0.29575209950642406, 0.34714024493040513, 0.39456453933796704, 0.43778454318843235, 0.4772614246658993, 0.5130835867456196, 0.5458026953620968, 0.575504181755857, 0.6025559908487201, 0.6270836654837276, 0.6491544546205958, 0.6696723440606447, 0.6880652023691887, 0.7048751225031282, 0.7201515645073335, 0.7337772624899587, 0.7462845312993213, 0.7576021083839976, 0.7678733769789562, 0.7770066259245094, 0.7853862708904169, 0.7933328424083029, 0.8005081413282407, 0.8067675093302058, 0.8128252451724112, 0.8180655698645074, 0.8228805478460537, 0.8276250961299725, 0.8317781197436318, 0.8356165857719042, 0.8387507338588704, 0.8419651806631189, 0.8449202474518521, 0.8473936135669832, 0.849534193851851, 0.8516783836534129, 0.8535777104330264, 0.8552850455173593, 0.8569925455307589, 0.8584316392928185, 0.8597512377411722, 0.8609877044696905, 0.8622002397926763, 0.8633912368506828, 0.8641528108783404, 0.8649898594215304, 0.8657289370618322, 0.8662577705770346, 0.8668446135306263, 0.8676607404682263, 0.8682303862538585, 0.868893669870566, 0.8695618082957834, 0.86995355432991, 0.8702339130817834, 0.8705106500209695, 0.8707200036465081, 0.8708128246768121, 0.8710672620415857, 0.8714050894424723, 0.8717945833220203, 0.8719376082823636, 0.872116188380332, 0.8722301413608229, 0.8721576825711261, 0.8724119114902182, 0.8725217357308801, 0.8723733483964968, 0.8724605558667116, 0.8725156580360646, 0.8725662794971418, 0.8726474303972018, 0.8724895621221653, 0.8723586581972229, 0.8722764362498651, 0.8724852572333123, 0.8724381739986858, 0.8723459414538625, 0.872263961672181, 0.8723743148460774, 0.8723149412963341, 0.8722981261953152, 0.8725525768005578, 0.8727103991750954, 0.8725665300501311, 0.8724227817890939, 0.872492838888799, 0.8724307314400547, 0.8722771784861847, 0.8723262043310903, 0.8721261869665055, 0.8721272177898097, 0.8721535891019432, 0.8721009925693844, 0.8721177793723104, 0.8720596453074438, 0.872138513466835, 0.8722014058136757, 0.8723078665594919, 0.8721941026821571, 0.8722748206613059, 0.8724451230925397, 0.8725475081383309], "moving_var_accuracy_train": [0.029807828018122456, 0.0611013884789883, 0.09011681882454092, 0.1152774896884661, 0.13383465061402466, 0.14653695056343158, 0.15413039827015815, 0.15752679668917646, 0.15760947908579487, 0.1549930225694125, 0.15068137857640196, 0.14492526969051747, 0.13830118167761724, 0.1310103652746011, 0.1233893874511346, 0.11566575704470095, 0.10795883372734796, 0.10036773517931619, 0.09296753294409903, 0.0859060797340807, 0.07923634669450562, 0.07290516064574894, 0.06691906795848006, 0.061346841377445394, 0.05614318434189165, 0.0513225571288431, 0.046865058570541206, 0.04274420700525143, 0.03894239807840038, 0.03545256495589258, 0.03224427887995622, 0.029312735998864914, 0.026627597854324205, 0.02416479000298366, 0.02192294146382769, 0.019875932119873815, 0.01801464238119857, 0.01632282061804274, 0.014784271667143623, 0.013390903494317628, 0.01212315868468168, 0.010975281326337722, 0.009930558021712332, 0.008987483232467267, 0.008130749624501176, 0.007354783126007621, 0.006652980209855314, 0.006016666646643742, 0.005438950838509258, 0.0049158313459141765, 0.004444246289621062, 0.00401546262871579, 0.0036288115818384945, 0.003278484230639744, 0.00296176079541989, 0.0026778634671491093, 0.002421716665824334, 0.0021883420749659534, 0.0019759789374435583, 0.0017850144925201281, 0.0016128637712969974, 0.0014571097138786008, 0.0013160574666109418, 0.0011882257066623794, 0.0010738604660679547, 0.0009710121573415424, 0.0008776663478460355, 0.0007933961368944544, 0.0007173514836829411, 0.0006487112620132183, 0.0005859449067064384, 0.0005302050642286477, 0.0004792901770523465, 0.00043344993673655023, 0.00039160260033596503, 0.000353717683774354, 0.0003197475318021145, 0.00028983290989845174, 0.00026244674422512017, 0.00023726822687235562, 0.00021477520016762963, 0.00019507801651885793, 0.00017671223660405851, 0.00015972867724931334, 0.00014498556812126307, 0.0001314284093572639, 0.00011932560848272643, 0.0001083815740575114, 9.838240784997814e-05, 8.94865117415065e-05, 8.171107370882423e-05, 7.396721119592458e-05, 6.683227985739083e-05, 6.058846794730185e-05, 5.48070694225825e-05, 4.991519374575179e-05, 4.5695476228628156e-05, 4.1514381102157206e-05, 3.767005232732318e-05, 3.4218369812642637e-05, 3.1119094531605015e-05, 2.8598752206556858e-05, 2.6084264820998402e-05, 2.390404791615877e-05, 2.178330855380172e-05, 1.9810561036661398e-05, 1.7909257803705688e-05], "duration": 139005.809821, "accuracy_train": [0.5754981225775194, 0.6746608527131783, 0.7439881384966777, 0.7979258230666297, 0.821519594695921, 0.839539137481543, 0.8521890284699151, 0.8618830965646919, 0.8699026970284238, 0.8745515526947213, 0.8831756918489295, 0.8875233596345515, 0.8937075344915099, 0.89714803375323, 0.90130824768134, 0.9056795274432448, 0.9089583477528608, 0.9112827755860096, 0.9126088313838132, 0.9161627041805095, 0.9204184474783131, 0.9219534061807864, 0.9226262573827981, 0.9258145951573459, 0.9271388485142118, 0.9295090583471761, 0.9315784407876523, 0.9329284690499261, 0.934043459013935, 0.9358574355735512, 0.9367170196567, 0.9386930356566077, 0.9396470676448875, 0.9397157406446106, 0.9413437052994648, 0.9418774080610927, 0.943178770418051, 0.9443667592400333, 0.945225622346807, 0.9469233414659468, 0.9474105412513842, 0.9488285215370063, 0.9489687514419527, 0.9507344225844407, 0.9511315003229974, 0.9519914448943337, 0.9530599318821521, 0.9535965185492802, 0.9537585579895718, 0.954270090727667, 0.9555024195967147, 0.9552695442275747, 0.9562697187038575, 0.9565018730966224, 0.9569905148348099, 0.958664621977667, 0.9595245665490033, 0.9591761547157622, 0.958757627930048, 0.9597112994301403, 0.9603848716085271, 0.9606649709302326, 0.9608033983942414, 0.960803758882429, 0.9620131967515688, 0.9627801353705242, 0.9628491688584349, 0.9632684166205242, 0.9637094739179586, 0.9641280007036729, 0.9636862224298633, 0.9649657752514765, 0.9647339813468992, 0.965198290132429, 0.9649418027870063, 0.9650348087393872, 0.9655932049418604, 0.9668258942990956, 0.9667325278585271, 0.9663830345607235, 0.9669879337393872, 0.9681032841915835, 0.9676625873823367, 0.9672208091085271, 0.968429526001292, 0.9683368805370985, 0.9688255222752861, 0.9690802071797711, 0.9691506826204319, 0.9696386033822444, 0.9703368690014765, 0.969266219084533, 0.9690108132036729, 0.9696854668466224, 0.9694525914774824, 0.9704302354420451, 0.9710565836678663, 0.9704985479535806, 0.9704760174418604, 0.9706852808347176, 0.9708938232511997, 0.971753767822536, 0.9714053559892949, 0.9718235222868217, 0.9715913678940569, 0.9715448649178663, 0.9711259776439645], "end": "2016-02-05 06:01:03.295000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0], "moving_var_accuracy_valid": [0.030173231394638907, 0.06160146036473975, 0.088586023083318, 0.11155048794231151, 0.12845491810244533, 0.13937609970324685, 0.14568006303341813, 0.14792377532558446, 0.1471572153336998, 0.14399053946492157, 0.1392263461363409, 0.13324331616869534, 0.1265051879285962, 0.1192691305427437, 0.11172629508658621, 0.1043425196615942, 0.09695294282625835, 0.08980080927781736, 0.08292105547280625, 0.07629988673514905, 0.07007778401926103, 0.06422279357694324, 0.05875000484619698, 0.05362575048829032, 0.04889514148725321, 0.04457395932852779, 0.04057992762698916, 0.03687455205434646, 0.03351736232071727, 0.03041277511455288, 0.02758015371976257, 0.025024734993552237, 0.022677489940417527, 0.020542345339427583, 0.018576516763564133, 0.01681185910152381, 0.015209264968904299, 0.013743396331469175, 0.012410295453925945, 0.011210643857679448, 0.010122046451853316, 0.009136076744479731, 0.008248709076693594, 0.0074424770867282265, 0.006713901438639481, 0.006056270944512128, 0.005463876027246309, 0.004930254690451299, 0.004442449176402593, 0.0040045101111352436, 0.0036089752218472658, 0.0032505946836437504, 0.0029286346771489977, 0.0026417657780385857, 0.002380509667124524, 0.002146418206817805, 0.0019357940667332936, 0.0017435958446572488, 0.0015699436694592925, 0.0014136385525149542, 0.0012726691577281941, 0.0011454797836483752, 0.001031514450636874, 0.0009293901517482949, 0.0008378164859133132, 0.0007542189425755124, 0.0006790840659804749, 0.0006112925269182924, 0.000550210526712302, 0.0004957711651307969, 0.0004463026008922499, 0.000401870510012073, 0.0003617519052966171, 0.00032560404100856195, 0.00029306669969860014, 0.0002638192989459649, 0.0002376616705817353, 0.0002140497260616498, 0.00019270559749313065, 0.00017382749357196839, 0.00015646469569361753, 0.0001408947877051772, 0.0001268657950961003, 0.00011428881599339128, 0.0001028916613597342, 9.260503995236128e-05, 8.392724195169997e-05, 7.575868887367214e-05, 6.836910491236701e-05, 6.171816648409129e-05, 5.559052181065393e-05, 5.006618564629435e-05, 4.527177366844453e-05, 4.0766228102818476e-05, 3.7049667807756174e-05, 3.334471059035072e-05, 3.0016498546248428e-05, 2.703974624875845e-05, 2.43383077946549e-05, 2.1934893140670614e-05, 1.9797385505695285e-05, 1.7853245980745812e-05, 1.6169926396268823e-05, 1.4669413734718016e-05, 1.3261110890666931e-05, 1.219602606435775e-05, 1.107076773633699e-05], "accuracy_test": 0.730695950255102, "start": "2016-02-03 15:24:17.485000", "learning_rate_per_epoch": [0.0029538366943597794, 0.0014769183471798897, 0.000984612270258367, 0.0007384591735899448, 0.0005907673621550202, 0.0004923061351291835, 0.0004219766706228256, 0.0003692295867949724, 0.0003282040706835687, 0.0002953836810775101, 0.0002685305953491479, 0.00024615306756459177, 0.0002272182027809322, 0.0002109883353114128, 0.00019692243949975818, 0.0001846147933974862, 0.0001737550919642672, 0.00016410203534178436, 0.0001554650953039527, 0.00014769184053875506, 0.00014065888535697013, 0.00013426529767457396, 0.00012842768046539277, 0.00012307653378229588, 0.00011815346806542948, 0.0001136091013904661, 0.0001094013568945229, 0.0001054941676557064, 0.00010185643623117357, 9.846121974987909e-05, 9.528505324851722e-05, 9.23073966987431e-05, 8.951020572567359e-05, 8.68775459821336e-05, 8.439533121418208e-05, 8.205101767089218e-05, 7.98334222054109e-05, 7.773254765197635e-05, 7.57394009269774e-05, 7.384592026937753e-05, 7.204480061773211e-05, 7.032944267848507e-05, 6.869387289043516e-05, 6.713264883728698e-05, 6.564081559190527e-05, 6.421384023269638e-05, 6.284759001573548e-05, 6.153826689114794e-05, 6.0282382037257776e-05, 5.907673403271474e-05, 5.791836520074867e-05, 5.680455069523305e-05, 5.573276575887576e-05, 5.470067844726145e-05, 5.3706120525021106e-05, 5.27470838278532e-05, 5.182169479667209e-05, 5.092821811558679e-05, 5.006502760807052e-05, 4.9230609874939546e-05, 4.8423553380416706e-05, 4.764252662425861e-05, 4.6886296331649646e-05, 4.615369834937155e-05, 4.54436412837822e-05, 4.475510286283679e-05, 4.408711538417265e-05, 4.34387729910668e-05, 4.280922803445719e-05, 4.219766560709104e-05, 4.160333264735527e-05, 4.102550883544609e-05, 4.046351750730537e-05, 3.991671110270545e-05, 3.938448935514316e-05, 3.8866273825988173e-05, 3.836151518044062e-05, 3.78697004634887e-05, 3.739033854799345e-05, 3.6922960134688765e-05, 3.6467117752181366e-05, 3.6022400308866054e-05, 3.558839307515882e-05, 3.516472133924253e-05, 3.4751021303236485e-05, 3.434693644521758e-05, 3.3952146623050794e-05, 3.356632441864349e-05, 3.318917515571229e-05, 3.282040779595263e-05, 3.245974221499637e-05, 3.210692011634819e-05, 3.17616832035128e-05, 3.142379500786774e-05, 3.109301906079054e-05, 3.076913344557397e-05, 3.045192534045782e-05, 3.0141191018628888e-05, 2.9836734029231593e-05, 2.953836701635737e-05, 2.9245908081065863e-05, 2.8959182600374334e-05, 2.867802686523646e-05, 2.8402275347616524e-05, 2.8131777071394026e-05, 2.786638287943788e-05, 2.7605950890574604e-05], "accuracy_train_first": 0.5754981225775194, "accuracy_train_last": 0.9711259776439645, "batch_size_eval": 1024, "accuracy_train_std": [0.01850763221918711, 0.021599027652540653, 0.019925007848302284, 0.017889091836876862, 0.01665918836651462, 0.015509073820468754, 0.012781187740583398, 0.013246980566407147, 0.012628725900355114, 0.012944169034690698, 0.012757036206721091, 0.013325190684368842, 0.011453644321974332, 0.011342594850277247, 0.011745074905477127, 0.010793190432034886, 0.01206612486890653, 0.010997959237937609, 0.011214949896025462, 0.011914562278382506, 0.010554474161738027, 0.009916388407013858, 0.0109494997062874, 0.009858772450031235, 0.009612281881693907, 0.010304424194866761, 0.009764657996453647, 0.00891426049477859, 0.00924311538919598, 0.009292803069460065, 0.008962299887787848, 0.009246448550544564, 0.008149274527550673, 0.008558946279254454, 0.008949489394190236, 0.008228659700596061, 0.008773083031173287, 0.008099095347482096, 0.009115414489891862, 0.009359540430024989, 0.008856353060155711, 0.00924333344636878, 0.008688024375193927, 0.008991197855193414, 0.008398588612966489, 0.008553932322633184, 0.008258578349706745, 0.007389779906664454, 0.00858164273711313, 0.007578294853977492, 0.007658509117876948, 0.007632973588267643, 0.0073751558349443, 0.007284932913752678, 0.007109389603837181, 0.006763339554477564, 0.007109137764287863, 0.0068911905926665835, 0.0065834097780462794, 0.007461685590426811, 0.006483485943263674, 0.006416216782255902, 0.007045176219750676, 0.006777966207813412, 0.006772988755065817, 0.006399315882711955, 0.005836685106197536, 0.0061850535304598, 0.00669473549557199, 0.006165207787431494, 0.00656571540064728, 0.006736208963093578, 0.006095925773373928, 0.0066156686421191815, 0.006831113710113312, 0.006782436328491211, 0.006432243480677408, 0.005997293118483998, 0.006810533214794189, 0.00615002357671361, 0.006283920781501187, 0.005883722836318628, 0.005573190484210079, 0.005171874489496221, 0.005584082888201741, 0.006078992920086478, 0.005683932417619864, 0.0051048324930996185, 0.005731875249662125, 0.005668327757982568, 0.004876208292145711, 0.005841372592731105, 0.005882170936742075, 0.005362515179350817, 0.00557346650461277, 0.00555844880523687, 0.004862193610141407, 0.00528653220876098, 0.005301485552771352, 0.005414336671603102, 0.005871088760581038, 0.0051424934336649865, 0.004731508918694585, 0.0052264321373143, 0.005640011080306194, 0.005425667604124379, 0.005404582660678764], "accuracy_test_std": 0.009911881312174459, "error_valid": [0.42098521037274095, 0.3234480892319277, 0.27337720020707834, 0.22491381541792166, 0.2017189853162651, 0.1903664462537651, 0.17861681099397586, 0.17323542215737953, 0.16744664203689763, 0.16451695453689763, 0.1597253270896084, 0.15718244070030118, 0.15397772731551207, 0.15216726280120485, 0.1522084431475903, 0.14566665097891573, 0.14639907285391573, 0.14383559629141573, 0.1423604574548193, 0.14359145566641573, 0.14115004941641573, 0.14053969785391573, 0.13968520566641573, 0.14079413356551207, 0.13919692441641573, 0.13514801393072284, 0.1349141683923193, 0.1368981786521084, 0.13265513224774095, 0.1347715079066265, 0.13378465032003017, 0.12967396931475905, 0.13084466773343373, 0.1298372199736446, 0.13304193335843373, 0.1291047980986446, 0.12848415144954817, 0.13034609139683728, 0.13120058358433728, 0.12902390813253017, 0.12932834855045183, 0.1293489387236446, 0.1276399543486446, 0.1286165168486446, 0.1283723762236446, 0.1278840949736446, 0.12688694230045183, 0.12588978962725905, 0.12899302287274095, 0.12747670368975905, 0.12761936417545183, 0.1289827277861446, 0.12787379988704817, 0.12499411709337349, 0.12664280167545183, 0.12513677757906627, 0.12442494587725905, 0.12652073136295183, 0.1272428581513554, 0.1269987175263554, 0.1273958137236446, 0.12835178605045183, 0.12664280167545183, 0.12555446394954817, 0.12469997176204817, 0.12677516707454817, 0.12627659073795183, 0.12674428181475905, 0.1284944465361446, 0.12530002823795183, 0.12648984610316272, 0.12896213761295183, 0.1267545769013554, 0.12698842243975905, 0.12697812735316272, 0.12662221150225905, 0.12893125235316272, 0.12881947712725905, 0.1284635612763554, 0.12563535391566272, 0.12798557511295183, 0.12848415144954817, 0.12847385636295183, 0.1266325065888554, 0.1282194206513554, 0.1278532097138554, 0.12515736775225905, 0.12586919945406627, 0.12872829207454817, 0.12887095256024095, 0.1268766472138554, 0.1281282355986446, 0.1291047980986446, 0.12723256306475905, 0.12967396931475905, 0.12786350480045183, 0.1276090690888554, 0.1283723762236446, 0.1277311394013554, 0.1284635612763554, 0.1271516730986446, 0.12723256306475905, 0.12673398672816272, 0.1288297722138554, 0.1269987175263554, 0.1260221550263554, 0.12653102644954817], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.025690483370139783, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.002953836676487469, "optimization": "adam", "nb_data_augmentation": 4, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.210058356644393e-05, "rotation_range": [0, 0], "momentum": 0.8948605597478041}, "accuracy_valid_max": 0.875575054122741, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8734689735504518, "accuracy_valid_std": [0.024063832298488064, 0.014558181688646176, 0.009760455600214364, 0.009669831593880312, 0.007532346924858265, 0.006403915925044235, 0.005637912040770771, 0.006965204775609355, 0.005972578579836966, 0.007900303217834076, 0.00601719534947737, 0.008786405635317612, 0.006989728507753816, 0.007957113465588424, 0.009898210240788168, 0.005367745313060991, 0.006846797947861033, 0.00717169373539729, 0.007881768918897383, 0.007919370057216876, 0.007577092242172043, 0.007507957451589413, 0.007180500094449592, 0.009834025749559043, 0.008349568852978257, 0.006933221977869774, 0.008553996957792059, 0.010152054001294012, 0.006860967281450617, 0.006577910427906756, 0.005974444143560937, 0.00737449917575339, 0.007389721664520954, 0.004835242651736251, 0.005305520794094494, 0.004435865444599524, 0.004925912807070347, 0.00545678814183107, 0.005545993022979184, 0.006039166845480284, 0.004761944696569148, 0.005543105943584236, 0.00431157234331386, 0.004987743681396181, 0.0056205560147699965, 0.005824973111926255, 0.004441886447268567, 0.0069381967657734965, 0.007048075539907647, 0.007389043134265011, 0.006420698560015469, 0.0048870031586414255, 0.004671949290099245, 0.00863809238083954, 0.004988350783036753, 0.007465148751473748, 0.007272630000289739, 0.0041858348247899866, 0.005717337656779993, 0.005555399883838868, 0.005841592794970086, 0.006915144509906759, 0.006928942611068081, 0.006521794206400725, 0.006565732317653114, 0.007066966159590539, 0.007725864735428483, 0.007963136722173829, 0.008045313060221477, 0.0071230776633646594, 0.008000337602089624, 0.0073784566668885524, 0.008376406792419935, 0.006768378030908569, 0.008391560464099687, 0.006444405185994656, 0.007887609897246834, 0.008156494504039316, 0.007578375609203795, 0.009500488035794815, 0.00647414809738052, 0.0064366726402833455, 0.009076958427086586, 0.007986408678988194, 0.0077470664496489434, 0.007202061791146556, 0.007260869961481305, 0.009012085774003043, 0.006558030521112567, 0.008455587053908652, 0.006136255144665788, 0.006768596460177402, 0.00672217897756816, 0.007089714694501657, 0.008911632224169233, 0.007847475367164698, 0.007519382841146813, 0.006394448725271642, 0.006834480382377856, 0.006763820407003607, 0.005208473957673806, 0.0067096084616912145, 0.008925761320082546, 0.007966376624063902, 0.007100196571622862, 0.005916471213242014, 0.007354389638954654], "accuracy_valid": [0.579014789627259, 0.6765519107680723, 0.7266227997929217, 0.7750861845820783, 0.7982810146837349, 0.8096335537462349, 0.8213831890060241, 0.8267645778426205, 0.8325533579631024, 0.8354830454631024, 0.8402746729103916, 0.8428175592996988, 0.8460222726844879, 0.8478327371987951, 0.8477915568524097, 0.8543333490210843, 0.8536009271460843, 0.8561644037085843, 0.8576395425451807, 0.8564085443335843, 0.8588499505835843, 0.8594603021460843, 0.8603147943335843, 0.8592058664344879, 0.8608030755835843, 0.8648519860692772, 0.8650858316076807, 0.8631018213478916, 0.867344867752259, 0.8652284920933735, 0.8662153496799698, 0.870326030685241, 0.8691553322665663, 0.8701627800263554, 0.8669580666415663, 0.8708952019013554, 0.8715158485504518, 0.8696539086031627, 0.8687994164156627, 0.8709760918674698, 0.8706716514495482, 0.8706510612763554, 0.8723600456513554, 0.8713834831513554, 0.8716276237763554, 0.8721159050263554, 0.8731130576995482, 0.874110210372741, 0.871006977127259, 0.872523296310241, 0.8723806358245482, 0.8710172722138554, 0.8721262001129518, 0.8750058829066265, 0.8733571983245482, 0.8748632224209337, 0.875575054122741, 0.8734792686370482, 0.8727571418486446, 0.8730012824736446, 0.8726041862763554, 0.8716482139495482, 0.8733571983245482, 0.8744455360504518, 0.8753000282379518, 0.8732248329254518, 0.8737234092620482, 0.873255718185241, 0.8715055534638554, 0.8746999717620482, 0.8735101538968373, 0.8710378623870482, 0.8732454230986446, 0.873011577560241, 0.8730218726468373, 0.873377788497741, 0.8710687476468373, 0.871180522872741, 0.8715364387236446, 0.8743646460843373, 0.8720144248870482, 0.8715158485504518, 0.8715261436370482, 0.8733674934111446, 0.8717805793486446, 0.8721467902861446, 0.874842632247741, 0.8741308005459337, 0.8712717079254518, 0.871129047439759, 0.8731233527861446, 0.8718717644013554, 0.8708952019013554, 0.872767436935241, 0.870326030685241, 0.8721364951995482, 0.8723909309111446, 0.8716276237763554, 0.8722688605986446, 0.8715364387236446, 0.8728483269013554, 0.872767436935241, 0.8732660132718373, 0.8711702277861446, 0.8730012824736446, 0.8739778449736446, 0.8734689735504518], "seed": 275490178, "model": "residualv3", "loss_std": [0.3110922574996948, 0.26374685764312744, 0.2572091519832611, 0.2475699633359909, 0.24013903737068176, 0.23455703258514404, 0.223903089761734, 0.21881692111492157, 0.21390214562416077, 0.20799346268177032, 0.2055436670780182, 0.2001728117465973, 0.19734027981758118, 0.19443240761756897, 0.18992166221141815, 0.18856550753116608, 0.18464820086956024, 0.18194690346717834, 0.17923399806022644, 0.17740501463413239, 0.17582251131534576, 0.17158696055412292, 0.17015548050403595, 0.16792839765548706, 0.16582396626472473, 0.16546258330345154, 0.16214264929294586, 0.1592452973127365, 0.15763255953788757, 0.1557907611131668, 0.15451493859291077, 0.15158601105213165, 0.150691419839859, 0.14906206727027893, 0.14885812997817993, 0.1477627158164978, 0.14550721645355225, 0.14407141506671906, 0.14450649917125702, 0.1423862874507904, 0.14136290550231934, 0.13795138895511627, 0.13799992203712463, 0.13670603930950165, 0.1343344897031784, 0.13506624102592468, 0.1324177086353302, 0.1339941769838333, 0.13123160600662231, 0.12972676753997803, 0.13080990314483643, 0.12725871801376343, 0.12771975994110107, 0.12706753611564636, 0.1236431673169136, 0.12466483563184738, 0.1229953020811081, 0.12179194390773773, 0.12085893005132675, 0.12072496116161346, 0.12083032727241516, 0.12039598077535629, 0.11989106982946396, 0.11728809773921967, 0.11847234517335892, 0.11571958661079407, 0.11433570086956024, 0.1149822324514389, 0.11288962513208389, 0.1118059828877449, 0.11366083472967148, 0.11181402206420898, 0.11080711334943771, 0.11114207655191422, 0.11130029708147049, 0.10879826545715332, 0.10772055387496948, 0.10952889919281006, 0.10791831463575363, 0.10760095715522766, 0.10680406540632248, 0.10539548844099045, 0.10319127142429352, 0.10479365289211273, 0.10275665670633316, 0.10312037914991379, 0.1017659604549408, 0.10402059555053711, 0.09918062388896942, 0.1009879931807518, 0.10011175274848938, 0.09995507448911667, 0.09837080538272858, 0.0996728464961052, 0.09909332543611526, 0.09818801283836365, 0.09907266497612, 0.09708841890096664, 0.09643076360225677, 0.09660869091749191, 0.09619784355163574, 0.09379026293754578, 0.09396445751190186, 0.09350965172052383, 0.09385228157043457, 0.09453748911619186, 0.09366653114557266]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:38 2016", "state": "available"}], "summary": "5ac0d48b7603ada7cc9f239c3064b640"}