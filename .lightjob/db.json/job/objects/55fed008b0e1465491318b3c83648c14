{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 32, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.12408658836415162, 0.11657139297165664, 0.11262590191914734, 0.11007028376847594, 0.10790556970281383, 0.105293580377244, 0.10361664453005916, 0.09905074580483356, 0.09978767955004682, 0.09849252588142725, 0.09681264695001092, 0.093121247922242, 0.09350226829397074, 0.09815723297849338, 0.09772028020854637, 0.09682913321564177, 0.0963406305460842, 0.09502646226172187, 0.09403482905643032, 0.09588070699107286, 0.09738754786586665, 0.09353049383998395, 0.09459050806393685, 0.09631646905023937, 0.09589642357720174, 0.09489997391819938, 0.09462255570941282, 0.09463574864431408, 0.09276202515211793, 0.09697269374798141, 0.09421871591806887, 0.09481226280401193, 0.09453194593800163, 0.09587707972071212, 0.09270048023655202, 0.09258882580547623, 0.0956840784463809, 0.09183325837818757, 0.0915785891680462, 0.09230832603553432, 0.09052004992193621, 0.08916490826438128, 0.09381808044988006, 0.08935243025206277, 0.0923284175896501, 0.09165801216292628, 0.09383129145964865, 0.09250056207417487, 0.09123714255602429, 0.09158043926120978, 0.09195126135840145, 0.09095747439442274, 0.09270538609522887, 0.08940670514591906, 0.08979723099267231, 0.09044554343045311, 0.09068658073484764, 0.08989003412623799, 0.09148223528836813, 0.09068176233403792, 0.08996232437505045, 0.08973196324642184, 0.0906132937089068, 0.08963252994624968, 0.09182480991786024, 0.09071332312250403, 0.09132506504202975, 0.08973196324642184, 0.0905448717948718, 0.08945566395463067, 0.08971138959872589, 0.089775679009371, 0.08963889698110314, 0.09150845280899275, 0.0909595331951819, 0.0900613935626895, 0.08888443854814146, 0.08908856778920202, 0.08884590509121658, 0.09011355920112053, 0.08861888077824817, 0.09015956264045188, 0.08994448033277673, 0.09066051898402468, 0.09052359632049276, 0.0896492424482883, 0.0893431483395621, 0.08931689426926263, 0.08996143225699761, 0.08970333775647116, 0.09076796319555318, 0.09010643398804795, 0.09057520040179465, 0.09079978865063872, 0.09066730559520138, 0.0899789756217997, 0.08958157737602924, 0.08950110884204786, 0.08975630760516334, 0.09013888873060158, 0.09056338526913754, 0.09048300145196822, 0.08970025599597416, 0.09059173899871827, 0.09000424394904302, 0.09044159957834838, 0.09003515084414702, 0.08928454023835974, 0.09010326704582194, 0.08963252994624966, 0.0897074134706531, 0.09049719201975115, 0.09007169050271456, 0.09018952644833877, 0.09009693282741219, 0.09026789950001408, 0.08933446438705057, 0.09048300145196822, 0.09005416519961054, 0.09077346468910856, 0.08950110884204786, 0.08968743075867905, 0.08983495933810097, 0.091140525541267, 0.09016431004607742, 0.09081000186413285, 0.09028202510491208, 0.09004624303944832, 0.09003485371298067, 0.09052270973387895], "moving_avg_accuracy_train": [0.05679593373493975, 0.12003341490963852, 0.1827759318524096, 0.24294336878765055, 0.3000284558546686, 0.35336757186558726, 0.4022693312452936, 0.4477351692051016, 0.4903863510797722, 0.5297183861525179, 0.5658466981396757, 0.5989575290787201, 0.6296161812913301, 0.6576184185838838, 0.6838017009724834, 0.7079149420198134, 0.7301204357696394, 0.7505571873733984, 0.7697503391179862, 0.7875089271941393, 0.8035151880891832, 0.8185326451838794, 0.8321495425630818, 0.8445412336380989, 0.8559737819610359, 0.8668301876504745, 0.8764644693372342, 0.8854577061384505, 0.8935869166993042, 0.9012467679510606, 0.908455957872822, 0.9150148236819253, 0.9209013307715641, 0.9263003731462149, 0.931110094867738, 0.9357282834231329, 0.9397434633639521, 0.9437806945877978, 0.9475671582615481, 0.9510808678872006, 0.9542408533876371, 0.9570307175970661, 0.9596945509578414, 0.9622426033921777, 0.9644111329625985, 0.9664616424073025, 0.9682459186786204, 0.9699764849432885, 0.9715645856959476, 0.9731185939938227, 0.974613681130585, 0.975935727927165, 0.9771349826946893, 0.9781954866842565, 0.9791593529254693, 0.9801609628136452, 0.9810600585503529, 0.9818010029965225, 0.9825007972751836, 0.9831823817042917, 0.983784041877236, 0.9843796587738497, 0.9849321861193563, 0.9854341670556135, 0.9858859498982449, 0.9863066734325168, 0.9866617929868554, 0.987077880254435, 0.9874100018675458, 0.9877159708072972, 0.9880195808048807, 0.988238707061742, 0.9884429801808691, 0.9886597702651918, 0.9888784129675883, 0.9890681319117933, 0.9892153473350718, 0.9893831386557814, 0.9895482698203237, 0.9896874752178094, 0.9898292322141007, 0.9899426945348593, 0.9900706954126987, 0.9901670709015493, 0.9902585151668162, 0.9903314023549538, 0.9904017071495789, 0.9904673346273921, 0.9905358120080264, 0.9906139137891514, 0.9906865585548146, 0.9907095819162006, 0.9907679535438577, 0.9907969563822431, 0.9908230589367899, 0.9908842018382916, 0.9909274646363901, 0.9909311037149198, 0.9909461446988495, 0.9909620347470368, 0.9910210458807668, 0.9910623900878708, 0.991090187223662, 0.9911246172964765, 0.9911720765005638, 0.9912100834589411, 0.9912207580949747, 0.9912185994541518, 0.9912143035147608, 0.9912551472596703, 0.9912660218409323, 0.991264043150815, 0.9912669686550106, 0.9912625421208349, 0.9912538519147756, 0.9912695623558281, 0.9912413448250645, 0.9912606591377388, 0.9912945141576999, 0.9912755672600021, 0.9912373365882188, 0.9912264606101198, 0.9912354975310355, 0.9912200991333535, 0.9912415380151989, 0.9912584798462092, 0.9912925527953232, 0.9913067463109716, 0.9912936356858986, 0.9912912487739353], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234423, "moving_var_accuracy_train": [0.029032002799413013, 0.06211961374735517, 0.09133726326344489, 0.11478462114330619, 0.13263452351801688, 0.14497662283765136, 0.15200139918776248, 0.1554055410614739, 0.15623709679308254, 0.15453646796044754, 0.1508301155077755, 0.14561404808626383, 0.13951221987708137, 0.13261812552986962, 0.125526391466653, 0.11820678786424746, 0.11082386465188472, 0.10350042553171994, 0.09646577664356493, 0.08965750603333485, 0.08299755892056326, 0.0767275191868263, 0.07072354631626525, 0.06503317775392667, 0.059706188428940626, 0.05479632348648963, 0.05015206559041918, 0.04586476380484203, 0.04187304400344209, 0.03821379949388918, 0.0348601713184525, 0.03176132267292368, 0.02889704909707862, 0.026269691114440236, 0.02385092281034263, 0.02165777951910699, 0.019637096596810708, 0.017820080060722793, 0.016167107819024192, 0.014661512435122396, 0.013285230766876882, 0.012026757770952674, 0.010887946067423222, 0.009857584601554047, 0.008914148825878746, 0.008060575244136255, 0.007283170496034124, 0.0065818071827983765, 0.005946325040523906, 0.005373427012580298, 0.004856201881240877, 0.004386311962707918, 0.003960624674413998, 0.00357468422537959, 0.00322557714602018, 0.002912048432730989, 0.002628118947751785, 0.0023702480410273917, 0.002137630645216673, 0.0019280485967010299, 0.0017385016917042934, 0.0015678443579216503, 0.0014138075003372783, 0.0012746946140468408, 0.001149062122274222, 0.001035748984677412, 0.0009333090752905342, 0.0008415363252896569, 0.0007583754356537486, 0.0006833804450172074, 0.0006158720117911807, 0.0005547169574600772, 0.0004996208092788504, 0.00045008170981691127, 0.0004055037805170212, 0.00036527734196543156, 0.00032894465919654806, 0.0002963035786226428, 0.0002669186354739065, 0.00024040117521071823, 0.0002165419131036243, 0.00019500358507734924, 0.00017565068459216316, 0.0001581692102466076, 0.00014242754770479844, 0.00012823260581407012, 0.00011545383010998862, 0.00010394720979158685, 9.359469117735496e-05, 8.429012105355365e-05, 7.59086043060026e-05, 6.832251455192794e-05, 6.152092831897315e-05, 5.537640596878548e-05, 4.9844897462091736e-05, 4.4894053805519015e-05, 4.0421493452260964e-05, 3.6379463293067774e-05, 3.274355304453916e-05, 2.947147018276781e-05, 2.6555663989627996e-05, 2.3915481681814756e-05, 2.1530887640457043e-05, 1.938846774563746e-05, 1.7469892355547103e-05, 1.5735903879958278e-05, 1.41633390226525e-05, 1.2747047057959067e-05, 1.1472508448020428e-05, 1.0340271506702443e-05, 9.307308664690789e-06, 8.376613035152934e-06, 7.539028758810828e-06, 6.785302230773028e-06, 6.107451684827918e-06, 5.498927877967722e-06, 4.956201151552466e-06, 4.463938420463936e-06, 4.027860039806614e-06, 3.6283049002172564e-06, 3.278628668580585e-06, 2.9518303838190136e-06, 2.6573823388938487e-06, 2.393778100864979e-06, 2.1585369216714995e-06, 1.9452664602461615e-06, 1.7611885069734812e-06, 1.5868827592542855e-06, 1.4297414797371157e-06, 1.2868186079018912e-06], "duration": 130278.959972, "accuracy_train": [0.5679593373493976, 0.6891707454819277, 0.7474585843373494, 0.7844503012048193, 0.8137942394578314, 0.8334196159638554, 0.8423851656626506, 0.8569277108433735, 0.8742469879518072, 0.8837067018072289, 0.8910015060240963, 0.8969550075301205, 0.9055440512048193, 0.9096385542168675, 0.9194512424698795, 0.9249341114457831, 0.9299698795180723, 0.9344879518072289, 0.9424887048192772, 0.9473362198795181, 0.9475715361445783, 0.9536897590361446, 0.9547016189759037, 0.956066453313253, 0.9588667168674698, 0.9645378388554217, 0.9631730045180723, 0.9663968373493976, 0.9667498117469879, 0.9701854292168675, 0.9733386671686747, 0.9740446159638554, 0.9738798945783133, 0.9748917545180723, 0.9743975903614458, 0.9772919804216867, 0.9758800828313253, 0.9801157756024096, 0.9816453313253012, 0.9827042545180723, 0.9826807228915663, 0.9821394954819277, 0.9836690512048193, 0.9851750753012049, 0.9839278990963856, 0.9849162274096386, 0.9843044051204819, 0.9855515813253012, 0.9858574924698795, 0.9871046686746988, 0.9880694653614458, 0.9878341490963856, 0.9879282756024096, 0.9877400225903614, 0.9878341490963856, 0.9891754518072289, 0.9891519201807228, 0.9884695030120482, 0.9887989457831325, 0.9893166415662651, 0.9891989834337349, 0.9897402108433735, 0.9899049322289156, 0.9899519954819277, 0.9899519954819277, 0.9900931852409639, 0.9898578689759037, 0.9908226656626506, 0.9903990963855421, 0.9904696912650602, 0.9907520707831325, 0.990210843373494, 0.9902814382530121, 0.9906108810240963, 0.9908461972891566, 0.9907756024096386, 0.9905402861445783, 0.9908932605421686, 0.9910344503012049, 0.9909403237951807, 0.9911050451807228, 0.9909638554216867, 0.991222703313253, 0.9910344503012049, 0.9910815135542169, 0.9909873870481928, 0.9910344503012049, 0.9910579819277109, 0.9911521084337349, 0.9913168298192772, 0.9913403614457831, 0.9909167921686747, 0.9912932981927711, 0.9910579819277109, 0.9910579819277109, 0.9914344879518072, 0.9913168298192772, 0.9909638554216867, 0.9910815135542169, 0.9911050451807228, 0.9915521460843374, 0.9914344879518072, 0.9913403614457831, 0.9914344879518072, 0.9915992093373494, 0.9915521460843374, 0.9913168298192772, 0.991199171686747, 0.991175640060241, 0.9916227409638554, 0.9913638930722891, 0.991246234939759, 0.9912932981927711, 0.991222703313253, 0.991175640060241, 0.9914109563253012, 0.9909873870481928, 0.9914344879518072, 0.9915992093373494, 0.9911050451807228, 0.9908932605421686, 0.9911285768072289, 0.9913168298192772, 0.9910815135542169, 0.9914344879518072, 0.9914109563253012, 0.9915992093373494, 0.9914344879518072, 0.991175640060241, 0.9912697665662651], "end": "2016-01-22 04:21:09.596000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0], "accuracy_valid": [0.5610309829059829, 0.6742788461538461, 0.7314369658119658, 0.7593482905982906, 0.7811164529914529, 0.7923344017094017, 0.7996794871794872, 0.8056891025641025, 0.8123664529914529, 0.8181089743589743, 0.8221153846153846, 0.8237179487179487, 0.8297275641025641, 0.8262553418803419, 0.8293269230769231, 0.8315972222222222, 0.8333333333333334, 0.8372061965811965, 0.8382745726495726, 0.8382745726495726, 0.8362713675213675, 0.8388087606837606, 0.8408119658119658, 0.8401442307692307, 0.8405448717948718, 0.843482905982906, 0.8421474358974359, 0.843482905982906, 0.844551282051282, 0.8420138888888888, 0.8472222222222222, 0.8438835470085471, 0.8454861111111112, 0.8474893162393162, 0.8472222222222222, 0.8477564102564102, 0.8446848290598291, 0.8484241452991453, 0.8520299145299145, 0.8496260683760684, 0.8525641025641025, 0.8521634615384616, 0.8530982905982906, 0.8530982905982906, 0.8517628205128205, 0.8530982905982906, 0.8529647435897436, 0.8528311965811965, 0.8565705128205128, 0.8553685897435898, 0.8545673076923077, 0.8572382478632479, 0.8543002136752137, 0.8557692307692307, 0.8581730769230769, 0.8581730769230769, 0.8565705128205128, 0.8556356837606838, 0.858573717948718, 0.8572382478632479, 0.8571047008547008, 0.858573717948718, 0.858840811965812, 0.8580395299145299, 0.8587072649572649, 0.8576388888888888, 0.8589743589743589, 0.858573717948718, 0.8597756410256411, 0.859642094017094, 0.8597756410256411, 0.8583066239316239, 0.860176282051282, 0.8595085470085471, 0.8595085470085471, 0.8613782051282052, 0.8607104700854701, 0.8611111111111112, 0.859909188034188, 0.8620459401709402, 0.8611111111111112, 0.8576388888888888, 0.8595085470085471, 0.859642094017094, 0.8613782051282052, 0.8620459401709402, 0.8612446581196581, 0.8613782051282052, 0.8604433760683761, 0.8617788461538461, 0.8605769230769231, 0.859642094017094, 0.8611111111111112, 0.8613782051282052, 0.8611111111111112, 0.8608440170940171, 0.8609775641025641, 0.858840811965812, 0.8600427350427351, 0.8609775641025641, 0.8616452991452992, 0.8611111111111112, 0.8608440170940171, 0.8595085470085471, 0.8607104700854701, 0.8608440170940171, 0.8615117521367521, 0.8605769230769231, 0.860176282051282, 0.8607104700854701, 0.8603098290598291, 0.8600427350427351, 0.8608440170940171, 0.8607104700854701, 0.8607104700854701, 0.8597756410256411, 0.8603098290598291, 0.8611111111111112, 0.8604433760683761, 0.8600427350427351, 0.859909188034188, 0.859909188034188, 0.8611111111111112, 0.8600427350427351, 0.8608440170940171, 0.8608440170940171, 0.8607104700854701, 0.8609775641025641, 0.8600427350427351, 0.859375], "accuracy_test": 0.8586, "start": "2016-01-20 16:09:50.636000", "learning_rate_per_epoch": [0.00193887110799551, 0.001832545269280672, 0.001732050208374858, 0.0016370662488043308, 0.0015472911763936281, 0.0014624391915276647, 0.0013822404434904456, 0.0013064397498965263, 0.0012347958981990814, 0.001167080830782652, 0.0011030792957171798, 0.001042587449774146, 0.0009854129748418927, 0.0009313738555647433, 0.0008802982047200203, 0.0008320235065184534, 0.0007863961509428918, 0.0007432709680870175, 0.0007025107042863965, 0.0006639856728725135, 0.0006275733467191458, 0.0005931578343734145, 0.0005606296472251415, 0.0005298852338455617, 0.0005008268635720015, 0.0004733620153274387, 0.00044740329030901194, 0.0004228681209497154, 0.00039967845077626407, 0.00037776047247461975, 0.00035704445326700807, 0.00033746447297744453, 0.0003189582494087517, 0.0003014668764080852, 0.0002849347365554422, 0.00026930918102152646, 0.0002545405295677483, 0.00024058177950792015, 0.00022738850384484977, 0.00021491873485501856, 0.000203132804017514, 0.00019199319649487734, 0.00018146447837352753, 0.00017151313659269363, 0.00016210752073675394, 0.00015321769751608372, 0.00014481539255939424, 0.00013687385944649577, 0.00012936783605255187, 0.00012227342813275754, 0.00011556807294255123, 0.00010923043009825051, 0.00010324033792130649, 9.757874067872763e-05, 9.222761582350358e-05, 8.716994489077479e-05, 8.238962618634105e-05, 7.787146023474634e-05, 7.360106246778741e-05, 6.95648486725986e-05, 6.574997678399086e-05, 6.214431050466374e-05, 5.873637564945966e-05, 5.55153310415335e-05, 5.247092485660687e-05, 4.959346915711649e-05, 4.687381078838371e-05, 4.430329499882646e-05, 4.18737436120864e-05, 3.957742956117727e-05, 3.740704050869681e-05, 3.535567520884797e-05, 3.341680348967202e-05, 3.158425897709094e-05, 2.9852208172087558e-05, 2.8215141355758533e-05, 2.6667850761441514e-05, 2.5205412384821102e-05, 2.3823171432013623e-05, 2.2516731405630708e-05, 2.1281935914885253e-05, 2.01148559426656e-05, 1.9011777112609707e-05, 1.7969188775168732e-05, 1.6983776731649414e-05, 1.6052403225330636e-05, 1.5172105122474022e-05, 1.4340081179398112e-05, 1.3553684766520746e-05, 1.2810413863917347e-05, 1.2107902875868604e-05, 1.144391717389226e-05, 1.0816344001796097e-05, 1.0223186109215021e-05, 9.662556294642854e-06, 9.13267103896942e-06, 8.63184413901763e-06, 8.15848216006998e-06, 7.711078978900332e-06, 7.288210781553062e-06, 6.888532425364247e-06, 6.510771981993457e-06, 6.153727554192301e-06, 5.816263183078263e-06, 5.49730475540855e-06, 5.195837729843333e-06, 4.9109030442195944e-06, 4.641593932319665e-06, 4.387053195387125e-06, 4.146471383137396e-06, 3.919082701031584e-06, 3.7041641007817816e-06, 3.501031414998579e-06, 3.309038220322691e-06, 3.1275737910618773e-06, 2.956060598080512e-06, 2.793953171931207e-06, 2.6407353743707063e-06, 2.4959199436125346e-06, 2.359046220590244e-06, 2.22967832996801e-06, 2.107404952766956e-06, 1.9918368252547225e-06, 1.882606397884956e-06, 1.7793661299947416e-06, 1.6817874666230637e-06, 1.5895598153292667e-06, 1.502389864072029e-06, 1.4200002169673098e-06, 1.3421288258541608e-06], "accuracy_train_last": 0.9912697665662651, "error_valid": [0.43896901709401714, 0.32572115384615385, 0.2685630341880342, 0.24065170940170943, 0.21888354700854706, 0.20766559829059827, 0.20032051282051277, 0.19431089743589747, 0.18763354700854706, 0.18189102564102566, 0.17788461538461542, 0.17628205128205132, 0.1702724358974359, 0.1737446581196581, 0.17067307692307687, 0.1684027777777778, 0.16666666666666663, 0.16279380341880345, 0.1617254273504274, 0.1617254273504274, 0.16372863247863245, 0.16119123931623935, 0.15918803418803418, 0.15985576923076927, 0.1594551282051282, 0.15651709401709402, 0.1578525641025641, 0.15651709401709402, 0.15544871794871795, 0.15798611111111116, 0.1527777777777778, 0.15611645299145294, 0.15451388888888884, 0.15251068376068377, 0.1527777777777778, 0.15224358974358976, 0.1553151709401709, 0.15157585470085466, 0.1479700854700855, 0.15037393162393164, 0.14743589743589747, 0.14783653846153844, 0.14690170940170943, 0.14690170940170943, 0.14823717948717952, 0.14690170940170943, 0.1470352564102564, 0.14716880341880345, 0.14342948717948723, 0.14463141025641024, 0.1454326923076923, 0.14276175213675213, 0.1456997863247863, 0.14423076923076927, 0.14182692307692313, 0.14182692307692313, 0.14342948717948723, 0.14436431623931623, 0.14142628205128205, 0.14276175213675213, 0.1428952991452992, 0.14142628205128205, 0.14115918803418803, 0.14196047008547008, 0.1412927350427351, 0.14236111111111116, 0.14102564102564108, 0.14142628205128205, 0.14022435897435892, 0.14035790598290598, 0.14022435897435892, 0.14169337606837606, 0.13982371794871795, 0.14049145299145294, 0.14049145299145294, 0.13862179487179482, 0.13928952991452992, 0.13888888888888884, 0.14009081196581197, 0.13795405982905984, 0.13888888888888884, 0.14236111111111116, 0.14049145299145294, 0.14035790598290598, 0.13862179487179482, 0.13795405982905984, 0.1387553418803419, 0.13862179487179482, 0.13955662393162394, 0.13822115384615385, 0.13942307692307687, 0.14035790598290598, 0.13888888888888884, 0.13862179487179482, 0.13888888888888884, 0.13915598290598286, 0.1390224358974359, 0.14115918803418803, 0.1399572649572649, 0.1390224358974359, 0.1383547008547008, 0.13888888888888884, 0.13915598290598286, 0.14049145299145294, 0.13928952991452992, 0.13915598290598286, 0.13848824786324787, 0.13942307692307687, 0.13982371794871795, 0.13928952991452992, 0.1396901709401709, 0.1399572649572649, 0.13915598290598286, 0.13928952991452992, 0.13928952991452992, 0.14022435897435892, 0.1396901709401709, 0.13888888888888884, 0.13955662393162394, 0.1399572649572649, 0.14009081196581197, 0.14009081196581197, 0.13888888888888884, 0.1399572649572649, 0.13915598290598286, 0.13915598290598286, 0.13928952991452992, 0.1390224358974359, 0.1399572649572649, 0.140625], "accuracy_train_std": [0.124727530010953, 0.11734804535586371, 0.10918394749994431, 0.10229503077223454, 0.09707838114917797, 0.09424629696156224, 0.09045031976008416, 0.08697879973934516, 0.08178201395901562, 0.0786172025763392, 0.07772775147034362, 0.07609616833234378, 0.07170189696141134, 0.07268516811685016, 0.06797275996937199, 0.06634267917702125, 0.06398193425122167, 0.061716561523523276, 0.059176983252075994, 0.05652198719093211, 0.056558595961695615, 0.05341245885639899, 0.05277572027211341, 0.05217041202092486, 0.05043556076642205, 0.046685365196088006, 0.04812667519547202, 0.046278759104091245, 0.04507199729015342, 0.04292121585385252, 0.0405438862088519, 0.03968701421772843, 0.04047896684596331, 0.03978717917925507, 0.03997086262080759, 0.03789459222863224, 0.03883467747828655, 0.035856076035037196, 0.03406326282504091, 0.03307015772641066, 0.03325744013264134, 0.03408847490843219, 0.031778779860167576, 0.03011309061937941, 0.031562911948146435, 0.0310682977159412, 0.03184432517901677, 0.030198293166171043, 0.029382390193812506, 0.028773627680398997, 0.027660453546115474, 0.02798145895831026, 0.027705499884204978, 0.027622711808948316, 0.028295066094906365, 0.02641234037705596, 0.026318996030164174, 0.027106758688940896, 0.027082111289570374, 0.026302548553470438, 0.026560765090439028, 0.025276954221475762, 0.025545496306523616, 0.02585009954759288, 0.025274752491130974, 0.025388422222078492, 0.02552684769295769, 0.024577970008967633, 0.024892857441304712, 0.02483128545311499, 0.024819597507023606, 0.025692915639291648, 0.0251706029447072, 0.02458773474981807, 0.02443674155752008, 0.024500390018995327, 0.024828665065892402, 0.024454319423473033, 0.024325727135622213, 0.02447179415330849, 0.024139320086633487, 0.02451051325114427, 0.0239685412939203, 0.02408267400899939, 0.02403903939487663, 0.02418702096647547, 0.02408267400899939, 0.02399967503218066, 0.02397326532564524, 0.02394144964668794, 0.02398065558610182, 0.024553084479977307, 0.02371684449111855, 0.0239383154866215, 0.02387679825738304, 0.02364412116577104, 0.023632302990157963, 0.02426931079741464, 0.023916364865964154, 0.02395584300969573, 0.023593093196932473, 0.02370624219514588, 0.023857680853610442, 0.02364412116577104, 0.02348497587810842, 0.023530673455850942, 0.02394144964668794, 0.023805959266600743, 0.02395125425816189, 0.023462058309912187, 0.023835377196069877, 0.023823305246596538, 0.02390215623234982, 0.023907101841191658, 0.02395125425816189, 0.023728737241441943, 0.02406510080191267, 0.023768200864351684, 0.023296345894916123, 0.024260867273686912, 0.0243941034914587, 0.024056447494316603, 0.023756444415045257, 0.023977780583598177, 0.02364412116577104, 0.023728737241441943, 0.023359391802202995, 0.023581836493123454, 0.02419562757590987, 0.02410804512407725], "accuracy_test_std": 0.08649300549755454, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.9053883142998653, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0020513659529436787, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "nesterov_momentum", "nb_data_augmentation": 1, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 6.022206917720906e-10, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.054839032037360194}, "accuracy_valid_max": 0.8620459401709402, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234423\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.859375, "loss_train": [1.5950196981430054, 1.1599458456039429, 0.9497914910316467, 0.8201791048049927, 0.7289673089981079, 0.6604974269866943, 0.5998007655143738, 0.5566582083702087, 0.5150361657142639, 0.4799547791481018, 0.448211669921875, 0.4174930453300476, 0.3925321698188782, 0.36967915296554565, 0.34680473804473877, 0.3298892080783844, 0.3089894652366638, 0.29376867413520813, 0.27871477603912354, 0.26447999477386475, 0.2507294714450836, 0.24087637662887573, 0.22789043188095093, 0.22056743502616882, 0.2096346765756607, 0.20166067779064178, 0.1928471326828003, 0.18645456433296204, 0.1831493228673935, 0.17329221963882446, 0.17099742591381073, 0.16507749259471893, 0.15901339054107666, 0.15422514081001282, 0.15224899351596832, 0.14995327591896057, 0.143336221575737, 0.13876183331012726, 0.13773813843727112, 0.1337382197380066, 0.13072262704372406, 0.12980757653713226, 0.12561854720115662, 0.12292107194662094, 0.12383007258176804, 0.12014388293027878, 0.1178412213921547, 0.11748147755861282, 0.11292892694473267, 0.11330156028270721, 0.1114971935749054, 0.1095774918794632, 0.10988567024469376, 0.10711249709129333, 0.10571107268333435, 0.10528755187988281, 0.10331287980079651, 0.10408245772123337, 0.10353535413742065, 0.098604716360569, 0.10058093070983887, 0.09946002811193466, 0.09875147044658661, 0.09729194641113281, 0.09757876396179199, 0.09670022130012512, 0.09490296244621277, 0.09513682126998901, 0.09511124342679977, 0.09559033811092377, 0.09533542394638062, 0.09305208921432495, 0.09289226680994034, 0.09166056662797928, 0.0927438735961914, 0.09151584655046463, 0.09075795859098434, 0.09044454246759415, 0.09050575643777847, 0.08920319378376007, 0.09023571759462357, 0.09145821630954742, 0.0907287672162056, 0.09002283215522766, 0.08807677775621414, 0.09061118215322495, 0.08859919011592865, 0.08899620920419693, 0.08832841366529465, 0.08944114297628403, 0.08869733661413193, 0.08847610652446747, 0.08742455393075943, 0.08653514832258224, 0.08656512945890427, 0.08682933449745178, 0.0864439457654953, 0.08623826503753662, 0.08720505237579346, 0.08512943238019943, 0.08719334751367569, 0.086851567029953, 0.08516688644886017, 0.0867236778140068, 0.08679337054491043, 0.08603688329458237, 0.08735183626413345, 0.08671097457408905, 0.08726326376199722, 0.08458075672388077, 0.08735678344964981, 0.08562988042831421, 0.08450374007225037, 0.08753778040409088, 0.08566844463348389, 0.08601630479097366, 0.08545872569084167, 0.08700535446405411, 0.08654583245515823, 0.08588113635778427, 0.08365894854068756, 0.0853111520409584, 0.08593884110450745, 0.0856085941195488, 0.08575762063264847, 0.08705534040927887, 0.08614349365234375, 0.08491839468479156, 0.08599017560482025, 0.0856340080499649], "accuracy_train_first": 0.5679593373493976, "model": "residualv4", "loss_std": [0.32240742444992065, 0.2602481245994568, 0.2515493631362915, 0.24475353956222534, 0.23564065992832184, 0.22800740599632263, 0.22036774456501007, 0.21338193118572235, 0.20590518414974213, 0.1973353773355484, 0.19100479781627655, 0.18260107934474945, 0.172860786318779, 0.16962166130542755, 0.1630409061908722, 0.16064868867397308, 0.15345321595668793, 0.15079057216644287, 0.14139877259731293, 0.13869939744472504, 0.13348788022994995, 0.13027532398700714, 0.12609748542308807, 0.12400040030479431, 0.11823094636201859, 0.11362681537866592, 0.1103031113743782, 0.10987113416194916, 0.10794465988874435, 0.1039617583155632, 0.10146729648113251, 0.10155496746301651, 0.09761614352464676, 0.09648154675960541, 0.09600783884525299, 0.09537100791931152, 0.09026418626308441, 0.0874752551317215, 0.08761771768331528, 0.08739273250102997, 0.08661246299743652, 0.08726346492767334, 0.0840715765953064, 0.08241550624370575, 0.08289331942796707, 0.08280910551548004, 0.08103775233030319, 0.08185210078954697, 0.0789891928434372, 0.07731214165687561, 0.07911713421344757, 0.07751177996397018, 0.07886306196451187, 0.07434549927711487, 0.07643807679414749, 0.07409936189651489, 0.0732152909040451, 0.07438834011554718, 0.07136010378599167, 0.07279336452484131, 0.0718255490064621, 0.07270730286836624, 0.07355593889951706, 0.07173524051904678, 0.07125280797481537, 0.07182131707668304, 0.07056672126054764, 0.07099686563014984, 0.07001999765634537, 0.07035648077726364, 0.07011827081441879, 0.06812587380409241, 0.06964191794395447, 0.0668545588850975, 0.06733432412147522, 0.06669639050960541, 0.066647008061409, 0.06831758469343185, 0.0659184679389, 0.06640259921550751, 0.06751005351543427, 0.0689079761505127, 0.06717559695243835, 0.06660877168178558, 0.06519874930381775, 0.06778337806463242, 0.06990775465965271, 0.06888367980718613, 0.06572475284337997, 0.06700947880744934, 0.0658826231956482, 0.06636134535074234, 0.06612970679998398, 0.06548426300287247, 0.066350057721138, 0.06547416001558304, 0.06574510782957077, 0.06434459239244461, 0.065357506275177, 0.06400412321090698, 0.06551963835954666, 0.06453972309827805, 0.06455253809690475, 0.06513762474060059, 0.06487070769071579, 0.0642104372382164, 0.06553894281387329, 0.0653877854347229, 0.06654637306928635, 0.0643128752708435, 0.06509131193161011, 0.06612449139356613, 0.06480469554662704, 0.0667378157377243, 0.06375308334827423, 0.06521735340356827, 0.06274546682834625, 0.06505700945854187, 0.06587328016757965, 0.06436995416879654, 0.06413837522268295, 0.0647738054394722, 0.06503347307443619, 0.06470556557178497, 0.06539338827133179, 0.06638668477535248, 0.06581570953130722, 0.06596909463405609, 0.06592169404029846, 0.0649830624461174]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:07 2016", "state": "available"}], "summary": "dd80b49e0162e424b80a8f5ba56bc92e"}