{"content": {"hp_model": {"f0": 64, "f1": 16, "f2": 32, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.011650299268601668, 0.019112954268643984, 0.01094124784001609, 0.01782421394611259, 0.01268229069152778, 0.011651979774613028, 0.011535946299561868, 0.014152989532109517, 0.01297960128884381, 0.013637096476409278, 0.01064688881719078, 0.012716597362482549, 0.009538149271774756, 0.009263492421745727, 0.009130926349150421, 0.00981583277204231, 0.010154537597227929, 0.008046802254483999, 0.008190349853709923, 0.009179978907363714, 0.009146888345797195, 0.010648514289097298, 0.011864801425541971, 0.011965237337537364, 0.011598631103911078, 0.01103847668150326, 0.008987126721121688, 0.009123268706833084, 0.007778091954137691, 0.008971251173782351, 0.009458460613599468, 0.01102874610334916, 0.010014556117362705, 0.013029027250654168, 0.013912207799649304, 0.0137502667372711, 0.013663985524789571, 0.013686585743460994, 0.013745379545207947, 0.012572811920766312, 0.013012637063761493, 0.012063097549495258, 0.011926982029160215, 0.010517605411994247, 0.011113823399026735, 0.01229742375214148, 0.012794913399121779, 0.010796155405642755, 0.012048430828247456, 0.01155942201696285, 0.012184941059844025, 0.013187560948511647, 0.012555291462837015, 0.01258469922514801, 0.012254337385852198, 0.011998745839030604, 0.012301341243807483, 0.01191164672162208, 0.012974989543136552, 0.01277126246321281, 0.0113583136113045, 0.010835486690698401, 0.012027624448359468, 0.012055586864085, 0.012397111541878699, 0.013641752673194599], "moving_avg_accuracy_train": [0.05651472450050755, 0.11094872019483663, 0.17208558728572582, 0.23160510078168256, 0.2882040801945627, 0.340854218848233, 0.38966709315204023, 0.4343983149230304, 0.4754212803287986, 0.5129835100213248, 0.5474358720648272, 0.5793265184027797, 0.608913621315178, 0.6362930370018126, 0.6615947091864135, 0.6849521156037356, 0.7065108907543255, 0.7263532054660377, 0.7445344483422837, 0.761376547585667, 0.7769296040558747, 0.7911412685695379, 0.804166570612778, 0.8161147376909428, 0.8271144817374632, 0.8372630423019505, 0.8465803975171228, 0.8551148627834061, 0.8629469801468613, 0.8702143776644288, 0.876934035839754, 0.8831049610844516, 0.888837794214194, 0.8941252993131235, 0.8990909921462076, 0.9036159553162306, 0.907953489133537, 0.911831692932208, 0.9155290145950595, 0.9189425985487596, 0.9221659948285273, 0.9250670154314996, 0.9278035280587077, 0.9303175787458233, 0.9327220223927896, 0.9349093092119731, 0.93692433427661, 0.93880296100145, 0.9405029896002252, 0.9420818434641229, 0.9435586515618781, 0.9448808394522481, 0.9461010354881049, 0.9472364143013283, 0.9482884821677533, 0.9492446798915926, 0.9501471105216194, 0.9509732489815006, 0.9517190987442032, 0.9524043504723115, 0.9529862718931037, 0.9534960502789595, 0.9539455862798103, 0.9543130023484425, 0.9546669282983067, 0.9549901119508034], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 937725023, "moving_var_accuracy_train": [0.02874522676831442, 0.05253824307673512, 0.08092386742826303, 0.10471463306659513, 0.12307416999515224, 0.1357150868978934, 0.1435878484882976, 0.14723700344959736, 0.14765925632078333, 0.14559162058397188, 0.1417151457789638, 0.1366967511157387, 0.13090564593290446, 0.12456177296968785, 0.11786716721075215, 0.11099056640057256, 0.10407453683445854, 0.09721054022908034, 0.09046450453889772, 0.0839709608473237, 0.07775094285268057, 0.07179359124165273, 0.06614115855734615, 0.060811870970361205, 0.05581963319512573, 0.05116460940939277, 0.04682946644230477, 0.04280205367450686, 0.03907392686861061, 0.035641869781854664, 0.03248406705760814, 0.029578383217228135, 0.026916333276746567, 0.024476319340612753, 0.022250610354364365, 0.02020982694413851, 0.018358172046271153, 0.016657719023980277, 0.015114978808889571, 0.01370835392668124, 0.012431031086200904, 0.01126367126243064, 0.0102047006484174, 0.009241114641292136, 0.008369035320425848, 0.007575189801047631, 0.0068542137550429046, 0.006200555524880159, 0.005606510847522028, 0.005068294778481729, 0.004581093960051917, 0.0041387181914036945, 0.0037382462775566103, 0.0033760234152466003, 0.0030483826948820153, 0.002751773252177493, 0.0024839253563378387, 0.0022416753634981095, 0.002022514453965011, 0.0018244891379463884, 0.0016450879170115398, 0.001482917991334557, 0.00133644493574565, 0.0012040153932784867, 0.001084741226152523, 0.000977207132596441], "duration": 25651.605591, "accuracy_train": [0.5651472450050756, 0.6008546814437985, 0.7223173911037284, 0.7672807222452934, 0.7975948949104835, 0.8147054667312662, 0.8289829618863048, 0.8369793108619417, 0.8446279689807125, 0.8510435772540605, 0.8575071304563492, 0.8663423354443521, 0.8751975475267626, 0.8827077781815246, 0.889309758847822, 0.8951687733596345, 0.9005398671096345, 0.904934037871447, 0.9081656342284975, 0.9129554407761166, 0.9169071122877446, 0.9190462491925065, 0.921394289001938, 0.923648241394426, 0.9261121781561462, 0.9286000873823367, 0.9304365944536729, 0.9319250501799556, 0.9334360364179586, 0.935620955322536, 0.9374109594176817, 0.9386432882867294, 0.9404332923818751, 0.9417128452034883, 0.9437822276439645, 0.9443406238464378, 0.9469912934892949, 0.9467355271202473, 0.9488049095607235, 0.9496648541320598, 0.9511765613464378, 0.9511762008582503, 0.9524321417035806, 0.9529440349298633, 0.9543620152154854, 0.9545948905846253, 0.9550595598583426, 0.9557106015250092, 0.9558032469892026, 0.9562915282392026, 0.9568499244416758, 0.9567805304655776, 0.9570827998108158, 0.9574548236203396, 0.9577570929655776, 0.9578504594061462, 0.9582689861918604, 0.9584084951204319, 0.9584317466085271, 0.9585716160252861, 0.9582235646802326, 0.9580840557516611, 0.9579914102874677, 0.9576197469661315, 0.9578522618470838, 0.9578987648232743], "end": "2016-01-23 17:50:25.210000", "learning_rate_per_epoch": [0.00120470579713583, 0.000602352898567915, 0.00040156859904527664, 0.0003011764492839575, 0.0002409411536063999, 0.00020078429952263832, 0.00017210083024110645, 0.00015058822464197874, 0.00013385619968175888, 0.00012047057680319995, 0.00010951871081488207, 0.00010039214976131916, 9.266967390431091e-05, 8.605041512055323e-05, 8.031371544348076e-05, 7.529411232098937e-05, 7.086504774633795e-05, 6.692809984087944e-05, 6.340556865325198e-05, 6.0235288401599973e-05, 5.736694220104255e-05, 5.4759355407441035e-05, 5.237851291894913e-05, 5.019607488065958e-05, 4.818823072127998e-05, 4.6334836952155456e-05, 4.461873322725296e-05, 4.302520756027661e-05, 4.154157795710489e-05, 4.015685772174038e-05, 3.886147533194162e-05, 3.7647056160494685e-05, 3.650623693829402e-05, 3.5432523873168975e-05, 3.442016532062553e-05, 3.346404992043972e-05, 3.2559615647187456e-05, 3.170278432662599e-05, 3.088989251409657e-05, 3.0117644200799987e-05, 2.9383067158050835e-05, 2.8683471100521274e-05, 2.801641312544234e-05, 2.7379677703720517e-05, 2.6771240300149657e-05, 2.6189256459474564e-05, 2.5632038159528747e-05, 2.509803744032979e-05, 2.458583185216412e-05, 2.409411536063999e-05, 2.362168197578285e-05, 2.3167418476077728e-05, 2.2730297132511623e-05, 2.230936661362648e-05, 2.1903741071582772e-05, 2.1512603780138306e-05, 2.113518894475419e-05, 2.0770788978552446e-05, 2.041874176939018e-05, 2.007842886087019e-05, 1.9749275452340953e-05, 1.943073766597081e-05, 1.912231346068438e-05, 1.8823528080247343e-05, 1.8533935872255825e-05, 1.825311846914701e-05], "accuracy_valid": [0.5677107845444277, 0.5923689876694277, 0.7015763248305723, 0.7267331042921686, 0.7504368058170181, 0.7592876388365963, 0.7603965667356928, 0.7586052216679217, 0.7591243881777108, 0.7573845185429217, 0.7571506730045181, 0.7577507294804217, 0.7587581772402108, 0.7613319488893072, 0.7615863846009037, 0.7629291580384037, 0.7605892319277108, 0.7593788238893072, 0.7581478256777108, 0.7575271790286144, 0.7564388413027108, 0.7544754212161144, 0.7515148484563253, 0.7523796357304217, 0.7520340149661144, 0.7505897613893072, 0.7508441971009037, 0.7504676910768072, 0.751220703125, 0.7486160462161144, 0.7467952866152108, 0.7444347703313253, 0.7437229386295181, 0.7435699830572289, 0.7434685029179217, 0.7419727739081325, 0.7403858598456325, 0.7402637895331325, 0.7384224397590362, 0.7386768754706325, 0.7375576524849398, 0.7363472444465362, 0.7363575395331325, 0.7371002564947289, 0.7379650437688253, 0.7378223832831325, 0.7380768189947289, 0.7372326218938253, 0.7364899049322289, 0.7366222703313253, 0.7357471879706325, 0.7369678910956325, 0.7377209031438253, 0.7352692018072289, 0.7356457078313253, 0.7356457078313253, 0.7352794968938253, 0.7344147096197289, 0.7336822877447289, 0.7336822877447289, 0.7336822877447289, 0.7334381471197289, 0.7321968538215362, 0.7313423616340362, 0.7324512895331325, 0.7315865022590362], "accuracy_test": 0.10922523961661341, "start": "2016-01-23 10:42:53.604000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0], "accuracy_train_last": 0.9578987648232743, "batch_size_eval": 1024, "accuracy_train_std": [0.023518228940468604, 0.024629144385791054, 0.02556008550478647, 0.025245286402224593, 0.02596522649229692, 0.02518245642361795, 0.024197466313653642, 0.024066557656787568, 0.02355531717052026, 0.02387087511008197, 0.022975237918892808, 0.022081506845385762, 0.021965179027766516, 0.02077530311464084, 0.020348655895510652, 0.019664771590167867, 0.01874303558831437, 0.017828471403468243, 0.01810764431235543, 0.017191273204370465, 0.017398798109454553, 0.017175838791216404, 0.01768223733472143, 0.01791706634446378, 0.017351006218729828, 0.016516897843769286, 0.015586030639912608, 0.01590378043470291, 0.015701337090621912, 0.015609312507403968, 0.01523801543926066, 0.015248281817018813, 0.015189308247986344, 0.014459886896577562, 0.014658363450813944, 0.013602388873805691, 0.01401631927859087, 0.01353831713095055, 0.013525754709251426, 0.013446376591713277, 0.01294304044110579, 0.013108638511101924, 0.012456771243031, 0.012027877154780925, 0.012057320648042893, 0.011557375091077252, 0.011285518162353972, 0.011099277188163174, 0.011356696560574668, 0.011245499570255595, 0.010986559026265557, 0.011068280513698337, 0.010649709118122838, 0.010557861413516556, 0.010252013139978616, 0.00992248711765797, 0.009793598952947945, 0.009906478273492229, 0.009613521667423575, 0.009249178503605788, 0.008846916623686809, 0.00902956240174595, 0.008978232310756748, 0.00868553180975118, 0.008595613901971822, 0.008667354593768328], "accuracy_test_std": 0.05364144794877954, "error_valid": [0.4322892154555723, 0.4076310123305723, 0.2984236751694277, 0.27326689570783136, 0.2495631941829819, 0.24071236116340367, 0.23960343326430722, 0.24139477833207834, 0.24087561182228923, 0.24261548145707834, 0.2428493269954819, 0.24224927051957834, 0.24124182275978923, 0.23866805111069278, 0.23841361539909633, 0.23707084196159633, 0.23941076807228923, 0.24062117611069278, 0.24185217432228923, 0.24247282097138556, 0.24356115869728923, 0.24552457878388556, 0.24848515154367468, 0.24762036426957834, 0.24796598503388556, 0.24941023861069278, 0.24915580289909633, 0.24953230892319278, 0.248779296875, 0.25138395378388556, 0.2532047133847892, 0.2555652296686747, 0.2562770613704819, 0.2564300169427711, 0.25653149708207834, 0.25802722609186746, 0.25961414015436746, 0.25973621046686746, 0.2615775602409638, 0.26132312452936746, 0.26244234751506024, 0.2636527555534638, 0.26364246046686746, 0.2628997435052711, 0.2620349562311747, 0.26217761671686746, 0.2619231810052711, 0.2627673781061747, 0.2635100950677711, 0.2633777296686747, 0.26425281202936746, 0.26303210890436746, 0.2622790968561747, 0.2647307981927711, 0.2643542921686747, 0.2643542921686747, 0.2647205031061747, 0.2655852903802711, 0.2663177122552711, 0.2663177122552711, 0.2663177122552711, 0.2665618528802711, 0.2678031461784638, 0.2686576383659638, 0.26754871046686746, 0.2684134977409638], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.512739272731044, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0012047057855553295, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "rmsprop", "nb_data_augmentation": 0, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 4.063095330998311e-05, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.028042668361977274}, "accuracy_valid_max": 0.7629291580384037, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7315865022590362, "loss_train": [1.9735122919082642, 1.5154824256896973, 1.260082483291626, 1.1246118545532227, 1.0357728004455566, 0.969438910484314, 0.9159404635429382, 0.8696852922439575, 0.8297480940818787, 0.7950289845466614, 0.7637265920639038, 0.7349631190299988, 0.7085935473442078, 0.6839844584465027, 0.6609653830528259, 0.6392104625701904, 0.6192265152931213, 0.6002923846244812, 0.582598090171814, 0.5661280155181885, 0.5505229234695435, 0.5356636643409729, 0.5217669606208801, 0.5087628960609436, 0.4964890778064728, 0.4849614202976227, 0.47405779361724854, 0.4637397825717926, 0.45393651723861694, 0.44472455978393555, 0.43626174330711365, 0.428207129240036, 0.4208109378814697, 0.41379958391189575, 0.4074312150478363, 0.4014506936073303, 0.3959508240222931, 0.39088529348373413, 0.3861890435218811, 0.38179925084114075, 0.37769728899002075, 0.37380334734916687, 0.3701636791229248, 0.36670541763305664, 0.36339718103408813, 0.36018282175064087, 0.3570958971977234, 0.3540571928024292, 0.35107940435409546, 0.34816405177116394, 0.3452974557876587, 0.3424524962902069, 0.339640736579895, 0.33686599135398865, 0.3341151475906372, 0.33140671253204346, 0.32873719930648804, 0.32610633969306946, 0.32351717352867126, 0.3209727704524994, 0.3184778094291687, 0.3160291910171509, 0.31362584233283997, 0.31127122044563293, 0.30895528197288513, 0.3066891133785248], "accuracy_train_first": 0.5651472450050756, "model": "residualv4", "loss_std": [0.30718085169792175, 0.19626781344413757, 0.18487462401390076, 0.17754697799682617, 0.17301489412784576, 0.16808533668518066, 0.16367770731449127, 0.15913906693458557, 0.15472625195980072, 0.15030455589294434, 0.14471016824245453, 0.13916124403476715, 0.13389012217521667, 0.12878140807151794, 0.1238112673163414, 0.11850166320800781, 0.11351592093706131, 0.10857593268156052, 0.10376337170600891, 0.09886909276247025, 0.09414076060056686, 0.08933884650468826, 0.08471371233463287, 0.08033017069101334, 0.07613678276538849, 0.07185612618923187, 0.06765420734882355, 0.06358131021261215, 0.05946115776896477, 0.05541591718792915, 0.05164717510342598, 0.047826867550611496, 0.04427167773246765, 0.04065006226301193, 0.03747348114848137, 0.03427107632160187, 0.031324077397584915, 0.02835662104189396, 0.025616200640797615, 0.022999785840511322, 0.020593399181962013, 0.01831214129924774, 0.016290096566081047, 0.014517811127007008, 0.012966104783117771, 0.011594818904995918, 0.010483459569513798, 0.00951953511685133, 0.00863979198038578, 0.007838648743927479, 0.007140105590224266, 0.006492081563919783, 0.005917966365814209, 0.005427889060229063, 0.004894382320344448, 0.004394445568323135, 0.003866020357236266, 0.0034090159460902214, 0.003003097139298916, 0.002632182789966464, 0.0023173673544079065, 0.0020745517686009407, 0.0018785345600917935, 0.0017279938329011202, 0.0015976032009348273, 0.0015061937738209963]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:09 2016", "state": "available"}], "summary": "1a2cf65ae9d35f068d23d8743b468e3f"}