{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 32, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.0536383708710381, 0.05482523544229642, 0.055002564001293074, 0.05263243313365677, 0.05059828101087776, 0.05004681940732199, 0.05302741232362551, 0.05206723671929284, 0.05551444896054094, 0.05548295613887297, 0.05585458948026746, 0.05692487256061341, 0.05557064175097004, 0.056564055788325324, 0.0560976922532271, 0.05694460730641709, 0.05793601087735534, 0.05493929807846582, 0.05340845252280082, 0.05508777680532278, 0.05426282997864427, 0.0540434895157604, 0.05675480762945908, 0.05853138536167896, 0.055363893173476605, 0.05674726529524835, 0.05779083812761076, 0.0595185322263003, 0.058448141712554164, 0.05795601677854489, 0.056000959952618015, 0.05346452370360679, 0.055106551248995, 0.05414569546330581, 0.05277827790084506, 0.050695823520189906, 0.05184170336509574, 0.052726550885775963, 0.051395996096832236, 0.05437216839907004, 0.055175120726996686, 0.05615552461342347, 0.05715843168298889, 0.05564665238331766, 0.05761094739914619, 0.0553003956111377, 0.0555282616015737, 0.05632391593201914, 0.05670167568490953, 0.058936762701056286, 0.05848108743179333, 0.05730365040129047, 0.05840052063947167, 0.059748221199769906, 0.058293230987054356, 0.057253831419355014, 0.05859351220213757, 0.05908908088302278, 0.05860446892544508, 0.058063929533629224, 0.056292875965194354, 0.0552074352595466, 0.05619203635224423, 0.05480148319154715, 0.0555202314144872, 0.054684197913833305, 0.056154889416637994, 0.057072560574573744, 0.05787410271768635, 0.057729701101849946, 0.059329451590027435, 0.05641535271834655, 0.056794388451121246, 0.060269514907505796, 0.06035054183269276, 0.05836997387581208, 0.057899982835610084, 0.05712191320712056, 0.0584838320706493, 0.05888862819059559, 0.0572902658067932, 0.0566132216528457, 0.056677451122460636, 0.05829873782369375, 0.05630396363203203, 0.057939396976995336, 0.05850761354740628, 0.06006350015052722, 0.05718681881950095, 0.05724573173970569, 0.05786948005013138, 0.057031921952802066, 0.05555619760477122, 0.05683300028324961, 0.055179322686396276, 0.0533463049702723, 0.055914906078157064, 0.05623772197393482, 0.056992819027992844, 0.05631663257827597, 0.0571500064135635, 0.057222984203067706, 0.05728497336033503, 0.05770652622351441], "moving_avg_accuracy_train": [0.01995717243975903, 0.0403000282379518, 0.06142768731174698, 0.08289222279743974, 0.10392576030685238, 0.12461140340267317, 0.14477686321300826, 0.16418707222303275, 0.18270577087422346, 0.20044799499162042, 0.21734549594426558, 0.23340979800646552, 0.24889835510340932, 0.2639534555870443, 0.27860903246809887, 0.2927144319321323, 0.3064540956666299, 0.31970458218430425, 0.3325195155321389, 0.3448506776837443, 0.3569017544936831, 0.36843720027925453, 0.3796427084139797, 0.39045008666896724, 0.40091091384544403, 0.4109280679428273, 0.4206706338895084, 0.4301731299885094, 0.4392783697005018, 0.44810373303165646, 0.45664191018029804, 0.46491456027672606, 0.47290587909845105, 0.48058281754402765, 0.4879697541631189, 0.4951380460660841, 0.501996605917307, 0.508628176500275, 0.515095460506874, 0.5213795891549818, 0.5274753463539414, 0.5332909706040895, 0.5389415422183793, 0.5443612057676257, 0.5496812975402607, 0.554737640677801, 0.5596036732967679, 0.5643407833767295, 0.5690183390752013, 0.5736281768544281, 0.578174715343684, 0.5826689907972675, 0.5871185826813963, 0.5914408923349433, 0.5955733467761478, 0.5995443441768462, 0.6034265061447038, 0.6072310693856551, 0.6109610874470895, 0.6145604794553926, 0.6181317281966003, 0.6215929341420006, 0.6250092243121379, 0.6282627258267073, 0.6314709035452414, 0.6346829999377052, 0.637926861088513, 0.6410910650399028, 0.6442777040178402, 0.6473927611762972, 0.6505092832514385, 0.6535753541732827, 0.6564383571595689, 0.6591915470460217, 0.6619870949016604, 0.6647101662849884, 0.667335064566128, 0.669808071663732, 0.6722479158527804, 0.6746532070988277, 0.6770721107865353, 0.6794656150693276, 0.6816103562732381, 0.6838230028748299, 0.6859061581596361, 0.6880327863195761, 0.6900667629587028, 0.6920926544339169, 0.6940900907977541, 0.6960101479830388, 0.6976958425220844, 0.6994788749867433, 0.7010483067651774, 0.7026914053055271, 0.7039795878171431, 0.7049907028306095, 0.705961888571645, 0.7068006582988179, 0.7078214584327915, 0.7087472380413196, 0.7095874991769466, 0.7102943177833483, 0.7107633799809171, 0.7115126255671628], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234423, "moving_var_accuracy_train": [0.0035845985861124975, 0.006950624765735834, 0.010272964090608929, 0.013392204234097113, 0.016034671112126222, 0.0182822664735027, 0.02011385175041259, 0.021493272499686848, 0.022430425047320676, 0.02302046119227608, 0.023288144919050277, 0.023281886633855664, 0.023112756578977694, 0.022841385376231038, 0.022490320242056456, 0.022031948864210414, 0.021527759213622995, 0.020955161828856327, 0.020337648296355693, 0.01967240150680279, 0.019012217426633917, 0.01830859426921725, 0.01760780555531205, 0.01689821982249825, 0.016193257987193458, 0.015477022574370622, 0.014783578637962435, 0.01411789766316995, 0.013452256408768553, 0.012808014109233696, 0.012183316919504594, 0.01158091588411551, 0.010997574884298063, 0.010428235850942803, 0.00987651375937876, 0.009351322062695936, 0.008839548445521605, 0.008351393156541435, 0.007892685702685392, 0.007458829588210573, 0.007047370931847532, 0.006647027207432966, 0.006269685122803633, 0.005907071387406542, 0.005571094636889217, 0.005244084626521243, 0.0049327806249087666, 0.004641464469604969, 0.004374233768455211, 0.004128065830766774, 0.003901298357798672, 0.0036929551286929456, 0.0035018494272413842, 0.0033198057311875657, 0.0031415197754464832, 0.002969287181109021, 0.002807999096900235, 0.002657471500299793, 0.0025169416629174618, 0.0023818481020906413, 0.002258447650025777, 0.0021404224043914625, 0.0020314195106915065, 0.001923545008570101, 0.0018238221461763784, 0.0017342980006690523, 0.0016555719170936272, 0.0015801244051981732, 0.0015135039764597473, 0.0014494858087178567, 0.0013919516164496624, 0.0013373635728846972, 0.0012773982904915803, 0.0012178789524002133, 0.001166426847478692, 0.0011165202225591182, 0.0010668790191801789, 0.0010152329942053617, 0.0009672852517863289, 0.0009226255604125001, 0.0008830228598248988, 0.0008462803386081128, 0.0008030515382330694, 0.0007668086292615862, 0.0007291835898009705, 0.0006969681567967217, 0.000664504889833669, 0.0006349925272744071, 0.0006074010427951799, 0.0005798405148685371, 0.0005474305580923941, 0.0005213003452134074, 0.0004913383556564962, 0.0004665024754105413, 0.00043478695551858607, 0.00040050944210084217, 0.0003689473135830729, 0.00033838439412176013, 0.0003139242509312694, 0.0002902454367902401, 0.00026757524209562397, 0.00024531405076726274, 0.00022276281979722895, 0.0002055388583540832], "duration": 28447.303652, "accuracy_train": [0.19957172439759036, 0.22338573042168675, 0.2515766189759036, 0.2760730421686747, 0.29322759789156627, 0.31078219126506024, 0.3262660015060241, 0.338878953313253, 0.34937405873493976, 0.3601280120481928, 0.3694230045180723, 0.3779885165662651, 0.3882953689759036, 0.39944935993975905, 0.41050922439759036, 0.41966302710843373, 0.43011106927710846, 0.4389589608433735, 0.4478539156626506, 0.4558311370481928, 0.46536144578313254, 0.4722562123493976, 0.48049228162650603, 0.48771649096385544, 0.4950583584337349, 0.5010824548192772, 0.5083537274096386, 0.5156955948795181, 0.5212255271084337, 0.5275320030120482, 0.5334855045180723, 0.5393684111445783, 0.5448277484939759, 0.5496752635542169, 0.5544521837349398, 0.5596526731927711, 0.5637236445783133, 0.5683123117469879, 0.5733010165662651, 0.5779367469879518, 0.5823371611445783, 0.5856315888554217, 0.5897966867469879, 0.5931381777108434, 0.5975621234939759, 0.6002447289156626, 0.6033979668674698, 0.6069747740963856, 0.6111163403614458, 0.6151167168674698, 0.6190935617469879, 0.6231174698795181, 0.6271649096385542, 0.6303416792168675, 0.6327654367469879, 0.6352833207831325, 0.6383659638554217, 0.6414721385542169, 0.64453125, 0.6469550075301205, 0.6502729668674698, 0.6527437876506024, 0.6557558358433735, 0.6575442394578314, 0.6603445030120482, 0.6635918674698795, 0.6671216114457831, 0.6695689006024096, 0.6729574548192772, 0.6754282756024096, 0.6785579819277109, 0.6811699924698795, 0.6822053840361446, 0.6839702560240963, 0.6871470256024096, 0.6892178087349398, 0.6909591490963856, 0.6920651355421686, 0.6942065135542169, 0.696300828313253, 0.6988422439759037, 0.7010071536144579, 0.7009130271084337, 0.7037368222891566, 0.7046545557228916, 0.7071724397590361, 0.7083725527108434, 0.7103256777108434, 0.7120670180722891, 0.7132906626506024, 0.712867093373494, 0.7155261671686747, 0.7151731927710844, 0.7174792921686747, 0.7155732304216867, 0.7140907379518072, 0.7147025602409639, 0.7143495858433735, 0.7170086596385542, 0.7170792545180723, 0.7171498493975904, 0.7166556852409639, 0.7149849397590361, 0.7182558358433735], "end": "2016-01-21 04:23:13.676000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0], "accuracy_valid": [0.20726495726495728, 0.22783119658119658, 0.26175213675213677, 0.2795138888888889, 0.29126602564102566, 0.3106303418803419, 0.3263888888888889, 0.33560363247863245, 0.343349358974359, 0.35149572649572647, 0.3613782051282051, 0.36939102564102566, 0.37753739316239315, 0.3868856837606838, 0.3999732905982906, 0.4114583333333333, 0.42094017094017094, 0.4284188034188034, 0.437366452991453, 0.4467147435897436, 0.4563301282051282, 0.46434294871794873, 0.4738247863247863, 0.48223824786324787, 0.48998397435897434, 0.49225427350427353, 0.49772970085470086, 0.5033386752136753, 0.5082799145299145, 0.5112179487179487, 0.5152243589743589, 0.5185630341880342, 0.5205662393162394, 0.5227029914529915, 0.5256410256410257, 0.5279113247863247, 0.530982905982906, 0.5324519230769231, 0.5373931623931624, 0.5400641025641025, 0.5420673076923077, 0.5440705128205128, 0.546073717948718, 0.5483440170940171, 0.5478098290598291, 0.5486111111111112, 0.5502136752136753, 0.5508814102564102, 0.5536858974358975, 0.5558226495726496, 0.5554220085470085, 0.5574252136752137, 0.5559561965811965, 0.5583600427350427, 0.5559561965811965, 0.5568910256410257, 0.5575587606837606, 0.5582264957264957, 0.5563568376068376, 0.5564903846153846, 0.5558226495726496, 0.5558226495726496, 0.5543536324786325, 0.5548878205128205, 0.5548878205128205, 0.5552884615384616, 0.5543536324786325, 0.5539529914529915, 0.5542200854700855, 0.5542200854700855, 0.5538194444444444, 0.5511485042735043, 0.5490117521367521, 0.5484775641025641, 0.5458066239316239, 0.5452724358974359, 0.5442040598290598, 0.5408653846153846, 0.5395299145299145, 0.5373931623931624, 0.5360576923076923, 0.5327190170940171, 0.5301816239316239, 0.5305822649572649, 0.5284455128205128, 0.5276442307692307, 0.5281784188034188, 0.5243055555555556, 0.5235042735042735, 0.5247061965811965, 0.5225694444444444, 0.5192307692307693, 0.5169604700854701, 0.5181623931623932, 0.5169604700854701, 0.5134882478632479, 0.5144230769230769, 0.5105502136752137, 0.5101495726495726, 0.5094818376068376, 0.5080128205128205, 0.5076121794871795, 0.5066773504273504, 0.5029380341880342], "accuracy_test": 0.24048477564102563, "start": "2016-01-20 20:29:06.372000", "learning_rate_per_epoch": [0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905, 0.00012364162830635905], "accuracy_train_last": 0.7182558358433735, "error_valid": [0.7927350427350427, 0.7721688034188035, 0.7382478632478633, 0.7204861111111112, 0.7087339743589743, 0.6893696581196581, 0.6736111111111112, 0.6643963675213675, 0.656650641025641, 0.6485042735042735, 0.6386217948717949, 0.6306089743589743, 0.6224626068376069, 0.6131143162393162, 0.6000267094017093, 0.5885416666666667, 0.579059829059829, 0.5715811965811965, 0.5626335470085471, 0.5532852564102564, 0.5436698717948718, 0.5356570512820513, 0.5261752136752137, 0.5177617521367521, 0.5100160256410257, 0.5077457264957265, 0.5022702991452992, 0.49666132478632474, 0.4917200854700855, 0.4887820512820513, 0.4847756410256411, 0.4814369658119658, 0.47943376068376065, 0.4772970085470085, 0.47435897435897434, 0.47208867521367526, 0.469017094017094, 0.46754807692307687, 0.46260683760683763, 0.45993589743589747, 0.4579326923076923, 0.4559294871794872, 0.45392628205128205, 0.45165598290598286, 0.4521901709401709, 0.45138888888888884, 0.44978632478632474, 0.44911858974358976, 0.44631410256410253, 0.4441773504273504, 0.4445779914529915, 0.4425747863247863, 0.44404380341880345, 0.4416399572649573, 0.44404380341880345, 0.44310897435897434, 0.44244123931623935, 0.44177350427350426, 0.44364316239316237, 0.4435096153846154, 0.4441773504273504, 0.4441773504273504, 0.44564636752136755, 0.4451121794871795, 0.4451121794871795, 0.44471153846153844, 0.44564636752136755, 0.4460470085470085, 0.4457799145299145, 0.4457799145299145, 0.4461805555555556, 0.44885149572649574, 0.45098824786324787, 0.4515224358974359, 0.45419337606837606, 0.4547275641025641, 0.45579594017094016, 0.4591346153846154, 0.4604700854700855, 0.46260683760683763, 0.4639423076923077, 0.46728098290598286, 0.46981837606837606, 0.4694177350427351, 0.4715544871794872, 0.4723557692307693, 0.47182158119658124, 0.4756944444444444, 0.47649572649572647, 0.47529380341880345, 0.4774305555555556, 0.4807692307692307, 0.4830395299145299, 0.4818376068376068, 0.4830395299145299, 0.48651175213675213, 0.48557692307692313, 0.4894497863247863, 0.4898504273504274, 0.49051816239316237, 0.4919871794871795, 0.4923878205128205, 0.4933226495726496, 0.4970619658119658], "accuracy_train_std": [0.05074199545902291, 0.05247989030262755, 0.05384134363539565, 0.05520645961561374, 0.05813170540275406, 0.058106895532404064, 0.05780436162983055, 0.05814957262035594, 0.05881757700307891, 0.057775275710267665, 0.05750720781147847, 0.05678110285704547, 0.05751945460742167, 0.05843576644040337, 0.05928049219670568, 0.05850329605411176, 0.0592614195332139, 0.06071011880031587, 0.06134940089920751, 0.06186460133037193, 0.06178636217521192, 0.06122680963123151, 0.061658964098029345, 0.061388939929506794, 0.06110062836265503, 0.0613685149120705, 0.06163273502739693, 0.06166190968017987, 0.062165818658739734, 0.0616482447629973, 0.060647091846242764, 0.06034422607158743, 0.06073249306916424, 0.06092009977807983, 0.06050883107387785, 0.060512175799587016, 0.06097516222655253, 0.060153129612877144, 0.059559095228655476, 0.059596723370974575, 0.05912238154462886, 0.0591379269828547, 0.05842405766675453, 0.05745787167667279, 0.05676739456502841, 0.056425680479922406, 0.056215336873330524, 0.05589884260198181, 0.055752040104725974, 0.056578799963727576, 0.05696667546122575, 0.05648782504698025, 0.05628206254749615, 0.05659523486493078, 0.05581874422730655, 0.05536038568953241, 0.055865250955823405, 0.05587817473411551, 0.05540388409860446, 0.055042758095069075, 0.05552432755386726, 0.05617846006254161, 0.05560207654107334, 0.05652063031088159, 0.05637067804521336, 0.056053421009819936, 0.056253621769289476, 0.05586332799098235, 0.0563589676378639, 0.05643018963582515, 0.05597837133682767, 0.05559442755314864, 0.05533833597996721, 0.05496953701901676, 0.05575503456656558, 0.05607170852280793, 0.05647511431896993, 0.056614472167387284, 0.05691680755153305, 0.0563019527088712, 0.0560229664449042, 0.05632303529640868, 0.05846557507820452, 0.0576941544765203, 0.057635669380433414, 0.05849309182511583, 0.05915769937042378, 0.059827872732660374, 0.059090608377409044, 0.059624591078116254, 0.05962130336650483, 0.05966094349884483, 0.059569883742275856, 0.05970676212898071, 0.05989718873886231, 0.05991377623312234, 0.06053798025151746, 0.06096433174197551, 0.061245387783773204, 0.06187745333892485, 0.061518782976119256, 0.0609966948716147, 0.06228031616387537, 0.06188020955020733], "accuracy_test_std": 0.05323958107817107, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.9124434269278967, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0001236416215668152, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 3.443700377594999e-05, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.02038381162727242}, "accuracy_valid_max": 0.5583600427350427, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234423\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-6, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.5029380341880342, "loss_train": [2.852597951889038, 2.6831490993499756, 2.5921430587768555, 2.512908935546875, 2.451683759689331, 2.4038946628570557, 2.3661088943481445, 2.3344032764434814, 2.3063747882843018, 2.2804110050201416, 2.256068229675293, 2.2326486110687256, 2.2097156047821045, 2.1869890689849854, 2.164276123046875, 2.1411256790161133, 2.1173174381256104, 2.0929129123687744, 2.06835675239563, 2.0447118282318115, 2.0222628116607666, 2.001232385635376, 1.9813940525054932, 1.9625999927520752, 1.9444196224212646, 1.9270074367523193, 1.9096187353134155, 1.8926596641540527, 1.8761577606201172, 1.8601397275924683, 1.8441846370697021, 1.8287032842636108, 1.8133459091186523, 1.7985821962356567, 1.7839876413345337, 1.7697941064834595, 1.755570411682129, 1.741632342338562, 1.7280259132385254, 1.714526891708374, 1.7013448476791382, 1.6883527040481567, 1.6755367517471313, 1.662689447402954, 1.6499985456466675, 1.6374783515930176, 1.6248589754104614, 1.6125491857528687, 1.600179672241211, 1.5878835916519165, 1.5755828619003296, 1.563349723815918, 1.5512524843215942, 1.5389539003372192, 1.526734471321106, 1.5143402814865112, 1.5019999742507935, 1.489878535270691, 1.4773778915405273, 1.46492338180542, 1.4523800611495972, 1.4395865201950073, 1.427042841911316, 1.4140595197677612, 1.401174783706665, 1.3880724906921387, 1.3749542236328125, 1.361778736114502, 1.3485387563705444, 1.3353427648544312, 1.3219839334487915, 1.3084852695465088, 1.294821858406067, 1.280937910079956, 1.2673652172088623, 1.2535043954849243, 1.2396384477615356, 1.2256886959075928, 1.211690902709961, 1.1976277828216553, 1.1834319829940796, 1.1692323684692383, 1.1551923751831055, 1.141284465789795, 1.1272343397140503, 1.1129348278045654, 1.0990116596221924, 1.0847375392913818, 1.070838451385498, 1.0571036338806152, 1.043376088142395, 1.0296305418014526, 1.0154659748077393, 1.0015097856521606, 0.9882171750068665, 0.9744197726249695, 0.9615989923477173, 0.9485626816749573, 0.9356091022491455, 0.9228579998016357, 0.9106922745704651, 0.8984540104866028, 0.8874596953392029, 0.8763720393180847], "accuracy_train_first": 0.19957172439759036, "model": "residualv4", "loss_std": [0.11677365005016327, 0.06380634009838104, 0.064891017973423, 0.06900592893362045, 0.07214394211769104, 0.07512051612138748, 0.07754059880971909, 0.07969532161951065, 0.0816563069820404, 0.08359787613153458, 0.08532299101352692, 0.08705025911331177, 0.08884275704622269, 0.09065980464220047, 0.09247315675020218, 0.09417498111724854, 0.0959361270070076, 0.09769189357757568, 0.09938197582960129, 0.1012280285358429, 0.10288497060537338, 0.10432568192481995, 0.10581370443105698, 0.10715094953775406, 0.1082058921456337, 0.10910134017467499, 0.10979080945253372, 0.11025715619325638, 0.11079290509223938, 0.11125005036592484, 0.11165500432252884, 0.11215037852525711, 0.11249007284641266, 0.1127004325389862, 0.11300641298294067, 0.11335314810276031, 0.11365906894207001, 0.11385779827833176, 0.11406023055315018, 0.11410743743181229, 0.11431664228439331, 0.11439390480518341, 0.11444464325904846, 0.11431331932544708, 0.11438219994306564, 0.11431535333395004, 0.11423970758914948, 0.1140243411064148, 0.1136493980884552, 0.11333858966827393, 0.11323902010917664, 0.11287873983383179, 0.11239738762378693, 0.11201155185699463, 0.1115148663520813, 0.1109681949019432, 0.1105213612318039, 0.1101553812623024, 0.10983666032552719, 0.10907413810491562, 0.10877401381731033, 0.10798663645982742, 0.1074305921792984, 0.10664018988609314, 0.10644643753767014, 0.10600726306438446, 0.10552699863910675, 0.10485805571079254, 0.1042381078004837, 0.10368962585926056, 0.10299059748649597, 0.10232611745595932, 0.10139559954404831, 0.10074955970048904, 0.09995056688785553, 0.09938737004995346, 0.0986936166882515, 0.09759043157100677, 0.09673584997653961, 0.09618013352155685, 0.09530122578144073, 0.09462994337081909, 0.0936053916811943, 0.09232176840305328, 0.09115959703922272, 0.08985984325408936, 0.08906114846467972, 0.08784614503383636, 0.08650920540094376, 0.08531094342470169, 0.08410193771123886, 0.08272324502468109, 0.08142205327749252, 0.08005012571811676, 0.07873503118753433, 0.07739783823490143, 0.07598771899938583, 0.07395540177822113, 0.07276832312345505, 0.07093911617994308, 0.06974353641271591, 0.06830050051212311, 0.06715252250432968, 0.0664268508553505]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:05 2016", "state": "available"}], "summary": "1785d8f0937a60f9222e928a143859e1"}