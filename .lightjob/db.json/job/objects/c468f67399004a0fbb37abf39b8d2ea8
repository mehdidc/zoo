{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 16, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [2.579159736633301, 2.0509347915649414, 1.9453011751174927, 1.87575364112854, 1.8266496658325195, 1.7919588088989258, 1.7659196853637695, 1.7449504137039185, 1.7270630598068237, 1.7112846374511719, 1.6971074342727661, 1.6841423511505127, 1.672202706336975, 1.6611201763153076, 1.6507599353790283, 1.6410579681396484, 1.6319279670715332, 1.623288631439209, 1.6150697469711304, 1.6072888374328613, 1.5998839139938354, 1.5928291082382202, 1.586137056350708, 1.579771637916565, 1.573726773262024, 1.5679587125778198, 1.5624850988388062, 1.5572737455368042, 1.552305817604065, 1.5475770235061646, 1.543060064315796, 1.538766622543335, 1.5346769094467163, 1.5307828187942505, 1.5270717144012451, 1.52351975440979, 1.5201210975646973, 1.516865611076355, 1.5137478113174438, 1.5107545852661133, 1.5078850984573364, 1.5051316022872925, 1.5024884939193726, 1.4999467134475708, 1.4974985122680664, 1.4951436519622803, 1.4928772449493408, 1.4907015562057495, 1.4886064529418945, 1.4865864515304565, 1.4846352338790894, 1.482753038406372, 1.4809348583221436, 1.4791761636734009, 1.4774786233901978, 1.4758405685424805, 1.4742581844329834, 1.4727329015731812, 1.4712575674057007, 1.4698315858840942, 1.468449592590332, 1.4671159982681274, 1.4658268690109253, 1.4645804166793823, 1.4633772373199463, 1.4622124433517456, 1.4610826969146729, 1.4599932432174683, 1.458938479423523, 1.4579161405563354, 1.456925630569458, 1.4559670686721802, 1.4550387859344482, 1.4541380405426025, 1.4532641172409058, 1.4524167776107788, 1.451596736907959, 1.4507997035980225, 1.450027585029602, 1.44927978515625, 1.4485552310943604, 1.447852373123169, 1.4471698999404907, 1.446509599685669, 1.4458680152893066, 1.445246934890747, 1.4446438550949097, 1.4440584182739258, 1.443490982055664, 1.4429386854171753, 1.4424046277999878, 1.4418864250183105, 1.4413840770721436, 1.4408955574035645, 1.440422534942627, 1.439964771270752, 1.4395191669464111, 1.4390875101089478, 1.4386677742004395, 1.4382609128952026, 1.4378662109375, 1.4374828338623047, 1.4371110200881958, 1.4367505311965942, 1.436400055885315, 1.4360597133636475, 1.4357295036315918, 1.4354093074798584, 1.43509840965271, 1.4347964525222778, 1.4345037937164307, 1.4342193603515625, 1.433943510055542, 1.43367600440979, 1.4334160089492798, 1.4331637620925903, 1.4329193830490112, 1.4326815605163574, 1.4324508905410767, 1.4322270154953003, 1.4320094585418701, 1.4317985773086548, 1.431593418121338, 1.4313946962356567, 1.431201696395874, 1.4310142993927002, 1.4308327436447144, 1.4306563138961792, 1.4304850101470947, 1.4303189516067505, 1.4301573038101196, 1.4300004243850708, 1.4298481941223145, 1.4297006130218506, 1.429556965827942, 1.429417610168457, 1.429282546043396, 1.4291512966156006, 1.42902410030365, 1.4289004802703857, 1.428780436515808, 1.428663969039917, 1.4285507202148438, 1.4284409284591675, 1.4283342361450195, 1.428231120109558, 1.4281307458877563, 1.4280333518981934, 1.42793869972229, 1.4278470277786255, 1.427757978439331, 1.4276715517044067, 1.4275875091552734, 1.4275059700012207, 1.427427053451538, 1.4273502826690674, 1.4272757768630981, 1.4272035360336304, 1.427133560180664, 1.427065372467041, 1.4269994497299194, 1.4269355535507202, 1.4268733263015747, 1.4268131256103516, 1.4267547130584717, 1.4266979694366455, 1.4266431331634521, 1.4265896081924438, 1.4265378713607788, 1.426487684249878, 1.4264390468597412, 1.426391839981079, 1.4263463020324707, 1.4263019561767578, 1.4262590408325195, 1.4262175559997559, 1.4261771440505981, 1.426138162612915, 1.4261001348495483, 1.4260636568069458, 1.4260281324386597, 1.4259939193725586, 1.425960659980774, 1.4259284734725952, 1.425897479057312, 1.4258674383163452, 1.4258382320404053, 1.4258102178573608, 1.4257829189300537, 1.4257566928863525, 1.4257311820983887, 1.4257066249847412, 1.4256829023361206, 1.4256598949432373, 1.4256377220153809, 1.4256163835525513, 1.4255956411361694, 1.4255757331848145, 1.4255565404891968, 1.4255378246307373, 1.4255200624465942, 1.4255026578903198, 1.4254859685897827, 1.425469994544983, 1.4254546165466309, 1.4254395961761475, 1.4254251718521118, 1.425411343574524, 1.4253979921340942, 1.4253851175308228, 1.425372838973999, 1.425360918045044, 1.425349473953247, 1.4253383874893188, 1.4253277778625488, 1.4253177642822266, 1.4253079891204834, 1.4252984523773193, 1.425289511680603, 1.4252809286117554, 1.4252725839614868, 1.425264596939087, 1.4252569675445557, 1.4252495765686035, 1.42524254322052, 1.4252357482910156, 1.4252293109893799, 1.4252229928970337, 1.4252171516418457, 1.4252114295959473, 1.425205945968628, 1.4252008199691772, 1.4251956939697266, 1.4251909255981445, 1.425186276435852, 1.4251818656921387, 1.4251776933670044, 1.4251736402511597, 1.425169825553894, 1.425166130065918, 1.425162434577942, 1.425159215927124, 1.4251558780670166, 1.4251528978347778, 1.4251497983932495, 1.4251471757888794, 1.4251445531845093, 1.4251419305801392, 1.4251394271850586, 1.4251370429992676, 1.4251348972320557, 1.4251328706741333, 1.425130844116211, 1.4251289367675781, 1.4251270294189453, 1.425125241279602, 1.4251235723495483, 1.4251220226287842, 1.42512047290802, 1.425119161605835, 1.4251177310943604, 1.4251164197921753, 1.4251152276992798, 1.4251140356063843, 1.4251128435134888, 1.4251117706298828, 1.4251108169555664, 1.42510986328125, 1.4251089096069336, 1.4251080751419067, 1.4251072406768799, 1.4251065254211426, 1.4251058101654053, 1.425105094909668], "moving_avg_accuracy_train": [0.02530588235294117, 0.053062352941176456, 0.08082905882352939, 0.10784262352941174, 0.13359718470588233, 0.15797864270588233, 0.18087019019999995, 0.20230552412117642, 0.22231732465023524, 0.2409726510087411, 0.2582989153196317, 0.27438431790531553, 0.2893505919971369, 0.3032602386797762, 0.31618362657650445, 0.3282217345070893, 0.3394842669387333, 0.34998289906838936, 0.35985284445566806, 0.3690322658924542, 0.37759492165615, 0.38558366478465267, 0.3930252983061874, 0.39996747435792157, 0.4064295504515412, 0.41248306599462237, 0.4181806417481013, 0.42350375404387947, 0.4285180845218444, 0.4331439231284835, 0.4374530602273998, 0.4414889306752481, 0.4452647434900762, 0.44878532796459797, 0.4520785598740205, 0.4551954097689714, 0.4581182217332507, 0.4608805172069844, 0.4634701125451095, 0.4659419248200103, 0.46829832057330334, 0.47052495910420833, 0.47263481613496394, 0.4746466286391146, 0.4765466716575561, 0.47835553390356517, 0.4801011569837969, 0.4817545706971819, 0.4832826430392284, 0.4847261434411879, 0.4860558820382456, 0.4873161761873622, 0.4885257350392142, 0.48968492624117516, 0.49078702185235173, 0.49184008437299886, 0.49285607593569897, 0.4938881154009526, 0.4948545979785044, 0.4957809028865363, 0.49664751848023564, 0.49746276663221206, 0.49823884291016735, 0.49896084097209176, 0.49966475687488254, 0.5003712223638649, 0.5010376295392431, 0.501675043055907, 0.5022934211032575, 0.5028876084046964, 0.5034459063877561, 0.5039954333960392, 0.5045417724093765, 0.5050640657566741, 0.5055788356515949, 0.506049187380553, 0.5064889745248506, 0.5069059594253067, 0.5073118340710113, 0.5076818271344984, 0.5080454091269309, 0.5083985152730612, 0.508728075510461, 0.5090364444300031, 0.5093422117517087, 0.5096362258706555, 0.5099196621071194, 0.5101912253081721, 0.5104521027773549, 0.5107057160290311, 0.5109574973673044, 0.5111958652776328, 0.5114292199263402, 0.5116580626395886, 0.5118899034344533, 0.5121150307380667, 0.5123341158995541, 0.5125642337213634, 0.5127831044668741, 0.5130036175495984, 0.5132256087358151, 0.5134371655092924, 0.5136393313113043, 0.5138495158272327, 0.514062211303333, 0.5142630489965291, 0.5144626264498173, 0.514658716746012, 0.5148493156596461, 0.515037325270152, 0.5152159456843133, 0.5153767040570585, 0.515530798357235, 0.5156906596979821, 0.5158439466693604, 0.5159866108259538, 0.5161197144492408, 0.5162324488866696, 0.5163550863509438, 0.5164701659511436, 0.5165831493560292, 0.5166989520674852, 0.5168149392136778, 0.51693344529231, 0.5170424537042555, 0.5171452671573593, 0.5172377992651528, 0.5173210781621669, 0.517403087993009, 0.5174792497819434, 0.5175501483331608, 0.5176139570292565, 0.5176666789733897, 0.5177282463701683, 0.5177789511449162, 0.5178363501480717, 0.5178950680744411, 0.5179502671493499, 0.5180070051402973, 0.5180627752145028, 0.5181176741636407, 0.5181647302766884, 0.5182141396019607, 0.5182680197594117, 0.5183188648422941, 0.5183669783580647, 0.5184220452281405, 0.5184739583523853, 0.5185347978112644, 0.5185989650889615, 0.5186590685800654, 0.5187155146632353, 0.5187733749616177, 0.5188348609948676, 0.5188925513659691, 0.5189468256411369, 0.5190003783711409, 0.5190509287693209, 0.5191058358923888, 0.5191528993619734, 0.5191929035434231, 0.5192312602479043, 0.5192728401054668, 0.519310261977273, 0.5193439416618987, 0.5193742533780618, 0.5194062398049615, 0.5194350275891713, 0.5194679954184894, 0.5195023723472287, 0.5195333115830941, 0.519561156895373, 0.5195885706176003, 0.519613242967605, 0.5196425069061387, 0.5196782562155249, 0.5197104305939724, 0.5197440934169281, 0.5197743899575882, 0.5197993039030059, 0.5198311382185876, 0.5198597891026112, 0.5198879278394088, 0.5199132527025268, 0.5199360450793329, 0.5199565582184584, 0.5199726671024949, 0.5199871650981278, 0.5200002132941974, 0.5200143096118365, 0.5200269962977117, 0.5200384143149993, 0.5200510434717347, 0.520064762653973, 0.5200818158003404, 0.5200995165732476, 0.520115447268864, 0.5201297848949188, 0.5201403358171917, 0.5201498316472373, 0.5201607308354548, 0.5201705401048505, 0.5201840743296595, 0.5201962551319876, 0.5202095707952594, 0.5202215548922041, 0.5202346935206308, 0.5202418124038618, 0.5202482193987698, 0.5202563386353634, 0.5202659988894741, 0.5202723401769973, 0.5202804002769447, 0.5202876543668973, 0.5202918301066781, 0.5202979412136574, 0.5203034412099387, 0.5203083912065919, 0.5203151991447562, 0.5203213262891041, 0.5203268407190172, 0.520331803705939, 0.5203362703941686, 0.5203426433547518, 0.5203507319604531, 0.5203603646467607, 0.5203690340644376, 0.5203768365403467, 0.520383858768665, 0.5203901787741514, 0.5203935138379128, 0.520396515395298, 0.5203992167969447, 0.5204016480584267, 0.5204038361937605, 0.5204058055155609, 0.5204099308463578, 0.5204136436440749, 0.5204193381031968, 0.5204244631164066, 0.5204290756282953, 0.5204332268889952, 0.5204369630236251, 0.520440325544792, 0.5204433518138423, 0.5204460754559874, 0.5204485267339181, 0.5204507328840556, 0.5204527184191795, 0.520454505400791, 0.5204537607430648, 0.5204530905511112, 0.520452487378353, 0.5204519445228707, 0.5204514559529365, 0.5204533691811722, 0.5204550910865845, 0.5204566408014555, 0.5204580355448394, 0.5204592908138849, 0.5204604205560258, 0.5204614373239527, 0.5204647053562633, 0.5204699995265193, 0.5204747642797497, 0.520479052557657, 0.5204829120077736, 0.5204863855128786, 0.5204895116674731, 0.5204923252066082, 0.5204948573918297, 0.520497136358529, 0.5204991874285585, 0.5205010333915849], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.025159999999999995, 0.05227066666666666, 0.07999026666666666, 0.10696457333333333, 0.13257478266666667, 0.1564773044, 0.17874957395999996, 0.1996079498973333, 0.21915382157426663, 0.2374917727501733, 0.2543959288084893, 0.27035633592764036, 0.2852407023348763, 0.299089965434722, 0.31199430222458313, 0.3239548720021248, 0.33511938480191233, 0.34535411298838775, 0.3548320350228823, 0.3636954981872607, 0.3719126150352013, 0.37973468686501444, 0.38709455151184635, 0.3939850963606617, 0.40039992005792885, 0.40633326138546927, 0.411926601913589, 0.4172006083888968, 0.4221605475500071, 0.4268111594616731, 0.43129004351550576, 0.43542770583062185, 0.4392849352475597, 0.4429164417228037, 0.44631813088385663, 0.4496996511288043, 0.4527696860159238, 0.45575938408099814, 0.45855677900623165, 0.4612077677722751, 0.4636869909950476, 0.4660116252288762, 0.4681437960393219, 0.47020941643538966, 0.4721351414585173, 0.4740416273126656, 0.47585079791473234, 0.4775190514565924, 0.47904714631093315, 0.48042243167983983, 0.48175352184518916, 0.48300483632733693, 0.4841310193612699, 0.4851979174251429, 0.4862114590159619, 0.4872436464476991, 0.4881326151362625, 0.4889593536226362, 0.4897700849270393, 0.4905530764343353, 0.4912977687909018, 0.49204799191181164, 0.49274985938729715, 0.49343487344856746, 0.49409138610371073, 0.49466891416000636, 0.49533535607733903, 0.4960284871362718, 0.4967189717559779, 0.4973270745803801, 0.49795436712234215, 0.4984655970767746, 0.49896570403576385, 0.49941580029885413, 0.49980755360230206, 0.5001734649087385, 0.5005694517511979, 0.5009791732427449, 0.5014012559184704, 0.5017811303266233, 0.5021496839606276, 0.5024547155645649, 0.5027825773414417, 0.5031443196072976, 0.5034565543132344, 0.5037375655485776, 0.5039904756603866, 0.5042447614276813, 0.5044736186182465, 0.5046929234230885, 0.5048636310807796, 0.5050172679727016, 0.5051555411754315, 0.5052933203912217, 0.5054173216854329, 0.5055022561835563, 0.5056186972318674, 0.5057368275086807, 0.505856478091146, 0.5059774969486981, 0.5061397472538283, 0.5063124391951122, 0.5064145286089343, 0.5064930757480408, 0.5065637681732367, 0.5066540580225797, 0.5067353188869883, 0.5068217869982895, 0.5068996082984606, 0.5069829808019478, 0.507084682721753, 0.507189547782911, 0.5072972596712866, 0.5073942003708246, 0.5074814470004089, 0.5075466356337013, 0.5075919720703311, 0.507632774863298, 0.5076828307103016, 0.5077278809726048, 0.5077684262086777, 0.5078315835878099, 0.5078750918956956, 0.5079142493727927, 0.5079761577688467, 0.5080452086586287, 0.5081073544594324, 0.5081632856801559, 0.5082002904454737, 0.5082735947342596, 0.508339568594167, 0.5083989450680836, 0.5084657172279419, 0.5085258121718144, 0.5085665642879662, 0.5085899078591696, 0.5086109170732527, 0.5086298253659274, 0.5086868428293346, 0.5087381585464011, 0.5088110093584277, 0.508889908422585, 0.5089609175803265, 0.5090248258222938, 0.5090690099067311, 0.5091087755827247, 0.5091445646911189, 0.5091767748886737, 0.5092190973998063, 0.5092838543264924, 0.5093421355605098, 0.5093945886711254, 0.5094417964706796, 0.5094976168236116, 0.5095478551412504, 0.5096064029604588, 0.5096590959977463, 0.5097198530646383, 0.5097745344248411, 0.5098104143156903, 0.5098427062174546, 0.5098851022623758, 0.5099365920361383, 0.5099829328325245, 0.5100113062159387, 0.5100368422610115, 0.5100598247015771, 0.510080508898086, 0.510099124674944, 0.5101158788741162, 0.5101309576533712, 0.5101445285547007, 0.5101567423658973, 0.5101677347959742, 0.5101776279830434, 0.5101865318514057, 0.5101945453329318, 0.5102017574663054, 0.5102082483863415, 0.510214090214374, 0.5102193478596033, 0.5102240797403096, 0.5102283384329452, 0.5102321712563174, 0.5102356207973523, 0.5102387253842837, 0.510241519512522, 0.5102440342279364, 0.5102462974718094, 0.5102483343912951, 0.5102501676188322, 0.5102518175236157, 0.5102533024379208, 0.5102546388607954, 0.5102558416413825, 0.5102569241439109, 0.5102578983961864, 0.5102587752232344, 0.5102595643675776, 0.5102602745974865, 0.5102609138044044, 0.5102614890906306, 0.5102753401815676, 0.5102878061634109, 0.510312358880403, 0.5103344563256961, 0.5103543440264597, 0.5103722429571471, 0.5103883519947657, 0.5104028501286225, 0.5104158984490936, 0.5104276419375176, 0.5104382110770991, 0.5104610566360559, 0.5104682843057836, 0.5104747892085386, 0.5104806436210181, 0.5104859125922496, 0.510490654666358, 0.5105082558663889, 0.5105240969464168, 0.5105516872517751, 0.5105765185265976, 0.5106122000072711, 0.5106443133398774, 0.5106732153392229, 0.5106992271386339, 0.5107226377581038, 0.5107437073156267, 0.5107626699173973, 0.5107797362589909, 0.5107950959664251, 0.510808919703116, 0.5108213610661378, 0.5108325582928575, 0.510842635796905, 0.5108517055505479, 0.5108598683288265, 0.5108672148292772, 0.5108738266796828, 0.510879777345048, 0.5108851329438765, 0.5108899529828221, 0.5108942910178733, 0.5108981952494194, 0.5109017090578107, 0.510904871485363, 0.51090771767016, 0.5109102792364772, 0.5109125846461628, 0.5109146595148799, 0.5109165268967253, 0.5109182075403861, 0.5109197201196808, 0.5109210814410461, 0.5109223066302748, 0.5109234093005807, 0.5109244017038559, 0.5109252948668037, 0.5109260987134567, 0.5109268221754444, 0.5109274732912332, 0.5109280592954433, 0.5109285866992324, 0.5109290613626425, 0.5109294885597115, 0.5109298730370737, 0.5109302190666997, 0.510930530493363, 0.51093081077736, 0.5109310630329573, 0.510931290062995, 0.5109314943900288, 0.5109316782843593], "moving_var_accuracy_train": [0.005763489134948096, 0.012120935157093424, 0.01784775124139792, 0.022630570220328165, 0.026337189990828444, 0.029053570439597474, 0.030864419915716722, 0.0319132397869561, 0.03232616525199404, 0.03222573954067592, 0.03170496050134589, 0.030863126038303035, 0.02979271767619645, 0.028554750346099517, 0.02720240090405329, 0.02578640519658365, 0.024349366407889785, 0.022906421256445427, 0.021492521528331664, 0.020101625376725656, 0.01875133450260106, 0.017450581203299738, 0.01620392426818923, 0.015017276116369754, 0.013891374351690348, 0.012832042370394241, 0.011840999458554498, 0.010911919233320225, 0.010047018901268441, 0.009234902456473652, 0.008478530173661602, 0.007777271408741776, 0.0071278551295811815, 0.006526620252003258, 0.005971566614486083, 0.005461842732446373, 0.004992543927208546, 0.004561962021045582, 0.004166119855078177, 0.0038044965728715125, 0.003474020324099601, 0.0031712395640154365, 0.0028941790778259538, 0.0026411876760100714, 0.0024095603796564185, 0.0021980521853161104, 0.0020056717662286384, 0.0018297085817742613, 0.0016677527693395831, 0.0015197307330997404, 0.0013836715024183105, 0.0012595994242571575, 0.0011468067753762838, 0.0010442196160229884, 0.0009507291870462623, 0.0008656367343931622, 0.0007883632106531463, 0.0007191128387084008, 0.0006556083519919612, 0.0005977698838365612, 0.0005447520987380903, 0.0004962585548079903, 0.0004520533488300354, 0.0004115395447598357, 0.00037484506866766856, 0.00034185240318500916, 0.0003116640495770684, 0.00028415430854039436, 0.00025918040037135984, 0.00023643988727694565, 0.00021560116829024862, 0.00019675887085671807, 0.00017976936062849514, 0.00016424753763132806, 0.0001502076762706465, 0.00013717798538398726, 0.00012520090143619376, 0.00011424569895744993, 0.00010430373711393766, 9.510541720580108e-05, 8.678460227221177e-05, 7.922829759890648e-05, 7.228295738969073e-05, 6.591048416557792e-05, 6.016087864422712e-05, 5.492278949906488e-05, 5.0153535450425575e-05, 4.580190105487729e-05, 4.183422443473455e-05, 3.8229679124093424e-05, 3.497725579240862e-05, 3.19909035592366e-05, 2.9281902731972727e-05, 2.6825033345437168e-05, 2.4626281398364772e-05, 2.261979398401884e-05, 2.0789799357472775e-05, 1.9187407328953794e-05, 1.7699806225222194e-05, 1.6367459779573346e-05, 1.5174234582436798e-05, 1.4059617539830277e-05, 1.3021494889375403e-05, 1.2116943177062338e-05, 1.1312403149337733e-05, 1.0544184845479012e-05, 9.848246799680395e-06, 9.209484758068027e-06, 8.615487795167684e-06, 8.07206753843442e-06, 7.552008055787262e-06, 7.029396539877514e-06, 6.540162366011721e-06, 6.116146963799636e-06, 5.716004327768555e-06, 5.327581449180192e-06, 4.954272475051267e-06, 4.57322670798776e-06, 4.25126356598146e-06, 3.94532703882248e-06, 3.6656815829561962e-06, 3.4198058364854247e-06, 3.19890241557415e-06, 3.0054053900718114e-06, 2.811810355938478e-06, 2.6257647755969055e-06, 2.4402480167915638e-06, 2.2586415873034344e-06, 2.093307939765887e-06, 1.9361827086324856e-06, 1.7878038788518032e-06, 1.6456674382435e-06, 1.5061171249577803e-06, 1.3896203115769259e-06, 1.2737970480593197e-06, 1.1760691533225223e-06, 1.089492391884262e-06, 1.007965593532952e-06, 9.36141830730343e-07, 8.705203582493562e-07, 8.105933739724833e-07, 7.494625365516525e-07, 6.964878157112919e-07, 6.52966676442655e-07, 6.109370108782134e-07, 5.706775033886599e-07, 5.409009946693826e-07, 5.110656474221228e-07, 4.932720404902397e-07, 4.81001792184703e-07, 4.6541347975206367e-07, 4.475475745239057e-07, 4.3292314423156437e-07, 4.2365562037178115e-07, 4.1124366859503126e-07, 3.9663057424043707e-07, 3.827785708342852e-07, 3.674987985562902e-07, 3.578820481730084e-07, 3.4202857487981854e-07, 3.2222872819299427e-07, 3.032469863816326e-07, 2.8848224873774023e-07, 2.7223759226932843e-07, 2.5522272345074033e-07, 2.3796965233642554e-07, 2.2338087065507737e-07, 2.0850141226692653e-07, 1.9743317096979854e-07, 1.8832581293873276e-07, 1.7810835848824824e-07, 1.6727577538256685e-07, 1.573118073415655e-07, 1.470591503001862e-07, 1.4006063815666185e-07, 1.3755669243526327e-07, 1.3311773884809218e-07, 1.3000463580740732e-07, 1.252650956104083e-07, 1.1832492813582812e-07, 1.1561324815928336e-07, 1.1143978174135669e-07, 1.0742190014435522e-07, 1.0245184835740076e-07, 9.688209548592129e-08, 9.098098582838022e-08, 8.421835254965969e-08, 7.768824419102882e-08, 7.145171855795627e-08, 6.609490224099727e-08, 6.093398000335559e-08, 5.601392207205105e-08, 5.1847990263463734e-08, 4.8357134888705185e-08, 4.6138709609087665e-08, 4.434469490176809e-08, 4.21943089769972e-08, 3.982498576727633e-08, 3.6844384837812576e-08, 3.397148344831394e-08, 3.16434658376766e-08, 2.9345115148605097e-08, 2.8059180804386236e-08, 2.6588610232168742e-08, 2.5525511204271077e-08, 2.4265527300052477e-08, 2.3392586582448048e-08, 2.150943441031788e-08, 1.9727937223039005e-08, 1.8348441526503476e-08, 1.7353481959206223e-08, 1.5980041110350464e-08, 1.496672389976323e-08, 1.3943647899149254e-08, 1.270621433369191e-08, 1.1771703556928804e-08, 1.0866782833087029e-08, 1.00006267515776e-08, 9.417696274865746e-09, 8.813803728120616e-09, 8.206103790709111e-09, 7.607174564312772e-09, 7.026018841548126e-09, 6.6889485967408824e-09, 6.608883616785664e-09, 6.783093064617389e-09, 6.781212983859256e-09, 6.650999358293589e-09, 6.429704637448772e-09, 6.146216397841591e-09, 5.6316986106867445e-09, 5.1496128702476936e-09, 4.700329720932829e-09, 4.283496040384443e-09, 3.898237862497296e-09, 3.54331813143012e-09, 3.342151505937623e-09, 3.132000157340741e-09, 3.110641923827568e-09, 3.0359695750439056e-09, 2.9238500108548227e-09, 2.7865616983547904e-09, 2.6335338462736007e-09, 2.4719393990270655e-09, 2.307170198402605e-09, 2.1432172173777332e-09, 1.98297436708058e-09, 1.8284808162395635e-09, 1.6811138821679264e-09, 1.5417422234684767e-09, 1.3925586372837973e-09, 1.2573451888467434e-09, 1.1348850263480127e-09, 1.0240487523858379e-09, 9.237921823720696e-10, 8.643569446739604e-10, 8.046058744431705e-10, 7.45759832630377e-10, 6.886916313287991e-10, 6.340037715846501e-10, 5.820902501710207e-10, 5.331855783071926e-10, 5.759873371250736e-10, 7.70642751710452e-10, 8.979043366609852e-10, 9.736178496936461e-10, 1.0103142615506408e-09, 1.0178699748250665e-09, 1.0040385602803427e-09, 9.748787264319219e-10, 9.350985117543769e-10, 8.883318635312575e-10, 8.373606715695028e-10, 7.842928198696307e-10], "duration": 39827.572696, "accuracy_train": [0.2530588235294118, 0.3028705882352941, 0.3307294117647059, 0.35096470588235296, 0.3653882352941176, 0.37741176470588234, 0.38689411764705883, 0.3952235294117647, 0.4024235294117647, 0.4088705882352941, 0.41423529411764703, 0.41915294117647056, 0.4240470588235294, 0.4284470588235294, 0.4324941176470588, 0.43656470588235297, 0.4408470588235294, 0.4444705882352941, 0.4486823529411765, 0.4516470588235294, 0.4546588235294118, 0.45748235294117645, 0.46, 0.46244705882352943, 0.46458823529411764, 0.46696470588235295, 0.46945882352941176, 0.47141176470588236, 0.4736470588235294, 0.4747764705882353, 0.47623529411764703, 0.4778117647058824, 0.4792470588235294, 0.4804705882352941, 0.48171764705882353, 0.4832470588235294, 0.4844235294117647, 0.48574117647058823, 0.4867764705882353, 0.48818823529411765, 0.4895058823529412, 0.49056470588235296, 0.4916235294117647, 0.4927529411764706, 0.49364705882352944, 0.49463529411764706, 0.49581176470588234, 0.49663529411764706, 0.4970352941176471, 0.49771764705882354, 0.4980235294117647, 0.49865882352941177, 0.49941176470588233, 0.5001176470588236, 0.5007058823529412, 0.5013176470588235, 0.502, 0.5031764705882353, 0.5035529411764705, 0.5041176470588236, 0.5044470588235294, 0.5048, 0.5052235294117647, 0.5054588235294117, 0.506, 0.5067294117647059, 0.507035294117647, 0.5074117647058823, 0.5078588235294118, 0.508235294117647, 0.5084705882352941, 0.5089411764705882, 0.5094588235294117, 0.5097647058823529, 0.5102117647058824, 0.5102823529411765, 0.5104470588235294, 0.5106588235294117, 0.510964705882353, 0.5110117647058824, 0.5113176470588235, 0.5115764705882353, 0.5116941176470589, 0.5118117647058823, 0.5120941176470588, 0.5122823529411765, 0.5124705882352941, 0.5126352941176471, 0.5128, 0.5129882352941176, 0.5132235294117647, 0.5133411764705882, 0.5135294117647059, 0.5137176470588235, 0.5139764705882353, 0.5141411764705882, 0.5143058823529412, 0.5146352941176471, 0.5147529411764706, 0.5149882352941176, 0.5152235294117647, 0.5153411764705882, 0.5154588235294117, 0.5157411764705883, 0.5159764705882353, 0.5160705882352942, 0.5162588235294118, 0.5164235294117647, 0.5165647058823529, 0.5167294117647059, 0.5168235294117647, 0.5168235294117647, 0.5169176470588235, 0.5171294117647058, 0.5172235294117647, 0.5172705882352941, 0.5173176470588235, 0.5172470588235294, 0.5174588235294117, 0.5175058823529411, 0.5176, 0.5177411764705883, 0.5178588235294118, 0.518, 0.5180235294117647, 0.5180705882352942, 0.5180705882352942, 0.5180705882352942, 0.5181411764705882, 0.518164705882353, 0.5181882352941176, 0.5181882352941176, 0.5181411764705882, 0.5182823529411764, 0.518235294117647, 0.5183529411764706, 0.5184235294117647, 0.5184470588235294, 0.5185176470588235, 0.5185647058823529, 0.5186117647058823, 0.5185882352941177, 0.5186588235294117, 0.5187529411764706, 0.5187764705882353, 0.5188, 0.5189176470588235, 0.5189411764705882, 0.5190823529411764, 0.5191764705882352, 0.5192, 0.5192235294117648, 0.5192941176470588, 0.5193882352941176, 0.5194117647058824, 0.5194352941176471, 0.5194823529411765, 0.5195058823529412, 0.5196, 0.5195764705882353, 0.5195529411764706, 0.5195764705882353, 0.5196470588235295, 0.5196470588235295, 0.5196470588235295, 0.5196470588235295, 0.5196941176470589, 0.5196941176470589, 0.5197647058823529, 0.5198117647058823, 0.5198117647058823, 0.5198117647058823, 0.5198352941176471, 0.5198352941176471, 0.5199058823529412, 0.52, 0.52, 0.5200470588235294, 0.5200470588235294, 0.5200235294117647, 0.5201176470588236, 0.5201176470588236, 0.5201411764705882, 0.5201411764705882, 0.5201411764705882, 0.5201411764705882, 0.5201176470588236, 0.5201176470588236, 0.5201176470588236, 0.5201411764705882, 0.5201411764705882, 0.5201411764705882, 0.520164705882353, 0.5201882352941176, 0.520235294117647, 0.5202588235294118, 0.5202588235294118, 0.5202588235294118, 0.520235294117647, 0.520235294117647, 0.5202588235294118, 0.5202588235294118, 0.5203058823529412, 0.5203058823529412, 0.5203294117647059, 0.5203294117647059, 0.5203529411764706, 0.5203058823529412, 0.5203058823529412, 0.5203294117647059, 0.5203529411764706, 0.5203294117647059, 0.5203529411764706, 0.5203529411764706, 0.5203294117647059, 0.5203529411764706, 0.5203529411764706, 0.5203529411764706, 0.5203764705882353, 0.5203764705882353, 0.5203764705882353, 0.5203764705882353, 0.5203764705882353, 0.5204, 0.5204235294117647, 0.5204470588235294, 0.5204470588235294, 0.5204470588235294, 0.5204470588235294, 0.5204470588235294, 0.5204235294117647, 0.5204235294117647, 0.5204235294117647, 0.5204235294117647, 0.5204235294117647, 0.5204235294117647, 0.5204470588235294, 0.5204470588235294, 0.5204705882352941, 0.5204705882352941, 0.5204705882352941, 0.5204705882352941, 0.5204705882352941, 0.5204705882352941, 0.5204705882352941, 0.5204705882352941, 0.5204705882352941, 0.5204705882352941, 0.5204705882352941, 0.5204705882352941, 0.5204470588235294, 0.5204470588235294, 0.5204470588235294, 0.5204470588235294, 0.5204470588235294, 0.5204705882352941, 0.5204705882352941, 0.5204705882352941, 0.5204705882352941, 0.5204705882352941, 0.5204705882352941, 0.5204705882352941, 0.5204941176470588, 0.5205176470588235, 0.5205176470588235, 0.5205176470588235, 0.5205176470588235, 0.5205176470588235, 0.5205176470588235, 0.5205176470588235, 0.5205176470588235, 0.5205176470588235, 0.5205176470588235, 0.5205176470588235], "end": "2016-02-04 11:52:37.902000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0, 256.0, 257.0, 258.0, 259.0, 260.0, 261.0, 262.0, 263.0, 264.0, 265.0, 266.0, 267.0, 268.0, 269.0, 270.0, 271.0, 272.0, 273.0], "moving_var_accuracy_valid": [0.005697230399999999, 0.011742401583999997, 0.017483547443039996, 0.0222837116800624, 0.02595828591093054, 0.028504432226749737, 0.030118474926254486, 0.03102227405431717, 0.03135841654538576, 0.031249098970816614, 0.030695943502130067, 0.029918960510598492, 0.028920963729642334, 0.02775508615235683, 0.02647827470899662, 0.02511794430272795, 0.02372796698696473, 0.022297917237727567, 0.020876604568782427, 0.019495992925300826, 0.018154082716405125, 0.01688933771416143, 0.015687912411322457, 0.014546437645012036, 0.013462143548114015, 0.01243277004708451, 0.01147106216674762, 0.010574292248787153, 0.00973827199224568, 0.008959098513397476, 0.00824373228336682, 0.0075734412999355245, 0.006950001138915985, 0.006373691578542041, 0.0058404658230236635, 0.005359331353224216, 0.004908224245774976, 0.004497846471880259, 0.004118490590001734, 0.00376989120394076, 0.0034482210136416937, 0.0031520342311673138, 0.002877746179334833, 0.002628372649987212, 0.0023989111367707927, 0.0021917322179023204, 0.0020020168805185323, 0.001826862821386038, 0.0016651922042021988, 0.0015156956723953383, 0.0013800723144104124, 0.0012561571743684656, 0.0011419560509648853, 0.0010380048891766606, 0.0009434497992658746, 0.0008586935173874121, 0.0007799365536118864, 0.0007080943669743619, 0.0006432004975083773, 0.0005843981290620195, 0.0005309494165091742, 0.00048291998743858587, 0.00043906155027302683, 0.00039937859362296625, 0.0003633198140579388, 0.000329989680554422, 0.0003009880159615825, 0.0002752130903491395, 0.00025198270240468237, 0.00023011253356962767, 0.00021064274361147514, 0.00019193067384710866, 0.0001749885691962626, 0.00015931299209106685, 0.00014476292873882136, 0.00013149165562254155, 0.0001197537402748967, 0.00010928921155312558, 9.996367446414148e-05, 9.12660481114534e-05, 8.336192933054824e-05, 7.586313491209811e-05, 6.924426152351991e-05, 6.349755257332647e-05, 5.802521192031741e-05, 5.2933396557787784e-05, 4.8215728623905715e-05, 4.3976107024552896e-05, 4.0049876845158184e-05, 3.647774053748356e-05, 3.30922364232848e-05, 2.9995451431991493e-05, 2.7167981596130827e-05, 2.4622031447251557e-05, 2.229821519122079e-05, 2.0133318492841938e-05, 1.8242013303143715e-05, 1.6543404833529154e-05, 1.5017910707134736e-05, 1.3647929711370127e-05, 1.2520063193866695e-05, 1.1536459433739456e-05, 1.0476613726096393e-05, 9.484479231043196e-06, 8.581008078759603e-06, 7.796277582933069e-06, 7.076079777399802e-06, 6.4357624081077505e-06, 5.846691560139794e-06, 5.324581173165444e-06, 4.885212580277508e-06, 4.4956614517148586e-06, 4.150511964620238e-06, 3.8200382612004745e-06, 3.506542204444651e-06, 3.1941340051950205e-06, 2.8932191370521696e-06, 2.6188810345720395e-06, 2.379543221487998e-06, 2.159854634541458e-06, 1.9586644166011415e-06, 1.7986976657906967e-06, 1.6358646549072993e-06, 1.4860779615300618e-06, 1.37196401089493e-06, 1.2776798382226102e-06, 1.1846707594182655e-06, 1.0943583965409516e-06, 9.972467307928437e-07, 9.458837265033127e-07, 8.904683055726844e-07, 8.331515659083765e-07, 7.899631013068316e-07, 7.43469411687476e-07, 6.840690852564512e-07, 6.20566477579553e-07, 5.624823135090816e-07, 5.094517939450352e-07, 4.877655347511171e-07, 4.626887066384812e-07, 4.6418500329097843e-07, 4.737920638858648e-07, 4.717935618457049e-07, 4.613725761833621e-07, 4.3280541842307323e-07, 4.0375665746578125e-07, 3.749087342360634e-07, 3.467553322511117e-07, 3.282005535631483e-07, 3.3312163419122034e-07, 3.303797909194518e-07, 3.2210377114686686e-07, 3.0995058108086384e-07, 3.0699872918589994e-07, 2.9901385329993874e-07, 2.999630921763594e-07, 2.949557885659148e-07, 2.9868300030523526e-07, 2.957251606574027e-07, 2.777389436978323e-07, 2.5934995160404623e-07, 2.4959177806828636e-07, 2.484933714803969e-07, 2.42971259019694e-07, 2.2591957309505522e-07, 2.091964221671866e-07, 1.9303051311959267e-07, 1.7757798567462493e-07, 1.6293911143942712e-07, 1.4917152900461987e-07, 1.3630070235855858e-07, 1.2432815638876873e-07, 1.1323793540540672e-07, 1.030016435358334e-07, 9.358235553573353e-08, 8.493762982848227e-08, 7.702180982115499e-08, 6.9787762649211e-08, 6.318817477052862e-08, 5.7176499886328995e-08, 5.1707635397906944e-08, 4.673838811328721e-08, 4.222777746864746e-08, 3.813721453680101e-08, 3.443058708328576e-08, 3.107427451509075e-08, 2.803711143709008e-08, 2.529031443592306e-08, 2.2807383447789887e-08, 2.056398647193283e-08, 1.853783433356636e-08, 1.670855057235947e-08, 1.505754024956482e-08, 1.3567860459505814e-08, 1.222409454382219e-08, 1.101223139495623e-08, 9.919550762928852e-09, 8.934515117685336e-09, 8.046668345066849e-09, 7.246541349271732e-09, 6.5255644837009835e-09, 5.875986623509631e-09, 7.01506244244679e-09, 7.712162528044867e-09, 1.2366469480552189e-08, 1.552449632880042e-08, 1.7531732470927275e-08, 1.866190470159063e-08, 1.9131224068413673e-08, 1.9109864629527327e-08, 1.8731206170617337e-08, 1.8099271236830395e-08, 1.7294704516600827e-08, 2.0262510141362017e-08, 1.8706412014465382e-08, 1.72165946516828e-08, 1.5803402495832155e-08, 1.4472920766796105e-08, 1.3228014091759498e-08, 1.469343286532307e-08, 1.5482547926808262e-08, 2.0785317682017792e-08, 2.4256115797607313e-08, 3.328901678537677e-08, 3.924151028653887e-08, 4.283528935344244e-08, 4.4641283795501994e-08, 4.510966935165209e-08, 4.459403870440677e-08, 4.3370857227183616e-08, 4.1655111642972834e-08, 3.96128859908665e-08, 3.737145865665317e-08, 3.502740041553341e-08, 3.2653061349860564e-08, 3.030176000533731e-08, 2.801192788507794e-08, 2.581041363959198e-08, 2.3715111895480052e-08, 2.173704979800787e-08, 1.988203858278797e-08, 1.815197667381938e-08, 1.654587398537897e-08, 1.5060653519783644e-08, 1.3691775383488352e-08, 1.2433719489842745e-08, 1.1280356073068147e-08, 1.0225227376851362e-08, 9.261759237149472e-09, 8.383417537801096e-09, 7.583821505757906e-09, 6.856823389788939e-09, 6.196562118841541e-09, 5.597496972062947e-09, 5.054426037592122e-09, 4.5624932316485786e-09, 4.117186844714414e-09, 3.7143319385898877e-09, 3.3500784051918665e-09, 3.0208860896459626e-09, 2.7235080559096976e-09, 2.454972816253719e-09, 2.212566143035699e-09, 1.993812921542028e-09, 1.796459377563794e-09, 1.618455915829951e-09, 1.457940729825226e-09, 1.3132242853611095e-09, 1.1827747359249175e-09, 1.065204294403372e-09, 9.592565609405145e-10, 8.63794788588216e-10, 7.777910555602018e-10, 7.003163041271378e-10], "accuracy_test": 0.5171, "start": "2016-02-04 00:48:50.329000", "learning_rate_per_epoch": [0.00016534315363969654, 0.00016053780564107, 0.00015587211237289011, 0.00015134202840272337, 0.00014694359560962766, 0.00014267300139181316, 0.0001385265204589814, 0.00013450054393615574, 0.00013059157936368138, 0.00012679622159339488, 0.0001231111673405394, 0.00011953320790780708, 0.00011605922918533906, 0.00011268621892668307, 0.00010941123764496297, 0.000106231433164794, 0.00010314404062228277, 0.00010014638246502727, 9.723583934828639e-05, 9.4409886514768e-05, 9.166606469079852e-05, 8.900198736228049e-05, 8.641533349873498e-05, 8.390385482925922e-05, 8.146536856656894e-05, 7.909775013104081e-05, 7.679894042667001e-05, 7.456694584107026e-05, 7.239981641760096e-05, 7.029566768324003e-05, 6.82526733726263e-05, 6.626905815210193e-05, 6.434309034375474e-05, 6.247309647733346e-05, 6.065745037631132e-05, 5.889457315788604e-05, 5.718292959500104e-05, 5.552103175432421e-05, 5.390743172029033e-05, 5.234072887105867e-05, 5.081955896457657e-05, 4.934259777655825e-05, 4.79085611004848e-05, 4.65162011096254e-05, 4.5164306357037276e-05, 4.3851701775565743e-05, 4.2577245039865375e-05, 4.1339830204378814e-05, 4.0138376789400354e-05, 3.897184069501236e-05, 3.7839206925127655e-05, 3.673949322546832e-05, 3.567173916962929e-05, 3.463501707301475e-05, 3.3628424716880545e-05, 3.2651085348334163e-05, 3.1702151318313554e-05, 3.078079680562951e-05, 2.9886219635955058e-05, 2.901764128182549e-05, 2.817430504364893e-05, 2.7355479687685147e-05, 2.6560452170087956e-05, 2.5788529455894604e-05, 2.5039040338015184e-05, 2.4311333618243225e-05, 2.3604776288266294e-05, 2.291875352966599e-05, 2.2252668713917956e-05, 2.160594340239186e-05, 2.09780137083726e-05, 2.0368332116049714e-05, 1.977636929950677e-05, 1.9201612303731963e-05, 1.864355908764992e-05, 1.810172398108989e-05, 1.7575635865796357e-05, 1.706483817542903e-05, 1.6568885257584043e-05, 1.6087346011772752e-05, 1.5619802070432343e-05, 1.5165845979936421e-05, 1.4725083019584417e-05, 1.4297130292106885e-05, 1.3881614904676098e-05, 1.3478175787895452e-05, 1.3086461876810063e-05, 1.2706132110906765e-05, 1.2336855434114113e-05, 1.1978310794802383e-05, 1.1630187145783566e-05, 1.1292180715827271e-05, 1.0963997738144826e-05, 1.0645352631399874e-05, 1.0335967999708373e-05, 1.0035575542133301e-05, 9.743913324200548e-06, 9.460727596888319e-06, 9.185771887132432e-06, 8.918806997826323e-06, 8.659601007821038e-06, 8.407928362430539e-06, 8.163569873431697e-06, 7.926313628559e-06, 7.695952263020445e-06, 7.472285687981639e-06, 7.2551197263237555e-06, 7.044265203148825e-06, 6.839538855274441e-06, 6.640762421739055e-06, 6.447763098549331e-06, 6.26037262918544e-06, 6.078428214095766e-06, 5.901771601202199e-06, 5.730249085900141e-06, 5.5637115110585e-06, 5.402014267019695e-06, 5.24501638210495e-06, 5.092581432109e-06, 4.944576630805386e-06, 4.800873284693807e-06, 4.66134633825277e-06, 4.525874373939587e-06, 4.394339612190379e-06, 4.2666274566727225e-06, 4.1426269490330014e-06, 4.022230314149056e-06, 3.905332960130181e-06, 3.791832796196104e-06, 3.6816313695453573e-06, 3.574632728486904e-06, 3.4707436498138122e-06, 3.369873866176931e-06, 3.2719356113375397e-06, 3.176843847541022e-06, 3.0845155833958415e-06, 2.994870783368242e-06, 2.907831230913871e-06, 2.8233214379724814e-06, 2.7412677354732295e-06, 2.6615987280820264e-06, 2.5842450668278616e-06, 2.509139449102804e-06, 2.4362166186620016e-06, 2.3654131382500054e-06, 2.2966673896007705e-06, 2.2299195734376553e-06, 2.165111709473422e-06, 2.10218740903656e-06, 2.041091875071288e-06, 1.9817719021375524e-06, 1.9241758764110273e-06, 1.8682537756831152e-06, 1.8139569419872714e-06, 1.7612381952858414e-06, 1.7100516060963855e-06, 1.660352609178517e-06, 1.6120980035339016e-06, 1.5652458387194201e-06, 1.5197553011603304e-06, 1.4755868278371054e-06, 1.4327021062854328e-06, 1.3910637335357023e-06, 1.3506354434866807e-06, 1.311382106905512e-06, 1.2732696177408798e-06, 1.2362647794361692e-06, 1.200335418616305e-06, 1.165450271400914e-06, 1.1315789834043244e-06, 1.0986921097355662e-06, 1.0667610013115336e-06, 1.0357579185438226e-06, 1.0056558039650554e-06, 9.764286232893937e-07, 9.480507969783503e-07, 9.204977118315583e-07, 8.937454367696773e-07, 8.677706659909745e-07, 8.425507758147432e-07, 8.180638246813032e-07, 7.942885531520005e-07, 7.712042702223698e-07, 7.487908533221344e-07, 7.270288620020438e-07, 7.058993105601985e-07, 6.853838385723066e-07, 6.654645972048456e-07, 6.46124306058482e-07, 6.273460826378141e-07, 6.091136128816288e-07, 5.914110374760639e-07, 5.742229518546083e-07, 5.575344061981014e-07, 5.413308485913149e-07, 5.255982387097902e-07, 5.103228772895818e-07, 4.954914629706764e-07, 4.810910922969924e-07, 4.6710920287296176e-07, 4.53533687050367e-07, 4.403526929763757e-07, 4.275547951237968e-07, 4.1512882376082416e-07, 4.0306400705958367e-07, 3.913498289875861e-07, 3.7997608615114586e-07, 3.6893288779538125e-07, 3.582106558042142e-07, 3.4780003943524207e-07, 3.376919721631566e-07, 3.278776716797438e-07, 3.183486114721745e-07, 3.090964924012951e-07, 3.0011327112333674e-07, 2.9139113166820607e-07, 2.8292248543948517e-07, 2.7469994279272214e-07, 2.6671636987884995e-07, 2.5896483180076757e-07, 2.514385641916306e-07, 2.4413103005826997e-07, 2.3703589135948278e-07, 2.3014695216261316e-07, 2.2345822969782603e-07, 2.1696389751468814e-07, 2.1065829969302285e-07, 2.045359650537648e-07, 1.9859156452639581e-07, 1.9281992535979953e-07, 1.8721603112226148e-07, 1.8177499327975966e-07, 1.764920938285286e-07, 1.7136272845164058e-07, 1.6638243494071503e-07, 1.615468789850638e-07, 1.5685186838254594e-07, 1.5229331040700345e-07, 1.4786722601911606e-07, 1.4356977828811068e-07, 1.3939722975919722e-07, 1.3534594245356857e-07, 1.3141240629011008e-07, 1.2759318224198068e-07, 1.2388495918003173e-07, 1.2028451124024286e-07, 1.1678869782372203e-07, 1.1339448491298754e-07, 1.1009891665025862e-07, 1.0689912954831016e-07, 1.0379233827961798e-07, 1.0077583567635884e-07, 9.784699983583778e-08, 9.500328701506078e-08, 9.224221741988003e-08, 8.956139652127604e-08, 8.695848663364814e-08, 8.443122112566925e-08, 8.197741152571325e-08, 7.959491199471813e-08, 7.728165485332283e-08, 7.503562926558516e-08, 7.285488123898176e-08, 7.073751362440817e-08, 6.86816790107514e-08, 6.668559393574469e-08, 6.474751756968544e-08, 6.286577303171725e-08, 6.103871186269316e-08, 5.92647531050261e-08, 5.7542351328265795e-08, 5.587000728723979e-08, 5.424626436933977e-08, 5.266971214723526e-08], "accuracy_train_first": 0.2530588235294118, "accuracy_train_last": 0.5205176470588235, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.7484, 0.7037333333333333, 0.6705333333333333, 0.6502666666666667, 0.6369333333333334, 0.6284000000000001, 0.6208, 0.6126666666666667, 0.6049333333333333, 0.5974666666666666, 0.5934666666666666, 0.5860000000000001, 0.5808, 0.5762666666666667, 0.5718666666666667, 0.5684, 0.5644, 0.5625333333333333, 0.5598666666666667, 0.5565333333333333, 0.5541333333333334, 0.5498666666666667, 0.5466666666666666, 0.544, 0.5418666666666667, 0.5402666666666667, 0.5377333333333334, 0.5353333333333333, 0.5332, 0.5313333333333333, 0.5284, 0.5273333333333333, 0.526, 0.5244, 0.5230666666666667, 0.5198666666666667, 0.5196000000000001, 0.5173333333333333, 0.5162666666666667, 0.5149333333333334, 0.514, 0.5130666666666667, 0.5126666666666666, 0.5112, 0.5105333333333333, 0.5087999999999999, 0.5078666666666667, 0.5074666666666667, 0.5072, 0.5072, 0.5062666666666666, 0.5057333333333334, 0.5057333333333334, 0.5052, 0.5046666666666666, 0.5034666666666667, 0.5038666666666667, 0.5036, 0.5029333333333333, 0.5024, 0.502, 0.5012, 0.5009333333333333, 0.5004, 0.5, 0.5001333333333333, 0.4986666666666667, 0.49773333333333336, 0.49706666666666666, 0.4972, 0.49639999999999995, 0.49693333333333334, 0.4965333333333334, 0.4965333333333334, 0.4966666666666667, 0.4965333333333334, 0.4958666666666667, 0.4953333333333333, 0.4948, 0.4948, 0.4945333333333334, 0.4948, 0.49426666666666663, 0.49360000000000004, 0.49373333333333336, 0.49373333333333336, 0.49373333333333336, 0.4934666666666667, 0.4934666666666667, 0.4933333333333333, 0.49360000000000004, 0.49360000000000004, 0.49360000000000004, 0.4934666666666667, 0.4934666666666667, 0.49373333333333336, 0.4933333333333333, 0.49319999999999997, 0.49306666666666665, 0.49293333333333333, 0.49239999999999995, 0.4921333333333333, 0.4926666666666667, 0.4928, 0.4928, 0.4925333333333334, 0.4925333333333334, 0.49239999999999995, 0.49239999999999995, 0.49226666666666663, 0.492, 0.4918666666666667, 0.49173333333333336, 0.49173333333333336, 0.49173333333333336, 0.4918666666666667, 0.492, 0.492, 0.4918666666666667, 0.4918666666666667, 0.4918666666666667, 0.49160000000000004, 0.49173333333333336, 0.49173333333333336, 0.4914666666666667, 0.4913333333333333, 0.4913333333333333, 0.4913333333333333, 0.4914666666666667, 0.49106666666666665, 0.49106666666666665, 0.49106666666666665, 0.49093333333333333, 0.49093333333333333, 0.49106666666666665, 0.49119999999999997, 0.49119999999999997, 0.49119999999999997, 0.4908, 0.4908, 0.4905333333333334, 0.49039999999999995, 0.49039999999999995, 0.49039999999999995, 0.4905333333333334, 0.4905333333333334, 0.4905333333333334, 0.4905333333333334, 0.49039999999999995, 0.4901333333333333, 0.4901333333333333, 0.4901333333333333, 0.4901333333333333, 0.49, 0.49, 0.4898666666666667, 0.4898666666666667, 0.48973333333333335, 0.48973333333333335, 0.4898666666666667, 0.4898666666666667, 0.48973333333333335, 0.48960000000000004, 0.48960000000000004, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48973333333333335, 0.48960000000000004, 0.48960000000000004, 0.4894666666666667, 0.4894666666666667, 0.4894666666666667, 0.4894666666666667, 0.4894666666666667, 0.4894666666666667, 0.4894666666666667, 0.4894666666666667, 0.4894666666666667, 0.4893333333333333, 0.4894666666666667, 0.4894666666666667, 0.4894666666666667, 0.4894666666666667, 0.4894666666666667, 0.4893333333333333, 0.4893333333333333, 0.48919999999999997, 0.48919999999999997, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665, 0.48906666666666665], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.02906285795243181, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.00017029233589142576, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.035212195366529e-05, "rotation_range": [0, 0], "momentum": 0.830374983963364}, "accuracy_valid_max": 0.5109333333333334, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.5109333333333334, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.2516, 0.2962666666666667, 0.3294666666666667, 0.34973333333333334, 0.36306666666666665, 0.3716, 0.3792, 0.3873333333333333, 0.3950666666666667, 0.40253333333333335, 0.40653333333333336, 0.414, 0.4192, 0.42373333333333335, 0.4281333333333333, 0.4316, 0.4356, 0.43746666666666667, 0.4401333333333333, 0.4434666666666667, 0.4458666666666667, 0.45013333333333333, 0.4533333333333333, 0.456, 0.45813333333333334, 0.4597333333333333, 0.46226666666666666, 0.4646666666666667, 0.4668, 0.4686666666666667, 0.4716, 0.4726666666666667, 0.474, 0.4756, 0.4769333333333333, 0.48013333333333336, 0.4804, 0.4826666666666667, 0.48373333333333335, 0.48506666666666665, 0.486, 0.48693333333333333, 0.48733333333333334, 0.4888, 0.48946666666666666, 0.4912, 0.4921333333333333, 0.4925333333333333, 0.4928, 0.4928, 0.49373333333333336, 0.4942666666666667, 0.4942666666666667, 0.4948, 0.49533333333333335, 0.4965333333333333, 0.4961333333333333, 0.4964, 0.49706666666666666, 0.4976, 0.498, 0.4988, 0.49906666666666666, 0.4996, 0.5, 0.4998666666666667, 0.5013333333333333, 0.5022666666666666, 0.5029333333333333, 0.5028, 0.5036, 0.5030666666666667, 0.5034666666666666, 0.5034666666666666, 0.5033333333333333, 0.5034666666666666, 0.5041333333333333, 0.5046666666666667, 0.5052, 0.5052, 0.5054666666666666, 0.5052, 0.5057333333333334, 0.5064, 0.5062666666666666, 0.5062666666666666, 0.5062666666666666, 0.5065333333333333, 0.5065333333333333, 0.5066666666666667, 0.5064, 0.5064, 0.5064, 0.5065333333333333, 0.5065333333333333, 0.5062666666666666, 0.5066666666666667, 0.5068, 0.5069333333333333, 0.5070666666666667, 0.5076, 0.5078666666666667, 0.5073333333333333, 0.5072, 0.5072, 0.5074666666666666, 0.5074666666666666, 0.5076, 0.5076, 0.5077333333333334, 0.508, 0.5081333333333333, 0.5082666666666666, 0.5082666666666666, 0.5082666666666666, 0.5081333333333333, 0.508, 0.508, 0.5081333333333333, 0.5081333333333333, 0.5081333333333333, 0.5084, 0.5082666666666666, 0.5082666666666666, 0.5085333333333333, 0.5086666666666667, 0.5086666666666667, 0.5086666666666667, 0.5085333333333333, 0.5089333333333333, 0.5089333333333333, 0.5089333333333333, 0.5090666666666667, 0.5090666666666667, 0.5089333333333333, 0.5088, 0.5088, 0.5088, 0.5092, 0.5092, 0.5094666666666666, 0.5096, 0.5096, 0.5096, 0.5094666666666666, 0.5094666666666666, 0.5094666666666666, 0.5094666666666666, 0.5096, 0.5098666666666667, 0.5098666666666667, 0.5098666666666667, 0.5098666666666667, 0.51, 0.51, 0.5101333333333333, 0.5101333333333333, 0.5102666666666666, 0.5102666666666666, 0.5101333333333333, 0.5101333333333333, 0.5102666666666666, 0.5104, 0.5104, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5102666666666666, 0.5104, 0.5104, 0.5105333333333333, 0.5105333333333333, 0.5105333333333333, 0.5105333333333333, 0.5105333333333333, 0.5105333333333333, 0.5105333333333333, 0.5105333333333333, 0.5105333333333333, 0.5106666666666667, 0.5105333333333333, 0.5105333333333333, 0.5105333333333333, 0.5105333333333333, 0.5105333333333333, 0.5106666666666667, 0.5106666666666667, 0.5108, 0.5108, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334, 0.5109333333333334], "seed": 290879692, "model": "residualv3", "loss_std": [0.4637625217437744, 0.0777442455291748, 0.07002244889736176, 0.07017602771520615, 0.0712021216750145, 0.07234089821577072, 0.0733870193362236, 0.07431328296661377, 0.07512069493532181, 0.07585852593183517, 0.07658947259187698, 0.07727310061454773, 0.07787003368139267, 0.07844485342502594, 0.07895583659410477, 0.07943513244390488, 0.07992248237133026, 0.08038721978664398, 0.08080673217773438, 0.08121541142463684, 0.08161140978336334, 0.08202195912599564, 0.08239991962909698, 0.0827435776591301, 0.08307892829179764, 0.08336273580789566, 0.08363397419452667, 0.08388715982437134, 0.08413340151309967, 0.08436819911003113, 0.08459693938493729, 0.08480493724346161, 0.08501419425010681, 0.08521749824285507, 0.08541455119848251, 0.08561096340417862, 0.08579430729150772, 0.08598015457391739, 0.08615989238023758, 0.08632620424032211, 0.08648889511823654, 0.08664362132549286, 0.08678717166185379, 0.0869281217455864, 0.08707992732524872, 0.08721549063920975, 0.08734352886676788, 0.08747191727161407, 0.08759995549917221, 0.08772558718919754, 0.08785039186477661, 0.08796397596597672, 0.08807109296321869, 0.08816687017679214, 0.08826436847448349, 0.08835633099079132, 0.0884498879313469, 0.08854128420352936, 0.08863187581300735, 0.08871494233608246, 0.08879415690898895, 0.08887367695569992, 0.0889463946223259, 0.08901725709438324, 0.08908908814191818, 0.08916065841913223, 0.08922820538282394, 0.08929578214883804, 0.08935635536909103, 0.089415542781353, 0.08947058767080307, 0.08952782303094864, 0.08958660066127777, 0.08964360505342484, 0.08969796448945999, 0.08974864333868027, 0.08979383111000061, 0.08984164148569107, 0.08989005535840988, 0.08993515372276306, 0.0899784192442894, 0.09002266824245453, 0.09006880223751068, 0.09011019766330719, 0.0901486799120903, 0.0901862233877182, 0.09022559970617294, 0.09026218950748444, 0.09029576182365417, 0.09032925218343735, 0.09036093205213547, 0.09039147198200226, 0.09042070806026459, 0.09045049548149109, 0.09047947824001312, 0.09050621837377548, 0.09053245931863785, 0.09055811166763306, 0.0905822291970253, 0.09060551971197128, 0.09062927216291428, 0.09065213799476624, 0.0906735509634018, 0.09069398790597916, 0.09071411192417145, 0.09073431044816971, 0.0907532200217247, 0.09077253937721252, 0.09079181402921677, 0.09081079810857773, 0.09082895517349243, 0.09084702283143997, 0.09086568653583527, 0.09088341146707535, 0.09090040624141693, 0.09091664105653763, 0.09093201160430908, 0.09094715118408203, 0.09096287935972214, 0.09097762405872345, 0.09099099040031433, 0.09100476652383804, 0.0910186842083931, 0.09103196859359741, 0.09104491770267487, 0.09105715155601501, 0.0910695493221283, 0.09108110517263412, 0.09109298139810562, 0.0911034494638443, 0.09111332148313522, 0.0911230593919754, 0.0911325067281723, 0.09114143997430801, 0.09115023165941238, 0.09115957468748093, 0.09116905182600021, 0.09117817133665085, 0.09118669480085373, 0.09119495004415512, 0.09120318293571472, 0.09121093899011612, 0.09121815860271454, 0.09122518450021744, 0.091232530772686, 0.0912398025393486, 0.09124665707349777, 0.09125292301177979, 0.09125906229019165, 0.09126506745815277, 0.0912705808877945, 0.09127601981163025, 0.09128136187791824, 0.09128661453723907, 0.09129172563552856, 0.09129690378904343, 0.09130176156759262, 0.09130624681711197, 0.09131060540676117, 0.09131506830453873, 0.09131927043199539, 0.0913231149315834, 0.09132681041955948, 0.0913304015994072, 0.09133395552635193, 0.09133721143007278, 0.09134034812450409, 0.09134355932474136, 0.09134669601917267, 0.09134962409734726, 0.09135233610868454, 0.09135498106479645, 0.09135764837265015, 0.09136036038398743, 0.09136289358139038, 0.09136539697647095, 0.09136781841516495, 0.09137021005153656, 0.09137266129255295, 0.09137499332427979, 0.09137716889381409, 0.091379314661026, 0.09138128906488419, 0.09138315916061401, 0.09138503670692444, 0.09138688445091248, 0.09138863533735275, 0.09139027446508408, 0.09139195084571838, 0.0913936123251915, 0.09139518439769745, 0.09139670431613922, 0.09139814972877502, 0.09139961004257202, 0.09140104800462723, 0.09140247106552124, 0.09140384942293167, 0.0914052277803421, 0.09140655398368835, 0.09140786528587341, 0.09140916168689728, 0.09141038358211517, 0.0914115309715271, 0.09141259640455246, 0.0914136990904808, 0.09141473472118378, 0.09141570329666138, 0.09141664206981659, 0.09141752123832703, 0.09141834080219269, 0.09141913056373596, 0.09141989797353745, 0.09142063558101654, 0.09142133593559265, 0.09142203629016876, 0.09142272174358368, 0.09142335504293442, 0.09142395853996277, 0.09142455458641052, 0.09142515063285828, 0.09142571687698364, 0.09142624586820602, 0.09142676740884781, 0.0914272740483284, 0.0914277508854866, 0.09142821282148361, 0.09142863005399704, 0.09142901003360748, 0.09142937511205673, 0.09142974019050598, 0.09143009036779404, 0.0914304181933403, 0.09143071621656418, 0.09143102914094925, 0.09143131226301193, 0.09143159538507462, 0.09143190085887909, 0.09143219143152237, 0.09143247455358505, 0.09143271297216415, 0.09143297374248505, 0.09143319725990295, 0.09143339842557907, 0.091433584690094, 0.09143376350402832, 0.09143391996622086, 0.0914340540766716, 0.09143421053886414, 0.09143434464931488, 0.09143445640802383, 0.09143459796905518, 0.09143473207950592, 0.09143485873937607, 0.09143498539924622, 0.09143511950969696, 0.0914352536201477, 0.09143539518117905, 0.0914355143904686, 0.09143565595149994, 0.09143577516078949, 0.09143588691949844, 0.0914359986782074, 0.09143609553575516, 0.09143620729446411, 0.09143630415201187, 0.09143640100955963, 0.0914364755153656, 0.09143653512001038, 0.09143660962581635, 0.09143666177988052, 0.0914367064833641, 0.09143675118684769, 0.09143677353858948, 0.09143681824207306]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:37 2016", "state": "available"}], "summary": "601e49bc5cd59684a73789fd45140274"}