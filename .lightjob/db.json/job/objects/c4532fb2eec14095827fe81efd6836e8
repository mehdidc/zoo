{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 32, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.8368914127349854, 1.4673042297363281, 1.3183830976486206, 1.2135785818099976, 1.1314915418624878, 1.0657336711883545, 1.0118670463562012, 0.9679102897644043, 0.9271919131278992, 0.8932159543037415, 0.8622568249702454, 0.8357624411582947, 0.8100911974906921, 0.7862125635147095, 0.7660402655601501, 0.744512677192688, 0.7279844284057617, 0.7090457081794739, 0.692855954170227, 0.6772570013999939, 0.6614758968353271, 0.6510459184646606, 0.6365140080451965, 0.6211850643157959, 0.6109508275985718, 0.600577175617218, 0.5898796916007996, 0.5784470438957214, 0.5712994933128357, 0.5629541873931885, 0.553638756275177, 0.5467707514762878, 0.5375959277153015, 0.5289167761802673, 0.520785391330719, 0.5150958299636841, 0.5108765363693237, 0.502633810043335, 0.49482497572898865, 0.48953860998153687, 0.4838911294937134, 0.4802989065647125, 0.4748346507549286, 0.4674820899963379, 0.46437153220176697, 0.4589785039424896, 0.4567464292049408, 0.45124581456184387, 0.44729354977607727, 0.4449232816696167, 0.440625935792923, 0.43645697832107544, 0.4333150386810303, 0.42802178859710693, 0.4302191436290741, 0.4250873029232025, 0.41986483335494995, 0.41711223125457764, 0.4170851409435272, 0.41270530223846436, 0.4105933606624603, 0.4087684750556946, 0.4060516953468323, 0.40666958689689636, 0.40450987219810486, 0.40068280696868896, 0.3999066948890686, 0.39530155062675476, 0.3973886966705322, 0.39058613777160645, 0.3919495940208435, 0.39124831557273865, 0.3892284333705902, 0.38775473833084106, 0.3867206871509552, 0.38288629055023193, 0.384607195854187, 0.3832928538322449, 0.38028669357299805, 0.3786414563655853, 0.3788430988788605, 0.37643513083457947, 0.375221312046051, 0.3767104148864746, 0.37416672706604004, 0.37209218740463257, 0.37082114815711975, 0.3699105679988861, 0.37054765224456787, 0.3668879270553589, 0.36953580379486084, 0.36700573563575745, 0.3649543523788452, 0.3657034933567047, 0.365835040807724, 0.36547213792800903, 0.36522671580314636, 0.36312156915664673, 0.3649456202983856, 0.3645510971546173, 0.3650340437889099, 0.36324745416641235, 0.3611823618412018, 0.36122676730155945, 0.3602869212627411, 0.36126548051834106, 0.35842806100845337, 0.3593143820762634, 0.35800206661224365, 0.3592817783355713, 0.3579573929309845, 0.35727423429489136, 0.35526043176651, 0.3574257791042328, 0.3575844466686249, 0.3562450408935547, 0.3573204576969147, 0.35409116744995117, 0.35362616181373596, 0.3567740023136139, 0.35442402958869934, 0.35389092564582825, 0.35319051146507263, 0.3534850478172302, 0.35368117690086365, 0.35289266705513, 0.35319602489471436, 0.3514499366283417, 0.3521381914615631, 0.3516720235347748, 0.35085052251815796, 0.3521987199783325, 0.35064032673835754, 0.35174304246902466, 0.3521720767021179, 0.3509809076786041, 0.35025081038475037, 0.3497472405433655, 0.35019248723983765, 0.3494431674480438, 0.3496955633163452, 0.35078004002571106, 0.35186949372291565, 0.34759825468063354, 0.3483986258506775, 0.34952256083488464, 0.3493864834308624, 0.3479386568069458, 0.349762886762619, 0.34817740321159363, 0.34602147340774536, 0.34843698143959045, 0.34747084975242615, 0.34580332040786743, 0.348868191242218, 0.348034530878067, 0.34739574790000916, 0.3476845920085907, 0.3471013903617859, 0.34858667850494385, 0.34752851724624634, 0.3496091067790985, 0.34697213768959045, 0.34668633341789246, 0.34757161140441895, 0.3486233353614807, 0.3489364683628082, 0.34723585844039917, 0.34688085317611694, 0.3477603495121002, 0.3474833369255066, 0.3459852933883667, 0.34701448678970337, 0.3478313982486725, 0.3471997380256653, 0.3467790186405182, 0.3456563353538513, 0.3450314402580261, 0.34574195742607117, 0.3462250828742981, 0.3451366424560547, 0.34557437896728516, 0.3454713523387909, 0.34607750177383423, 0.34736454486846924, 0.34631988406181335, 0.34698525071144104, 0.34753093123435974, 0.3467535674571991, 0.3461611568927765], "moving_avg_accuracy_train": [0.051263529411764694, 0.10497717647058821, 0.15821592941176468, 0.21048845411764705, 0.2575713734117647, 0.30327070665882355, 0.34733187128705884, 0.3881704488642353, 0.42484516868369415, 0.46081712240356004, 0.49128364545732167, 0.5228917514998248, 0.5540284587027835, 0.5817762010677993, 0.6080856397845487, 0.6315358993355056, 0.6540881917548962, 0.6742723137558772, 0.6942097882626423, 0.7128217506128487, 0.7289348696692108, 0.7440884415258191, 0.7588560679614725, 0.772358696459443, 0.7847557679899693, 0.7966990147203842, 0.8077091132483457, 0.8183993783940993, 0.8276300287899835, 0.8366505553227499, 0.844860793908122, 0.852520596870251, 0.8603461842420493, 0.8674550952296092, 0.8732578210007659, 0.8793861565477481, 0.8845604820694439, 0.8898997279801466, 0.8951238728291907, 0.8993126620168598, 0.9028637487563502, 0.906836197410127, 0.9111290482573495, 0.9148349669610263, 0.9185538232061001, 0.9214631467678431, 0.9246885967969413, 0.9275679724113648, 0.9303241163466989, 0.933006998829676, 0.9352827695349436, 0.9379662572873316, 0.9407555139115397, 0.9428752566380327, 0.9447265545036412, 0.9460703696415124, 0.9480986267950082, 0.9500746464684486, 0.9518812994686626, 0.9536766989335611, 0.9555396172754991, 0.9570585967244197, 0.9584797958755071, 0.9598318162879564, 0.9611427523062196, 0.9623014182520682, 0.9634995117209789, 0.9647377958429987, 0.9658051927292871, 0.966852908750476, 0.9676970296401343, 0.9684049737349443, 0.968955064596744, 0.9697113228429519, 0.9706107787939509, 0.9713497009145557, 0.9720147308231002, 0.9726838459760843, 0.9733872260843581, 0.9741002681818046, 0.9746149472459771, 0.9753346289919677, 0.9757658719751239, 0.9762763436011409, 0.9768040033586738, 0.9773541912581005, 0.9778352427205257, 0.9782846596249437, 0.9786820760153905, 0.9790303390020867, 0.979452010984231, 0.9797844569446313, 0.98010483477958, 0.980494351301622, 0.9808519749949892, 0.9811150127896079, 0.9813917468047648, 0.9816972780066412, 0.9819722560883299, 0.9821632657736147, 0.9824316450786061, 0.9826637746883925, 0.9828891619254356, 0.983004951615245, 0.9831938682184264, 0.9833615402201131, 0.9835712685510429, 0.983825906401821, 0.9840244922322271, 0.9842408665384161, 0.9843979563551627, 0.9846052195431758, 0.984758815235917, 0.9849276395946783, 0.985032522694034, 0.98517632924816, 0.9852304610292264, 0.9853403561027744, 0.9854651440219087, 0.9855986296197178, 0.9856858254812755, 0.9857737135213832, 0.9858904598163037, 0.9859931785405558, 0.986057390098265, 0.9861669452060855, 0.986265544803124, 0.9863448726757528, 0.9863786207022951, 0.9864607586320655, 0.9865323298276825, 0.9865190968449142, 0.9865565989251286, 0.9866209390326157, 0.9866341392470013, 0.9866530782634776, 0.9867124763194828, 0.9867918169228287, 0.9868679293481929, 0.9869152540604325, 0.986962552183801, 0.9869933557889503, 0.9869999025629965, 0.9870363828949322, 0.9870386269583802, 0.9871065289684245, 0.9871558760715821, 0.9872167590526592, 0.9872950831473932, 0.9872926336561834, 0.9873021938199769, 0.9873272685556262, 0.9873616005235931, 0.9873854404712338, 0.987435131718228, 0.9874751479581699, 0.9875511625741176, 0.9875960463167058, 0.9875940887438588, 0.9876440916341788, 0.9876514471766432, 0.98763218481192, 0.9876454369189633, 0.9876903049917728, 0.9877165686102425, 0.987688441160983, 0.9877125382213552, 0.9877483432227491, 0.9877546853710625, 0.9877556874221914, 0.9877918833858547, 0.987826812694328, 0.9878535431896011, 0.9878728947529939, 0.9879020758659298, 0.9879448094558074, 0.9880138579219914, 0.9880030603650863, 0.9880262837403424, 0.9880589494839552, 0.9880459957120302, 0.9880578667290625, 0.9880285506443915, 0.9880115779328936, 0.9880268907278396, 0.9880618487138791, 0.9880462520777853, 0.9880204503994184, 0.9880207583006531, 0.9880845648235289], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.050639999999999984, 0.10317599999999996, 0.15557839999999995, 0.20710055999999993, 0.25316383733333325, 0.2977674535999999, 0.3403773749066666, 0.3800196374159999, 0.41545767367439995, 0.4497652396402933, 0.4782153823429306, 0.5074871774419709, 0.5360451263644405, 0.5612139470613298, 0.5848792190218635, 0.6054179637863438, 0.6252628340743761, 0.6434698840002718, 0.6605762289335779, 0.6760386060402201, 0.6895414121028648, 0.702373937559245, 0.7143765438033205, 0.7248588894229885, 0.7348130004806896, 0.7441317004326207, 0.7523851970560252, 0.7603866773504228, 0.7668546762820471, 0.7734092086538423, 0.7791082877884581, 0.7846107923429456, 0.7898163797753177, 0.7945814084644526, 0.7987232676180074, 0.8029709408562067, 0.8061805134372526, 0.8094691287601941, 0.8124822158841747, 0.8148473276290905, 0.8171892615328481, 0.8194436687128966, 0.8216193018416069, 0.8238307049907796, 0.8255943011583684, 0.8270748710425316, 0.8285807172716118, 0.8299893122111172, 0.8314570476566722, 0.8327246762243383, 0.8336788752685712, 0.8347909877417141, 0.8360185556342093, 0.837123366737455, 0.8377843633970429, 0.8382059270573385, 0.8392253343516047, 0.8399961342497776, 0.8408765208247998, 0.8418688687423198, 0.8426153152014212, 0.843673783681279, 0.8443197386464844, 0.8451277647818359, 0.8456683216369857, 0.8462081561399538, 0.8468673405259585, 0.8475539398066959, 0.8480118791593596, 0.848264024576757, 0.8484242887857479, 0.8488085265738399, 0.8489276739164558, 0.8493549065248103, 0.8494727492056626, 0.8498321409517631, 0.8501689268565867, 0.8506587008375947, 0.8509661640871685, 0.851482881011785, 0.8519345929106066, 0.8522478002862126, 0.8525563535909247, 0.852767384898499, 0.8529173130753158, 0.8530389151011176, 0.8532150235910059, 0.8533335212319053, 0.8536001691087148, 0.8536934855311766, 0.853964136978059, 0.854327723280253, 0.8543216176188944, 0.854489455857005, 0.8548138436046377, 0.8549191259108406, 0.8552138799864233, 0.8554258253211143, 0.8554832427890029, 0.8555749185101026, 0.8558174266590923, 0.8559823506598497, 0.8560507822605314, 0.8559523707011449, 0.856183800297697, 0.8563254202679272, 0.8562262115744679, 0.8561635904170211, 0.8563605647086523, 0.8565378415711203, 0.8565773907473416, 0.8567996516726074, 0.8569063531720134, 0.856975717854812, 0.8570114794026641, 0.8570303314623977, 0.8570339649828246, 0.8571172351512089, 0.8572588449694213, 0.8574529604724792, 0.8576809977585647, 0.8576995646493749, 0.8577162748511041, 0.8578513140326605, 0.8577995159627277, 0.8578862310331217, 0.8579509412631429, 0.8577558471368286, 0.8578069290898124, 0.8579462361808311, 0.8578582792294146, 0.8578591179731399, 0.8579532061758258, 0.8579578855582433, 0.8580954303357523, 0.8580992206355104, 0.8581026319052927, 0.8583190353814301, 0.8585137985099538, 0.8587557519922918, 0.8586401767930626, 0.8586028257804229, 0.858675876535714, 0.8585949555488093, 0.8585754599939284, 0.8585979139945356, 0.8585247892617487, 0.8584056436689071, 0.8583250793020164, 0.8582925713718148, 0.8583433142346333, 0.8583889828111699, 0.8584167511967196, 0.8583350760770476, 0.8582615684693429, 0.8582620782890752, 0.8583425371268344, 0.8584016167474843, 0.8585347884060692, 0.858587976232129, 0.8585691786089161, 0.8585789274146912, 0.858721034673222, 0.8587955978725664, 0.8587160380853098, 0.8586577676101121, 0.858631990849101, 0.8586087917641909, 0.8586279125877718, 0.8585917879956613, 0.8586659425294285, 0.858652681609819, 0.8587074134488372, 0.8587433387706201, 0.8586690048935581, 0.858748771070869, 0.8587805606304488, 0.8587025045674039, 0.8587789207773302, 0.8588343620329305, 0.8588709258296374, 0.8589304999133404, 0.8589041165886729, 0.858987038263139, 0.8589550011034918, 0.8589795009931426, 0.8589615508938283, 0.8589453958044454, 0.8589041895573342, 0.8589471039349341], "moving_var_accuracy_train": [0.023651545029757783, 0.04725279345002076, 0.06803679743760332, 0.08582486924598673, 0.09719359392469562, 0.1062700960652577, 0.1131155625143, 0.11681411102961353, 0.11723801559117443, 0.11716004712187454, 0.1137979236525556, 0.11140978259564714, 0.10899425515506744, 0.10502426449675832, 0.10075151713739607, 0.09562559748072168, 0.09064049077297684, 0.0852430307242335, 0.08029625365918139, 0.07538427457599274, 0.07018254056991398, 0.06523096317304311, 0.06067061197062588, 0.056244439560751106, 0.05200318204747287, 0.048086634124897665, 0.0443689711387667, 0.04096060994486867, 0.037631393110961135, 0.0346005828902201, 0.031747196759856676, 0.02910053031663877, 0.026741635644397757, 0.024522301618819412, 0.02237311609431469, 0.020473812954070875, 0.01866739446010404, 0.01705722293614823, 0.015597125847167558, 0.01419532685617941, 0.012889286123843937, 0.011742380646221576, 0.010733999697167916, 0.009784204228395479, 0.008930252831499655, 0.008113405020631903, 0.00739569626958059, 0.006730743877982962, 0.0061260364547151775, 0.005578213535000851, 0.005067004372227358, 0.004625113893659569, 0.004232622076934807, 0.003849799652280009, 0.0034956654211368674, 0.003162351431146126, 0.0028831407317578767, 0.002629968542330499, 0.002396347643666088, 0.0021857240124464974, 0.001998385793940409, 0.0018193129016425586, 0.0016555598747217677, 0.0015064555200107067, 0.0013712769472054534, 0.0012462318134515318, 0.0011345274837485997, 0.0010348748634753558, 0.000941641402143543, 0.0008573566416786925, 0.0007780338381980407, 0.0007047411179506251, 0.0006369904057616823, 0.0005784387040001319, 0.0005278760226702052, 0.00048000247350605803, 0.00043598260916877967, 0.00039641378404347817, 0.00036122509782956944, 0.0003296784493411911, 0.0002990946552589491, 0.000273846666072662, 0.0002481357340600887, 0.00022566739218279606, 0.0002056064763419943, 0.0001877701892298753, 0.000171075864892401, 0.00015578605838895097, 0.0001416289106366178, 0.00012855760354407933, 0.00011730210853440067, 0.0001065665805302397, 9.683370009135304e-05, 8.8515838170711e-05, 8.081530670815836e-05, 7.335647596992349e-05, 6.671006380923463e-05, 6.087920126619188e-05, 5.547179764825611e-05, 5.025298018228335e-05, 4.587592922618437e-05, 4.177329370522243e-05, 3.8053158994297515e-05, 3.436850836526312e-05, 3.125286287535512e-05, 2.8380601689166477e-05, 2.593841527540146e-05, 2.3928137663301144e-05, 2.18902508853139e-05, 2.012258636019169e-05, 1.8332422618901927e-05, 1.6885802618960072e-05, 1.5409547088522015e-05, 1.412510735667011e-05, 1.2811600801777227e-05, 1.1716563646685882e-05, 1.0571279529510025e-05, 9.622843921270003e-06, 8.800707751999889e-06, 8.08100262020206e-06, 7.341330422636848e-06, 6.676716148718985e-06, 6.131711810246062e-06, 5.613500856029042e-06, 5.0892588877170254e-06, 4.688353893791495e-06, 4.307015429237748e-06, 3.93295008869624e-06, 3.5499054434861576e-06, 3.2556346547003814e-06, 2.9761731136087437e-06, 2.6801318087443706e-06, 2.4247762820536415e-06, 2.219555498731386e-06, 1.9991681597966523e-06, 1.8024795209228065e-06, 1.6539847303452731e-06, 1.545240639364269e-06, 1.4428544870812217e-06, 1.3187256938700944e-06, 1.2069871367507382e-06, 1.0948281818874162e-06, 9.85731105952376e-07, 8.991353269203461e-07, 8.092671166151374e-07, 7.698365516661634e-07, 7.147691258099246e-07, 6.766528496924348e-07, 6.641995390664154e-07, 5.978335852244609e-07, 5.388727972878294e-07, 4.906441988700216e-07, 4.5218793520326636e-07, 4.120842296145387e-07, 3.9309878690369033e-07, 3.682006033451101e-07, 3.8338453954971537e-07, 3.63177038733291e-07, 3.2689382368302565e-07, 3.167070426779098e-07, 2.8552327445463874e-07, 2.6031029526172327e-07, 2.3585983080533326e-07, 2.3039214334358775e-07, 2.135609279053666e-07, 1.9932521573146322e-07, 1.84618709025589e-07, 1.7769482124638252e-07, 1.6028734472879847e-07, 1.442676472141058e-07, 1.416322125622708e-07, 1.3844950061988257e-07, 1.3103522495579081e-07, 1.213020495119333e-07, 1.1683568073034191e-07, 1.215875499917563e-07, 1.523380111338031e-07, 1.3815349513647654e-07, 1.2919207204738994e-07, 1.2587632209465356e-07, 1.147988917489375e-07, 1.0458729198246836e-07, 1.0186345816813037e-07, 9.426976877165991e-08, 8.695312709599936e-08, 8.92563614778867e-08, 8.252002084707769e-08, 8.025955822124592e-08, 7.223445562765392e-08, 1.0165246131847657e-07], "duration": 80931.804653, "accuracy_train": [0.5126352941176471, 0.5884, 0.6373647058823529, 0.6809411764705883, 0.6813176470588236, 0.714564705882353, 0.7438823529411764, 0.7557176470588235, 0.7549176470588236, 0.7845647058823529, 0.7654823529411765, 0.807364705882353, 0.8342588235294117, 0.8315058823529412, 0.8448705882352942, 0.8425882352941176, 0.8570588235294118, 0.8559294117647058, 0.8736470588235294, 0.8803294117647059, 0.8739529411764706, 0.8804705882352941, 0.8917647058823529, 0.8938823529411765, 0.8963294117647059, 0.9041882352941176, 0.9068, 0.9146117647058823, 0.9107058823529411, 0.9178352941176471, 0.9187529411764706, 0.9214588235294118, 0.9307764705882353, 0.931435294117647, 0.9254823529411764, 0.9345411764705882, 0.9311294117647059, 0.9379529411764705, 0.9421411764705883, 0.9370117647058823, 0.9348235294117647, 0.9425882352941176, 0.949764705882353, 0.9481882352941177, 0.9520235294117647, 0.9476470588235294, 0.9537176470588236, 0.9534823529411764, 0.9551294117647059, 0.9571529411764705, 0.955764705882353, 0.9621176470588235, 0.9658588235294118, 0.9619529411764706, 0.9613882352941177, 0.9581647058823529, 0.9663529411764706, 0.9678588235294118, 0.9681411764705883, 0.969835294117647, 0.9723058823529411, 0.9707294117647058, 0.9712705882352941, 0.972, 0.9729411764705882, 0.9727294117647058, 0.9742823529411765, 0.9758823529411764, 0.9754117647058823, 0.9762823529411765, 0.9752941176470589, 0.9747764705882352, 0.9739058823529412, 0.9765176470588235, 0.9787058823529412, 0.978, 0.978, 0.9787058823529412, 0.9797176470588236, 0.9805176470588235, 0.9792470588235294, 0.9818117647058824, 0.9796470588235294, 0.9808705882352942, 0.9815529411764706, 0.9823058823529411, 0.9821647058823529, 0.9823294117647059, 0.9822588235294117, 0.9821647058823529, 0.9832470588235294, 0.9827764705882353, 0.9829882352941176, 0.984, 0.9840705882352941, 0.9834823529411765, 0.9838823529411764, 0.9844470588235295, 0.9844470588235295, 0.9838823529411764, 0.9848470588235294, 0.9847529411764706, 0.9849176470588236, 0.9840470588235294, 0.9848941176470588, 0.9848705882352942, 0.9854588235294117, 0.9861176470588235, 0.9858117647058824, 0.9861882352941177, 0.9858117647058824, 0.9864705882352941, 0.9861411764705882, 0.9864470588235295, 0.9859764705882353, 0.9864705882352941, 0.9857176470588235, 0.9863294117647059, 0.9865882352941177, 0.9868, 0.9864705882352941, 0.9865647058823529, 0.9869411764705882, 0.9869176470588236, 0.986635294117647, 0.9871529411764706, 0.9871529411764706, 0.9870588235294118, 0.9866823529411765, 0.9872, 0.9871764705882353, 0.9864, 0.9868941176470588, 0.9872, 0.9867529411764706, 0.9868235294117647, 0.9872470588235294, 0.9875058823529411, 0.9875529411764706, 0.9873411764705883, 0.9873882352941177, 0.9872705882352941, 0.9870588235294118, 0.9873647058823529, 0.9870588235294118, 0.9877176470588235, 0.9876, 0.987764705882353, 0.988, 0.9872705882352941, 0.9873882352941177, 0.9875529411764706, 0.9876705882352941, 0.9876, 0.9878823529411764, 0.987835294117647, 0.9882352941176471, 0.988, 0.9875764705882353, 0.9880941176470588, 0.9877176470588235, 0.9874588235294117, 0.987764705882353, 0.9880941176470588, 0.9879529411764706, 0.9874352941176471, 0.9879294117647058, 0.9880705882352941, 0.9878117647058824, 0.987764705882353, 0.9881176470588235, 0.9881411764705882, 0.9880941176470588, 0.9880470588235294, 0.988164705882353, 0.9883294117647059, 0.9886352941176471, 0.9879058823529412, 0.9882352941176471, 0.9883529411764705, 0.9879294117647058, 0.988164705882353, 0.987764705882353, 0.9878588235294118, 0.988164705882353, 0.9883764705882353, 0.9879058823529412, 0.9877882352941176, 0.9880235294117647, 0.9886588235294118], "end": "2016-02-08 04:16:26.814000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0], "moving_var_accuracy_valid": [0.023079686399999993, 0.04561199942399999, 0.06576490321343999, 0.08307920963168638, 0.09386771833670597, 0.10238628975961185, 0.1084881093274936, 0.1117828791864743, 0.11190728099249192, 0.11130963463577984, 0.10746336675040571, 0.10442857197024692, 0.10132572279314767, 0.09689437633128237, 0.09224534457084832, 0.0868173704422677, 0.08168000328878036, 0.07649547296293886, 0.07147956899944022, 0.06648337805158831, 0.06147597219051802, 0.056810438357764025, 0.052425957531840504, 0.048172277905868, 0.044246809057822634, 0.04060367167118745, 0.03715638636268166, 0.03401696090852818, 0.030991779909290813, 0.028279258969877947, 0.025743648599733666, 0.023441781747109702, 0.021341486837043365, 0.019411687639013537, 0.017624913850343153, 0.016024807016755464, 0.014515038520456939, 0.013160869585091809, 0.011926490872732906, 0.010784185567553067, 0.009755128900483885, 0.008825357176036585, 0.007985421874029603, 0.007230892421620179, 0.006535795622439163, 0.005901944844832265, 0.005332158516139753, 0.004816799921858182, 0.004354508155715609, 0.003933519279814118, 0.00354836181417684, 0.0032046567801354356, 0.002897753408498059, 0.002618963535812949, 0.0023609994314875305, 0.0021264989316159147, 0.0019232017595387505, 0.001736228775932085, 0.001569581623032191, 0.0014214862502336288, 0.0012843522660570105, 0.0011660002391569832, 0.001053155535594947, 0.0009537161381541529, 0.0008609743397615821, 0.0007774996974007772, 0.00070366044415347, 0.0006375371668889061, 0.0005756708262564783, 0.000518675939434461, 0.0004670395070411664, 0.0004216643044372293, 0.0003796256387967785, 0.00034330582423187244, 0.0003091002238855594, 0.0002793526633414893, 0.0002524382197185315, 0.00022935330471893014, 0.00020726877709558385, 0.00018894486680769146, 0.0001718867728827546, 0.00015558098533568517, 0.00014087973307875528, 0.00012719256768586802, 0.0001146756170411136, 0.00010334113881111404, 9.328615273189908e-05, 8.408391267679772e-05, 7.63154312209809e-05, 6.876225969119253e-05, 6.254530357336883e-05, 5.748052820832064e-05, 5.1732810899394214e-05, 4.6813056877003304e-05, 4.307879788663174e-05, 3.8870677373963167e-05, 3.576552932221983e-05, 3.2593263814073354e-05, 2.936360832323463e-05, 2.6502887431463545e-05, 2.438189050925506e-05, 2.2188500792562118e-05, 2.0011796669052636e-05, 1.809778051733528e-05, 1.6770039389044466e-05, 1.5273541393852313e-05, 1.3834768538188371e-05, 1.2486584368609307e-05, 1.1587115775820979e-05, 1.0711247971937696e-05, 9.654200410801979e-06, 9.133379639822031e-06, 8.32250856561908e-06, 7.533560842035013e-06, 6.791714752574535e-06, 6.115741878722869e-06, 5.504286513086816e-06, 5.0162631502627396e-06, 4.695116900763941e-06, 4.564732667434218e-06, 4.576268435297597e-06, 4.1217441566770575e-06, 3.7120828185858187e-06, 3.5049947617256665e-06, 3.178642645991893e-06, 2.9284539122934636e-06, 2.67329524588865e-06, 2.7485211844008124e-06, 2.497153359246486e-06, 2.422096213794821e-06, 2.249514420137609e-06, 2.0245693095431773e-06, 1.9017856875510267e-06, 1.7118041883742e-06, 1.7108908619167819e-06, 1.53993107307541e-06, 1.3860426966216174e-06, 1.668912607318589e-06, 1.8434154326776148e-06, 2.185947277948997e-06, 2.0875711902458712e-06, 1.891369954528111e-06, 1.750260674712617e-06, 1.6341684623360782e-06, 1.474172306043506e-06, 1.3312927147285537e-06, 1.2462884821620852e-06, 1.2494206845879367e-06, 1.1828941710416514e-06, 1.074115643671441e-06, 9.898776224474716e-07, 9.096604301486988e-07, 8.256341362581491e-07, 8.031081491931863e-07, 7.714276497881595e-07, 6.942872240547792e-07, 6.83121122811269e-07, 6.462226247153606e-07, 7.41212578096115e-07, 6.925518238551534e-07, 6.264767972157244e-07, 5.646844704205096e-07, 6.899662797229214e-07, 6.710066880189807e-07, 6.608738569519889e-07, 6.253455057746303e-07, 5.687909278712305e-07, 5.167556129501003e-07, 4.6837050470480064e-07, 4.332783296306825e-07, 4.3944055057168975e-07, 3.9707916341453087e-07, 3.8433141489379927e-07, 3.575139321112934e-07, 3.714922664118024e-07, 3.9160682715570676e-07, 3.615413293246126e-07, 3.802219371947534e-07, 3.9475467773075145e-07, 3.8294280536052456e-07, 3.5668072589111484e-07, 3.5295429634339506e-07, 3.239235850935875e-07, 3.5341526345049195e-07, 3.273111534898036e-07, 2.9998223947694465e-07, 2.728838701177751e-07, 2.479443653227014e-07, 2.384315219993355e-07, 2.3116316404249043e-07], "accuracy_test": 0.851, "start": "2016-02-07 05:47:35.010000", "learning_rate_per_epoch": [0.0003345588338561356, 0.00032311645918525755, 0.0003120654437225312, 0.000301392370602116, 0.0002910843468271196, 0.0002811288577504456, 0.00027151385438628495, 0.0002622276952024549, 0.00025325914612039924, 0.0002445973514113575, 0.00023623179004061967, 0.00022815233387518674, 0.0002203492185799405, 0.00021281297085806727, 0.0002055344812106341, 0.0001985049166250974, 0.00019171577878296375, 0.00018515883130021393, 0.00017882614338304847, 0.00017271004617214203, 0.00016680313274264336, 0.0001610982435522601, 0.0001555884664412588, 0.0001502671220805496, 0.0001451277785236016, 0.00014016420755069703, 0.0001353703992208466, 0.00013074054731987417, 0.0001262690348085016, 0.00012195046292617917, 0.00011777959298342466, 0.00011375136818969622, 0.00010986091365339234, 0.00010610352182993665, 0.00010247463796986267, 9.896986739477143e-05, 9.558496094541624e-05, 9.231582225766033e-05, 8.915849321056157e-05, 8.610915392637253e-05, 8.316410094266757e-05, 8.031977631617337e-05, 7.757273124298081e-05, 7.491964061046019e-05, 7.235728844534606e-05, 6.988256791373715e-05, 6.749248859705403e-05, 6.518415466416627e-05, 6.29547648713924e-05, 6.080162711441517e-05, 5.872212932445109e-05, 5.671375402016565e-05, 5.4774067393736914e-05, 5.290071931085549e-05, 5.109144331072457e-05, 4.934404569212347e-05, 4.765641278936528e-05, 4.60265000583604e-05, 4.4452332076616585e-05, 4.293200254323892e-05, 4.146367064095102e-05, 4.0045557398116216e-05, 3.8675945688737556e-05, 3.735317659447901e-05, 3.607564576668665e-05, 3.4841810702346265e-05, 3.365017255418934e-05, 3.249929068260826e-05, 3.1387771741719916e-05, 3.0314267860376276e-05, 2.9277478461153805e-05, 2.8276148441364057e-05, 2.730906635406427e-05, 2.637505895108916e-05, 2.547299664001912e-05, 2.460178620822262e-05, 2.3760370822856203e-05, 2.2947733668843284e-05, 2.2162890672916546e-05, 2.140488868462853e-05, 2.0672812752309255e-05, 1.9965775209129788e-05, 1.928291931108106e-05, 1.8623417417984456e-05, 1.798647099349182e-05, 1.737130878609605e-05, 1.6777186829131097e-05, 1.6203384802793153e-05, 1.564920603414066e-05, 1.511398204456782e-05, 1.4597063454857562e-05, 1.4097823623160366e-05, 1.3615658644994255e-05, 1.3149984624760691e-05, 1.2700236766249873e-05, 1.226587119163014e-05, 1.1846361303469166e-05, 1.1441199603723362e-05, 1.104989496525377e-05, 1.0671973541320767e-05, 1.0306977856089361e-05, 9.954464985639788e-06, 9.614008376956917e-06, 9.285196028940845e-06, 8.967629582912195e-06, 8.660924322612118e-06, 8.36470917420229e-06, 8.07862488727551e-06, 7.802324944350403e-06, 7.535474651376717e-06, 7.277751137735322e-06, 7.02884199199616e-06, 6.788446171412943e-06, 6.556272182933753e-06, 6.332038537948392e-06, 6.1154742070357315e-06, 5.9063168009743094e-06, 5.7043125707423314e-06, 5.5092173170123715e-06, 5.320794571161969e-06, 5.138816050020978e-06, 4.96306165587157e-06, 4.79331811220618e-06, 4.629379873222206e-06, 4.471048669074662e-06, 4.318132596381474e-06, 4.170446572970832e-06, 4.027811428386485e-06, 3.890054813382449e-06, 3.7570096083072713e-06, 3.6285148325987393e-06, 3.504414735289174e-06, 3.3845590223791078e-06, 3.268802402089932e-06, 3.1570048122375738e-06, 3.0490309654851444e-06, 2.9447498945955886e-06, 2.8440354071790352e-06, 2.7467654035717715e-06, 2.652822331583593e-06, 2.562092277003103e-06, 2.4744651909713866e-06, 2.3898351173556875e-06, 2.308099510628381e-06, 2.2291594632406486e-06, 2.1529192508751294e-06, 2.079286559819593e-06, 2.0081720322195906e-06, 1.9394897208258044e-06, 1.87315652055986e-06, 1.809092054827488e-06, 1.7472186755185248e-06, 1.6874614630069118e-06, 1.6297479987770203e-06, 1.5740083654236514e-06, 1.5201751466520363e-06, 1.4681830862173229e-06, 1.4179692016114132e-06, 1.3694726703761262e-06, 1.3226348301031976e-06, 1.2773989510606043e-06, 1.2337101225057268e-06, 1.1915155937458621e-06, 1.150764092017198e-06, 1.1114063909190008e-06, 1.0733947419794276e-06, 1.0366832157160388e-06, 1.0012272468884476e-06, 9.66983861871995e-07, 9.339116786577506e-07, 9.019706226354174e-07, 8.711219834367512e-07, 8.413284149355604e-07, 8.125538215608685e-07, 7.847633582969138e-07, 7.579233738397306e-07, 7.320013537537307e-07, 7.069659204717027e-07, 6.827867196079751e-07, 6.594344768018345e-07, 6.368809408741072e-07, 6.15098770140321e-07, 5.940615324107057e-07, 5.737438186770305e-07, 5.541210157389287e-07, 5.351693062038976e-07, 5.168657821741363e-07, 4.99188274716289e-07, 4.821153538614453e-07, 4.656263570268493e-07], "accuracy_train_first": 0.5126352941176471, "accuracy_train_last": 0.9886588235294118, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.49360000000000004, 0.42400000000000004, 0.3728, 0.32920000000000005, 0.3322666666666667, 0.30079999999999996, 0.27613333333333334, 0.2632, 0.26559999999999995, 0.24146666666666672, 0.2657333333333334, 0.22906666666666664, 0.2069333333333333, 0.21226666666666671, 0.2021333333333334, 0.20973333333333333, 0.19613333333333338, 0.19266666666666665, 0.18546666666666667, 0.18479999999999996, 0.1889333333333333, 0.18213333333333337, 0.17759999999999998, 0.18079999999999996, 0.17559999999999998, 0.17200000000000004, 0.17333333333333334, 0.16759999999999997, 0.17493333333333339, 0.16759999999999997, 0.16959999999999997, 0.16586666666666672, 0.16333333333333333, 0.1625333333333333, 0.16400000000000003, 0.15880000000000005, 0.16493333333333338, 0.16093333333333337, 0.1604, 0.16386666666666672, 0.16173333333333328, 0.16026666666666667, 0.15880000000000005, 0.15626666666666666, 0.1585333333333333, 0.15959999999999996, 0.1578666666666667, 0.15733333333333333, 0.15533333333333332, 0.1558666666666667, 0.15773333333333328, 0.1552, 0.15293333333333337, 0.15293333333333337, 0.15626666666666666, 0.15800000000000003, 0.15159999999999996, 0.15306666666666668, 0.1512, 0.1492, 0.15066666666666662, 0.14680000000000004, 0.1498666666666667, 0.14759999999999995, 0.14946666666666664, 0.14893333333333336, 0.1472, 0.14626666666666666, 0.1478666666666667, 0.14946666666666664, 0.15013333333333334, 0.14773333333333338, 0.15000000000000002, 0.14680000000000004, 0.14946666666666664, 0.14693333333333336, 0.14680000000000004, 0.14493333333333336, 0.14626666666666666, 0.1438666666666667, 0.14400000000000002, 0.14493333333333336, 0.14466666666666672, 0.14533333333333331, 0.14573333333333338, 0.1458666666666667, 0.1452, 0.14559999999999995, 0.14400000000000002, 0.14546666666666663, 0.14359999999999995, 0.14239999999999997, 0.14573333333333338, 0.14400000000000002, 0.14226666666666665, 0.14413333333333334, 0.14213333333333333, 0.14266666666666672, 0.14400000000000002, 0.14359999999999995, 0.14200000000000002, 0.1425333333333333, 0.1433333333333333, 0.14493333333333336, 0.14173333333333338, 0.14239999999999997, 0.14466666666666672, 0.14439999999999997, 0.1418666666666667, 0.1418666666666667, 0.14306666666666668, 0.1412, 0.14213333333333333, 0.14239999999999997, 0.14266666666666672, 0.14280000000000004, 0.14293333333333336, 0.14213333333333333, 0.14146666666666663, 0.14080000000000004, 0.14026666666666665, 0.14213333333333333, 0.14213333333333333, 0.14093333333333335, 0.14266666666666672, 0.1413333333333333, 0.14146666666666663, 0.14400000000000002, 0.14173333333333338, 0.14080000000000004, 0.14293333333333336, 0.14213333333333333, 0.1412, 0.14200000000000002, 0.14066666666666672, 0.1418666666666667, 0.1418666666666667, 0.13973333333333338, 0.13973333333333338, 0.13906666666666667, 0.14239999999999997, 0.14173333333333338, 0.14066666666666672, 0.14213333333333333, 0.14159999999999995, 0.1412, 0.14213333333333333, 0.14266666666666672, 0.14239999999999997, 0.14200000000000002, 0.1412, 0.1412, 0.1413333333333333, 0.14239999999999997, 0.14239999999999997, 0.14173333333333338, 0.14093333333333335, 0.14106666666666667, 0.14026666666666665, 0.14093333333333335, 0.14159999999999995, 0.1413333333333333, 0.14, 0.1405333333333333, 0.14200000000000002, 0.1418666666666667, 0.14159999999999995, 0.14159999999999995, 0.1412, 0.14173333333333338, 0.14066666666666672, 0.14146666666666663, 0.14080000000000004, 0.14093333333333335, 0.14200000000000002, 0.1405333333333333, 0.14093333333333335, 0.14200000000000002, 0.1405333333333333, 0.14066666666666672, 0.14080000000000004, 0.1405333333333333, 0.1413333333333333, 0.14026666666666665, 0.1413333333333333, 0.14080000000000004, 0.1412, 0.1412, 0.14146666666666663, 0.14066666666666672], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.034201366431269434, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0003464063904293422, "optimization": "rmsprop", "nb_data_augmentation": 1, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 9.721767797187249e-05, "rotation_range": [0, 0], "momentum": 0.6024969310219157}, "accuracy_valid_max": 0.8609333333333333, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8593333333333333, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.5064, 0.576, 0.6272, 0.6708, 0.6677333333333333, 0.6992, 0.7238666666666667, 0.7368, 0.7344, 0.7585333333333333, 0.7342666666666666, 0.7709333333333334, 0.7930666666666667, 0.7877333333333333, 0.7978666666666666, 0.7902666666666667, 0.8038666666666666, 0.8073333333333333, 0.8145333333333333, 0.8152, 0.8110666666666667, 0.8178666666666666, 0.8224, 0.8192, 0.8244, 0.828, 0.8266666666666667, 0.8324, 0.8250666666666666, 0.8324, 0.8304, 0.8341333333333333, 0.8366666666666667, 0.8374666666666667, 0.836, 0.8412, 0.8350666666666666, 0.8390666666666666, 0.8396, 0.8361333333333333, 0.8382666666666667, 0.8397333333333333, 0.8412, 0.8437333333333333, 0.8414666666666667, 0.8404, 0.8421333333333333, 0.8426666666666667, 0.8446666666666667, 0.8441333333333333, 0.8422666666666667, 0.8448, 0.8470666666666666, 0.8470666666666666, 0.8437333333333333, 0.842, 0.8484, 0.8469333333333333, 0.8488, 0.8508, 0.8493333333333334, 0.8532, 0.8501333333333333, 0.8524, 0.8505333333333334, 0.8510666666666666, 0.8528, 0.8537333333333333, 0.8521333333333333, 0.8505333333333334, 0.8498666666666667, 0.8522666666666666, 0.85, 0.8532, 0.8505333333333334, 0.8530666666666666, 0.8532, 0.8550666666666666, 0.8537333333333333, 0.8561333333333333, 0.856, 0.8550666666666666, 0.8553333333333333, 0.8546666666666667, 0.8542666666666666, 0.8541333333333333, 0.8548, 0.8544, 0.856, 0.8545333333333334, 0.8564, 0.8576, 0.8542666666666666, 0.856, 0.8577333333333333, 0.8558666666666667, 0.8578666666666667, 0.8573333333333333, 0.856, 0.8564, 0.858, 0.8574666666666667, 0.8566666666666667, 0.8550666666666666, 0.8582666666666666, 0.8576, 0.8553333333333333, 0.8556, 0.8581333333333333, 0.8581333333333333, 0.8569333333333333, 0.8588, 0.8578666666666667, 0.8576, 0.8573333333333333, 0.8572, 0.8570666666666666, 0.8578666666666667, 0.8585333333333334, 0.8592, 0.8597333333333333, 0.8578666666666667, 0.8578666666666667, 0.8590666666666666, 0.8573333333333333, 0.8586666666666667, 0.8585333333333334, 0.856, 0.8582666666666666, 0.8592, 0.8570666666666666, 0.8578666666666667, 0.8588, 0.858, 0.8593333333333333, 0.8581333333333333, 0.8581333333333333, 0.8602666666666666, 0.8602666666666666, 0.8609333333333333, 0.8576, 0.8582666666666666, 0.8593333333333333, 0.8578666666666667, 0.8584, 0.8588, 0.8578666666666667, 0.8573333333333333, 0.8576, 0.858, 0.8588, 0.8588, 0.8586666666666667, 0.8576, 0.8576, 0.8582666666666666, 0.8590666666666666, 0.8589333333333333, 0.8597333333333333, 0.8590666666666666, 0.8584, 0.8586666666666667, 0.86, 0.8594666666666667, 0.858, 0.8581333333333333, 0.8584, 0.8584, 0.8588, 0.8582666666666666, 0.8593333333333333, 0.8585333333333334, 0.8592, 0.8590666666666666, 0.858, 0.8594666666666667, 0.8590666666666666, 0.858, 0.8594666666666667, 0.8593333333333333, 0.8592, 0.8594666666666667, 0.8586666666666667, 0.8597333333333333, 0.8586666666666667, 0.8592, 0.8588, 0.8588, 0.8585333333333334, 0.8593333333333333], "seed": 612286753, "model": "residualv3", "loss_std": [0.3103829622268677, 0.18356390297412872, 0.18274813890457153, 0.18382996320724487, 0.18052777647972107, 0.17918822169303894, 0.17743952572345734, 0.1765078753232956, 0.17125046253204346, 0.17336691915988922, 0.17269235849380493, 0.1694968044757843, 0.16863229870796204, 0.16387635469436646, 0.1646537333726883, 0.15836842358112335, 0.1586952954530716, 0.15478812158107758, 0.15070855617523193, 0.14900138974189758, 0.1437719613313675, 0.1469431072473526, 0.14080838859081268, 0.13654355704784393, 0.13575363159179688, 0.13215303421020508, 0.13244026899337769, 0.1291002780199051, 0.12905892729759216, 0.12486414611339569, 0.12344968318939209, 0.12249957770109177, 0.12204433232545853, 0.11738882213830948, 0.1150674894452095, 0.11427021026611328, 0.11307715624570847, 0.11136163771152496, 0.11009447276592255, 0.10883690416812897, 0.10794663429260254, 0.10325821489095688, 0.10395581275224686, 0.09958138316869736, 0.10165878385305405, 0.09980316460132599, 0.09901068359613419, 0.09714294224977493, 0.09591151773929596, 0.0974477231502533, 0.09561262279748917, 0.09431707113981247, 0.0928560420870781, 0.09069225192070007, 0.09095867723226547, 0.08976146578788757, 0.08783397078514099, 0.0878327339887619, 0.08685038238763809, 0.087679423391819, 0.08877826482057571, 0.08793296664953232, 0.0859532579779625, 0.08536655455827713, 0.08330978453159332, 0.08224158734083176, 0.08481467515230179, 0.08237367868423462, 0.08246291428804398, 0.08014102280139923, 0.08043105900287628, 0.08097600191831589, 0.08115385472774506, 0.08118169009685516, 0.08219016343355179, 0.07677166908979416, 0.07917354255914688, 0.07959529012441635, 0.07950254529714584, 0.07712571322917938, 0.0776015892624855, 0.07722535729408264, 0.07623760402202606, 0.07759860157966614, 0.07591073215007782, 0.07697125524282455, 0.07612714916467667, 0.07691478729248047, 0.07451611757278442, 0.07486778497695923, 0.07529271394014359, 0.07415902614593506, 0.07344210147857666, 0.07331819087266922, 0.0750449150800705, 0.07415516674518585, 0.0749824270606041, 0.07564626634120941, 0.07513558864593506, 0.07296378910541534, 0.07388154417276382, 0.07338186353445053, 0.073839470744133, 0.07476136833429337, 0.07213374227285385, 0.074518121778965, 0.07078833878040314, 0.07368890196084976, 0.07285019755363464, 0.07279415428638458, 0.07259228825569153, 0.07117333263158798, 0.07137328386306763, 0.07162991166114807, 0.07317063212394714, 0.07234051078557968, 0.07156650722026825, 0.07224828749895096, 0.07013136148452759, 0.07417924702167511, 0.07275065779685974, 0.07318483293056488, 0.07260320335626602, 0.06904815882444382, 0.07089077681303024, 0.07109087705612183, 0.0719585195183754, 0.06986596435308456, 0.07087771594524384, 0.07129178941249847, 0.07104308158159256, 0.07139500975608826, 0.0688922181725502, 0.0728812962770462, 0.07137730717658997, 0.0710504800081253, 0.06985944509506226, 0.07002820074558258, 0.07282422482967377, 0.06996800750494003, 0.06936757266521454, 0.06990943104028702, 0.07023444026708603, 0.06835374236106873, 0.07011375576257706, 0.0732400044798851, 0.06901726871728897, 0.0696134939789772, 0.07100009173154831, 0.06905846297740936, 0.06702841073274612, 0.06937120109796524, 0.06854672729969025, 0.06847475469112396, 0.07124850153923035, 0.06920715421438217, 0.06896349042654037, 0.06979754567146301, 0.06975695490837097, 0.0702098160982132, 0.07020357996225357, 0.07239463925361633, 0.0680679902434349, 0.06963205337524414, 0.07028339058160782, 0.07223255187273026, 0.06963648647069931, 0.06955201178789139, 0.06962666660547256, 0.0704389289021492, 0.07181936502456665, 0.06860723346471786, 0.06922882050275803, 0.07014527916908264, 0.06893771886825562, 0.06955166161060333, 0.06837791204452515, 0.06876608729362488, 0.06903213262557983, 0.06894861161708832, 0.06998400390148163, 0.06588426977396011, 0.06726009398698807, 0.06920845806598663, 0.06960809975862503, 0.06641935557126999, 0.06866198033094406, 0.07172759622335434, 0.06942375004291534, 0.0695144385099411]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:45 2016", "state": "available"}], "summary": "27adbb15ac4d6b5390958a276c30d140"}