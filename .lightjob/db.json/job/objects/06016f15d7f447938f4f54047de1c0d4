{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 64, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.9773463010787964, 1.7295974493026733, 1.6278419494628906, 1.5614371299743652, 1.5096932649612427, 1.4643219709396362, 1.4224903583526611, 1.3833897113800049, 1.3469799757003784, 1.3130236864089966, 1.2814067602157593, 1.2518912553787231, 1.224172830581665, 1.198009729385376, 1.1731128692626953, 1.1496500968933105, 1.1275086402893066, 1.1065071821212769, 1.0864895582199097, 1.0673316717147827, 1.0490111112594604, 1.0315557718276978, 1.0149996280670166, 0.9992356896400452, 0.9841530323028564, 0.9695773720741272, 0.9555909633636475, 0.9420779943466187, 0.9290714859962463, 0.9164767861366272, 0.904335618019104, 0.8925082683563232, 0.8809967637062073, 0.8698006272315979, 0.8588210940361023, 0.8480528593063354, 0.8375140428543091, 0.8272515535354614, 0.8171078562736511, 0.8070126175880432, 0.7970591187477112, 0.7872076034545898, 0.7774993777275085, 0.7678655385971069, 0.7583573460578918, 0.7489261031150818, 0.7395171523094177, 0.7301497459411621, 0.7208843231201172, 0.7116291522979736, 0.7024281620979309, 0.6932518482208252, 0.6840755343437195, 0.674888014793396, 0.6657412648200989, 0.6565404534339905, 0.647352397441864, 0.6381151676177979, 0.6288784146308899, 0.6195788383483887, 0.6103030443191528, 0.6009493470191956, 0.5915635228157043, 0.5821795463562012, 0.5727619528770447, 0.5632860064506531, 0.5536705255508423, 0.5440496802330017, 0.5344106554985046, 0.5247098207473755, 0.5149598121643066, 0.5052068829536438, 0.49540209770202637, 0.48561128973960876, 0.47568783164024353, 0.4657573401927948, 0.4557170569896698, 0.4456244111061096, 0.43542370200157166, 0.42528417706489563, 0.41506823897361755, 0.40483197569847107, 0.39457976818084717, 0.38430118560791016, 0.37409448623657227, 0.3638356029987335, 0.3536204993724823, 0.34338006377220154, 0.3332379460334778, 0.32303258776664734, 0.3129427433013916, 0.30292320251464844, 0.29303908348083496, 0.2831747531890869, 0.2734008729457855, 0.2637728154659271, 0.2543293833732605, 0.24499376118183136, 0.23586276173591614, 0.2268264889717102, 0.2179848849773407, 0.20935718715190887, 0.20088832080364227, 0.19264467060565948, 0.18457897007465363, 0.1767250895500183, 0.16914288699626923, 0.1617589294910431, 0.15463630855083466, 0.1477552354335785, 0.14109976589679718, 0.134690061211586, 0.12856841087341309, 0.12267936766147614, 0.11702319234609604, 0.11167532205581665, 0.10653132200241089, 0.10160090029239655, 0.09689273685216904, 0.0924244150519371, 0.08819955587387085], "moving_avg_accuracy_train": [0.03396, 0.0693545882352941, 0.1040097176470588, 0.13722286352941174, 0.16919940070588232, 0.19977828416470586, 0.22901692633647056, 0.25679052782047057, 0.28319382797959997, 0.3080909157698753, 0.3315688830164172, 0.35372023000889313, 0.3747646775962391, 0.394542327483674, 0.41310927120589486, 0.43062893232059946, 0.4471048626179513, 0.4626767292973326, 0.47730787989701107, 0.49102650367201583, 0.5040909121283437, 0.5163712326802151, 0.5280188152945465, 0.5389651690592095, 0.5492639462709357, 0.5589916692909009, 0.5681019141265168, 0.5768564285962181, 0.5851237269130669, 0.592985471868819, 0.6005857482113489, 0.6078730557431552, 0.6149045736982514, 0.6215811751519555, 0.6279007046955836, 0.6339200459907312, 0.6396174531563639, 0.6451286490171981, 0.650444019409596, 0.6556254998215775, 0.6605711851335374, 0.6654034783848896, 0.6699619540758124, 0.6744128174917606, 0.6788162416249375, 0.6830169704036202, 0.6871693910103169, 0.6912242166151675, 0.6951700302477685, 0.6989824389876975, 0.7026018421477513, 0.7061369520506233, 0.709598550963208, 0.712977519396299, 0.7162468262801986, 0.7194644965933553, 0.722623929286961, 0.7257380069465001, 0.7287924415459678, 0.7318473150384298, 0.7348155247110574, 0.7377433840046574, 0.7406749279571329, 0.7435156704555372, 0.74644645635116, 0.749206516598397, 0.7519658649385573, 0.754691631385878, 0.7573754094237608, 0.7600402214225612, 0.7627373757508933, 0.7654495205287452, 0.7680080978876354, 0.7705861116282836, 0.7731016181125141, 0.7756385151247921, 0.7782581930240775, 0.7808864913687286, 0.783484901055385, 0.7859858227145524, 0.7884578286783913, 0.7909555752223169, 0.7934529588765558, 0.7958276629889002, 0.798254308454716, 0.8005277011386562, 0.8027972839659671, 0.8049952026281939, 0.8070862706006686, 0.8092082317758958, 0.8112874085983063, 0.8132739618561228, 0.8152453891999223, 0.8171984973387536, 0.8191139417225253, 0.8209296063738022, 0.8228154692658337, 0.8246750988098385, 0.8265770006935604, 0.8284910653300868, 0.8303454882088428, 0.8320497629173703, 0.833863610155045, 0.8355972491395405, 0.8372610536373511, 0.8389067129794984, 0.8405972181521368, 0.8422833786898642, 0.843960923173819, 0.8457365955623194, 0.8474052889472639, 0.8492506424054787, 0.8512408722825779, 0.853286196819026, 0.8554916947841822, 0.8577095841292934, 0.8599645080693052, 0.8621774690270806, 0.8645408985949607, 0.8669409263825234, 0.869282127861918], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.03358666666666666, 0.06861466666666666, 0.10309986666666665, 0.13630988, 0.167598892, 0.19730566946666667, 0.22550843585333333, 0.25210425893466665, 0.27760049970786665, 0.30164044973708, 0.32406307143003865, 0.34532343095370144, 0.36533775452499795, 0.3838706457391648, 0.4013635811652483, 0.41764055638205677, 0.4329165007438511, 0.4472915173361326, 0.46081569893585267, 0.4735741290422674, 0.485670049471374, 0.49716971119090325, 0.5078527400718129, 0.517920799397965, 0.5272087194581685, 0.536021180845685, 0.5442723960944498, 0.5519518231516715, 0.559063307503171, 0.5656503100861873, 0.5718586124109019, 0.5777927511698117, 0.5833601427194972, 0.5885441284475476, 0.5933697156027927, 0.5978594107091801, 0.6020068029715955, 0.605966122674436, 0.6097295104069923, 0.6132232260329598, 0.6164075700963305, 0.6195934797533642, 0.6226074651113611, 0.625386718600225, 0.6279947134068692, 0.630341908732849, 0.6326677178595641, 0.6347876127402744, 0.6367221847995803, 0.6384632996529557, 0.6401236363543268, 0.6417246060522275, 0.6431388121136714, 0.6445715975689709, 0.6457544378120739, 0.6469123273641998, 0.6479677612944464, 0.6490509851650017, 0.6499192199818349, 0.6507272979836514, 0.6514012348519529, 0.6520077780334242, 0.6526603335634151, 0.6533409668737402, 0.6539802035196995, 0.6545555165010629, 0.6550999648509566, 0.6556033016991943, 0.6560696381959416, 0.656449341043014, 0.656897740272046, 0.6572212995781748, 0.6573391696203573, 0.657338585991655, 0.6573647273924894, 0.6571882546532405, 0.6568294291879164, 0.6563598196024581, 0.6558571709755456, 0.6553247872113244, 0.6549256418235252, 0.6544330776411726, 0.6540164365437221, 0.6533881262226833, 0.652849313600415, 0.6522443822403735, 0.6514466106830028, 0.6506886162813692, 0.649886421319899, 0.6490844458545758, 0.648322667935785, 0.6475170678088732, 0.6466186943613192, 0.6457968249251873, 0.6450171424326686, 0.6442220948560684, 0.6434398853704616, 0.6426558968334154, 0.6418569738167406, 0.6410179431017332, 0.64023614879156, 0.639372533912404, 0.6385552805211636, 0.6376197524690472, 0.6366844438888092, 0.635802666166595, 0.6350757328832688, 0.6343548262616086, 0.6336926769687811, 0.6331367426052362, 0.6326230683447126, 0.6322007615102414, 0.632007352025884, 0.6319932834899622, 0.6319272884742992, 0.6318545596268693, 0.6316824369975158, 0.6314875266310975, 0.6314987739679878, 0.6315622299045224, 0.6316993402474035], "moving_var_accuracy_train": [0.0103795344, 0.020616572847114184, 0.029363717513318198, 0.0363553632966085, 0.041922317335131506, 0.046145698623913156, 0.049225212525958316, 0.05124504772789091, 0.05239475128873956, 0.05273406098379569, 0.05242158939968336, 0.051595570021944676, 0.050421831988059176, 0.04890004770488285, 0.04711262552705161, 0.04516380970451329, 0.04309053524653072, 0.040963829008801494, 0.0387940812187556, 0.03660847884140118, 0.03448373987208499, 0.03239262234058695, 0.030374355733347336, 0.02841532410668297, 0.026528375004525595, 0.024727194860451492, 0.023001444423090118, 0.021391073693182983, 0.01986710031700262, 0.018436653589046004, 0.01711286603448678, 0.015879523090606185, 0.014736550984321136, 0.013664088948633477, 0.01265710813664521, 0.011717489549627903, 0.010837884630364145, 0.010027455685676017, 0.00927898857878383, 0.008592719374243187, 0.007953585665663291, 0.007368386621700532, 0.006818565265153087, 0.006315000404964611, 0.00585801166133795, 0.005431025595652636, 0.005043106408141655, 0.004686770263499263, 0.004358218244158308, 0.004053206563345063, 0.003765786620125622, 0.0035016809763415077, 0.003259356881991827, 0.003036178042839071, 0.0028287555460651775, 0.002639060611656184, 0.0024649926849993837, 0.0023057707335262154, 0.002159159796675419, 0.002027234085502402, 0.001903803094898283, 0.0017905740257965414, 0.001688862172724545, 0.0015926043169322561, 0.0015106494389328614, 0.001428145888154974, 0.0013538573287005854, 0.0012853398203585829, 0.0012216298193323224, 0.0011633778442996447, 0.001112511833107246, 0.0010674622134607826, 0.001019632855027534, 0.0009774849631475203, 0.000936686422682618, 0.0009009403984725019, 0.0008726107692892959, 0.0008475212620568264, 0.0008235347319485395, 0.0007974727410613181, 0.000772722788322481, 0.0007515991496694622, 0.0007325715607506531, 0.0007100673812662576, 0.0006920581170905143, 0.0006693671340399968, 0.0006487894765262157, 0.0006273881468854788, 0.000604002419586517, 0.0005841266506904121, 0.0005646207719510087, 0.0005436762393711772, 0.0005242873473809823, 0.0005061902952606058, 0.0004885916104204483, 0.000469402192511471, 0.0004544702828881981, 0.00044014725296779997, 0.0004286876046487689, 0.00041879163507909745, 0.0004078624294904721, 0.0003932171570805629, 0.0003835058175870893, 0.00037220477298544367, 0.00035989850434931415, 0.00034828240594795125, 0.00033917443500161077, 0.00033084522773235486, 0.00032308810441994255, 0.00031915640585949436, 0.0003123016037901637, 0.000311719407882855, 0.00031619660176785373, 0.00032222711372563786, 0.00033378239382184864, 0.00034467545276408335, 0.00035597004526482145, 0.0003644478065440815, 0.00037827521979064566, 0.0003922888982412416, 0.0004023910277211977], "duration": 23172.148211, "accuracy_train": [0.3396, 0.38790588235294116, 0.4159058823529412, 0.43614117647058825, 0.45698823529411764, 0.47498823529411766, 0.49216470588235295, 0.5067529411764706, 0.5208235294117647, 0.532164705882353, 0.5428705882352941, 0.5530823529411765, 0.5641647058823529, 0.5725411764705882, 0.5802117647058823, 0.5883058823529411, 0.5953882352941177, 0.6028235294117648, 0.6089882352941176, 0.6144941176470589, 0.6216705882352941, 0.6268941176470588, 0.6328470588235294, 0.6374823529411765, 0.6419529411764706, 0.6465411764705883, 0.6500941176470588, 0.6556470588235294, 0.6595294117647059, 0.6637411764705883, 0.6689882352941177, 0.6734588235294118, 0.6781882352941176, 0.6816705882352941, 0.6847764705882353, 0.6880941176470589, 0.6908941176470588, 0.6947294117647059, 0.6982823529411765, 0.7022588235294117, 0.7050823529411765, 0.7088941176470588, 0.7109882352941177, 0.7144705882352941, 0.7184470588235294, 0.7208235294117648, 0.7245411764705882, 0.7277176470588236, 0.7306823529411764, 0.7332941176470589, 0.7351764705882353, 0.7379529411764706, 0.7407529411764706, 0.7433882352941177, 0.7456705882352941, 0.7484235294117647, 0.7510588235294118, 0.7537647058823529, 0.7562823529411765, 0.7593411764705882, 0.7615294117647059, 0.7640941176470588, 0.7670588235294118, 0.7690823529411764, 0.7728235294117647, 0.7740470588235294, 0.7768, 0.7792235294117648, 0.7815294117647059, 0.7840235294117647, 0.7870117647058823, 0.7898588235294117, 0.7910352941176471, 0.7937882352941177, 0.7957411764705883, 0.7984705882352942, 0.8018352941176471, 0.8045411764705882, 0.8068705882352941, 0.8084941176470588, 0.8107058823529412, 0.813435294117647, 0.8159294117647059, 0.8172, 0.8200941176470589, 0.8209882352941177, 0.8232235294117647, 0.8247764705882353, 0.8259058823529412, 0.8283058823529412, 0.83, 0.8311529411764705, 0.8329882352941177, 0.8347764705882353, 0.8363529411764706, 0.8372705882352941, 0.8397882352941176, 0.8414117647058823, 0.8436941176470588, 0.8457176470588236, 0.8470352941176471, 0.8473882352941177, 0.8501882352941177, 0.8512, 0.8522352941176471, 0.8537176470588236, 0.8558117647058824, 0.8574588235294117, 0.8590588235294118, 0.8617176470588235, 0.8624235294117647, 0.8658588235294118, 0.8691529411764706, 0.8716941176470588, 0.8753411764705883, 0.8776705882352941, 0.8802588235294118, 0.8820941176470588, 0.8858117647058823, 0.8885411764705883, 0.8903529411764706], "end": "2016-02-05 16:22:08.355000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0], "moving_var_accuracy_valid": [0.010152577599999999, 0.020179966896, 0.028865031377759996, 0.035904673110385595, 0.04112522624677233, 0.044955137269181586, 0.047618187829011484, 0.04922240929447258, 0.0501506930071102, 0.050336896483062846, 0.049828172507026425, 0.04891338124000238, 0.04762720144815114, 0.045955693814141316, 0.04411414954111734, 0.04208719388688299, 0.03997866478349642, 0.03784056822340211, 0.03570264279254176, 0.03359737636231002, 0.03155444034532452, 0.029589176287764533, 0.02765740261362123, 0.025803954719613362, 0.0239999483790546, 0.02229888882250736, 0.02068174291798967, 0.0191443310255354, 0.017685056810116465, 0.01630704855636278, 0.015023230860522013, 0.013837833799759773, 0.01273301305779138, 0.011701575124269904, 0.010740994234378727, 0.009848311070275725, 0.009018287726453234, 0.008257544866391617, 0.0075592581647824595, 0.006913186788180376, 0.006313128533387656, 0.005773165863133911, 0.005277606246464499, 0.004819363871416308, 0.004398642216478023, 0.004008361927914931, 0.0036562102279686388, 0.003331034793919129, 0.0030316144360010393, 0.0027557363207947323, 0.002504973150372538, 0.002277543771097649, 0.002067789203045907, 0.001879486150189577, 0.001704129534536953, 0.0015457829550175595, 0.0014012301265458472, 0.001271667479474931, 0.0011512852168018897, 0.0010420336056348788, 0.0009419179631934955, 0.0008510372185530507, 0.000769765955175241, 0.0006969587149858346, 0.0006309404548930868, 0.0005708252746425051, 0.0005164105632295725, 0.00046704963875175974, 0.0004223019024303701, 0.00038136928045600786, 0.00034504190922777497, 0.00031147993392624003, 0.000280456980655213, 0.00025241128565529387, 0.0002271763074453028, 0.00020473896035005476, 0.0001854238657461345, 0.00016886627763630998, 0.00015425355065191204, 0.00014137908783837813, 0.00012867503241995197, 0.0001179911044415862, 0.00010775430223419115, 0.0001005318367464878, 9.309152444908006e-05, 8.707584955742693e-05, 8.409621972143106e-05, 8.085759736545902e-05, 7.856348843478746e-05, 7.649562141413248e-05, 7.406880965073609e-05, 7.250285276598575e-05, 7.25162411508176e-05, 7.134384136616615e-05, 6.968060033181152e-05, 6.840144614015093e-05, 6.706796664049577e-05, 6.589291221242407e-05, 6.504812287033754e-05, 6.487906344983619e-05, 6.389197819562708e-05, 6.421525631156097e-05, 6.380485862985033e-05, 6.5301287393535e-05, 6.664437791658391e-05, 6.697772768746571e-05, 6.503584290438526e-05, 6.320961582832857e-05, 6.083462941942358e-05, 5.753273362661058e-05, 5.415421147726994e-05, 5.034387789151323e-05, 4.5646155160116746e-05, 4.10833209574319e-05, 3.7014186940519815e-05, 3.33603736137042e-05, 3.0290972048154167e-05, 2.760378530177439e-05, 2.484454529488106e-05, 2.2396330668326244e-05, 2.032589081661837e-05], "accuracy_test": 0.4254, "start": "2016-02-05 09:55:56.206000", "learning_rate_per_epoch": [0.00048745941603556275, 0.0004857633903156966, 0.00048407327267341316, 0.000482389034004882, 0.0004807106452062726, 0.00047903810627758503, 0.0004773713881149888, 0.00047571046161465347, 0.000474055326776579, 0.00047240592539310455, 0.0004707622865680605, 0.000469124352093786, 0.000467492121970281, 0.00046586556709371507, 0.0004642446874640882, 0.0004626294248737395, 0.0004610197793226689, 0.0004594157508108765, 0.00045781731023453176, 0.00045622442848980427, 0.00045463707647286355, 0.0004530552541837096, 0.000451478932518512, 0.00044990808237344027, 0.0004483427037484944, 0.0004467827675398439, 0.00044522827374748886, 0.00044367919326759875, 0.00044213549699634314, 0.00044059715582989156, 0.00043906416976824403, 0.00043753653881140053, 0.00043601420475170016, 0.0004344971675891429, 0.0004329854273237288, 0.0004314789257477969, 0.0004299776628613472, 0.0004284816386643797, 0.000426990824053064, 0.00042550518992356956, 0.00042402473627589643, 0.00042254943400621414, 0.00042107925401069224, 0.0004196141962893307, 0.00041815423173829913, 0.000416699331253767, 0.00041524949483573437, 0.0004138047224842012, 0.0004123649559915066, 0.0004109301953576505, 0.000409500440582633, 0.0004080756625626236, 0.00040665583219379187, 0.00040524094947613776, 0.00040383098530583084, 0.0004024259396828711, 0.0004010257835034281, 0.0003996304876636714, 0.0003982400521636009, 0.0003968544478993863, 0.00039547367487102747, 0.000394097703974694, 0.00039272650610655546, 0.0003913600812666118, 0.00038999842945486307, 0.0003886414924636483, 0.000387289299396798, 0.00038594179204665124, 0.000384598970413208, 0.0003832608344964683, 0.0003819273551926017, 0.0003805985033977777, 0.0003792742791119963, 0.0003779546532314271, 0.00037663962575607, 0.00037532916758209467, 0.00037402327870950103, 0.00037272193003445864, 0.0003714251215569675, 0.00037013282417319715, 0.00036884500877931714, 0.00036756167537532747, 0.00036628282396122813, 0.0003650083963293582, 0.0003637384215835482, 0.0003624728706199676, 0.00036121171433478594, 0.00035995495272800326, 0.0003587025566957891, 0.00035745452623814344, 0.00035621083225123584, 0.0003549714747350663, 0.00035373642458580434, 0.00035250565269961953, 0.0003512791881803423, 0.0003500569728203118, 0.00034883900661952794, 0.00034762528957799077, 0.00034641579259186983, 0.0003452105156611651, 0.0003440094296820462, 0.00034281250555068254, 0.00034161974326707423, 0.0003404311428312212, 0.0003392466751392931, 0.0003380663401912898, 0.0003368901088833809, 0.00033571795211173594, 0.00033454986987635493, 0.00033338586217723787, 0.0003322258999105543, 0.0003310699830763042, 0.00032991808257065713, 0.0003287701983936131, 0.00032762630144134164, 0.00032648639171384275, 0.000325350440107286, 0.0003242184466216713, 0.0003230903821531683, 0.000321966246701777, 0.00032084601116366684], "accuracy_train_first": 0.3396, "accuracy_train_last": 0.8903529411764706, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.6641333333333334, 0.6161333333333333, 0.5865333333333334, 0.5648, 0.5508, 0.5353333333333333, 0.5206666666666666, 0.5085333333333333, 0.49293333333333333, 0.482, 0.4741333333333333, 0.4633333333333334, 0.45453333333333334, 0.44933333333333336, 0.44120000000000004, 0.4358666666666666, 0.4296, 0.42333333333333334, 0.41746666666666665, 0.41159999999999997, 0.40546666666666664, 0.3993333333333333, 0.396, 0.39146666666666663, 0.3892, 0.3846666666666667, 0.3814666666666666, 0.37893333333333334, 0.37693333333333334, 0.37506666666666666, 0.37226666666666663, 0.3688, 0.3665333333333334, 0.3648, 0.36319999999999997, 0.36173333333333335, 0.3606666666666667, 0.35840000000000005, 0.35640000000000005, 0.3553333333333333, 0.3549333333333333, 0.35173333333333334, 0.3502666666666666, 0.3496, 0.34853333333333336, 0.34853333333333336, 0.34640000000000004, 0.3461333333333333, 0.34586666666666666, 0.34586666666666666, 0.3449333333333333, 0.34386666666666665, 0.3441333333333333, 0.34253333333333336, 0.3436, 0.3426666666666667, 0.34253333333333336, 0.34119999999999995, 0.3422666666666667, 0.34199999999999997, 0.34253333333333336, 0.34253333333333336, 0.3414666666666667, 0.34053333333333335, 0.3402666666666667, 0.3402666666666667, 0.33999999999999997, 0.33986666666666665, 0.33973333333333333, 0.3401333333333333, 0.3390666666666666, 0.33986666666666665, 0.3416, 0.3426666666666667, 0.34240000000000004, 0.34440000000000004, 0.34640000000000004, 0.34786666666666666, 0.3486666666666667, 0.3494666666666667, 0.3486666666666667, 0.35, 0.34973333333333334, 0.3522666666666666, 0.352, 0.35319999999999996, 0.35573333333333335, 0.3561333333333333, 0.3573333333333333, 0.3581333333333333, 0.35853333333333337, 0.35973333333333335, 0.3614666666666667, 0.36160000000000003, 0.362, 0.36293333333333333, 0.36360000000000003, 0.36439999999999995, 0.3653333333333333, 0.3665333333333334, 0.3668, 0.36839999999999995, 0.3688, 0.3708, 0.37173333333333336, 0.3721333333333333, 0.3714666666666666, 0.3721333333333333, 0.37226666666666663, 0.3718666666666667, 0.372, 0.37160000000000004, 0.36973333333333336, 0.3681333333333333, 0.3686666666666667, 0.3688, 0.3698666666666667, 0.37026666666666663, 0.36839999999999995, 0.3678666666666667, 0.36706666666666665], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.0034793168831986025, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "valid_ratio": 0.15, "learning_rate": 0.000489161328872585, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.2343506885672533e-08, "rotation_range": [0, 0], "momentum": 0.8052907274527985}, "accuracy_valid_max": 0.6609333333333334, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6329333333333333, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.33586666666666665, 0.3838666666666667, 0.41346666666666665, 0.4352, 0.4492, 0.4646666666666667, 0.47933333333333333, 0.49146666666666666, 0.5070666666666667, 0.518, 0.5258666666666667, 0.5366666666666666, 0.5454666666666667, 0.5506666666666666, 0.5588, 0.5641333333333334, 0.5704, 0.5766666666666667, 0.5825333333333333, 0.5884, 0.5945333333333334, 0.6006666666666667, 0.604, 0.6085333333333334, 0.6108, 0.6153333333333333, 0.6185333333333334, 0.6210666666666667, 0.6230666666666667, 0.6249333333333333, 0.6277333333333334, 0.6312, 0.6334666666666666, 0.6352, 0.6368, 0.6382666666666666, 0.6393333333333333, 0.6416, 0.6436, 0.6446666666666667, 0.6450666666666667, 0.6482666666666667, 0.6497333333333334, 0.6504, 0.6514666666666666, 0.6514666666666666, 0.6536, 0.6538666666666667, 0.6541333333333333, 0.6541333333333333, 0.6550666666666667, 0.6561333333333333, 0.6558666666666667, 0.6574666666666666, 0.6564, 0.6573333333333333, 0.6574666666666666, 0.6588, 0.6577333333333333, 0.658, 0.6574666666666666, 0.6574666666666666, 0.6585333333333333, 0.6594666666666666, 0.6597333333333333, 0.6597333333333333, 0.66, 0.6601333333333333, 0.6602666666666667, 0.6598666666666667, 0.6609333333333334, 0.6601333333333333, 0.6584, 0.6573333333333333, 0.6576, 0.6556, 0.6536, 0.6521333333333333, 0.6513333333333333, 0.6505333333333333, 0.6513333333333333, 0.65, 0.6502666666666667, 0.6477333333333334, 0.648, 0.6468, 0.6442666666666667, 0.6438666666666667, 0.6426666666666667, 0.6418666666666667, 0.6414666666666666, 0.6402666666666667, 0.6385333333333333, 0.6384, 0.638, 0.6370666666666667, 0.6364, 0.6356, 0.6346666666666667, 0.6334666666666666, 0.6332, 0.6316, 0.6312, 0.6292, 0.6282666666666666, 0.6278666666666667, 0.6285333333333334, 0.6278666666666667, 0.6277333333333334, 0.6281333333333333, 0.628, 0.6284, 0.6302666666666666, 0.6318666666666667, 0.6313333333333333, 0.6312, 0.6301333333333333, 0.6297333333333334, 0.6316, 0.6321333333333333, 0.6329333333333333], "seed": 625882075, "model": "residualv3", "loss_std": [0.18656517565250397, 0.0742475762963295, 0.07266058772802353, 0.07323012501001358, 0.07489951699972153, 0.0770777240395546, 0.07956688851118088, 0.08198840916156769, 0.08410190045833588, 0.08575664460659027, 0.08684684336185455, 0.08753153681755066, 0.08796527981758118, 0.08833068609237671, 0.08874856680631638, 0.08921489864587784, 0.08962126821279526, 0.09009215980768204, 0.09052373468875885, 0.09092540293931961, 0.09130456298589706, 0.09148404002189636, 0.09158605337142944, 0.09164192527532578, 0.09169347584247589, 0.09162735193967819, 0.09156663715839386, 0.09148886054754257, 0.09136254340410233, 0.09122859686613083, 0.0910673663020134, 0.0909205824136734, 0.09078353643417358, 0.09058277308940887, 0.09045138210058212, 0.09031044691801071, 0.09013289213180542, 0.0898766741156578, 0.08968205749988556, 0.08942225575447083, 0.08913282305002213, 0.08885025233030319, 0.08859580755233765, 0.08835087716579437, 0.08806630223989487, 0.08776138722896576, 0.08735493570566177, 0.08692461252212524, 0.08653420954942703, 0.08608242869377136, 0.08565055578947067, 0.0852612555027008, 0.08479276299476624, 0.0843091681599617, 0.08379103988409042, 0.08323142677545547, 0.08271823823451996, 0.08217120915651321, 0.08164111524820328, 0.08112078905105591, 0.08050405979156494, 0.07988181710243225, 0.07924233376979828, 0.07863213866949081, 0.07795421779155731, 0.07727867364883423, 0.07657993584871292, 0.0758267492055893, 0.07514569163322449, 0.07439693808555603, 0.07355053722858429, 0.07281424105167389, 0.07195569574832916, 0.07111833989620209, 0.07023806869983673, 0.06933943182229996, 0.06835968792438507, 0.06744303554296494, 0.06635836511850357, 0.06533122807741165, 0.06417886167764664, 0.06304800510406494, 0.06184973195195198, 0.06064481660723686, 0.05946914851665497, 0.05833183974027634, 0.05715225636959076, 0.055958591401576996, 0.054769959300756454, 0.05352276191115379, 0.052310168743133545, 0.051042262464761734, 0.04978939890861511, 0.04851099103689194, 0.04719068482518196, 0.04589644446969032, 0.04457765445113182, 0.043213024735450745, 0.041916701942682266, 0.04060627892613411, 0.03931223228573799, 0.038057733327150345, 0.03677702695131302, 0.03552975133061409, 0.0343049056828022, 0.03305324539542198, 0.031892381608486176, 0.03075140155851841, 0.029626665636897087, 0.028539439663290977, 0.027462013065814972, 0.026428652927279472, 0.025425273925065994, 0.024454087018966675, 0.023476820439100266, 0.02253381907939911, 0.02167612873017788, 0.020805375650525093, 0.019958194345235825, 0.01914263516664505, 0.018363643437623978]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:40 2016", "state": "available"}], "summary": "aa114ba453ab66498f080238c221ffb4"}