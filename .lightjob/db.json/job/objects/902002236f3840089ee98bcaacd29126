{"content": {"hp_model": {"f1": 226, "f2": 97, "f3": 63, "nonlin": "rectify", "ds2": 1092, "ds1": 230, "do2": 0.7096622957811484, "do3": 0.9303854834065528, "do1": 0.35035539727420184, "do4": 0.04145868928885699, "do5": 0.5192483492027187}, "accuracy_valid_std": [0.05382855668231031, 0.05815370386251416, 0.0630835414949734, 0.06375383687115391, 0.06969426731233463, 0.07514793528541483, 0.07255684068045674, 0.06827909978591054, 0.06694542504656083, 0.06768449086272094, 0.06694689027507628, 0.06730875219624055, 0.06492307847022498, 0.06427538580255436, 0.0674453381592717, 0.06530762778003897, 0.06662322833546006, 0.06662804669780255, 0.0666087711571211, 0.06757966879594579, 0.06825558733866258, 0.06790034768597203, 0.06848448936127072, 0.06893005268720993, 0.06915117748122293, 0.067622803959253, 0.06706825969329812, 0.06615700983961277, 0.06714958196463096, 0.06763242976536087, 0.06716989714864051, 0.06704073130541507, 0.06678044835687516, 0.06705988268456552, 0.0661861183970843, 0.06760856052701052, 0.0689388492078232, 0.06786566751866539, 0.06872275080258772, 0.06837828542913414, 0.06898591739311741, 0.06958824382689867, 0.0680759723966649, 0.06835741620094664, 0.06863549730225639, 0.06864537081984061, 0.06880250636439098, 0.068199647411488, 0.06707583799019369, 0.06915324073313892, 0.06968761357961384, 0.07073164476473472, 0.07175154040141758, 0.07063109253538145, 0.06960502882816251, 0.07007553987688442, 0.07006777694506808, 0.07040802965132589, 0.0697075728724384, 0.07060785812428469, 0.07196498740347454, 0.0706563386642698, 0.07213825709615422, 0.07273397246794602, 0.07231308469851988, 0.07385958538249839, 0.07331134770195825, 0.07393488534009833, 0.07321214610316759, 0.07386646692968066, 0.07326912736661838, 0.07325598177820447, 0.07478584783120193, 0.07356100908672117, 0.07370052748440332, 0.07444272506435115, 0.07371165819190707, 0.07462207237433752, 0.07460737232077917, 0.07438328610968067, 0.07395707453716642, 0.07424685232994309, 0.0732198192407735, 0.07353579005307802, 0.07406611486541007, 0.07432943848642186, 0.07291226388511106, 0.07334357452976206, 0.07340045376451668, 0.07359979072642847, 0.07356210009976673, 0.07370585107596908, 0.07370149543874414, 0.07332448336438888, 0.07350425409336854, 0.07279880055340968, 0.0723056853256077, 0.0725591757863575, 0.07232874420767152, 0.0721875628031872, 0.071854371942614, 0.07321543468918175, 0.07308487396298952, 0.07308328775951906, 0.0728335804846007, 0.07277221454413345, 0.07244466708284752, 0.07322956174886543, 0.07261728343064877, 0.07236966487015173, 0.07289085768627637, 0.07302774893252477, 0.07266773669987478, 0.07290406911152571, 0.07265116832240848, 0.07335366530373642, 0.07365368750122386, 0.07236966487015173, 0.07305216680462151, 0.07263987511477205, 0.07314963487837951, 0.07315792403750726, 0.07350376881907837, 0.07323358015586924, 0.07330550887422999, 0.07317705865656192, 0.07284178318822765, 0.0729759560230034, 0.07251848502857547, 0.07258669979257705, 0.07225831139644026, 0.07326608461613361, 0.0731276884511722, 0.0726697001090427, 0.07394079507027625, 0.07342474769132898, 0.07382154421152837, 0.07361881049787675, 0.07401456664307104, 0.07431852029332266, 0.07482542476051161, 0.07448475905198469, 0.07483841382720238, 0.07565882981348382, 0.07563100895215949, 0.07519740215404386, 0.07495509516766431, 0.07419218452987876, 0.07505579504709291, 0.07484365648152981, 0.07559704419959802, 0.07483221748946119, 0.07479824767596392, 0.0745643311198746, 0.0750111090708455, 0.07514508728038774, 0.07557580847789484, 0.0750692194107931, 0.07564055879093219, 0.07518779602857208, 0.0756489286457673, 0.07587738634685623, 0.07603411879867951, 0.07550356234874238, 0.07546623171622525, 0.07557675241445201, 0.07518921923571266, 0.07565706184285331, 0.07553851354894413, 0.07561744846740114, 0.07549931042350466, 0.07545169612816867, 0.07581424977769652, 0.07546623171622525, 0.07634591762493925, 0.07516691922793958, 0.0756319521997784, 0.07555456678758014, 0.0756405587909322, 0.07587738634685623, 0.07629976664849458, 0.07519787649881454, 0.07623651199999666, 0.0757334005532305, 0.07521732205884248, 0.07588020686657283, 0.0757856623122503, 0.0756782747644386, 0.07572433347007539, 0.07612554346745484, 0.07564055879093219, 0.07609426046478822, 0.07620948708867561, 0.07586093122511232, 0.07552009533785883, 0.07524482182923779, 0.07679045893157833, 0.07585093887384925, 0.07618034560337426, 0.07621873045252638, 0.07613549977843556, 0.07633435329160991, 0.0755098216918534, 0.07558536529097602, 0.07591592437937782, 0.07560978276977852, 0.07673609251772097, 0.07659185817407965, 0.07509665466951516, 0.07525335419743863, 0.07510627245200799, 0.07575082514582165, 0.07553993014840509, 0.07490225389695047, 0.07517118995429897, 0.07510627245200799, 0.07499505840765029, 0.07542935549742913, 0.07481827385326044, 0.07488879960346975], "moving_avg_accuracy_train": [0.01080336972891566, 0.021950065888554214, 0.033471644390060236, 0.046293060523343366, 0.06060200748305722, 0.07411306049981173, 0.08640478532332453, 0.09739203645966678, 0.10701230194020613, 0.1158187901196795, 0.12373521683060311, 0.13075410855115724, 0.137082876912909, 0.1427222925348711, 0.14776482231752855, 0.15241134460384798, 0.1566449842398487, 0.1604646725628518, 0.1640200501860847, 0.16724106851084972, 0.17020587355735511, 0.1729730309305353, 0.1755364206085661, 0.1778999472224083, 0.18004124015076986, 0.18205311764171697, 0.18388263268477417, 0.18562096956689916, 0.18730313089334177, 0.18888061147870638, 0.1903779983730044, 0.19181977308389672, 0.19320443734177212, 0.19446240098711298, 0.19569340109924505, 0.19686718975438078, 0.197982428610268, 0.19904026632153035, 0.20010997839419659, 0.20110801669935524, 0.20202037014990165, 0.20286972620720065, 0.20358473024310708, 0.2042682376404831, 0.20497281447884444, 0.20564693739842987, 0.20633600871882785, 0.20700794248549925, 0.20764562715261198, 0.20832778883494113, 0.20903821401771208, 0.20973407258582039, 0.21036975794772025, 0.21102894179150244, 0.21166691734126786, 0.2122881585890688, 0.21286139468799326, 0.21346672735774816, 0.21403270522438297, 0.2146126801838724, 0.21516524876187068, 0.2157237427109848, 0.21624991889169354, 0.21676583438204225, 0.21722545199805487, 0.21766263947897227, 0.21803728291059313, 0.2183979936255579, 0.218729692756978, 0.219105876342726, 0.21947973900965823, 0.21982798122315023, 0.22017199032975088, 0.22046983271243845, 0.2207967199231223, 0.22108621208743656, 0.22139617145098206, 0.22170337282998023, 0.22200573886023522, 0.22230375307662134, 0.22263079493763388, 0.22290395414868977, 0.22319215436635093, 0.22342564977308932, 0.22361932350059965, 0.22381480831921438, 0.22400251046922065, 0.2241785018921781, 0.22435336631139405, 0.22449897847543537, 0.2246535610495786, 0.2248091575048617, 0.2249633132905201, 0.2251302914494199, 0.2252852781177309, 0.22544594458306627, 0.22558348491391628, 0.22570727121168127, 0.22583985734352519, 0.22602272025375097, 0.2261990626862072, 0.2263318860862612, 0.22654084732703267, 0.22667008337746195, 0.2268075742867037, 0.22696661354478032, 0.22711445520235046, 0.22720280260380216, 0.22731525954221712, 0.22739764548558578, 0.22749061813582236, 0.2275672340330835, 0.22766913261772695, 0.22775378185595427, 0.22782761300770824, 0.22791759267079284, 0.2280197528314244, 0.22807875269888436, 0.22816009053140557, 0.22822388193007226, 0.22830011949007709, 0.22837108645673201, 0.22840671877491422, 0.22844820051188064, 0.228487887237801, 0.22855890273088836, 0.2285969318855104, 0.22866410240177865, 0.22868690526401042, 0.22870272151471782, 0.2287357814415593, 0.22877965435162023, 0.2288450247598317, 0.2288897391513184, 0.2289770453566685, 0.22904620829088118, 0.22909904228107017, 0.22915835868549328, 0.2292235092627271, 0.22931038273404475, 0.2293532714184716, 0.22939657755975695, 0.2294543783881186, 0.22949228015774048, 0.22953345123835198, 0.22956109256029988, 0.2295530254729446, 0.22953635244372245, 0.2295331125306755, 0.22953019660893323, 0.22957934185767845, 0.22959298146709134, 0.22961466976616535, 0.22963418923533196, 0.22965646308288312, 0.22966474373242612, 0.22972396589532806, 0.22971373045037358, 0.22972334385111937, 0.22972493642383876, 0.2297475482031416, 0.22975142666595996, 0.2297855083969543, 0.2298020629789456, 0.2298381405665932, 0.22986590407017488, 0.22988383173544655, 0.22991173244744406, 0.22995802155209724, 0.2299973285836345, 0.23003741123731924, 0.23007583878828614, 0.2300986577709033, 0.23012390118055995, 0.23015603289985334, 0.23014965400745835, 0.23016038514285708, 0.23019357479122196, 0.23022109231209975, 0.23024585808088976, 0.23025402829689717, 0.2303131510696171, 0.2303498894265108, 0.23035942232120912, 0.23039388671559424, 0.23041784518258904, 0.23044411412818555, 0.23039245497440314, 0.23034360857334837, 0.23037730117986893, 0.23039585871248447, 0.23042197314244084, 0.2304007660390401, 0.23042403657369032, 0.23047557116933332, 0.23053607128131565, 0.23053404547848527, 0.23054398806919096, 0.2305788211899827, 0.23060075834809288, 0.23062756127834383, 0.23062815228906366, 0.23066398163847057, 0.23068446223968375, 0.23071230743137802, 0.230659713736433, 0.2306759148025487, 0.2307140273885589, 0.23073420974006445, 0.23077590548292548, 0.23079225318764499, 0.23080696612189253, 0.23082962041331773, 0.23080529918523898, 0.23082341384502836, 0.23081853857498336, 0.23083297613314765, 0.23084361677284493, 0.23083436804736768, 0.23085663530889597], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.0010504151774968615, 0.002063613177224883, 0.003051972799999687, 0.004226273924965036, 0.005646360200331672, 0.006724661162892467, 0.007411973538835867, 0.007757253372749766, 0.007814473606719302, 0.007731014352544209, 0.007521941224114611, 0.007213130670566923, 0.006852297384300619, 0.006453294722885631, 0.006036809210077956, 0.005627439813285529, 0.005226009173064625, 0.004834718425722138, 0.004465012973543991, 0.004111886307625841, 0.0037798082975373116, 0.003470741907135088, 0.003182806416194493, 0.002914802097064107, 0.0026645881060031584, 0.0024345581547500594, 0.0022212264669100074, 0.002026300156260811, 0.0018491371411883427, 0.0016866194320443292, 0.0015381369964408362, 0.0014030317256494705, 0.001279984209047862, 0.0011662280409400693, 0.001063243488330685, 0.0009693191577599448, 0.0008835810613350762, 0.0008052941408118877, 0.00073506328199637, 0.0006705216779238088, 0.0006109610094999433, 0.0005563575599585834, 0.0005053228809049875, 0.0004589952340748987, 0.00041756356735780605, 0.00037989718601841896, 0.00034618084097793217, 0.0003156262117612783, 0.0002877233661971865, 0.00026313913062501126, 0.0002413675530253463, 0.00022158877004409941, 0.00020306675595369364, 0.00018667079041745547, 0.00017166682659459632, 0.00015797361012685992, 0.000145133645740165, 0.00013391812993580174, 0.00012340929545190613, 0.00011409570448942823, 0.00010543412234100501, 9.769794952767845e-05, 9.04199069332178e-05, 8.377343537853184e-05, 7.729732701722096e-05, 7.128779035673702e-05, 6.542223062877309e-05, 6.0051017544909446e-05, 5.503613461448222e-05, 5.080614796470989e-05, 4.698349281176995e-05, 4.337659728391328e-05, 4.010401794433948e-05, 3.689200691423057e-05, 3.416450345938562e-05, 3.1502304532241334e-05, 2.921674734246299e-05, 2.7144426793542137e-05, 2.525281106045737e-05, 2.3526842212925714e-05, 2.2136765401324202e-05, 2.059463245245387e-05, 1.9282703496347855e-05, 1.7845114091424455e-05, 1.639818829683178e-05, 1.5102298295928117e-05, 1.3909157340388097e-05, 1.2796998434940587e-05, 1.1792496677416024e-05, 1.0804073130525582e-05, 9.938727767531688e-06, 9.162747302848526e-06, 8.460348628831321e-06, 7.865249115894283e-06, 7.294912010492212e-06, 6.7977442271931665e-06, 6.288225887966796e-06, 5.797310726799404e-06, 5.375791395335465e-06, 5.139161851228145e-06, 4.905115547466543e-06, 4.573382493136976e-06, 4.509027445126055e-06, 4.208442311188442e-06, 3.957731831186727e-06, 3.7896000185541048e-06, 3.6073544181165306e-06, 3.3168663463942875e-06, 3.0989987787338633e-06, 2.8501858938431563e-06, 2.642962527687002e-06, 2.4314962363364638e-06, 2.281796506673879e-06, 2.118106297798657e-06, 1.955355018742651e-06, 1.8326865747877477e-06, 1.7433482030913673e-06, 1.6003422420248757e-06, 1.499850604815613e-06, 1.386489627228702e-06, 1.3001501545052303e-06, 1.2154619322605414e-06, 1.1053426979258394e-06, 1.0102950386490082e-06, 9.234408607126154e-07, 8.764855769673357e-07, 8.018529686819821e-07, 7.622745761154586e-07, 6.907268532375644e-07, 6.239055519917597e-07, 5.713516256574554e-07, 5.315399532266461e-07, 5.168455703315904e-07, 4.831554045526709e-07, 5.034412255310898e-07, 4.961487061981328e-07, 4.7165671025193355e-07, 4.5615696172990726e-07, 4.48742644981997e-07, 4.7179138065280527e-07, 4.41167195854319e-07, 4.1392927312614376e-07, 4.0260476764716965e-07, 3.7527318814667855e-07, 3.530013902404734e-07, 3.245776353276916e-07, 2.9270557288050336e-07, 2.6593692472344324e-07, 2.3943770558006464e-07, 2.1557045841852031e-07, 2.1575071184474397e-07, 1.95849991164692e-07, 1.8049843289873172e-07, 1.6587767669777024e-07, 1.53755027590576e-07, 1.3899664724320632e-07, 1.5666236372796865e-07, 1.4193900635591992e-07, 1.2857686298541767e-07, 1.1574200327767478e-07, 1.0876943601908289e-07, 9.802787468167588e-08, 9.867916670165932e-08, 9.127773769565846e-08, 9.386429490033065e-08, 9.141517459044824e-08, 8.51662677702405e-08, 8.365568856293038e-08, 9.457425059297962e-08, 9.902221008812497e-08, 1.0357956121701118e-07, 1.0651169515511276e-07, 1.0054687934874621e-07, 9.622725899371666e-08, 9.589655955709495e-08, 8.667311601506514e-08, 7.904221981607395e-08, 8.105197266173564e-08, 7.976170099290583e-08, 7.730562062746427e-08, 7.017583043118723e-08, 9.46177676748912e-08, 9.73033527126506e-08, 8.83909021733482e-08, 9.024196227900556e-08, 8.638383931776968e-08, 8.395597291076886e-08, 9.957838914532413e-08, 1.1109428829482509e-07, 1.1020158507270258e-07, 1.022808647164151e-07, 9.819044931229027e-08, 9.241907549289696e-08, 8.805082798976474e-08, 1.0314807612359137e-07, 1.2577564046010376e-07, 1.1323501130806084e-07, 1.0280120616672237e-07, 1.0344120228688186e-07, 9.742823221174648e-08, 9.415098262090843e-08, 8.47390280018562e-08, 8.781880571195759e-08, 8.28120203752426e-08, 8.150901064213714e-08, 9.825298030966106e-08, 9.078995316827359e-08, 9.478408076290251e-08, 8.89716184972571e-08, 9.57212714021259e-08, 8.855437130827668e-08, 8.164716808500339e-08, 7.81014035563066e-08, 7.561496241800827e-08, 7.100673426976413e-08, 6.41199751648932e-08, 5.958396542013599e-08, 5.464457779662883e-08, 4.9949970323550364e-08, 4.9417451714919306e-08], "duration": 123058.811846, "accuracy_train": [0.10803369728915663, 0.1222703313253012, 0.13716585090361447, 0.16168580572289157, 0.18938253012048192, 0.1957125376506024, 0.19703030873493976, 0.19627729668674698, 0.19359469126506024, 0.19507718373493976, 0.19498305722891565, 0.19392413403614459, 0.1940417921686747, 0.19347703313253012, 0.19314759036144577, 0.1942300451807229, 0.19474774096385541, 0.19484186746987953, 0.1960184487951807, 0.19623023343373494, 0.1968891189759036, 0.19787744728915663, 0.19860692771084337, 0.19917168674698796, 0.19931287650602408, 0.20016001506024098, 0.20034826807228914, 0.20126600150602408, 0.2024425828313253, 0.20307793674698796, 0.20385448042168675, 0.20479574548192772, 0.2056664156626506, 0.2057840737951807, 0.20677240210843373, 0.2074312876506024, 0.20801957831325302, 0.20856080572289157, 0.20973738704819278, 0.21009036144578314, 0.2102315512048193, 0.21051393072289157, 0.21001976656626506, 0.21041980421686746, 0.2113140060240964, 0.2117140436746988, 0.21253765060240964, 0.21305534638554216, 0.2133847891566265, 0.2144672439759036, 0.2154320406626506, 0.21599679969879518, 0.2160909262048193, 0.21696159638554216, 0.21740869728915663, 0.2178793298192771, 0.21802051957831325, 0.21891472138554216, 0.2191265060240964, 0.2198324548192771, 0.22013836596385541, 0.22075018825301204, 0.22098550451807228, 0.2214090737951807, 0.22136201054216867, 0.2215973268072289, 0.2214090737951807, 0.22164439006024098, 0.22171498493975902, 0.22249152861445784, 0.2228445030120482, 0.2229621611445783, 0.22326807228915663, 0.2231504141566265, 0.2237387048192771, 0.22369164156626506, 0.22418580572289157, 0.22446818524096385, 0.22472703313253012, 0.2249858810240964, 0.22557417168674698, 0.22536238704819278, 0.2257859563253012, 0.22552710843373494, 0.22536238704819278, 0.22557417168674698, 0.2256918298192771, 0.22576242469879518, 0.22592714608433734, 0.22580948795180722, 0.22604480421686746, 0.22620952560240964, 0.22635071536144577, 0.22663309487951808, 0.22668015813253012, 0.22689194277108435, 0.22682134789156627, 0.22682134789156627, 0.22703313253012047, 0.22766848644578314, 0.22778614457831325, 0.22752729668674698, 0.22842149849397592, 0.2278332078313253, 0.22804499246987953, 0.22839796686746988, 0.22844503012048192, 0.22799792921686746, 0.2283273719879518, 0.2281391189759036, 0.2283273719879518, 0.22825677710843373, 0.22858621987951808, 0.228515625, 0.22849209337349397, 0.22872740963855423, 0.22893919427710843, 0.22860975150602408, 0.2288921310240964, 0.22879800451807228, 0.22898625753012047, 0.2290097891566265, 0.22872740963855423, 0.2288215361445783, 0.22884506777108435, 0.2291980421686747, 0.22893919427710843, 0.22926863704819278, 0.2288921310240964, 0.22884506777108435, 0.22903332078313254, 0.22917451054216867, 0.22943335843373494, 0.2292921686746988, 0.2297628012048193, 0.22966867469879518, 0.2295745481927711, 0.2296922063253012, 0.22980986445783133, 0.2300922439759036, 0.22973926957831325, 0.2297863328313253, 0.2299745858433735, 0.22983339608433734, 0.22990399096385541, 0.22980986445783133, 0.22948042168674698, 0.2293862951807229, 0.22950395331325302, 0.22950395331325302, 0.23002164909638553, 0.22971573795180722, 0.22980986445783133, 0.22980986445783133, 0.22985692771084337, 0.22973926957831325, 0.23025696536144577, 0.22962161144578314, 0.22980986445783133, 0.22973926957831325, 0.22995105421686746, 0.2297863328313253, 0.2300922439759036, 0.22995105421686746, 0.2301628388554217, 0.23011577560240964, 0.23004518072289157, 0.2301628388554217, 0.23037462349397592, 0.23035109186746988, 0.23039815512048192, 0.23042168674698796, 0.23030402861445784, 0.23035109186746988, 0.23044521837349397, 0.2300922439759036, 0.23025696536144577, 0.23049228162650603, 0.23046875, 0.23046875, 0.23032756024096385, 0.2308452560240964, 0.23068053463855423, 0.23044521837349397, 0.23070406626506024, 0.23063347138554216, 0.23068053463855423, 0.22992752259036145, 0.22990399096385541, 0.23068053463855423, 0.23056287650602408, 0.2306570030120482, 0.23020990210843373, 0.23063347138554216, 0.23093938253012047, 0.23108057228915663, 0.23051581325301204, 0.23063347138554216, 0.23089231927710843, 0.23079819277108435, 0.2308687876506024, 0.23063347138554216, 0.23098644578313254, 0.2308687876506024, 0.2309629141566265, 0.23018637048192772, 0.23082172439759036, 0.2310570406626506, 0.23091585090361447, 0.2311511671686747, 0.23093938253012047, 0.23093938253012047, 0.23103350903614459, 0.23058640813253012, 0.23098644578313254, 0.2307746611445783, 0.2309629141566265, 0.23093938253012047, 0.23075112951807228, 0.2310570406626506], "end": "2016-01-18 09:55:53.795000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0], "accuracy_valid": [0.10844017094017094, 0.11899038461538461, 0.1360844017094017, 0.15678418803418803, 0.18389423076923078, 0.1893696581196581, 0.19003739316239315, 0.19324252136752137, 0.19110576923076922, 0.18856837606837606, 0.18803418803418803, 0.18883547008547008, 0.18776709401709402, 0.18803418803418803, 0.18776709401709402, 0.18723290598290598, 0.18883547008547008, 0.18856837606837606, 0.1909722222222222, 0.1908386752136752, 0.1908386752136752, 0.19404380341880342, 0.19257478632478633, 0.19270833333333334, 0.19297542735042736, 0.19337606837606838, 0.19257478632478633, 0.19404380341880342, 0.19444444444444445, 0.19591346153846154, 0.19564636752136752, 0.19618055555555555, 0.19738247863247863, 0.1985844017094017, 0.19778311965811965, 0.1984508547008547, 0.20045405982905984, 0.1996527777777778, 0.2012553418803419, 0.20392628205128205, 0.20392628205128205, 0.20392628205128205, 0.20352564102564102, 0.2049946581196581, 0.20579594017094016, 0.20552884615384615, 0.20405982905982906, 0.203125, 0.20446047008547008, 0.2047275641025641, 0.2049946581196581, 0.20579594017094016, 0.2065972222222222, 0.20673076923076922, 0.20699786324786323, 0.20713141025641027, 0.20726495726495728, 0.2075320512820513, 0.20819978632478633, 0.20886752136752137, 0.20873397435897437, 0.20966880341880342, 0.21006944444444445, 0.2110042735042735, 0.21167200854700854, 0.21340811965811965, 0.21367521367521367, 0.21367521367521367, 0.2143429487179487, 0.21487713675213677, 0.2143429487179487, 0.21514423076923078, 0.21474358974358973, 0.21634615384615385, 0.2155448717948718, 0.21514423076923078, 0.21581196581196582, 0.21607905982905984, 0.21647970085470086, 0.21647970085470086, 0.21634615384615385, 0.21661324786324787, 0.21634615384615385, 0.21768162393162394, 0.2172809829059829, 0.21794871794871795, 0.21794871794871795, 0.21915064102564102, 0.21915064102564102, 0.21901709401709402, 0.21995192307692307, 0.21981837606837606, 0.22008547008547008, 0.22142094017094016, 0.22115384615384615, 0.22235576923076922, 0.22182158119658119, 0.2219551282051282, 0.2219551282051282, 0.22248931623931623, 0.22369123931623933, 0.22355769230769232, 0.22556089743589744, 0.22382478632478633, 0.22449252136752137, 0.2248931623931624, 0.2250267094017094, 0.2248931623931624, 0.2250267094017094, 0.2248931623931624, 0.22449252136752137, 0.22556089743589744, 0.22542735042735043, 0.2250267094017094, 0.22529380341880342, 0.22542735042735043, 0.22556089743589744, 0.2248931623931624, 0.22529380341880342, 0.2247596153846154, 0.22516025641025642, 0.22569444444444445, 0.22622863247863248, 0.22689636752136752, 0.2264957264957265, 0.2263621794871795, 0.22756410256410256, 0.22609508547008547, 0.2266292735042735, 0.22729700854700854, 0.22689636752136752, 0.22783119658119658, 0.22756410256410256, 0.22756410256410256, 0.22716346153846154, 0.22769764957264957, 0.22769764957264957, 0.22796474358974358, 0.22796474358974358, 0.2282318376068376, 0.22702991452991453, 0.22756410256410256, 0.22716346153846154, 0.2264957264957265, 0.2267628205128205, 0.22743055555555555, 0.22849893162393162, 0.22876602564102563, 0.2283653846153846, 0.22849893162393162, 0.22756410256410256, 0.22769764957264957, 0.22863247863247863, 0.22849893162393162, 0.22783119658119658, 0.2280982905982906, 0.2267628205128205, 0.22930021367521367, 0.22956730769230768, 0.22863247863247863, 0.2297008547008547, 0.2298344017094017, 0.2297008547008547, 0.22916666666666666, 0.22943376068376067, 0.22943376068376067, 0.22943376068376067, 0.2298344017094017, 0.2297008547008547, 0.23010149572649571, 0.2299679487179487, 0.23010149572649571, 0.2297008547008547, 0.22943376068376067, 0.22943376068376067, 0.2299679487179487, 0.22943376068376067, 0.2299679487179487, 0.22956730769230768, 0.2298344017094017, 0.22956730769230768, 0.22876602564102563, 0.22943376068376067, 0.22930021367521367, 0.22903311965811965, 0.22903311965811965, 0.22849893162393162, 0.22930021367521367, 0.22916666666666666, 0.22863247863247863, 0.22876602564102563, 0.2282318376068376, 0.22903311965811965, 0.22876602564102563, 0.22863247863247863, 0.22796474358974358, 0.22876602564102563, 0.22863247863247863, 0.22863247863247863, 0.22916666666666666, 0.22876602564102563, 0.22849893162393162, 0.22849893162393162, 0.22876602564102563, 0.22876602564102563, 0.2283653846153846, 0.22876602564102563, 0.22903311965811965, 0.22889957264957264, 0.22876602564102563, 0.22903311965811965, 0.22876602564102563, 0.22889957264957264, 0.22930021367521367, 0.22916666666666666, 0.22903311965811965, 0.22903311965811965, 0.22889957264957264, 0.22889957264957264, 0.2283653846153846], "accuracy_test": 0.23006810897435898, "start": "2016-01-16 23:44:54.983000", "learning_rate_per_epoch": [0.004239719361066818, 0.002119859680533409, 0.0014132397482171655, 0.0010599298402667046, 0.0008479438838548958, 0.0007066198741085827, 0.0006056741694919765, 0.0005299649201333523, 0.0004710799257736653, 0.0004239719419274479, 0.0003854290407616645, 0.00035330993705429137, 0.0003261322563048452, 0.00030283708474598825, 0.0002826479612849653, 0.00026498246006667614, 0.00024939526338130236, 0.00023553996288683265, 0.00022314312809612602, 0.00021198597096372396, 0.00020189139468129724, 0.00019271452038083225, 0.00018433562945574522, 0.00017665496852714568, 0.00016958877677097917, 0.0001630661281524226, 0.0001570266467751935, 0.00015141854237299412, 0.0001461972133256495, 0.00014132398064248264, 0.00013676514208782464, 0.00013249123003333807, 0.00012847634206991643, 0.00012469763169065118, 0.00012113483535358682, 0.00011776998144341633, 0.00011458701192168519, 0.00011157156404806301, 0.00010871075210161507, 0.00010599298548186198, 0.00010340778680983931, 0.00010094569734064862, 9.859812416834757e-05, 9.635726019041613e-05, 9.421598224435002e-05, 9.216781472787261e-05, 9.020679135574028e-05, 8.832748426357284e-05, 8.652488759253174e-05, 8.479438838548958e-05, 8.313175203511491e-05, 8.15330640762113e-05, 7.99947083578445e-05, 7.851332338759676e-05, 7.708580960752442e-05, 7.570927118649706e-05, 7.438103784807026e-05, 7.309860666282475e-05, 7.185964932432398e-05, 7.066199032124132e-05, 6.95035996614024e-05, 6.838257104391232e-05, 6.729712913511321e-05, 6.624561501666903e-05, 6.522644980577752e-05, 6.423817103495821e-05, 6.327939627226442e-05, 6.234881584532559e-05, 6.144520739326254e-05, 6.056741767679341e-05, 5.97143589402549e-05, 5.8884990721708164e-05, 5.807834895676933e-05, 5.7293505960842595e-05, 5.6529592256993055e-05, 5.5785782024031505e-05, 5.506128945853561e-05, 5.4355376050807536e-05, 5.3667332394979894e-05, 5.299649274093099e-05, 5.2342213166411966e-05, 5.1703893404919654e-05, 5.1080955017823726e-05, 5.047284867032431e-05, 4.987905049347319e-05, 4.9299062084173784e-05, 4.873240686720237e-05, 4.8178630095208064e-05, 4.763729521073401e-05, 4.710799112217501e-05, 4.6590321289841086e-05, 4.6083907363936305e-05, 4.558838190860115e-05, 4.510339567787014e-05, 4.462862489162944e-05, 4.416374213178642e-05, 4.3708445446100086e-05, 4.326244379626587e-05, 4.2825449781958014e-05, 4.239719419274479e-05, 4.19774187321309e-05, 4.156587601755746e-05, 4.116232230444439e-05, 4.076653203810565e-05, 4.037827966385521e-05, 3.999735417892225e-05, 3.962354458053596e-05, 3.925666169379838e-05, 3.889650906785391e-05, 3.854290480376221e-05, 3.819567064056173e-05, 3.785463559324853e-05, 3.7519639590755105e-05, 3.719051892403513e-05, 3.686712443595752e-05, 3.6549303331412375e-05, 3.6236917367205024e-05, 3.592982466216199e-05, 3.562789424904622e-05, 3.533099516062066e-05, 3.5039003705605865e-05, 3.47517998307012e-05, 3.446926348260604e-05, 3.419128552195616e-05, 3.391775317140855e-05, 3.3648564567556605e-05, 3.338361784699373e-05, 3.312280750833452e-05, 3.286604260210879e-05, 3.261322490288876e-05, 3.236427073716186e-05, 3.211908551747911e-05, 3.1877589208306745e-05, 3.163969813613221e-05, 3.140532862744294e-05, 3.1174407922662795e-05, 3.0946855986258015e-05, 3.072260369663127e-05, 3.050157829420641e-05, 3.0283708838396706e-05, 3.006893166457303e-05, 2.985717947012745e-05, 2.9648386771441437e-05, 2.9442495360854082e-05, 2.923944339272566e-05, 2.9039174478384666e-05, 2.884162859118078e-05, 2.8646752980421297e-05, 2.8454493076424114e-05, 2.8264796128496528e-05, 2.8077611204935238e-05, 2.7892891012015752e-05, 2.7710584618034773e-05, 2.7530644729267806e-05, 2.7353027689969167e-05, 2.7177688025403768e-05, 2.7004582079825923e-05, 2.6833666197489947e-05, 2.6664902179618366e-05, 2.6498246370465495e-05, 2.633366057125386e-05, 2.6171106583205983e-05, 2.6010548026533797e-05, 2.5851946702459827e-05, 2.5695268050185405e-05, 2.5540477508911863e-05, 2.5387540517840534e-05, 2.5236424335162155e-05, 2.5087096219067462e-05, 2.4939525246736594e-05, 2.479368049534969e-05, 2.4649531042086892e-05, 2.450704778311774e-05, 2.4366203433601186e-05, 2.4226967070717365e-05, 2.4089315047604032e-05, 2.3953216441441327e-05, 2.3818647605367005e-05, 2.3685583073529415e-05, 2.3553995561087504e-05, 2.342386324016843e-05, 2.3295160644920543e-05, 2.3167865947471e-05, 2.3041953681968153e-05, 2.2917402020539157e-05, 2.2794190954300575e-05, 2.267229683639016e-05, 2.255169783893507e-05, 2.2432377591030672e-05, 2.231431244581472e-05, 2.219748421339318e-05, 2.208187106589321e-05, 2.196745845139958e-05, 2.1854222723050043e-05, 2.1742151147918776e-05, 2.1631221898132935e-05, 2.152141860278789e-05, 2.1412724890979007e-05, 2.1305122572812252e-05, 2.1198597096372396e-05, 2.1093130271765403e-05, 2.098870936606545e-05, 2.08853161893785e-05, 2.078293800877873e-05, 2.0681558453361504e-05, 2.0581161152222194e-05, 2.0481735191424377e-05, 2.0383266019052826e-05, 2.028573908319231e-05, 2.0189139831927605e-05, 2.0093457351322286e-05, 1.9998677089461125e-05, 1.99047863134183e-05, 1.981177229026798e-05, 1.9719624106073752e-05, 1.962833084689919e-05, 1.9537877960829064e-05, 1.9448254533926956e-05, 1.9359449652256444e-05, 1.9271452401881106e-05], "accuracy_train_last": 0.2310570406626506, "error_valid": [0.891559829059829, 0.8810096153846154, 0.8639155982905983, 0.843215811965812, 0.8161057692307692, 0.8106303418803419, 0.8099626068376069, 0.8067574786324786, 0.8088942307692308, 0.8114316239316239, 0.811965811965812, 0.8111645299145299, 0.812232905982906, 0.811965811965812, 0.812232905982906, 0.812767094017094, 0.8111645299145299, 0.8114316239316239, 0.8090277777777778, 0.8091613247863247, 0.8091613247863247, 0.8059561965811965, 0.8074252136752137, 0.8072916666666666, 0.8070245726495726, 0.8066239316239316, 0.8074252136752137, 0.8059561965811965, 0.8055555555555556, 0.8040865384615384, 0.8043536324786325, 0.8038194444444444, 0.8026175213675214, 0.8014155982905983, 0.8022168803418803, 0.8015491452991453, 0.7995459401709402, 0.8003472222222222, 0.7987446581196581, 0.796073717948718, 0.796073717948718, 0.796073717948718, 0.796474358974359, 0.7950053418803419, 0.7942040598290598, 0.7944711538461539, 0.795940170940171, 0.796875, 0.7955395299145299, 0.7952724358974359, 0.7950053418803419, 0.7942040598290598, 0.7934027777777778, 0.7932692307692308, 0.7930021367521367, 0.7928685897435898, 0.7927350427350427, 0.7924679487179487, 0.7918002136752137, 0.7911324786324786, 0.7912660256410257, 0.7903311965811965, 0.7899305555555556, 0.7889957264957265, 0.7883279914529915, 0.7865918803418803, 0.7863247863247863, 0.7863247863247863, 0.7856570512820513, 0.7851228632478633, 0.7856570512820513, 0.7848557692307692, 0.7852564102564102, 0.7836538461538461, 0.7844551282051282, 0.7848557692307692, 0.7841880341880342, 0.7839209401709402, 0.7835202991452992, 0.7835202991452992, 0.7836538461538461, 0.7833867521367521, 0.7836538461538461, 0.7823183760683761, 0.782719017094017, 0.782051282051282, 0.782051282051282, 0.780849358974359, 0.780849358974359, 0.780982905982906, 0.7800480769230769, 0.7801816239316239, 0.7799145299145299, 0.7785790598290598, 0.7788461538461539, 0.7776442307692308, 0.7781784188034189, 0.7780448717948718, 0.7780448717948718, 0.7775106837606838, 0.7763087606837606, 0.7764423076923077, 0.7744391025641025, 0.7761752136752137, 0.7755074786324786, 0.7751068376068376, 0.7749732905982906, 0.7751068376068376, 0.7749732905982906, 0.7751068376068376, 0.7755074786324786, 0.7744391025641025, 0.7745726495726496, 0.7749732905982906, 0.7747061965811965, 0.7745726495726496, 0.7744391025641025, 0.7751068376068376, 0.7747061965811965, 0.7752403846153846, 0.7748397435897436, 0.7743055555555556, 0.7737713675213675, 0.7731036324786325, 0.7735042735042735, 0.7736378205128205, 0.7724358974358975, 0.7739049145299145, 0.7733707264957265, 0.7727029914529915, 0.7731036324786325, 0.7721688034188035, 0.7724358974358975, 0.7724358974358975, 0.7728365384615384, 0.7723023504273504, 0.7723023504273504, 0.7720352564102564, 0.7720352564102564, 0.7717681623931624, 0.7729700854700855, 0.7724358974358975, 0.7728365384615384, 0.7735042735042735, 0.7732371794871795, 0.7725694444444444, 0.7715010683760684, 0.7712339743589743, 0.7716346153846154, 0.7715010683760684, 0.7724358974358975, 0.7723023504273504, 0.7713675213675214, 0.7715010683760684, 0.7721688034188035, 0.7719017094017094, 0.7732371794871795, 0.7706997863247863, 0.7704326923076923, 0.7713675213675214, 0.7702991452991453, 0.7701655982905983, 0.7702991452991453, 0.7708333333333334, 0.7705662393162394, 0.7705662393162394, 0.7705662393162394, 0.7701655982905983, 0.7702991452991453, 0.7698985042735043, 0.7700320512820513, 0.7698985042735043, 0.7702991452991453, 0.7705662393162394, 0.7705662393162394, 0.7700320512820513, 0.7705662393162394, 0.7700320512820513, 0.7704326923076923, 0.7701655982905983, 0.7704326923076923, 0.7712339743589743, 0.7705662393162394, 0.7706997863247863, 0.7709668803418803, 0.7709668803418803, 0.7715010683760684, 0.7706997863247863, 0.7708333333333334, 0.7713675213675214, 0.7712339743589743, 0.7717681623931624, 0.7709668803418803, 0.7712339743589743, 0.7713675213675214, 0.7720352564102564, 0.7712339743589743, 0.7713675213675214, 0.7713675213675214, 0.7708333333333334, 0.7712339743589743, 0.7715010683760684, 0.7715010683760684, 0.7712339743589743, 0.7712339743589743, 0.7716346153846154, 0.7712339743589743, 0.7709668803418803, 0.7711004273504274, 0.7712339743589743, 0.7709668803418803, 0.7712339743589743, 0.7711004273504274, 0.7706997863247863, 0.7708333333333334, 0.7709668803418803, 0.7709668803418803, 0.7711004273504274, 0.7711004273504274, 0.7716346153846154], "accuracy_train_std": [0.053714405274523105, 0.057575345645906856, 0.06082884487522574, 0.06415368343162213, 0.06897244571454225, 0.06903692767164867, 0.0693780314792645, 0.06735995804659088, 0.06981651959377391, 0.07051575322089111, 0.06992894680133338, 0.07015481170006654, 0.0697707765593717, 0.06984248588589752, 0.07050898388824418, 0.07039301900327581, 0.07007975968163212, 0.06981763393592014, 0.06963108087051481, 0.06944089684103363, 0.06924877116732571, 0.06939427981338235, 0.06972036141711636, 0.06996518863548831, 0.06993097392655184, 0.07064140974089132, 0.07090879939817615, 0.0709076865855788, 0.07144526948621932, 0.07145880447402857, 0.07062699680427792, 0.07149073502647024, 0.0711763826359169, 0.07148137774942084, 0.07134506259267959, 0.0719550774061964, 0.07207066865824319, 0.07204207743055754, 0.07242243709482447, 0.0730063772776922, 0.07288186550873628, 0.07257060525065923, 0.07244686939066976, 0.07263076833130298, 0.07227132186575015, 0.07241806348636756, 0.07263017365636561, 0.07202207872992013, 0.07254577191779212, 0.07228276787527965, 0.07256185655344884, 0.07223002378351476, 0.07248761232536984, 0.07226543726779608, 0.07274669413495703, 0.07297587629079415, 0.07308818261873912, 0.07258685221325968, 0.0730052850609343, 0.07320947312314054, 0.07304844173681127, 0.07364588225578286, 0.07322854261084344, 0.07360505824682602, 0.07390584871032374, 0.07387725190750699, 0.07333481424118926, 0.07413377785196082, 0.0738477329849593, 0.07439299211451766, 0.07428041097065814, 0.07398118457423593, 0.07449314647291073, 0.07449525381234985, 0.07417616598226502, 0.07417931621572803, 0.07444662487159842, 0.07465237443833887, 0.07480432770338578, 0.07487655213292181, 0.07466289548642153, 0.07466717839296232, 0.0750999426747324, 0.07424248673548701, 0.07503068982397562, 0.07500682232569404, 0.07459785039647762, 0.07462574440604917, 0.07483151576823385, 0.07440418988017918, 0.07427265768829165, 0.07474488809232765, 0.07446450378813356, 0.07487838615439961, 0.0746471819753266, 0.07483594441105368, 0.07490742870011831, 0.07497611593740831, 0.07503636125183691, 0.0751545445315738, 0.07504745933177083, 0.07475917005643144, 0.07475703682435204, 0.07478655964419434, 0.0748440943527804, 0.07452856564277878, 0.07468017393468467, 0.07508534204131895, 0.07485761767242223, 0.07474385091396951, 0.07494597696674705, 0.07463543454932102, 0.07504484729841226, 0.07408269919651893, 0.07433847522197796, 0.07456411651460293, 0.07433294423215922, 0.07473244100199031, 0.07440859186360402, 0.07472704661897563, 0.0744451074946804, 0.07449617923887429, 0.07462326602034408, 0.07451199519379911, 0.07434578592586917, 0.07468693591262326, 0.07446143998579022, 0.07430683687386774, 0.07447773915554086, 0.07441499157785043, 0.07450774052663958, 0.0747246753383988, 0.07464378813467613, 0.07489004371246341, 0.07484193764566005, 0.07494408918430635, 0.07489885684926033, 0.07470970489265226, 0.07490375094549351, 0.07487133453984793, 0.07480117047957255, 0.07481390219690709, 0.07477605599575496, 0.07473810172150132, 0.07474235809271042, 0.07463820928971705, 0.07480434250834181, 0.0746505089021274, 0.07453998819034315, 0.07463857652703969, 0.07458187445892997, 0.07452392926058954, 0.07446065542389704, 0.07472682801969613, 0.07433503376557053, 0.0744760439701613, 0.0743978152870145, 0.07441923666604933, 0.07465791143133363, 0.0743079993828575, 0.07475499612350181, 0.07479424114606102, 0.0746352416490949, 0.07457772030308712, 0.07465896092892052, 0.07455772588711837, 0.07418781072831378, 0.07420449461307085, 0.07457206226019072, 0.07448199181922453, 0.07436595274010012, 0.07455484788354153, 0.0742718040322639, 0.07446224313063485, 0.07446727002564815, 0.07431928446242429, 0.07434837783164873, 0.07440058400402894, 0.07436969431882892, 0.07452773349285538, 0.07437203224460223, 0.07441816518547743, 0.07438051591533654, 0.07446727002564815, 0.07438168099506917, 0.07462492447018898, 0.07458784356462378, 0.07443814501581547, 0.07447624100026826, 0.0745681229099106, 0.07481818386029845, 0.0746852009898317, 0.07445447903422929, 0.07460521361720075, 0.07483641796804151, 0.07466569887120522, 0.07434248260463559, 0.07464463011827886, 0.07445989688226366, 0.07456399769329035, 0.07434018471739462, 0.07471355895746801, 0.07463193259066082, 0.07460678711447953, 0.07441377120969786, 0.07453213189435925, 0.07465382826326317, 0.0747630808238226, 0.07471732389035493, 0.07457023184338958, 0.07454104677685047, 0.07442257006250921, 0.07465375779791206, 0.07447458667469435, 0.07479924943058981, 0.07469096537480364, 0.07491606239744057, 0.07468887838519205, 0.07471935820919548, 0.07488101878412813], "accuracy_test_std": 0.07333201554434336, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1.0, 1.2], "translation_range": [-3, 3], "momentum": 0.5855666814787946, "shear_range": [1, 1.1], "patience_check_each": 1, "learning_rate": 0.0042397193602935965, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "adadelta", "nb_data_augmentation": 0, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 2.4411389364265995e-09, "valid_ratio": 0.15, "rotation_range": [-90, 90], "learning_rate_decay": 0.8314163025448458}, "accuracy_valid_max": 0.23010149572649571, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.98, interval=[0.8, 1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256, 512],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1.0, 1.2)),\n        rotation_range=make_constant_param((-90, 90)),\n        shear_range=make_constant_param((1, 1.1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    instantiate = instantiate_random\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.2283653846153846, "loss_train": [2.3038392066955566, 2.3015453815460205, 2.2968146800994873, 2.2909295558929443, 2.285095453262329, 2.277509927749634, 2.2692441940307617, 2.2640323638916016, 2.2588400840759277, 2.2533621788024902, 2.2500035762786865, 2.2466187477111816, 2.242156982421875, 2.2388436794281006, 2.235182285308838, 2.2335567474365234, 2.228339672088623, 2.225088357925415, 2.222691774368286, 2.220113515853882, 2.216428279876709, 2.212344169616699, 2.2090799808502197, 2.2077770233154297, 2.2050673961639404, 2.2022705078125, 2.2008330821990967, 2.1972081661224365, 2.193491220474243, 2.1915454864501953, 2.190953493118286, 2.1875901222229004, 2.18585467338562, 2.182279109954834, 2.181344509124756, 2.178626298904419, 2.173739194869995, 2.1739883422851562, 2.1705052852630615, 2.170078754425049, 2.165637493133545, 2.1651690006256104, 2.1639554500579834, 2.1601617336273193, 2.1590182781219482, 2.155586004257202, 2.1529340744018555, 2.1537039279937744, 2.1488349437713623, 2.1492087841033936, 2.1478347778320312, 2.14521861076355, 2.1449971199035645, 2.142319440841675, 2.139867067337036, 2.139177083969116, 2.13714337348938, 2.1351757049560547, 2.135319948196411, 2.1328744888305664, 2.1327600479125977, 2.132251024246216, 2.129481554031372, 2.1291580200195312, 2.1297760009765625, 2.126636505126953, 2.1258201599121094, 2.124807119369507, 2.1229891777038574, 2.124143600463867, 2.121523857116699, 2.119572639465332, 2.1212213039398193, 2.1175968647003174, 2.1178619861602783, 2.117111921310425, 2.1167147159576416, 2.1146583557128906, 2.1148200035095215, 2.114626169204712, 2.1146256923675537, 2.1139748096466064, 2.110438108444214, 2.1111018657684326, 2.110440731048584, 2.110351085662842, 2.1106622219085693, 2.1085517406463623, 2.107595920562744, 2.106936454772949, 2.106665849685669, 2.1054904460906982, 2.104752540588379, 2.104233503341675, 2.1042416095733643, 2.1033520698547363, 2.1029181480407715, 2.1020054817199707, 2.1011674404144287, 2.1003060340881348, 2.099492073059082, 2.0997767448425293, 2.100125312805176, 2.098865270614624, 2.097205877304077, 2.097867965698242, 2.096607208251953, 2.097034215927124, 2.0959393978118896, 2.096513509750366, 2.0953831672668457, 2.096806764602661, 2.0944271087646484, 2.0935094356536865, 2.0931153297424316, 2.0926754474639893, 2.091622829437256, 2.0916292667388916, 2.091838836669922, 2.0911664962768555, 2.0906758308410645, 2.0900821685791016, 2.090934991836548, 2.090664863586426, 2.0886921882629395, 2.088182210922241, 2.089770555496216, 2.087709426879883, 2.0871734619140625, 2.088052272796631, 2.0860443115234375, 2.086143732070923, 2.085414409637451, 2.0867862701416016, 2.084926128387451, 2.0868890285491943, 2.084388256072998, 2.084484815597534, 2.08331561088562, 2.0829854011535645, 2.083189010620117, 2.081852436065674, 2.0821263790130615, 2.081737756729126, 2.081043243408203, 2.081624984741211, 2.08168363571167, 2.0800318717956543, 2.0813541412353516, 2.079341173171997, 2.07900333404541, 2.0799965858459473, 2.077877998352051, 2.0780739784240723, 2.0782582759857178, 2.078420877456665, 2.0773346424102783, 2.0776631832122803, 2.077540159225464, 2.0758588314056396, 2.0768656730651855, 2.075605630874634, 2.076420307159424, 2.0749616622924805, 2.074542760848999, 2.0748579502105713, 2.0746636390686035, 2.073981761932373, 2.0729737281799316, 2.073245048522949, 2.0744810104370117, 2.0734426975250244, 2.072730302810669, 2.0713255405426025, 2.0713062286376953, 2.0717129707336426, 2.071000576019287, 2.070042610168457, 2.071565866470337, 2.0707833766937256, 2.0721595287323, 2.070481061935425, 2.0690040588378906, 2.0691425800323486, 2.0680832862854004, 2.068631410598755, 2.068455934524536, 2.067363977432251, 2.0690934658050537, 2.0682430267333984, 2.068021297454834, 2.064709424972534, 2.0680432319641113, 2.0647315979003906, 2.066528081893921, 2.0663697719573975, 2.0653157234191895, 2.0667619705200195, 2.0664985179901123, 2.0670623779296875, 2.06482195854187, 2.0646111965179443, 2.065032958984375, 2.0638017654418945, 2.0646045207977295, 2.065084218978882, 2.0648205280303955, 2.062615394592285, 2.0632193088531494, 2.061727285385132, 2.0626602172851562, 2.0629794597625732, 2.0626044273376465, 2.0611979961395264, 2.0613086223602295, 2.061234474182129, 2.061718225479126, 2.0602805614471436, 2.060685396194458, 2.060866355895996], "accuracy_train_first": 0.10803369728915663, "model": "vgg", "loss_std": [0.008410001173615456, 0.008114112541079521, 0.013715477660298347, 0.02088714949786663, 0.02440856769680977, 0.028977468609809875, 0.03471004590392113, 0.038733795285224915, 0.04128306731581688, 0.04535630717873573, 0.047225721180438995, 0.0473826602101326, 0.05000397562980652, 0.05027355998754501, 0.05006083473563194, 0.05085020139813423, 0.05227118730545044, 0.05362732335925102, 0.055357519537210464, 0.055869702249765396, 0.05544544756412506, 0.05761926621198654, 0.05682224780321121, 0.057579923421144485, 0.05959387868642807, 0.05945699289441109, 0.05916036665439606, 0.0599227137863636, 0.059349849820137024, 0.059341270476579666, 0.059364791959524155, 0.060638587921857834, 0.061187904328107834, 0.061946868896484375, 0.06024037301540375, 0.061563052237033844, 0.06144434213638306, 0.060643840581178665, 0.06096367910504341, 0.059951361268758774, 0.06125488877296448, 0.06131792068481445, 0.06130930408835411, 0.06175650283694267, 0.060525305569171906, 0.06172935664653778, 0.061014559119939804, 0.06125793606042862, 0.062148407101631165, 0.06152765452861786, 0.06128533184528351, 0.06267017126083374, 0.06182461977005005, 0.06342262774705887, 0.06024167686700821, 0.06049107760190964, 0.06242963299155235, 0.06255809962749481, 0.06266704946756363, 0.0623040497303009, 0.06269290298223495, 0.06248660758137703, 0.061803750693798065, 0.06253525614738464, 0.06303698569536209, 0.06103181466460228, 0.06332714110612869, 0.06237449869513512, 0.06310125440359116, 0.06322576105594635, 0.0634814128279686, 0.06355509161949158, 0.06275828182697296, 0.0630534365773201, 0.06432387977838516, 0.06263985484838486, 0.0632459744811058, 0.06457602232694626, 0.06317169964313507, 0.06324943900108337, 0.0631217435002327, 0.06425602734088898, 0.06399311125278473, 0.06357801705598831, 0.06542856246232986, 0.06261980533599854, 0.06344889849424362, 0.06461977958679199, 0.06527431309223175, 0.06476890295743942, 0.06404072791337967, 0.06456153839826584, 0.0640045702457428, 0.06429512798786163, 0.06345219165086746, 0.06556288152933121, 0.06515081226825714, 0.0649675726890564, 0.06390027701854706, 0.06509163230657578, 0.06556728482246399, 0.0642516165971756, 0.06357624381780624, 0.06309428811073303, 0.06474175304174423, 0.06340502202510834, 0.06584534049034119, 0.06599263101816177, 0.06407967954874039, 0.06449346989393234, 0.06495688855648041, 0.06562978774309158, 0.06485208868980408, 0.06484945118427277, 0.06517589837312698, 0.06534620374441147, 0.06570762395858765, 0.06471439450979233, 0.06620137393474579, 0.06765878200531006, 0.06723856925964355, 0.06724947690963745, 0.06394099444150925, 0.06348278373479843, 0.06535816937685013, 0.06724750995635986, 0.06579050421714783, 0.06529287248849869, 0.06659241020679474, 0.06627443432807922, 0.06412553042173386, 0.06568873673677444, 0.06426782160997391, 0.06574372202157974, 0.0652294009923935, 0.06684591621160507, 0.06372957676649094, 0.06607125699520111, 0.0657212883234024, 0.06645625084638596, 0.0649595782160759, 0.06582819670438766, 0.06514342874288559, 0.06740028411149979, 0.06657551229000092, 0.06786762923002243, 0.06520761549472809, 0.06657979637384415, 0.06627427041530609, 0.0667121559381485, 0.06560994684696198, 0.06627541035413742, 0.0678873360157013, 0.0649399384856224, 0.06548745185136795, 0.0661633238196373, 0.06813264638185501, 0.06518357992172241, 0.06608186662197113, 0.06522322446107864, 0.065645731985569, 0.06784260272979736, 0.06791228801012039, 0.06691012531518936, 0.0667635127902031, 0.06657728552818298, 0.06533843278884888, 0.06721291691064835, 0.0680684745311737, 0.06816736608743668, 0.06614101678133011, 0.06667789071798325, 0.06696528941392899, 0.06677564978599548, 0.06729446351528168, 0.06491077691316605, 0.06682703644037247, 0.06604646146297455, 0.06527894735336304, 0.06816986203193665, 0.06708741933107376, 0.06442456692457199, 0.06917168200016022, 0.06684771925210953, 0.06765038520097733, 0.06630110740661621, 0.0669529065489769, 0.06789994239807129, 0.06676489114761353, 0.06635907292366028, 0.06805528700351715, 0.06827274709939957, 0.0651787742972374, 0.06727354228496552, 0.06752016395330429, 0.06819837540388107, 0.06678973883390427, 0.06845463812351227, 0.06669779866933823, 0.06681656837463379, 0.06785009801387787, 0.06699798256158829, 0.067335344851017, 0.06800717860460281, 0.06804357469081879, 0.06761600077152252, 0.06693662703037262, 0.06773641705513, 0.06785736978054047, 0.06783797591924667, 0.06796737760305405, 0.06782487034797668, 0.0659373477101326, 0.0670052021741867, 0.06816721707582474, 0.06746147572994232, 0.06829540431499481, 0.06727403402328491, 0.06757710129022598, 0.06710116565227509]}, "state": "available", "life": [{"dt": "Sun May 15 22:04:59 2016", "state": "available"}], "summary": "fb769fa2c994a96476eb7c72e3d0aa41"}