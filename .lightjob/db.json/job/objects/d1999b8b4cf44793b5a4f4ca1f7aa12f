{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 16, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.020868179815169655, 0.016928084965043857, 0.026824635016798437, 0.019863659215627803, 0.017686638049521405, 0.014825054772908512, 0.016684691663459553, 0.014634042038944242, 0.023098077309259345, 0.019868975208215907, 0.020120476186468176, 0.019222086600282438, 0.021993722235590586, 0.018902562114942845, 0.015348463928608528, 0.02230305084140945, 0.019835062238559336, 0.015175500068955643, 0.015808013706278327, 0.013644833147037308, 0.013054094163312869, 0.015395896806887579, 0.011803943939247642, 0.011136674667523778, 0.013003046868412062, 0.012892259341400304, 0.013917545255199193, 0.009849453564500234, 0.01281154295887169, 0.01334368557868362, 0.012989361164658523, 0.010857872699772314, 0.012692230278789286, 0.013184489871819784, 0.00889397963124804, 0.008820276257649806, 0.013091480497417434, 0.015604632929317314, 0.012014901657722146, 0.012432874845944221, 0.013225289625752557, 0.013404854022203233, 0.011437735872526741, 0.012548399799836402, 0.01095920995506372, 0.010989259736722538, 0.012603094635659824, 0.013129241363320783, 0.013631257590071012, 0.016134341995233146, 0.015585827130049293, 0.01376787278423142, 0.013359916454356378, 0.010042548518047206, 0.011889934181690726, 0.010822321791209498, 0.011042878602846256, 0.015287986486822575, 0.010833209126312748, 0.011496414959736936, 0.010991297446899015, 0.018241218647742478, 0.01222592972285702, 0.013169788861747352, 0.015979625833771573, 0.009656715518064731, 0.012329087088562228, 0.009672994936512216, 0.011305236074865901, 0.009604544482063042, 0.01442298673458323, 0.012493783301551438, 0.0090162629892127, 0.013070908720252523, 0.006598322824372573, 0.013962575330171334, 0.01070427303889576, 0.00919089824873356, 0.008766720699442027, 0.011791876455576568, 0.011033493614801961, 0.008496094355928768, 0.010168730350784647, 0.008762778504515087, 0.006069362114550479, 0.011427534291812442, 0.010408880134476493, 0.010210683541213435, 0.010362266920396098, 0.014463285665425736, 0.0105903251114313, 0.018179832458189923, 0.013528418133712396, 0.010377563595970054, 0.008946808261884084, 0.01660494216936177, 0.0101684164856226, 0.011955510034093678, 0.009259244644168189, 0.00982569739010523, 0.00890251291353322, 0.014485964965874787, 0.011484604436219733, 0.01043276714138971, 0.01323034563886391, 0.010137357290265965, 0.014183819637386276, 0.008585791342736918, 0.008994867561670516, 0.01689452837952726, 0.010489419551813142, 0.00996381078315847, 0.009140798904844014, 0.016092140250542526, 0.009061150512208821, 0.015145391366406951, 0.008695617471951297, 0.007091318702854746, 0.01187643156384419, 0.00995252714661026, 0.012469140740102507, 0.011973071355323257, 0.010919314854219795, 0.005945008250484833, 0.01413715922833944, 0.006858570991053122, 0.008784335510433098, 0.014720563498563136, 0.01017214937509915, 0.011442061228374273, 0.009806935919928502, 0.007800891683788458, 0.012571381147879788, 0.008718451618127615, 0.007938621750281094, 0.010250646591489975, 0.010505980197021645, 0.011811114047765215, 0.014183546277945562, 0.009729494238754483, 0.011722153145361864, 0.009367032905788724, 0.006038567779624804, 0.013782738445653395, 0.009974604696018756, 0.007438731885771939, 0.009950811678781351, 0.008996146194310549, 0.008836775781472036, 0.01104386538296212, 0.009403161355749189, 0.011776650367144747, 0.013340734997730192, 0.014004760578448681, 0.009420204234025233, 0.010894351197768088, 0.012982399939526348, 0.009798475042776342, 0.005259849661559942, 0.01001902426305964, 0.01295736426927132, 0.011247877089653963, 0.010258640403355005, 0.014594616273264308, 0.008138813554784375, 0.01153205005700062, 0.010464460250360706, 0.00797905795553913, 0.009295644034801922, 0.011337412738883189, 0.01676625285834145, 0.010053741125488297, 0.011781475535138246, 0.012616366034238569, 0.007288716990326596, 0.009932454559078971, 0.009233792957848052, 0.007864153392469528, 0.008181164601567726, 0.008464024625527115, 0.007752282657735851, 0.00869079435978226, 0.015505098512288807, 0.009404248543613488, 0.011043560132990465, 0.009950035301143663, 0.0074802862403065455, 0.007315894668390825, 0.009520264257048874, 0.008444770562416748, 0.007756171770036437, 0.007869196196164344, 0.010530892544522408, 0.009276253930695123, 0.00810586830641859, 0.011112981026590505, 0.007571118985120045, 0.008654175559357522, 0.006588450795288375, 0.009309164752965813, 0.012980619723933308, 0.010027431170427033, 0.008297075854312545, 0.009583115404754997, 0.008666050127517113, 0.006716051014450085, 0.007845868803400142, 0.01293484086111258, 0.009449338078808192, 0.006995035577762862, 0.011270274907943693, 0.009350549753362605, 0.010619752911120234, 0.006779545801472463, 0.009111535039508427, 0.007623504779577376, 0.008290184289761212, 0.012413781147500379, 0.008620224404707723, 0.009631458072969274, 0.008678683978099254, 0.008838279561623948, 0.013194328030792802, 0.01072744262841561, 0.009816816969736887, 0.006995274295060877, 0.010135338048491736, 0.008412764097777801, 0.008452877063613876, 0.009073206336976222, 0.007031696415110151, 0.008066376628397803, 0.008830926619621943, 0.011109890191205684, 0.008047413632815215, 0.009499546319538418], "moving_avg_accuracy_train": [0.04223468790374676, 0.08537174262873753, 0.13693256347476004, 0.188100306487057, 0.23191713163154065, 0.2806558255028089, 0.3212545729347631, 0.3634028761387028, 0.4009248337317816, 0.4339994481691424, 0.4698386641934057, 0.49906468625343947, 0.5291945800715709, 0.557185632313814, 0.5820196145616223, 0.603621536716802, 0.6247254236159043, 0.6459041291202239, 0.6662832874002926, 0.6845894002784805, 0.7004000535046007, 0.7150318921521565, 0.7283635236445359, 0.7426751185161473, 0.7538934329899756, 0.7647010510639478, 0.7746231116840665, 0.7846874586171272, 0.7933152543759386, 0.8005087525863699, 0.8065477737090083, 0.8128684499764427, 0.8203894921719509, 0.8266140569359296, 0.8316790558485105, 0.8370654159924801, 0.8410389202184905, 0.8447847017385388, 0.8474028092340278, 0.8487712061264703, 0.8488798606499436, 0.8536781993934579, 0.8566109155721446, 0.8581044583068737, 0.8605692963966828, 0.8630851255298551, 0.8628993137832096, 0.8635548254016125, 0.8646702334403088, 0.8669969661524795, 0.8695162754838447, 0.8716697215904067, 0.8733474424684645, 0.8760779862372604, 0.8766874149114302, 0.8791768952906729, 0.8794948540058839, 0.8816921449757275, 0.8833929059937969, 0.8836099899791348, 0.8864255559325409, 0.8875924859370627, 0.8870785466710605, 0.888954560301758, 0.8898221589908052, 0.8919258993371104, 0.8934820469737667, 0.8943456867646697, 0.8962573107561614, 0.8982105035223689, 0.899184657667871, 0.9017098548071378, 0.9037499092050693, 0.9052488476346453, 0.906890824922445, 0.9075597050374817, 0.9082591731469211, 0.9094094556811123, 0.9107469793071226, 0.9117949295514749, 0.9130148135285441, 0.912661888348401, 0.9130184406945779, 0.9145089237061463, 0.9153366086761093, 0.9162511889145338, 0.9178508026850405, 0.9173632672035889, 0.9173589636582945, 0.9149467049355068, 0.9162978399456014, 0.9167605853380383, 0.9169144585710304, 0.91584157799978, 0.9177169207938866, 0.9178448067991233, 0.919259518193085, 0.9193399931571837, 0.9203565031391767, 0.9213201181503329, 0.9225640457675163, 0.9191826332600245, 0.919979967104334, 0.9206137540606134, 0.9203752268284631, 0.9213416558171283, 0.9211001649712498, 0.9215546470206365, 0.9220799383055607, 0.921745837776269, 0.9231029063034594, 0.9240756572994054, 0.9238652887017093, 0.9230507082269332, 0.9242168358400096, 0.9249803934370256, 0.9253514110850636, 0.9251179185611363, 0.9256494977598401, 0.9253793653172817, 0.9261871412832464, 0.9247264990622215, 0.9259598155238934, 0.9266304553608543, 0.9263807376498427, 0.9262886333385314, 0.9230186093192316, 0.9236926901266496, 0.9245690801152304, 0.9242770334455309, 0.9253971863498243, 0.9259637259339725, 0.9266386250275446, 0.9275647598427579, 0.9287144293169077, 0.9300605215400249, 0.9273477301314543, 0.9284468427195456, 0.9283761006310535, 0.9301906843548622, 0.9303057759242209, 0.9303861068485486, 0.9300213848507093, 0.9305092622847967, 0.9282631655886628, 0.9298799236191284, 0.9305910303251373, 0.9292270004284671, 0.930821779736857, 0.930999247706093, 0.9306939751653195, 0.9311980465833575, 0.9334420033452875, 0.9339177737679569, 0.9345157030114547, 0.9352537660794029, 0.934334848843002, 0.9346609889933087, 0.9343824924726231, 0.9354244480980075, 0.9352810860620624, 0.9355916215011681, 0.93419023709999, 0.934247170269836, 0.9353468719916987, 0.9357041991140035, 0.9361374367157538, 0.9368620638394441, 0.9383651966662324, 0.9367794607009289, 0.9367052104999298, 0.9373964919773916, 0.9374441533070611, 0.9392868218287913, 0.9393385456709675, 0.9391292947110599, 0.9396268516971337, 0.940346587248858, 0.9397807657621136, 0.9409084311859484, 0.9422231300685532, 0.9425903399772109, 0.9423932364081535, 0.9414602419305441, 0.942536072982737, 0.9435903874844818, 0.9442673002229752, 0.945055522097134, 0.9445954800791426, 0.9450114843391316, 0.9457368774969036, 0.9439509746770767, 0.9442778975582061, 0.9448907816845837, 0.9459351647506676, 0.9467938013994472, 0.9467830352833582, 0.9450481574598488, 0.9462209621745966, 0.9474787022666608, 0.945681629728256, 0.945186986879323, 0.9458576274021234, 0.9460287261940724, 0.9468105413342166, 0.9464982291258596, 0.946542741069309, 0.9439298430755657, 0.9449122459323225, 0.9441668396761149, 0.9445001138918736, 0.9453254361705526, 0.9465239914368492, 0.946991357283705, 0.9464353879970565, 0.947076663704549, 0.9456243174187655, 0.9467674800209736, 0.9469454300451312, 0.9477658191823217, 0.9460839057879914, 0.9471322093306394, 0.9463645532393067, 0.9473383972070889, 0.9474916994495122, 0.9467392476201018, 0.947517475981938, 0.9480528319909332, 0.9482973790740011, 0.9481779997225719, 0.9490239053670091, 0.9496502897184126, 0.9497281155823041, 0.9495983042086437, 0.9479028781747764, 0.9483392040466213], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 325128973, "moving_var_accuracy_train": [0.016053919760942042, 0.03119577719796948, 0.052002863695013196, 0.07036581865026408, 0.08060846427691798, 0.09392676037172097, 0.09936840897194138, 0.10541988324148835, 0.10754897063189073, 0.10663944465032232, 0.1075355448323944, 0.10446943363823717, 0.10219278478784036, 0.09902499735970814, 0.09467303769229747, 0.08940552129025367, 0.08447333554147925, 0.08006284008887955, 0.07579434690982848, 0.0712309361372262, 0.0663576333224332, 0.06164868631006285, 0.057083409263293985, 0.05321846406688679, 0.049029272876901446, 0.04517758706510695, 0.04154585394114005, 0.038302888259727115, 0.03514254917065629, 0.03209401200212196, 0.029212838786986817, 0.026651113444587506, 0.024495096781488288, 0.022394293961848156, 0.020385752491523353, 0.01860829312277591, 0.01688956243300542, 0.015326884102468298, 0.013855886073942891, 0.01248715005704582, 0.011238541303590479, 0.010321903665509027, 0.0093671207166207, 0.008450484674062788, 0.007660115047937273, 0.0069510681091894105, 0.006256272032317193, 0.005634512088422226, 0.005082258095415097, 0.0046227554518985525, 0.004217602182272632, 0.0038375779352501724, 0.003479152867827198, 0.003198340404504267, 0.002881848993833944, 0.00264944170747826, 0.002385407416431642, 0.0021903194632438804, 0.0019973208092847576, 0.0017980128574664937, 0.001689558276461666, 0.0015328579795345794, 0.0013819493837033726, 0.0012754292896161035, 0.0011546609080216204, 0.0010790263282215104, 0.0009929180546029968, 0.0009003391123385783, 0.0008431939576683395, 0.0007932092197391902, 0.000722429084458062, 0.0007075757613417064, 0.000674274582726212, 0.0006270684721945322, 0.000588626429697932, 0.0005337903922027635, 0.0004848146537075909, 0.0004482415375130208, 0.00041951810881293866, 0.0003874500953633881, 0.0003620981380846409, 0.00032700932992118804, 0.0002954525631091474, 0.00028590116326819933, 0.00026347660862690426, 0.0002446570608768634, 0.00024322023272233085, 0.0002210374270611662, 0.00019893385103956848, 0.00023141139524660104, 0.00022470034806147047, 0.0002041575129393176, 0.00018395485439186862, 0.00017591902343417953, 0.0001899793164494271, 0.00017112857827750291, 0.0001720283954036007, 0.00015488384184186076, 0.00014869509054909647, 0.0001421825665017162, 0.00014189051310266885, 0.00023060701670479615, 0.00021326798636784862, 0.00019555636088461312, 0.00017651278196044738, 0.0001672673686755966, 0.00015106549226582533, 0.00013781792843817488, 0.00012651951400051282, 0.00011487217107351884, 0.00011995966885358307, 0.000116479902469251, 0.0001052302067443958, 0.00010067905821893208, 0.00010284983488685097, 9.781203323381389e-05, 8.926971676683295e-05, 8.083341391871857e-05, 7.529326052729638e-05, 6.842067830326982e-05, 6.745112857365479e-05, 7.990729699685421e-05, 8.5606192748847e-05, 8.109339359223386e-05, 7.354528464974633e-05, 6.626710502223092e-05, 0.00015587790830118647, 0.00014437958188543074, 0.0001368541584056511, 0.00012393636388062903, 0.00012283541025353753, 0.00011344057313184554, 0.00010619591489720126, 0.0001032958546710348, 0.00010486192830205877, 0.00011068341393008435, 0.00016584820757480556, 0.00016013582314902954, 0.00014416728082188452, 0.0001593849795560973, 0.00014356569622452469, 0.0001292672041187022, 0.00011753768292820336, 0.0001079261341516086, 0.00014253807405189536, 0.00015180942540838035, 0.00014117953759351992, 0.00014380678186526013, 0.00015231599306095165, 0.00013736784767579943, 0.0001244697848255726, 0.000114309598293361, 0.00014819671600872861, 0.0001354142618636397, 0.00012509051009934412, 0.00011748409291983296, 0.00011333536361404314, 0.00010295913383141729, 9.336126325658147e-05, 9.379618065835559e-05, 8.460153665267291e-05, 7.700927331787072e-05, 8.698325014487126e-05, 7.831409760284251e-05, 8.136678273617127e-05, 7.43792485135661e-05, 6.863057703834241e-05, 6.649327954999863e-05, 8.017862624971705e-05, 9.479179058966059e-05, 8.536222936183023e-05, 8.112683715538318e-05, 7.303459786095762e-05, 9.62899836036407e-05, 8.66850634459219e-05, 7.841063077933056e-05, 7.2797634290915e-05, 7.018004424156824e-05, 6.60434254111663e-05, 7.088374664306212e-05, 7.935127034605841e-05, 7.262973136460081e-05, 6.571640658055714e-05, 6.697907417974765e-05, 7.069787883753545e-05, 7.363230257108558e-05, 7.039297001379008e-05, 6.894531651853185e-05, 6.395553279153663e-05, 5.9117515411344196e-05, 5.7941520970291124e-05, 8.085240881005454e-05, 7.37290750609035e-05, 6.97368101261041e-05, 7.257975301199893e-05, 7.195708976244742e-05, 6.476242396950347e-05, 8.537439113509255e-05, 8.921619011199679e-05, 9.453176235346701e-05, 0.00011414381349271566, 0.00010493147607544934, 9.848615686530295e-05, 8.890101434823025e-05, 8.551202713363637e-05, 7.783867465967248e-05, 7.007263901169207e-05, 0.00012451049844189717, 0.00012074548695438347, 0.00011367161264008481, 0.0001033040967020829, 9.910409880502978e-05, 0.00010212250146183113, 9.387612882891391e-05, 8.727043257528998e-05, 8.224450011494205e-05, 9.300383770790817e-05, 9.546484055289959e-05, 8.620335239748926e-05, 8.364036218552141e-05, 0.0001007358199612205, 0.0001005527008228514, 9.580109361160745e-05, 9.475633291271779e-05, 8.549221381923405e-05, 8.203864623755838e-05, 7.928553606229973e-05, 7.393643696337524e-05, 6.708102274957134e-05, 6.050118334054328e-05, 6.089107224010476e-05, 5.833318121724369e-05, 5.255437488133343e-05, 4.745059632778461e-05, 6.857576162184217e-05, 6.343160785762968e-05], "duration": 159973.08673, "accuracy_train": [0.4223468790374677, 0.47360523515365455, 0.6009799510889626, 0.6486099935977299, 0.6262685579318937, 0.719304070344223, 0.6866432998223514, 0.7427376049741602, 0.7386224520694906, 0.7316709781053894, 0.7923916084117755, 0.7620988847937431, 0.8003636244347545, 0.8091051024940015, 0.8055254547918974, 0.7980388361134183, 0.8146604057078257, 0.8365124786590993, 0.8496957119209118, 0.8493444161821706, 0.8426959325396824, 0.8467184399801587, 0.8483482070759505, 0.8714794723606497, 0.8548582632544297, 0.8619696137296974, 0.8639216572651348, 0.8752665810146733, 0.8709654162052418, 0.865250236480251, 0.8608989638127538, 0.8697545363833518, 0.8880788719315246, 0.8826351398117387, 0.8772640460617387, 0.8855426572882059, 0.8768004582525839, 0.8784967354189737, 0.8709657766934293, 0.8610867781584534, 0.8498577513612033, 0.8968632480850868, 0.8830053611803249, 0.8715463429194352, 0.8827528392049648, 0.8857275877284054, 0.8612270080633997, 0.8694544299672389, 0.8747089057885751, 0.8879375605620154, 0.8921900594661315, 0.8910507365494648, 0.8884469303709857, 0.9006528801564231, 0.882172272978959, 0.9015822187038575, 0.8823564824427832, 0.901467763704319, 0.8986997551564231, 0.8855637458471761, 0.9117656495131967, 0.8980948559777593, 0.8824530932770396, 0.9058386829780363, 0.8976305471922297, 0.9108595624538575, 0.9074873757036729, 0.9021184448827981, 0.9134619266795865, 0.9157892384182356, 0.9079520449773901, 0.9244366290605389, 0.9221103987864526, 0.9187392935008305, 0.9216686205126431, 0.9135796260728128, 0.9145543861318751, 0.9197619984888336, 0.9227846919412146, 0.921226481750646, 0.9239937693221669, 0.9094855617271133, 0.9162274118101699, 0.9279232708102622, 0.9227857734057769, 0.9244824110603543, 0.9322473266196014, 0.9129754478705242, 0.917320231750646, 0.8932363764304172, 0.9284580550364526, 0.9209252938699704, 0.9182993176679586, 0.9061856528585271, 0.9345950059408453, 0.9189957808462532, 0.9319919207387413, 0.9200642678340717, 0.9295050929771133, 0.9299926532507383, 0.9337593943221669, 0.8887499206925988, 0.9271559717031194, 0.9263178366671282, 0.9182284817391103, 0.9300395167151162, 0.9189267473583426, 0.9256449854651162, 0.9268075598698781, 0.9187389330126431, 0.9353165230481728, 0.9328304162629198, 0.9219719713224437, 0.9157194839539498, 0.9347119843576966, 0.9318524118101699, 0.9286905699174051, 0.9230164858457919, 0.9304337105481728, 0.9229481733342562, 0.9334571249769288, 0.9115807190729974, 0.9370596636789406, 0.9326662138935032, 0.9241332782507383, 0.9254596945367294, 0.8935883931455334, 0.929759417393411, 0.9324565900124585, 0.9216486134182356, 0.9354785624884644, 0.9310625821913067, 0.9327127168696937, 0.9358999731796788, 0.9390614545842562, 0.9421753515480805, 0.902932607454319, 0.9383388560123662, 0.9277394218346253, 0.94652193786914, 0.9313416000484496, 0.9311090851674971, 0.926738886870155, 0.9349001591915835, 0.9080482953234589, 0.9444307458933187, 0.9369909906792175, 0.9169507313584349, 0.9451747935123662, 0.9325964594292175, 0.9279465222983574, 0.9357346893456996, 0.9536376142026578, 0.9381997075719823, 0.9398970662029347, 0.9418963336909376, 0.9260645937153931, 0.9375962503460686, 0.9318760237864526, 0.9448020487264673, 0.9339908277385567, 0.9383864404531194, 0.9215777774893872, 0.9347595687984496, 0.9452441874884644, 0.9389201432147471, 0.9400365751315062, 0.9433837079526578, 0.9518933921073275, 0.9225078370131967, 0.9360369586909376, 0.9436180252745479, 0.9378731052740864, 0.9558708385243633, 0.9398040602505537, 0.93724603607189, 0.9441048645717978, 0.946824207214378, 0.9346883723814139, 0.9510574200004615, 0.9540554200119971, 0.9458952291551311, 0.9406193042866371, 0.9330632916320598, 0.9522185524524732, 0.9530792180001846, 0.9503595148694168, 0.9521495189645626, 0.9404551019172205, 0.948755522679033, 0.9522654159168512, 0.9278778492986341, 0.9472202034883721, 0.9504067388219823, 0.9553346123454227, 0.9545215312384644, 0.9466861402385567, 0.9294342570482651, 0.9567762046073275, 0.9587983630952381, 0.9295079768826136, 0.9407352012389257, 0.9518933921073275, 0.9475686153216132, 0.953846877595515, 0.943687419250646, 0.9469433485603543, 0.9204137611318751, 0.9537538716431341, 0.9374581833702473, 0.9474995818337025, 0.9527533366786637, 0.957310988833518, 0.951197649905408, 0.9414316644172205, 0.9528481450719823, 0.9325532008467147, 0.9570559434408453, 0.9485469802625508, 0.9551493214170359, 0.930946685239018, 0.9565669412144703, 0.9394556484173128, 0.9561029929171282, 0.9488714196313216, 0.939967181155408, 0.9545215312384644, 0.95287103607189, 0.9504983028216132, 0.9471035855597084, 0.9566370561669435, 0.9552877488810447, 0.9504285483573275, 0.9484300018456996, 0.9326440438699704, 0.9522661368932264], "end": "2016-01-25 07:55:35.937000", "learning_rate_per_epoch": [0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647, 0.0011780851054936647], "accuracy_valid": [0.4203145590173193, 0.47149584666792166, 0.5981812994164157, 0.6360642766378012, 0.6134592079254518, 0.7070091891001506, 0.6647610951618976, 0.7196133165474398, 0.714599609375, 0.708972609186747, 0.766369187688253, 0.7340690888554217, 0.7758200771837349, 0.7815573818712349, 0.7750258847891567, 0.7709578548569277, 0.7823206890060241, 0.8029299816453314, 0.814598726939006, 0.8089320171310241, 0.8014445477221386, 0.8094614787274097, 0.8055949383471386, 0.824364351939006, 0.8128691523908133, 0.817406344126506, 0.8162665309676205, 0.8261851115399097, 0.8223906367658133, 0.8156870646649097, 0.8140280849962349, 0.8154017436935241, 0.8276190700301205, 0.8253512095256024, 0.8221567912274097, 0.826683687876506, 0.8183005459337349, 0.8223700465926205, 0.8154017436935241, 0.8052390224962349, 0.792980515813253, 0.8354521602033133, 0.8185961619917168, 0.8123293957078314, 0.8184637965926205, 0.8223391613328314, 0.8027873211596386, 0.8048522213855422, 0.8103453854480422, 0.8183005459337349, 0.8221656155873494, 0.8203154414533133, 0.8224715267319277, 0.826927828501506, 0.8142516354480422, 0.8299589961408133, 0.8114646084337349, 0.8291750988328314, 0.829247164439006, 0.8147810970444277, 0.8394510659826807, 0.8243231715926205, 0.8135398037462349, 0.835961031626506, 0.8298972256212349, 0.8370493693524097, 0.8339461361069277, 0.830712008189006, 0.8340887965926205, 0.8376803110881024, 0.8283309017319277, 0.8428072642131024, 0.8422072077371988, 0.8380053416792168, 0.8396540262612951, 0.8345564876694277, 0.8357065959149097, 0.8408747293862951, 0.8445471338478916, 0.8451574854103916, 0.8431837702371988, 0.8294707148908133, 0.8369272990399097, 0.8437029367469879, 0.8436926416603916, 0.8457163615399097, 0.8494608316076807, 0.8331225291792168, 0.8357683664344879, 0.8178328548569277, 0.8467443994728916, 0.8374655849962349, 0.8345976680158133, 0.8300104715737951, 0.8407320689006024, 0.8360522166792168, 0.8462561182228916, 0.8424925287085843, 0.8408850244728916, 0.8448015695594879, 0.8496946771460843, 0.8125838314194277, 0.8466635095067772, 0.8392363398908133, 0.8322474468185241, 0.8493181711219879, 0.8367140436746988, 0.8440794427710843, 0.8417292215737951, 0.8352683193712349, 0.8464899637612951, 0.8472429758094879, 0.8376906061746988, 0.8304781626506024, 0.8517698724585843, 0.8401011271649097, 0.8440691476844879, 0.8384436182228916, 0.8440485575112951, 0.8417086314006024, 0.8478842126317772, 0.8289824336408133, 0.8477312570594879, 0.8419733621987951, 0.8391760400978916, 0.8385553934487951, 0.8147502117846386, 0.8420542521649097, 0.8469076501317772, 0.8350859492658133, 0.8479856927710843, 0.8419733621987951, 0.8438661874058735, 0.8437338220067772, 0.8480062829442772, 0.8506712396460843, 0.8227362575301205, 0.8477930275790663, 0.8393187005835843, 0.8486666392131024, 0.8391142695783133, 0.8451986657567772, 0.8411188700112951, 0.848412203501506, 0.8242113963667168, 0.8525537697665663, 0.8502344338290663, 0.830712008189006, 0.8521463784826807, 0.8398466914533133, 0.8418512918862951, 0.8470606057040663, 0.8566320947853916, 0.845238375376506, 0.8507021249058735, 0.8510580407567772, 0.8409865046121988, 0.8514448418674698, 0.8466429193335843, 0.8487990046121988, 0.8443941782756024, 0.8463473032756024, 0.8356154108621988, 0.8441191523908133, 0.8562055840549698, 0.8453810358621988, 0.8449236398719879, 0.8493181711219879, 0.8560629235692772, 0.8399084619728916, 0.8380156367658133, 0.8502741434487951, 0.842186617564006, 0.856602680252259, 0.8531435311558735, 0.8510992211031627, 0.856602680252259, 0.8546083749058735, 0.841942476939006, 0.8573042168674698, 0.8565923851656627, 0.8505285791603916, 0.845482516001506, 0.8391054452183735, 0.8602853798004518, 0.8604986351656627, 0.856358539627259, 0.8602133141942772, 0.844872164439006, 0.8539568429969879, 0.8554422769201807, 0.8405188135353916, 0.8472326807228916, 0.8573248070406627, 0.8543745293674698, 0.8542318688817772, 0.8551878412085843, 0.8400202371987951, 0.8580263436558735, 0.8563173592808735, 0.8372126200112951, 0.8407320689006024, 0.8548731057040663, 0.8522478586219879, 0.8571012565888554, 0.8490740304969879, 0.8504579842808735, 0.8277014307228916, 0.8552393166415663, 0.8454633965549698, 0.8495314264871988, 0.8506712396460843, 0.8523390436746988, 0.8530008706701807, 0.8459516778049698, 0.8505388742469879, 0.8449339349585843, 0.8580469338290663, 0.8521360833960843, 0.8573645166603916, 0.8376803110881024, 0.8601324242281627, 0.8416571559676205, 0.8527464349585843, 0.8512007012424698, 0.8443132883094879, 0.8538759530308735, 0.8550260612763554, 0.8491152108433735, 0.8473444559487951, 0.8581998894013554, 0.8548525155308735, 0.8500608880835843, 0.8531832407756024, 0.8374052852033133, 0.8548216302710843], "accuracy_test": 0.4906489158163265, "start": "2016-01-23 11:29:22.850000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0], "accuracy_train_last": 0.9522661368932264, "batch_size_eval": 1024, "accuracy_train_std": [0.0170835477732148, 0.015481947498738368, 0.01734712335124031, 0.01606192305168593, 0.016167761057610284, 0.014950757418541788, 0.01575261260080167, 0.013839681076394252, 0.0141668223289221, 0.014631580262804941, 0.015273817393314047, 0.01631131732648682, 0.01648339661735034, 0.015528363420601111, 0.015405675397020669, 0.015530452664717528, 0.016678432756694173, 0.01559577478024487, 0.015350557900343018, 0.016772457260894713, 0.017114857261970106, 0.017538064168133468, 0.01641768803794812, 0.015464157747079849, 0.015464713082033088, 0.015870833711114543, 0.01630568469989478, 0.01608849047482836, 0.01627054815312424, 0.01447431178912481, 0.01590394227623264, 0.018126115957714378, 0.01512761920197913, 0.016427173026400187, 0.016500677909188875, 0.016590730662385075, 0.0176779117757438, 0.01657701330142612, 0.01610376488472232, 0.016301866522109888, 0.015560777462606914, 0.01677154086945431, 0.017681650156911526, 0.015955150435970695, 0.015817526083743826, 0.016187780058436605, 0.016042673556011668, 0.015508575262573961, 0.015492882391968948, 0.014241830516650057, 0.01625873901795235, 0.016768113804710723, 0.01651569376901125, 0.017276422323556466, 0.014738052566654489, 0.016192534387533798, 0.01623174931554425, 0.015233599271947607, 0.016006890821358418, 0.016273861165888935, 0.015360276240303412, 0.01721052347255287, 0.013834391307110665, 0.01572016773658737, 0.015614309899533583, 0.01453956047263922, 0.01555153906787669, 0.015277500517445355, 0.016145769529998202, 0.01357288316306594, 0.014944365681157267, 0.013960162409392705, 0.015024508913188543, 0.014221591133056407, 0.015472404904070024, 0.014977821194612495, 0.015131432761728209, 0.014578720028013782, 0.014279873482922327, 0.015868561198268426, 0.01510419687255283, 0.01749366083467385, 0.01639557874789248, 0.014806341050790732, 0.014870190568600068, 0.014920214495167866, 0.014305405572649055, 0.015156434395543946, 0.016812686229890437, 0.016373238849248142, 0.013909253565865413, 0.014900414483789811, 0.01595683879000858, 0.015962547336035915, 0.014550683578728074, 0.015016400222342139, 0.014640477030198058, 0.015366731874416926, 0.013935629117385393, 0.014335703953417234, 0.014037924172433335, 0.017658903032941743, 0.014686110192408777, 0.015492601879274401, 0.015276114899323784, 0.014010339275752116, 0.014897438941663434, 0.014452895450356374, 0.015026297040001718, 0.01647232814351228, 0.014274523316659946, 0.012842073360435297, 0.014786826071237899, 0.014510306392792542, 0.014400932728384589, 0.01460434168826868, 0.013574483587729677, 0.016035430315833276, 0.015816127063065226, 0.014876012750686495, 0.013912317361150122, 0.014715157316275415, 0.014918311955092281, 0.014188465829099195, 0.014839630145498392, 0.012993974016841261, 0.016338940440894404, 0.014544166073689762, 0.015607898937515787, 0.013724968349031986, 0.013509808294494214, 0.014345212124701592, 0.013336892440307046, 0.013348484907221543, 0.01239053962791951, 0.013060915987213609, 0.014779303189449632, 0.013318015046304192, 0.014775010407620599, 0.012942638020479258, 0.014391655289582991, 0.01349727765656535, 0.013910606018964322, 0.012235590746232395, 0.014022924148215009, 0.013078454081074884, 0.012791046449828258, 0.01562551996046165, 0.011970174811249915, 0.013804866793729494, 0.01394274620386384, 0.012379395724871518, 0.012191989013813454, 0.01393191252434978, 0.01270749702431009, 0.013176427144361383, 0.01445482771195931, 0.013152510933542754, 0.013232134747309832, 0.014432292199147961, 0.014596459125762856, 0.013235863338148942, 0.014025806969432264, 0.012880083943042579, 0.01328161912623641, 0.012920955009755665, 0.013033602960944019, 0.013544908042086485, 0.012762142847780792, 0.01311926564798127, 0.014601168865707894, 0.01183682273187122, 0.014259840319640468, 0.010267360710100155, 0.011510432170226454, 0.014630241684525825, 0.012387275919441092, 0.01220287450336554, 0.013919771563601689, 0.0120762437797819, 0.011599269439790054, 0.010571736053364582, 0.01355608534080154, 0.012357254972998314, 0.011803929465641644, 0.010841746995587353, 0.012279962334366878, 0.011332616168750765, 0.01220107414834888, 0.011546063333033051, 0.012052301559644875, 0.012515956101708954, 0.012486456318206714, 0.009504383135696412, 0.010329151438892728, 0.010303140465581125, 0.011427429506806093, 0.013778397387249227, 0.010768173267903873, 0.012203288293792207, 0.012638623603793845, 0.012604583665615815, 0.010929657000077132, 0.013756364537550615, 0.010573837533139301, 0.011563509026581816, 0.010701411417634454, 0.014800789779953024, 0.010804880959145783, 0.013161858462512319, 0.013467139570542786, 0.011762596974044215, 0.010538427094505082, 0.011530052889612894, 0.01365518223263892, 0.010159167985120504, 0.014348711424337815, 0.010722555988594883, 0.011033515788050499, 0.009433880884148647, 0.013796137776789933, 0.010544837370437382, 0.011157372941468919, 0.01029460372685902, 0.011630398701992378, 0.011840951045301269, 0.01131765049517076, 0.010394179631980143, 0.012612655041485942, 0.01185068167636835, 0.010250798600167655, 0.010839434599652309, 0.010868816968338503, 0.010161166490931172, 0.013469573506754396, 0.011753072240359035], "accuracy_test_std": 0.015195759563564252, "error_valid": [0.5796854409826807, 0.5285041533320783, 0.40181870058358427, 0.3639357233621988, 0.38654079207454817, 0.29299081089984935, 0.33523890483810237, 0.28038668345256024, 0.285400390625, 0.291027390813253, 0.23363081231174698, 0.26593091114457834, 0.2241799228162651, 0.2184426181287651, 0.22497411521084332, 0.2290421451430723, 0.21767931099397586, 0.19707001835466864, 0.18540127306099397, 0.19106798286897586, 0.19855545227786142, 0.1905385212725903, 0.19440506165286142, 0.17563564806099397, 0.18713084760918675, 0.18259365587349397, 0.18373346903237953, 0.1738148884600903, 0.17760936323418675, 0.1843129353350903, 0.1859719150037651, 0.18459825630647586, 0.17238092996987953, 0.17464879047439763, 0.1778432087725903, 0.17331631212349397, 0.1816994540662651, 0.17762995340737953, 0.18459825630647586, 0.1947609775037651, 0.20701948418674698, 0.16454783979668675, 0.1814038380082832, 0.18767060429216864, 0.18153620340737953, 0.17766083866716864, 0.19721267884036142, 0.19514777861445776, 0.18965461455195776, 0.1816994540662651, 0.17783438441265065, 0.17968455854668675, 0.1775284732680723, 0.17307217149849397, 0.18574836455195776, 0.17004100385918675, 0.1885353915662651, 0.17082490116716864, 0.17075283556099397, 0.1852189029555723, 0.1605489340173193, 0.17567682840737953, 0.1864601962537651, 0.16403896837349397, 0.1701027743787651, 0.1629506306475903, 0.1660538638930723, 0.16928799181099397, 0.16591120340737953, 0.16231968891189763, 0.1716690982680723, 0.15719273578689763, 0.15779279226280118, 0.1619946583207832, 0.16034597373870485, 0.1654435123305723, 0.1642934040850903, 0.15912527061370485, 0.1554528661521084, 0.1548425145896084, 0.15681622976280118, 0.17052928510918675, 0.1630727009600903, 0.15629706325301207, 0.1563073583396084, 0.1542836384600903, 0.1505391683923193, 0.1668774708207832, 0.16423163356551207, 0.1821671451430723, 0.1532556005271084, 0.1625344150037651, 0.16540233198418675, 0.16998952842620485, 0.15926793109939763, 0.1639477833207832, 0.1537438817771084, 0.15750747129141573, 0.1591149755271084, 0.15519843044051207, 0.15030532285391573, 0.1874161685805723, 0.15333649049322284, 0.16076366010918675, 0.16775255318147586, 0.15068182887801207, 0.16328595632530118, 0.15592055722891573, 0.15827077842620485, 0.1647316806287651, 0.15351003623870485, 0.15275702419051207, 0.16230939382530118, 0.16952183734939763, 0.14823012754141573, 0.1598988728350903, 0.15593085231551207, 0.1615563817771084, 0.15595144248870485, 0.15829136859939763, 0.15211578736822284, 0.17101756635918675, 0.15226874294051207, 0.15802663780120485, 0.1608239599021084, 0.16144460655120485, 0.18524978821536142, 0.1579457478350903, 0.15309234986822284, 0.16491405073418675, 0.15201430722891573, 0.15802663780120485, 0.1561338125941265, 0.15626617799322284, 0.15199371705572284, 0.14932876035391573, 0.17726374246987953, 0.15220697242093373, 0.16068129941641573, 0.15133336078689763, 0.16088573042168675, 0.15480133424322284, 0.15888112998870485, 0.15158779649849397, 0.1757886036332832, 0.14744623023343373, 0.14976556617093373, 0.16928799181099397, 0.1478536215173193, 0.16015330854668675, 0.15814870811370485, 0.15293939429593373, 0.1433679052146084, 0.15476162462349397, 0.1492978750941265, 0.14894195924322284, 0.15901349538780118, 0.14855515813253017, 0.15335708066641573, 0.15120099538780118, 0.15560582172439763, 0.15365269672439763, 0.16438458913780118, 0.15588084760918675, 0.14379441594503017, 0.15461896413780118, 0.15507636012801207, 0.15068182887801207, 0.14393707643072284, 0.1600915380271084, 0.16198436323418675, 0.14972585655120485, 0.15781338243599397, 0.14339731974774095, 0.1468564688441265, 0.14890077889683728, 0.14339731974774095, 0.1453916250941265, 0.15805752306099397, 0.14269578313253017, 0.14340761483433728, 0.1494714208396084, 0.15451748399849397, 0.1608945547816265, 0.13971462019954817, 0.13950136483433728, 0.14364146037274095, 0.13978668580572284, 0.15512783556099397, 0.14604315700301207, 0.1445577230798193, 0.1594811864646084, 0.1527673192771084, 0.14267519295933728, 0.14562547063253017, 0.14576813111822284, 0.14481215879141573, 0.15997976280120485, 0.1419736563441265, 0.1436826407191265, 0.16278737998870485, 0.15926793109939763, 0.14512689429593373, 0.14775214137801207, 0.1428987434111446, 0.15092596950301207, 0.1495420157191265, 0.1722985692771084, 0.14476068335843373, 0.15453660344503017, 0.15046857351280118, 0.14932876035391573, 0.14766095632530118, 0.1469991293298193, 0.15404832219503017, 0.14946112575301207, 0.15506606504141573, 0.14195306617093373, 0.14786391660391573, 0.1426354833396084, 0.16231968891189763, 0.13986757577183728, 0.15834284403237953, 0.14725356504141573, 0.14879929875753017, 0.15568671169051207, 0.1461240469691265, 0.1449739387236446, 0.1508847891566265, 0.15265554405120485, 0.1418001105986446, 0.1451474844691265, 0.14993911191641573, 0.14681675922439763, 0.16259471479668675, 0.14517836972891573], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.7376549206740131, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0011780851380770775, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "rmsprop", "nb_data_augmentation": 4, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 7.275056893728878e-06, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.06853500815602223}, "accuracy_valid_max": 0.8604986351656627, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8548216302710843, "loss_train": [1.5984379053115845, 1.229996681213379, 1.067405104637146, 0.9650285840034485, 0.8923314809799194, 0.8382709622383118, 0.792320191860199, 0.7571041584014893, 0.7267119884490967, 0.7016499042510986, 0.6773676872253418, 0.6571443676948547, 0.6386927366256714, 0.6238085031509399, 0.6101112961769104, 0.5977044701576233, 0.5835179090499878, 0.5753185749053955, 0.5641375184059143, 0.5548666715621948, 0.544796884059906, 0.5376348495483398, 0.5318143963813782, 0.5234663486480713, 0.5163445472717285, 0.5096622109413147, 0.5045749545097351, 0.4986818730831146, 0.49294719099998474, 0.48686033487319946, 0.4818074703216553, 0.477458655834198, 0.47386690974235535, 0.47011086344718933, 0.46518269181251526, 0.46061718463897705, 0.45648518204689026, 0.4524802267551422, 0.4502921402454376, 0.44522857666015625, 0.4409349858760834, 0.4396943151950836, 0.43543821573257446, 0.43046507239341736, 0.42798224091529846, 0.42753127217292786, 0.42292481660842896, 0.4200800955295563, 0.4193958342075348, 0.41558295488357544, 0.41252991557121277, 0.4122527241706848, 0.40838494896888733, 0.40495023131370544, 0.40436309576034546, 0.4014040231704712, 0.3996843695640564, 0.39765992760658264, 0.39363715052604675, 0.39331936836242676, 0.3902556598186493, 0.3907777667045593, 0.3854668438434601, 0.38492321968078613, 0.38462695479393005, 0.38215750455856323, 0.3829042911529541, 0.3793724775314331, 0.37800902128219604, 0.3746083676815033, 0.3747239112854004, 0.37315165996551514, 0.371648907661438, 0.36829036474227905, 0.369333952665329, 0.3685031831264496, 0.3662855625152588, 0.3659850060939789, 0.364886999130249, 0.3626323938369751, 0.36166292428970337, 0.361819326877594, 0.35757148265838623, 0.35802704095840454, 0.35479533672332764, 0.3562328815460205, 0.3538469076156616, 0.35495853424072266, 0.3516544997692108, 0.3532593548297882, 0.35188886523246765, 0.3504332900047302, 0.3494124710559845, 0.3496253788471222, 0.3466256558895111, 0.3444678783416748, 0.3441751003265381, 0.343618780374527, 0.3442424535751343, 0.3440419137477875, 0.3418256938457489, 0.33991533517837524, 0.3406233489513397, 0.3372993767261505, 0.33791062235832214, 0.3376193046569824, 0.3362311124801636, 0.33633050322532654, 0.33457353711128235, 0.3313111364841461, 0.33387503027915955, 0.3329923450946808, 0.3329181969165802, 0.33285877108573914, 0.3327937722206116, 0.33210453391075134, 0.3300326466560364, 0.329022616147995, 0.32903313636779785, 0.3301528990268707, 0.32828646898269653, 0.32788747549057007, 0.3295400142669678, 0.32489585876464844, 0.3262749910354614, 0.32471197843551636, 0.32333678007125854, 0.32113099098205566, 0.32328152656555176, 0.3212645649909973, 0.3209254741668701, 0.3227381706237793, 0.31973525881767273, 0.3202633559703827, 0.32034897804260254, 0.31913992762565613, 0.3165227770805359, 0.3183101713657379, 0.3168744742870331, 0.31931260228157043, 0.31592249870300293, 0.3172050416469574, 0.314866840839386, 0.31282925605773926, 0.31584954261779785, 0.31445246934890747, 0.3150152862071991, 0.314339280128479, 0.3130304217338562, 0.31236201524734497, 0.3130341172218323, 0.31191813945770264, 0.3132750988006592, 0.31117871403694153, 0.3105223774909973, 0.31006425619125366, 0.3126341998577118, 0.3095150887966156, 0.3072809875011444, 0.3105708360671997, 0.30951571464538574, 0.30836641788482666, 0.30916541814804077, 0.3080170750617981, 0.30785369873046875, 0.30669647455215454, 0.30812355875968933, 0.30812785029411316, 0.306793749332428, 0.30657508969306946, 0.30560922622680664, 0.3065917193889618, 0.3059905171394348, 0.3060787618160248, 0.30515792965888977, 0.30307090282440186, 0.30345088243484497, 0.3036693036556244, 0.3035133183002472, 0.3036311864852905, 0.30276423692703247, 0.3022709786891937, 0.3034994304180145, 0.30423086881637573, 0.3013797700405121, 0.3023478090763092, 0.30121663212776184, 0.30021366477012634, 0.3004556894302368, 0.30200138688087463, 0.29845988750457764, 0.3008492887020111, 0.3003758490085602, 0.3010917007923126, 0.3002517521381378, 0.29895642399787903, 0.2989858090877533, 0.29805788397789, 0.30032363533973694, 0.29932427406311035, 0.2989301383495331, 0.29835042357444763, 0.2979389727115631, 0.299896776676178, 0.29703691601753235, 0.29611918330192566, 0.2966707944869995, 0.2978602349758148, 0.29562342166900635, 0.2971170246601105, 0.2960767149925232, 0.29753485321998596, 0.29709285497665405, 0.2949504256248474, 0.29555243253707886, 0.29514291882514954, 0.2955261766910553, 0.29432734847068787, 0.29421401023864746, 0.29644665122032166, 0.2934623658657074, 0.2914564907550812, 0.2935416102409363, 0.29241684079170227, 0.2929297983646393, 0.29248878359794617, 0.29293179512023926, 0.2933189272880554, 0.2939147651195526, 0.29299673438072205, 0.29353976249694824, 0.29229211807250977, 0.29390087723731995, 0.291178435087204, 0.2929123640060425, 0.2895898222923279], "accuracy_train_first": 0.4223468790374677, "model": "residualv3", "loss_std": [0.24103674292564392, 0.1430426687002182, 0.13365983963012695, 0.12835083901882172, 0.12298531085252762, 0.12048309296369553, 0.11633571982383728, 0.11708364635705948, 0.1165233924984932, 0.11568655073642731, 0.1125250831246376, 0.11053705960512161, 0.10992167890071869, 0.10926991701126099, 0.10816274583339691, 0.1065930500626564, 0.10645478963851929, 0.10586313158273697, 0.10455752164125443, 0.10210064053535461, 0.10090507566928864, 0.0994286835193634, 0.09805677086114883, 0.09961173683404922, 0.09558200091123581, 0.09555789083242416, 0.0947040244936943, 0.09326477348804474, 0.09280099719762802, 0.09123057126998901, 0.09325896203517914, 0.09116082638502121, 0.0910363718867302, 0.08904177695512772, 0.08754541724920273, 0.08753103017807007, 0.08730018883943558, 0.08411242812871933, 0.08651380985975266, 0.08494438976049423, 0.08336585760116577, 0.08358468860387802, 0.08294517546892166, 0.08036024123430252, 0.08147025853395462, 0.08091980218887329, 0.07907915115356445, 0.0807594358921051, 0.08008182048797607, 0.07788877189159393, 0.07735077291727066, 0.07670272141695023, 0.07656759023666382, 0.07584713399410248, 0.07319855690002441, 0.07470502704381943, 0.07159647345542908, 0.07368864864110947, 0.0729447528719902, 0.07531634718179703, 0.0717216283082962, 0.07290858030319214, 0.07104387134313583, 0.07104767858982086, 0.07266636192798615, 0.07261832058429718, 0.0726281926035881, 0.07065233588218689, 0.06993115693330765, 0.06772272288799286, 0.06796049326658249, 0.06879405677318573, 0.06656046956777573, 0.06750941276550293, 0.06575099378824234, 0.0669775977730751, 0.06596367806196213, 0.06695521622896194, 0.06474506855010986, 0.06524264067411423, 0.0656232088804245, 0.06409592926502228, 0.06377704441547394, 0.06230184808373451, 0.06310431659221649, 0.06390003859996796, 0.06224936619400978, 0.06544411182403564, 0.0611482709646225, 0.062084417790174484, 0.062354736030101776, 0.062193915247917175, 0.05982092767953873, 0.060829613357782364, 0.061669379472732544, 0.05848636478185654, 0.061077021062374115, 0.06141909584403038, 0.060696590691804886, 0.05923263728618622, 0.05927696451544762, 0.056275323033332825, 0.05820389464497566, 0.05672675743699074, 0.05951321870088577, 0.05609922111034393, 0.05421963334083557, 0.05547165498137474, 0.05634435638785362, 0.05618663132190704, 0.055903177708387375, 0.05703091621398926, 0.05528819561004639, 0.05654188245534897, 0.05559766665101051, 0.05775241181254387, 0.052175603806972504, 0.05496160686016083, 0.0550699457526207, 0.05574122071266174, 0.055236492305994034, 0.05433408543467522, 0.05310742184519768, 0.0548793189227581, 0.05416984111070633, 0.05319736897945404, 0.05451734736561775, 0.05344212055206299, 0.050266388803720474, 0.052217744290828705, 0.05443707853555679, 0.05305780842900276, 0.05026797205209732, 0.05165522173047066, 0.05073276534676552, 0.051606517285108566, 0.04952100291848183, 0.05016122758388519, 0.051088739186525345, 0.05109047144651413, 0.048877205699682236, 0.0526922233402729, 0.05023343861103058, 0.049993399530649185, 0.05119860917329788, 0.0507391132414341, 0.04837154969573021, 0.04926234111189842, 0.0480366051197052, 0.05082099139690399, 0.048837702721357346, 0.048682067543268204, 0.05102340504527092, 0.048339806497097015, 0.04820108786225319, 0.04862792789936066, 0.04849746823310852, 0.046987298876047134, 0.046194206923246384, 0.04875662922859192, 0.04673488810658455, 0.04752286523580551, 0.0494169183075428, 0.05005226656794548, 0.04963834583759308, 0.047535527497529984, 0.04704577848315239, 0.047670647501945496, 0.04962494596838951, 0.046910785138607025, 0.04652785882353783, 0.04601616784930229, 0.045720018446445465, 0.04540508612990379, 0.04702452942728996, 0.04734381288290024, 0.047064121812582016, 0.045763444155454636, 0.045340023934841156, 0.043753378093242645, 0.04714496433734894, 0.04719483107328415, 0.04547591134905815, 0.0446869358420372, 0.04518218711018562, 0.04486854746937752, 0.044781118631362915, 0.04607115685939789, 0.045050062239170074, 0.04412907734513283, 0.04372430965304375, 0.04505067691206932, 0.04394011199474335, 0.04515501856803894, 0.04307812824845314, 0.04478001967072487, 0.04579649493098259, 0.0454384870827198, 0.04504142701625824, 0.044125091284513474, 0.04258522018790245, 0.044969603419303894, 0.04324406757950783, 0.04437367618083954, 0.043612655252218246, 0.044116150587797165, 0.043231233954429626, 0.04462509974837303, 0.043083108961582184, 0.04342672973871231, 0.04185238853096962, 0.04483887180685997, 0.04244460165500641, 0.042799416929483414, 0.040910784155130386, 0.04296186938881874, 0.04238021373748779, 0.04231024533510208, 0.0421387255191803, 0.0412028469145298, 0.042002297937870026, 0.040124233812093735, 0.04091811180114746, 0.04047474265098572, 0.04158160462975502, 0.041033100336790085, 0.04320035129785538, 0.042056843638420105, 0.04190598428249359, 0.040617093443870544, 0.04177607223391533, 0.039994336664676666, 0.04182087257504463, 0.041800059378147125, 0.04158243164420128, 0.04064586013555527]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:12 2016", "state": "available"}], "summary": "3c995866a545e385b5daa5ae4c0ded39"}