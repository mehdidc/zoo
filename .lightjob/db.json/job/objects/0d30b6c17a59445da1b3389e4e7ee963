{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 64, "f3": 32, "nonlin": "rectify", "nbg1": 2, "nbg3": 4, "nbg2": 3, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.52816641330719, 1.102241039276123, 0.9071986079216003, 0.7895254492759705, 0.7081570625305176, 0.6434521079063416, 0.595637321472168, 0.553695023059845, 0.5175859332084656, 0.48844650387763977, 0.46271246671676636, 0.43811315298080444, 0.4173305630683899, 0.3983560800552368, 0.3804541528224945, 0.36621320247650146, 0.3491581678390503, 0.33320051431655884, 0.32204291224479675, 0.3123924434185028, 0.29898837208747864, 0.2911708652973175, 0.2809106111526489, 0.2739403545856476, 0.264114648103714, 0.257810115814209, 0.25169795751571655, 0.24205394089221954, 0.23515212535858154, 0.22999002039432526, 0.2274916023015976, 0.21872445940971375, 0.21417392790317535, 0.20882107317447662, 0.20431676506996155, 0.20400995016098022, 0.1979169398546219, 0.1945391595363617, 0.18897618353366852, 0.1880849152803421, 0.1844402253627777, 0.17888805270195007, 0.17630112171173096, 0.17301279306411743, 0.17191721498966217, 0.16982032358646393, 0.16615508496761322, 0.16530536115169525, 0.15982981026172638, 0.15951243042945862, 0.1587657928466797, 0.1551913022994995, 0.15271076560020447, 0.14739324152469635, 0.14810067415237427, 0.15154887735843658, 0.147562175989151, 0.1415669322013855, 0.14257663488388062, 0.13849936425685883, 0.1362803727388382, 0.13692811131477356, 0.13556735217571259, 0.13544180989265442, 0.13425958156585693, 0.13396844267845154, 0.13021259009838104, 0.13234944641590118, 0.12857678532600403, 0.1264030635356903, 0.12834320962429047, 0.12466327100992203, 0.12408778071403503, 0.1239342987537384, 0.12132930755615234, 0.12260524928569794, 0.12007790803909302, 0.1173359677195549, 0.11902699619531631, 0.11645020544528961, 0.11816936731338501, 0.11684097349643707, 0.11443203687667847, 0.11821644008159637, 0.11349751800298691, 0.11208683997392654, 0.11372020840644836, 0.09832891076803207, 0.08555736392736435, 0.08137401193380356, 0.08005240559577942, 0.07803675532341003, 0.07720879465341568, 0.07636010646820068, 0.07572989910840988, 0.07512442022562027, 0.07599958032369614, 0.07539667934179306, 0.07537370920181274, 0.07504509389400482, 0.0750788226723671, 0.07609007507562637, 0.07567702233791351, 0.07510803639888763, 0.07533154636621475, 0.07544638216495514, 0.07543928921222687, 0.07517041265964508, 0.0759410709142685, 0.07545378804206848, 0.07515556365251541, 0.07615593820810318, 0.07468470185995102, 0.0751405656337738, 0.0757765918970108, 0.07548575103282928, 0.07458559423685074, 0.07582291215658188, 0.07543591409921646, 0.07596353441476822, 0.07545062899589539, 0.07633356004953384, 0.07588671892881393, 0.07487473636865616, 0.07576125115156174, 0.07578171044588089, 0.07541979104280472, 0.07541093230247498, 0.07601357996463776, 0.07477457821369171, 0.0760313868522644, 0.07512662559747696, 0.07543891668319702, 0.07536134123802185, 0.07584523409605026, 0.07582107931375504, 0.0759844109416008, 0.07629349082708359, 0.07591602206230164, 0.07591163367033005], "moving_avg_accuracy_train": [0.06287058823529411, 0.13017176470588232, 0.19500399999999996, 0.2558871294117646, 0.3133007694117646, 0.3665306924705881, 0.41565644675294106, 0.46108844913647046, 0.5034713689287058, 0.5411548202711294, 0.5761005147146047, 0.6088622279490266, 0.638526593389418, 0.6654551105210644, 0.6904554818218992, 0.7135275806985328, 0.7345042343933854, 0.7539949874246351, 0.7712919592704068, 0.7876357045198367, 0.8029215458325588, 0.8167964500728323, 0.8290085697714314, 0.8403453598531119, 0.8507908238678007, 0.8597752708927853, 0.8689577438035068, 0.8773631458937443, 0.8856080077749582, 0.89295544229158, 0.8990787215918338, 0.9047496729620621, 0.9103947056658559, 0.915618764511035, 0.9203110057069903, 0.9243810816068796, 0.9280535616814857, 0.9312740878662783, 0.934784326138474, 0.9382611876422736, 0.9411080100545168, 0.9431125031667122, 0.9459471352029822, 0.9475665393297428, 0.9495887089261802, 0.9506510145041503, 0.9523459130537353, 0.9545136746895383, 0.956544660161761, 0.9572996059102907, 0.9590849394369088, 0.9608376219638062, 0.9624126832968374, 0.9638161208495066, 0.9649545087645559, 0.9661908225939826, 0.9676423285698785, 0.9687157427717141, 0.9696676979063074, 0.9704468104686178, 0.9712586000099912, 0.9721139164795803, 0.9726648777727988, 0.9730148605837542, 0.97361925687832, 0.9744832135434291, 0.9748819510126157, 0.97512316767606, 0.9755096744378657, 0.9760034128764321, 0.9767324833534948, 0.9770686467828512, 0.9777758997516248, 0.9781088980117564, 0.9784650670341102, 0.978787972095405, 0.9793327042976292, 0.9796959044561017, 0.9800745493046091, 0.9806129767270894, 0.9803305025837922, 0.9805986287960012, 0.9809811188575775, 0.9813912422659374, 0.9811203533334613, 0.9813871415295269, 0.9813543097295153, 0.9827506434624461, 0.9841014614691427, 0.9853830800281108, 0.9865294779076527, 0.9875494712933579, 0.9885098182816692, 0.9893553070417376, 0.9901397763375639, 0.990843445762631, 0.991479101186368, 0.9920535440089077, 0.9925705425491934, 0.993033488294274, 0.9934407277001407, 0.9938190078713032, 0.9941547541429964, 0.9944639846110497, 0.9947446449734741, 0.9949878275349502, 0.9952161036049846, 0.9954121403033096, 0.9955956321553316, 0.995758421880975, 0.9959072855752304, 0.9960436158412368, 0.9961521954335837, 0.9962640347137547, 0.9963623371247322, 0.9964437504710825, 0.9965099636592684, 0.996576614352165, 0.9966507176228309, 0.9967150576252536, 0.9967635518627282, 0.9968095496176319, 0.9968462417146922, 0.9968745587196935, 0.99691651461243, 0.9969495690335399, 0.9969816709537154, 0.9970176215054027, 0.997038212296039, 0.9970473322429056, 0.9970649519597916, 0.9970831626461654, 0.9971042581462547, 0.9971020676257469, 0.9971259785102311, 0.997133380659208, 0.9971423955344637, 0.9971481559810174, 0.9971509874417391, 0.9971558886975651], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.06271999999999998, 0.12811466666666665, 0.1909165333333333, 0.24921154666666664, 0.3036370586666666, 0.35335335279999996, 0.3992313508533333, 0.441468215768, 0.48000139419119997, 0.51436125477208, 0.5461784626282052, 0.5756006163653847, 0.6019072213955129, 0.6257431659226284, 0.6475288493303655, 0.6672692977306622, 0.6853423679575961, 0.7019681311618365, 0.7161713180456528, 0.7300208529077542, 0.742912100950312, 0.7543142241886142, 0.7644428017697528, 0.7739051882594442, 0.7825280027668331, 0.7895418691568165, 0.7966410155744681, 0.8034169140170213, 0.8099818892819859, 0.8156637003537873, 0.8205639969850753, 0.8251342639532344, 0.8293808375579109, 0.8333627538021199, 0.8366398117552413, 0.8399224972463838, 0.8427169141884121, 0.8453385561029042, 0.8483513671592805, 0.8508628971100192, 0.852523274065684, 0.8539376133257822, 0.8556505186598706, 0.8566721334605503, 0.857911586781162, 0.8588804281030458, 0.8600057186260744, 0.8617651467634669, 0.8629886320871203, 0.8633297688784082, 0.8645167919905673, 0.8656784461248439, 0.8669906015123595, 0.8681582080277902, 0.8690490538916777, 0.8699974818358434, 0.8715844003189257, 0.8728659602870331, 0.8735126975916632, 0.8740547611658301, 0.8745559517159138, 0.8750603565443225, 0.8753943208898902, 0.8753882221342346, 0.8755693999208112, 0.8762791265953966, 0.876131213935857, 0.8759314258756047, 0.8759916166213776, 0.8765391216259064, 0.8774718761299825, 0.8776713551836508, 0.8786642196652857, 0.8788377976987571, 0.8785540179288814, 0.8785786161359933, 0.8793340878557273, 0.8794806790701545, 0.879532611163139, 0.8795660167134918, 0.8792360817088093, 0.8793391402045949, 0.8798985595174688, 0.8803353702323885, 0.8800884998758164, 0.8799196498882347, 0.8794076848994112, 0.8807735830761367, 0.8820828914351897, 0.8833146022916707, 0.8843831420625037, 0.8852648278562533, 0.8860983450706279, 0.8869151772302318, 0.8875036595072087, 0.8880066268898211, 0.8884726308675056, 0.8889320344474217, 0.8893188310026796, 0.889680281235745, 0.8900322531121705, 0.89029569446762, 0.8905194583541913, 0.8907875125187722, 0.8910687612668949, 0.8913618851402054, 0.8915323632928516, 0.8917924602968997, 0.8918932142672098, 0.8920372261738221, 0.8921268368897732, 0.8921541532007959, 0.8922187378807164, 0.8921968640926446, 0.8923105110167134, 0.8923061265817088, 0.8923155139235379, 0.8924039625311841, 0.8923368996113991, 0.8923565429835925, 0.8924275553518999, 0.8925314664833766, 0.892558319835039, 0.8926491545182018, 0.8926642390663817, 0.8926911484930768, 0.8927020336437692, 0.892818496946059, 0.8928299805847865, 0.8928269825263079, 0.8927709509403438, 0.8928671891796427, 0.8929538035950118, 0.8929250899021772, 0.8929525809119595, 0.8928973228207635, 0.8929275905386871, 0.892941498151485, 0.8928340150030032, 0.8928706135027028], "moving_var_accuracy_train": [0.03557439778546712, 0.07278199319584774, 0.10333276247534948, 0.12636028525054174, 0.14339119124793398, 0.15455289450277507, 0.16081766265678865, 0.16331249795630298, 0.16314805517170813, 0.15961363220022853, 0.15464308302143634, 0.14883874340578315, 0.14187464025803467, 0.1342134815464156, 0.12641730047839045, 0.11856646614970982, 0.11066999953684281, 0.10302200466668505, 0.09541247131531734, 0.08827528626278998, 0.0815506701382505, 0.0751282198335163, 0.06895762065796088, 0.06321856387636958, 0.05787867695507205, 0.052817291854667645, 0.04829442294800608, 0.044100837711892604, 0.040302553667665976, 0.03675816144668379, 0.03341979624651566, 0.03036725382685555, 0.02761732599221211, 0.025101210510351947, 0.022789243606285935, 0.02065940890613507, 0.01871485200460698, 0.016936712904308697, 0.015353937568426115, 0.013927340904832941, 0.0126075463949713, 0.011382953689205718, 0.010316974569314578, 0.009308879339915043, 0.008414793934814344, 0.00758347097960179, 0.006850978011482078, 0.006208172924920802, 0.005624479750324137, 0.005067161263040732, 0.004589131878948055, 0.004157865755414071, 0.003764406543697952, 0.003405692622006336, 0.0030767867032118756, 0.002782864279854175, 0.0025235396782513097, 0.0022815556728645013, 0.002061556072782558, 0.0018608636129670516, 0.0016807082720056974, 0.001519221541173481, 0.0013700314121757574, 0.00123413066266986, 0.0011140052503308372, 0.0010093225153704327, 0.0009098211879573888, 0.0008193627384701588, 0.0007387709519154372, 0.0006670878555353551, 0.0006051629638265392, 0.0005456637201050152, 0.0004955992089510665, 0.00044703727862721614, 0.0004034752581168545, 0.0003640661414126576, 0.00033033012582065217, 0.0002984843424346162, 0.00026992625548286585, 0.0002455427667380879, 0.00022170661483896236, 0.0002001829783461281, 0.0001814813683363574, 0.00016484704239348431, 0.0001490227654777782, 0.00013476107240404, 0.00012129466650746396, 0.0001267129309002023, 0.00013046402139512314, 0.00013220053443183376, 0.00013080853387261305, 0.00012709115904729524, 0.00012268244018419242, 0.00011684785735639085, 0.00011070160030559875, 0.00010408779621300786, 9.731553695124228e-05, 9.055384426342421e-05, 8.390404725299973e-05, 7.744251139369437e-05, 7.119085565754105e-05, 6.535963308283883e-05, 5.983819980515801e-05, 5.4714991165994384e-05, 4.995242420072093e-05, 4.548942160450363e-05, 4.14094691214064e-05, 3.7614395693077536e-05, 3.415597946159587e-05, 3.0978885968411496e-05, 2.8080440966776737e-05, 2.543967034296337e-05, 2.300180905953503e-05, 2.081420037488413e-05, 1.881975061343156e-05, 1.699742874876401e-05, 1.5337143550495247e-05, 1.3843410029218211e-05, 1.2508490678806776e-05, 1.1294898334131986e-05, 1.0186573720333006e-05, 9.18695848940529e-06, 8.280379430344935e-06, 7.459558162260674e-06, 6.729445018452476e-06, 6.066333869401445e-06, 5.468975281971841e-06, 4.933709733274239e-06, 4.444154585878029e-06, 4.000487688167888e-06, 3.6032330091593547e-06, 3.2458943701272564e-06, 2.9253101142307053e-06, 2.6328222882284918e-06, 2.3746856329769647e-06, 2.1377101959645525e-06, 1.924670588150975e-06, 1.7325021740363523e-06, 1.5593241111610889e-06, 1.403607900823035e-06], "duration": 123060.225653, "accuracy_train": [0.6287058823529412, 0.7358823529411764, 0.7784941176470588, 0.8038352941176471, 0.8300235294117647, 0.8456, 0.8577882352941176, 0.8699764705882352, 0.8849176470588235, 0.8803058823529412, 0.8906117647058823, 0.9037176470588235, 0.9055058823529412, 0.9078117647058823, 0.9154588235294118, 0.9211764705882353, 0.9232941176470588, 0.9294117647058824, 0.9269647058823529, 0.9347294117647059, 0.9404941176470588, 0.9416705882352941, 0.9389176470588235, 0.9423764705882353, 0.9448, 0.940635294117647, 0.9516, 0.9530117647058823, 0.9598117647058824, 0.9590823529411765, 0.9541882352941177, 0.9557882352941176, 0.9612, 0.962635294117647, 0.9625411764705882, 0.9610117647058823, 0.9611058823529411, 0.9602588235294117, 0.9663764705882353, 0.9695529411764706, 0.9667294117647058, 0.9611529411764705, 0.9714588235294118, 0.9621411764705883, 0.9677882352941176, 0.9602117647058823, 0.9676, 0.9740235294117647, 0.9748235294117648, 0.9640941176470588, 0.9751529411764706, 0.9766117647058824, 0.9765882352941176, 0.9764470588235294, 0.9752, 0.9773176470588235, 0.9807058823529412, 0.9783764705882353, 0.9782352941176471, 0.9774588235294117, 0.9785647058823529, 0.9798117647058824, 0.9776235294117647, 0.9761647058823529, 0.9790588235294118, 0.9822588235294117, 0.9784705882352941, 0.9772941176470589, 0.9789882352941176, 0.9804470588235294, 0.9832941176470589, 0.9800941176470588, 0.9841411764705882, 0.9811058823529412, 0.9816705882352941, 0.9816941176470588, 0.9842352941176471, 0.982964705882353, 0.9834823529411765, 0.9854588235294117, 0.9777882352941176, 0.9830117647058824, 0.9844235294117647, 0.9850823529411765, 0.9786823529411764, 0.9837882352941176, 0.9810588235294118, 0.9953176470588235, 0.9962588235294118, 0.9969176470588236, 0.9968470588235294, 0.9967294117647059, 0.9971529411764706, 0.996964705882353, 0.9972, 0.9971764705882353, 0.9972, 0.9972235294117647, 0.9972235294117647, 0.9972, 0.9971058823529412, 0.9972235294117647, 0.9971764705882353, 0.9972470588235294, 0.9972705882352941, 0.9971764705882353, 0.9972705882352941, 0.9971764705882353, 0.9972470588235294, 0.9972235294117647, 0.9972470588235294, 0.9972705882352941, 0.9971294117647059, 0.9972705882352941, 0.9972470588235294, 0.9971764705882353, 0.9971058823529412, 0.9971764705882353, 0.9973176470588235, 0.9972941176470588, 0.9972, 0.9972235294117647, 0.9971764705882353, 0.9971294117647059, 0.9972941176470588, 0.9972470588235294, 0.9972705882352941, 0.9973411764705883, 0.9972235294117647, 0.9971294117647059, 0.9972235294117647, 0.9972470588235294, 0.9972941176470588, 0.9970823529411764, 0.9973411764705883, 0.9972, 0.9972235294117647, 0.9972, 0.9971764705882353, 0.9972], "end": "2016-02-07 04:27:31.138000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0], "moving_var_accuracy_valid": [0.03540418559999999, 0.07035192889599998, 0.09881340611775997, 0.11951684272178557, 0.13422438565776632, 0.1430473362131587, 0.14768571894027777, 0.1489727218666282, 0.14743870223451286, 0.14332023218329917, 0.13809922140680822, 0.13208026744093543, 0.1251005779107424, 0.11770389038316564, 0.11020504535872847, 0.10269170855025864, 0.0953622605020819, 0.08831377847098497, 0.08129797528279599, 0.07489446429758545, 0.06890067635267969, 0.06318068444648459, 0.05778590875619041, 0.05281314870329402, 0.04820101020322415, 0.04382365807853058, 0.03989487318941084, 0.03631860106780391, 0.03307463106308988, 0.0300577147502817, 0.027268059438925036, 0.02472923955647475, 0.0224186160872467, 0.020319455391305082, 0.018384161831627617, 0.016642729864768675, 0.01504873577270486, 0.013605719252384773, 0.012326840601299101, 0.011150926585410306, 0.010060645591583399, 0.009072584232308957, 0.008191732211229999, 0.0073819522613157084, 0.006657583235989915, 0.00600027279395383, 0.005411642023409412, 0.004898338107404309, 0.004421976543698633, 0.0039808262581221025, 0.003595424847129093, 0.003248027325365321, 0.0029387203586776654, 0.0026571180675837853, 0.0023985487180042584, 0.0021667894862912995, 0.0019727753301097043, 0.0017902793606654334, 0.001615015846869691, 0.0014561587584486703, 0.0013128036103112416, 0.0011838130673584148, 0.0010664355502795677, 0.0009597923300049959, 0.0008641085255176352, 0.0007822310805394361, 0.0007042048758791613, 0.0006341436257124199, 0.0005707618694740682, 0.0005163835380965192, 0.0004725754629707343, 0.0004256760437093329, 0.0003919804582484291, 0.0003530535764269205, 0.00031847299740434486, 0.0002866311433100485, 0.0002631046666529043, 0.0002369876008449392, 0.0002133131132409811, 0.00019199184529403233, 0.00017377237473046277, 0.00015649072673939906, 0.00014365820377400533, 0.00013100961580262346, 0.00011845715897894781, 0.00010686803594580991, 9.854020569925803e-05, 0.00010547728559197178, 0.00011035815244454909, 0.00011297634190585326, 0.00011195470289193314, 0.00010775556115283914, 0.0001032327635574854, 9.8914420194405e-05, 9.213978068780689e-05, 8.520258831077447e-05, 7.863676684465755e-05, 7.26725550033494e-05, 6.675180367944835e-05, 6.125243975035061e-05, 5.624215359146592e-05, 5.124255036216956e-05, 4.656892781835416e-05, 4.255871235286106e-05, 3.901474884246062e-05, 3.588656840415556e-05, 3.255947676850672e-05, 2.9912383155289585e-05, 2.7012507102559734e-05, 2.4497911255518894e-05, 2.2120390853686467e-05, 1.9915067395948805e-05, 1.7961101284277717e-05, 1.616929731929137e-05, 1.4668608197515075e-05, 1.3201920387196361e-05, 1.1882521448156275e-05, 1.076467770909162e-05, 9.728686855073356e-06, 8.759290928206185e-06, 7.928746643459226e-06, 7.2330496883161685e-06, 6.516234641944066e-06, 5.938869634737173e-06, 5.34703056360756e-06, 4.818844562452366e-06, 4.33802648475748e-06, 4.026297143303831e-06, 3.6248542945992485e-06, 3.2624497603310963e-06, 2.9644606319288802e-06, 2.7513707570662737e-06, 2.543752193907177e-06, 2.2967972599222068e-06, 2.0739193344996294e-06, 1.8940085108332466e-06, 1.7128528724846742e-06, 1.5433083804798484e-06, 1.492951187299975e-06, 1.3557111201923875e-06], "accuracy_test": 0.8971, "start": "2016-02-05 18:16:30.913000", "learning_rate_per_epoch": [0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.008247657679021358, 0.0008247657679021358, 0.0008247657679021358, 0.0008247657679021358, 0.0008247657679021358, 0.0008247657679021358, 0.0008247657679021358, 0.0008247657679021358, 8.247657387983054e-05, 8.247657206084114e-06, 8.247657206084114e-07, 8.247657490301208e-08, 8.247657667936892e-09, 8.247657556914589e-10, 8.247657418136711e-11, 8.247657765081406e-12, 8.247657656661189e-13, 8.247657385610646e-14, 8.247657216204057e-15, 8.247657216204057e-16, 8.247657083855159e-17, 8.247657414727404e-18, 8.247657621522557e-19, 8.247657621522557e-20, 8.247657944639984e-21, 8.247658146588375e-22, 8.247658146588375e-23, 8.247658304360556e-24, 8.24765810714533e-25, 8.247657860626297e-26, 8.247657860626297e-27, 8.247658245812286e-28, 8.247658366182908e-29, 8.247658065256354e-30, 8.247657689098161e-31, 8.247657453999291e-32, 8.247657747872879e-33, 8.247657747872879e-34, 8.247657518284139e-35, 8.247657661777101e-36, 8.247658020509508e-37, 8.247658244717263e-38, 8.247658805236648e-39, 8.247664410430506e-40, 8.247622371476576e-41, 8.248042761015873e-42, 8.253647954873173e-43, 8.267660939516421e-44, 8.407790785948902e-45, 1.401298464324817e-45, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.6287058823529412, "accuracy_train_last": 0.9972, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.3728, 0.2833333333333333, 0.24386666666666668, 0.2261333333333333, 0.20653333333333335, 0.19920000000000004, 0.18786666666666663, 0.1784, 0.17320000000000002, 0.1764, 0.16746666666666665, 0.15959999999999996, 0.16133333333333333, 0.15973333333333328, 0.15639999999999998, 0.15506666666666669, 0.15200000000000002, 0.14839999999999998, 0.15600000000000003, 0.14533333333333331, 0.14106666666666667, 0.14306666666666668, 0.14439999999999997, 0.14093333333333335, 0.1398666666666667, 0.14733333333333332, 0.13946666666666663, 0.13560000000000005, 0.13093333333333335, 0.13319999999999999, 0.1353333333333333, 0.13373333333333337, 0.13239999999999996, 0.13080000000000003, 0.1338666666666667, 0.13053333333333328, 0.13213333333333332, 0.13106666666666666, 0.12453333333333338, 0.1265333333333334, 0.13253333333333328, 0.1333333333333333, 0.12893333333333334, 0.13413333333333333, 0.13093333333333335, 0.13239999999999996, 0.12986666666666669, 0.12239999999999995, 0.126, 0.13360000000000005, 0.12480000000000002, 0.12386666666666668, 0.12119999999999997, 0.1213333333333333, 0.12293333333333334, 0.12146666666666661, 0.11413333333333331, 0.11560000000000004, 0.1206666666666667, 0.12106666666666666, 0.12093333333333334, 0.12039999999999995, 0.12160000000000004, 0.1246666666666667, 0.12280000000000002, 0.11733333333333329, 0.12519999999999998, 0.12586666666666668, 0.12346666666666661, 0.11853333333333338, 0.11413333333333331, 0.12053333333333338, 0.11240000000000006, 0.11960000000000004, 0.124, 0.12119999999999997, 0.11386666666666667, 0.11919999999999997, 0.12, 0.12013333333333331, 0.12373333333333336, 0.11973333333333336, 0.11506666666666665, 0.11573333333333335, 0.12213333333333332, 0.12160000000000004, 0.12519999999999998, 0.10693333333333332, 0.1061333333333333, 0.10560000000000003, 0.10599999999999998, 0.1068, 0.10640000000000005, 0.10573333333333335, 0.10719999999999996, 0.10746666666666671, 0.10733333333333328, 0.10693333333333332, 0.10719999999999996, 0.10706666666666664, 0.1068, 0.10733333333333328, 0.10746666666666671, 0.1068, 0.10640000000000005, 0.10599999999999998, 0.10693333333333332, 0.10586666666666666, 0.10719999999999996, 0.10666666666666669, 0.10706666666666664, 0.10760000000000003, 0.10719999999999996, 0.10799999999999998, 0.10666666666666669, 0.10773333333333335, 0.10760000000000003, 0.1068, 0.10826666666666662, 0.10746666666666671, 0.10693333333333332, 0.10653333333333337, 0.10719999999999996, 0.10653333333333337, 0.10719999999999996, 0.10706666666666664, 0.10719999999999996, 0.1061333333333333, 0.10706666666666664, 0.10719999999999996, 0.10773333333333335, 0.10626666666666662, 0.10626666666666662, 0.10733333333333328, 0.1068, 0.10760000000000003, 0.1068, 0.10693333333333332, 0.1081333333333333, 0.1068], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.004141036188638448, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.008247657474640296, "optimization": "nesterov_momentum", "nb_data_augmentation": 2, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 8.048050830658738e-06, "rotation_range": [0, 0], "momentum": 0.8205199221014459}, "accuracy_valid_max": 0.8944, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8932, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.6272, 0.7166666666666667, 0.7561333333333333, 0.7738666666666667, 0.7934666666666667, 0.8008, 0.8121333333333334, 0.8216, 0.8268, 0.8236, 0.8325333333333333, 0.8404, 0.8386666666666667, 0.8402666666666667, 0.8436, 0.8449333333333333, 0.848, 0.8516, 0.844, 0.8546666666666667, 0.8589333333333333, 0.8569333333333333, 0.8556, 0.8590666666666666, 0.8601333333333333, 0.8526666666666667, 0.8605333333333334, 0.8644, 0.8690666666666667, 0.8668, 0.8646666666666667, 0.8662666666666666, 0.8676, 0.8692, 0.8661333333333333, 0.8694666666666667, 0.8678666666666667, 0.8689333333333333, 0.8754666666666666, 0.8734666666666666, 0.8674666666666667, 0.8666666666666667, 0.8710666666666667, 0.8658666666666667, 0.8690666666666667, 0.8676, 0.8701333333333333, 0.8776, 0.874, 0.8664, 0.8752, 0.8761333333333333, 0.8788, 0.8786666666666667, 0.8770666666666667, 0.8785333333333334, 0.8858666666666667, 0.8844, 0.8793333333333333, 0.8789333333333333, 0.8790666666666667, 0.8796, 0.8784, 0.8753333333333333, 0.8772, 0.8826666666666667, 0.8748, 0.8741333333333333, 0.8765333333333334, 0.8814666666666666, 0.8858666666666667, 0.8794666666666666, 0.8876, 0.8804, 0.876, 0.8788, 0.8861333333333333, 0.8808, 0.88, 0.8798666666666667, 0.8762666666666666, 0.8802666666666666, 0.8849333333333333, 0.8842666666666666, 0.8778666666666667, 0.8784, 0.8748, 0.8930666666666667, 0.8938666666666667, 0.8944, 0.894, 0.8932, 0.8936, 0.8942666666666667, 0.8928, 0.8925333333333333, 0.8926666666666667, 0.8930666666666667, 0.8928, 0.8929333333333334, 0.8932, 0.8926666666666667, 0.8925333333333333, 0.8932, 0.8936, 0.894, 0.8930666666666667, 0.8941333333333333, 0.8928, 0.8933333333333333, 0.8929333333333334, 0.8924, 0.8928, 0.892, 0.8933333333333333, 0.8922666666666667, 0.8924, 0.8932, 0.8917333333333334, 0.8925333333333333, 0.8930666666666667, 0.8934666666666666, 0.8928, 0.8934666666666666, 0.8928, 0.8929333333333334, 0.8928, 0.8938666666666667, 0.8929333333333334, 0.8928, 0.8922666666666667, 0.8937333333333334, 0.8937333333333334, 0.8926666666666667, 0.8932, 0.8924, 0.8932, 0.8930666666666667, 0.8918666666666667, 0.8932], "seed": 72121919, "model": "residualv5", "loss_std": [0.31133463978767395, 0.26167982816696167, 0.2544451951980591, 0.2452237606048584, 0.23889052867889404, 0.22868703305721283, 0.2246847152709961, 0.2154691368341446, 0.20685912668704987, 0.20282262563705444, 0.19605453312397003, 0.18870924413204193, 0.18134942650794983, 0.1786039024591446, 0.17155231535434723, 0.16849385201931, 0.1619216650724411, 0.15595410764217377, 0.15397478640079498, 0.1483277827501297, 0.14390070736408234, 0.14290973544120789, 0.14049385488033295, 0.1323447823524475, 0.1279786229133606, 0.1288052648305893, 0.12597279250621796, 0.11916720867156982, 0.1184224933385849, 0.11559537053108215, 0.11354921758174896, 0.11009705066680908, 0.10716898739337921, 0.10381732881069183, 0.10449202358722687, 0.10426326841115952, 0.10137724131345749, 0.09952171891927719, 0.09636630862951279, 0.09612271189689636, 0.09198316186666489, 0.09017228335142136, 0.09079951047897339, 0.08961478620767593, 0.0866827443242073, 0.08442142605781555, 0.0861438512802124, 0.08542180061340332, 0.08221885561943054, 0.07917709648609161, 0.0808965414762497, 0.07866410166025162, 0.07875503599643707, 0.0740305557847023, 0.07564648985862732, 0.0761980265378952, 0.07414375245571136, 0.07133053243160248, 0.07301147282123566, 0.06763321906328201, 0.06857115775346756, 0.06955588608980179, 0.0669204443693161, 0.06691927462816238, 0.06549388915300369, 0.06499672681093216, 0.06499909609556198, 0.06637644022703171, 0.06612817943096161, 0.06354551017284393, 0.06254380196332932, 0.06208261847496033, 0.061543770134449005, 0.06201927736401558, 0.05835244804620743, 0.05858256295323372, 0.05767298862338066, 0.055036380887031555, 0.05763445049524307, 0.056708820164203644, 0.056958772242069244, 0.05540270730853081, 0.05539821460843086, 0.05654625594615936, 0.05424479395151138, 0.05284292250871658, 0.0531504862010479, 0.045167334377765656, 0.03123062662780285, 0.028089530766010284, 0.02786489948630333, 0.024943247437477112, 0.024314455687999725, 0.02372884750366211, 0.023146852850914, 0.023421861231327057, 0.024630052968859673, 0.023473145440220833, 0.023338841274380684, 0.022634604945778847, 0.022381247952580452, 0.024610452353954315, 0.023832473903894424, 0.022659601643681526, 0.02225567027926445, 0.024638863280415535, 0.02379811368882656, 0.02322911098599434, 0.02406364306807518, 0.023205075412988663, 0.023874320089817047, 0.024293597787618637, 0.02250581979751587, 0.023655373603105545, 0.02483503706753254, 0.023077821359038353, 0.02123144455254078, 0.023741107434034348, 0.023069312795996666, 0.023562796413898468, 0.023385409265756607, 0.02440650761127472, 0.02428618259727955, 0.022933321073651314, 0.023339707404375076, 0.024073462933301926, 0.02296905778348446, 0.023298464715480804, 0.023549191653728485, 0.022798191756010056, 0.025357136502861977, 0.022857850417494774, 0.022409548982977867, 0.023661980405449867, 0.023659564554691315, 0.023992815986275673, 0.024290595203638077, 0.02469034679234028, 0.025236332789063454, 0.024172576144337654]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:43 2016", "state": "available"}], "summary": "5284573ebdc4de826b0db839f0fd300b"}