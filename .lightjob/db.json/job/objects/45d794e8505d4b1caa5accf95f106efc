{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 16, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 4, "nbg3": 4, "nbg2": 2, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.01225368630425532, 0.005365229388688539, 0.009110464797995366, 0.011095081479119466, 0.01511970058387228, 0.017220073637139944, 0.013752998839704796, 0.01716104339658676, 0.019939973810788988, 0.02027627192921133, 0.021055566942772795, 0.01846831038517299, 0.016859309106679377, 0.01721270916398111, 0.01639286149142334, 0.021477091614622464, 0.022218275682035934, 0.021297355767962697, 0.021545073067119266, 0.02164871825961551, 0.026569805331390434, 0.024780371156963802, 0.02584409542100129, 0.025564926658362554, 0.0261124532097192, 0.025038339952844466, 0.026940730751752184, 0.02732163253368218, 0.02506995702102135, 0.02548372342800264, 0.025379409936198437, 0.025920987175502078, 0.024861900104599703, 0.023667637042969536, 0.025491658393579513, 0.024086920009430645, 0.025635107020320097, 0.02456228279001438, 0.023884537147891044, 0.020997868107354822, 0.023273821362692702, 0.02351414920778171, 0.022535464166263817, 0.02274101717947303, 0.020162929338447554, 0.022175471972041352, 0.021363711222906374, 0.02176607235852258, 0.02053366634210165, 0.01991615220487703, 0.020274912962976194, 0.022648303044826443, 0.021170409073905674, 0.020404312674799963, 0.022426094545667456, 0.023225091747090115, 0.021449072517979482, 0.022036758879771294, 0.021850957649933337, 0.021399379935394987, 0.022538800936021963, 0.021377085566161243, 0.020695702634865223, 0.019925556505714603, 0.019755057980585466, 0.01962868178063636, 0.021936305286756295, 0.02274168036682069, 0.019606540534457952, 0.022966276336768728, 0.02177187423095416, 0.021427971516145526, 0.020722562129743466, 0.020746946578207224, 0.021289618463424892, 0.020963125020612396, 0.02166703296940096, 0.019304229394684597, 0.021594059968166327, 0.01985672275191984, 0.02021869503956154, 0.021334030101835152, 0.019791979438177173, 0.02079951571398117, 0.019880722138871546, 0.021495798234654136, 0.021117125750456336, 0.02034989474572277, 0.021337341261723687, 0.0221455817284857, 0.022325049664247153, 0.022102475912445355, 0.021100523851039513, 0.02111715018010684, 0.02098527108550621, 0.019405521192930507, 0.020998134083208754, 0.019130872930533095, 0.02262498584409242, 0.021856351965529798, 0.021409137939329224, 0.020680068992372926, 0.021250048510520748, 0.021480771981376094, 0.02225902188509269, 0.02104104922938122, 0.02143009735334737, 0.02037539715303729, 0.02147229443886219, 0.023146645765031837, 0.020600358974697565, 0.02133040701159099, 0.02077644466843324, 0.0220414947834801, 0.020228988208902225, 0.02108628872132242, 0.021254306152367383, 0.020776187514993917, 0.021623345752361092, 0.021116848366949634, 0.02052383862472033, 0.021190228852057992, 0.01879057513644192, 0.020827795961290002, 0.021423478053646545, 0.020943898174241162, 0.021242308230530055, 0.022374788659874512, 0.022012254535626153, 0.02073797969632833, 0.02203828534675214, 0.021032278080941744, 0.020750405573086276, 0.021585956497658425, 0.021617952478828, 0.021989107857077736, 0.018752012356162008, 0.020495755357112254, 0.020871349523932364, 0.020096422996658585, 0.019122590408425868, 0.02099172273549801, 0.02151663463000991, 0.022340954613573577, 0.021751662015162453, 0.01836073324179353, 0.021042093505493003, 0.02061708288010747, 0.02183146525247802, 0.021614498470345713, 0.021408996441656798, 0.021355360615250597, 0.022247521420837645, 0.02213023307180502, 0.020576047217711085, 0.01979666501298979, 0.020877651494919203, 0.021412374601093055, 0.019308855660677266, 0.021399060067181796, 0.021306978830513596, 0.021201477211760366, 0.018373713896066443, 0.019689320309154944, 0.02074338106238762, 0.019820890118973047, 0.02170711235192365, 0.022297433359774323, 0.02165278887344614, 0.021289618463424892], "moving_avg_accuracy_train": [0.015205842359957547, 0.03272887575269933, 0.052726221658418684, 0.07422700310668719, 0.09598162188862644, 0.11743903039582322, 0.13821303640939703, 0.15818113773223547, 0.17725894741439213, 0.19530544908306735, 0.21217036836818992, 0.22793691417833473, 0.24262889335504723, 0.2563678936986215, 0.2691375699006955, 0.28107209280519035, 0.29204575039782565, 0.3022474188692557, 0.31179636610308503, 0.32062064439449306, 0.32877404734960813, 0.33630742250921175, 0.343259485115941, 0.3498138163143414, 0.3558056842964639, 0.361426302161345, 0.3666148863289776, 0.37139153287626625, 0.3758067722093022, 0.3798431224316167, 0.38358980597218517, 0.3870315395741638, 0.39028023448856364, 0.3931830614746002, 0.39596079947396556, 0.39844674068289976, 0.4008213075995212, 0.4030118962470996, 0.40502756780848226, 0.4069764587470416, 0.4087444835822396, 0.4104565415743755, 0.41198809317205975, 0.413408342288547, 0.41476554945527183, 0.41593130443153314, 0.417052491425626, 0.4181197605381852, 0.41910587937639326, 0.42002601051175137, 0.4209353284978132, 0.4216492271841153, 0.42241489679105443, 0.42303893131945175, 0.4236981825961906, 0.42429383389406505, 0.42491133631930417, 0.4253205680782006, 0.4257144532981122, 0.42618295438533677, 0.42660220811739186, 0.4270330148988605, 0.42737184077954476, 0.4277000716090746, 0.42804892172945175, 0.4284210876556668, 0.42869314982494694, 0.4289264881797077, 0.4291271921037543, 0.42919847154371105, 0.4294277086051484, 0.4296038310747369, 0.42980412187830047, 0.4299031475884494, 0.4300410628037647, 0.4301302011189493, 0.4302174731466816, 0.4302797779787927, 0.43043347252887393, 0.4305438958382327, 0.4307175734321042, 0.43082745238803555, 0.4308613474281633, 0.43094987353806014, 0.4309575034726908, 0.4310271133828968, 0.43111308588781494, 0.4311717878541276, 0.4312269087237997, 0.43124167632318056, 0.4312293544768998, 0.43133681135571406, 0.43137768292639955, 0.4313332313269582, 0.43132584906843185, 0.4313912125512158, 0.4315127826547598, 0.43145485713130116, 0.43152130674947414, 0.431527596934392, 0.4314983448198565, 0.43155339812510785, 0.4315610934212626, 0.4316261839568587, 0.4316452739579521, 0.4316461789172695, 0.43172132604492236, 0.4317074701050204, 0.43175094752581217, 0.43177372906522066, 0.4317965936483165, 0.4318241111707126, 0.43188142902420246, 0.4318749945185615, 0.4319180315884846, 0.4319311162168731, 0.43197079416813705, 0.4319135344207124, 0.4318969139289918, 0.43194938480191947, 0.4319546477625267, 0.432059329777064, 0.4320862224211276, 0.43208241586861434, 0.43202558364637084, 0.43197672374634244, 0.43196777126373476, 0.43194801618770273, 0.43196972810021705, 0.43190087711789926, 0.43197148076477493, 0.43192810325054365, 0.43199605638179234, 0.4320129642748977, 0.4319863287001211, 0.43195777848284067, 0.4319714305729564, 0.4320069689421558, 0.43203205012564416, 0.4320639598348405, 0.4321205082611941, 0.4321412109592072, 0.43209252221839906, 0.43203696846116785, 0.4320777590296874, 0.43204700517706013, 0.43204025304898125, 0.43209455790512036, 0.4321643586149313, 0.4321202945061606, 0.4321340431332485, 0.43217664383215143, 0.43215227754094443, 0.43208842310264917, 0.43205195254510664, 0.4320469947802139, 0.43203097914540034, 0.43204435871332614, 0.4320098612994502, 0.4320347253448466, 0.43201525030713195, 0.43205112909817034, 0.4319881970553706, 0.43198721759300446, 0.4319655539328642, 0.4319506348387195, 0.4319535197444747, 0.4318794222977589, 0.4319428347825917, 0.43200691751418857], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 686878019, "moving_var_accuracy_train": [0.0020809587768829143, 0.004636373192742916, 0.007771780462925542, 0.011155154842608853, 0.014299010303475503, 0.017012892691730442, 0.019195637355225413, 0.020864599253655153, 0.022053804728706952, 0.022779510258133736, 0.023061388754763656, 0.02299250558033785, 0.022635943291464646, 0.022071190136284786, 0.02133165279540864, 0.020480383048488992, 0.019516135192283707, 0.01850118802946407, 0.01747171076598777, 0.016425350675666923, 0.01538111742583655, 0.014353771354910888, 0.013353374789810748, 0.012404670627954592, 0.0114873259023958, 0.010622915418803407, 0.009802916527904533, 0.009027972045253602, 0.008300623886040134, 0.007617190605490727, 0.006981810282920156, 0.006390239026311037, 0.005846201291501558, 0.005337418802949162, 0.004873119378192312, 0.004441426573621576, 0.004048031028633035, 0.003686416033375823, 0.003354340816628543, 0.0030530903179792758, 0.002775914492542241, 0.0025247033264039465, 0.002293343846430875, 0.0020821634297637334, 0.0018905251884280437, 0.0017137035315673412, 0.0015536467208921148, 0.0014085336190305098, 0.001276432130395078, 0.0011564086891118687, 0.001048209552998662, 0.0009479754597075307, 0.0008584541632596908, 0.0007761135187674104, 0.0007024136771036054, 0.0006353655136111809, 0.0005752607454566485, 0.0005192419066033891, 0.0004687140260412333, 0.0004238180628556856, 0.0003830182197966962, 0.00034638674816366087, 0.0003127813001440881, 0.0002824727894267639, 0.0002553207781424719, 0.00023103526761794415, 0.00020859790127173043, 0.00018822813223477947, 0.0001697678575974508, 0.00015283679866475073, 0.0001380260654713034, 0.00012450263104281872, 0.00011241341559246621, 0.00010126032885465406, 9.130548142872774e-05, 8.224644403896056e-05, 7.409034729648496e-05, 6.6716249595776e-05, 6.025722276872045e-05, 5.4341240257096194e-05, 4.917859139090336e-05, 4.436939271642206e-05, 3.994279330848723e-05, 3.6019045826839734e-05, 3.241766518727798e-05, 2.921950852494018e-05, 2.6364079116863272e-05, 2.3758684492817677e-05, 2.1410160835996638e-05, 1.927110749032023e-05, 1.734536319235011e-05, 1.5714749700355634e-05, 1.4158309097932785e-05, 1.2760261690375566e-05, 1.1484726001006565e-05, 1.0374704864840775e-05, 9.470247989037953e-06, 8.553421486545765e-06, 7.737819303689161e-06, 6.964393471156951e-06, 6.275655299884462e-06, 5.67536756766789e-06, 5.108363769147289e-06, 4.635658392650301e-06, 4.175372406660978e-06, 3.7578425365571753e-06, 3.4328821000517936e-06, 3.0913217736817205e-06, 2.7992021713819057e-06, 2.5239529410840834e-06, 2.27626274941701e-06, 2.055451400824692e-06, 1.8794742877003632e-06, 1.6918994846959232e-06, 1.5393792407144252e-06, 1.386982184143577e-06, 1.2624530240777746e-06, 1.1657158297461954e-06, 1.0516304134768742e-06, 9.712461046813335e-07, 8.74370783002381e-07, 8.855586222104143e-07, 8.035116887319669e-07, 7.232909284370972e-07, 6.800309489595889e-07, 6.335134625406691e-07, 5.708834387901671e-07, 5.173074621724299e-07, 4.6981938026045986e-07, 4.65501562129487e-07, 4.638152804857955e-07, 4.3436823110517987e-07, 4.3249006041318906e-07, 3.918139460152361e-07, 3.5901763600682576e-07, 3.3045190656700616e-07, 2.9908413199105317e-07, 2.805424999601091e-07, 2.5814984185069533e-07, 2.4149892353460844e-07, 2.4612855188868683e-07, 2.2537311204501763e-07, 2.2417114217384948e-07, 2.2953000743898305e-07, 2.215518410164637e-07, 2.0790885197760737e-07, 1.8752828788218473e-07, 1.953166156966218e-07, 2.196342059379898e-07, 2.1514559648001535e-07, 1.9533225955323153e-07, 1.921324095210559e-07, 1.782626138936199e-07, 1.9713285611427669e-07, 1.8939048461000352e-07, 1.706726510435866e-07, 1.5591389096556126e-07, 1.4193361740994147e-07, 1.3845089974612634e-07, 1.3016979655279703e-07, 1.2056631074339654e-07, 1.2009526848643782e-07, 1.4372971973626392e-07, 1.293653818813778e-07, 1.2065267122928638e-07, 1.1059061843723411e-07, 9.96064607244591e-08, 1.3905969914027111e-07, 1.6134401832030387e-07, 1.8216898488852323e-07], "duration": 137775.9379, "accuracy_train": [0.1520584235995755, 0.1904361762873754, 0.23270233480989294, 0.26773403614110375, 0.29177319092607973, 0.3105557069605943, 0.32517909053156147, 0.33789404963778147, 0.3489592345538021, 0.3577239641011443, 0.3639546419342931, 0.36983582646963825, 0.3748567059454596, 0.38001889679078993, 0.3840646557193614, 0.38848279894564414, 0.390808668731543, 0.39406243511212624, 0.3977368912075489, 0.40003914901716503, 0.40215467394564414, 0.40410779894564414, 0.40582804857650423, 0.40880279709994466, 0.40973249613556667, 0.41201186294527503, 0.41331214383767073, 0.41438135180186414, 0.41554392620662606, 0.4161702744324474, 0.4173099578373016, 0.4180071419919712, 0.4195184887181617, 0.41930850434892947, 0.420960441468254, 0.4208202115633075, 0.4221924098491141, 0.42272719407530457, 0.4231686118609265, 0.4245164771940753, 0.4246567070990218, 0.4258650635035991, 0.42577205755121816, 0.4261905843369324, 0.42698041395579545, 0.4264230992178848, 0.42714317437246213, 0.42772518255121816, 0.4279809489202658, 0.42830719072997414, 0.4291191903723699, 0.42807431536083423, 0.4293059232535068, 0.4286552420750277, 0.4296314440868401, 0.42965469557493535, 0.43046885814645625, 0.4290036539082687, 0.42925942027731634, 0.43039946417035807, 0.43037549170588774, 0.43091027593207826, 0.43042127370570327, 0.43065414907484306, 0.4311885728128461, 0.43177058099160204, 0.43114170934846807, 0.4310265333725544, 0.4309335274201735, 0.4298399865033223, 0.4314908421580842, 0.4311889333010336, 0.43160673911037284, 0.43079437897978956, 0.43128229974160204, 0.430932445955611, 0.43100292139627167, 0.4308405214677925, 0.431816723479605, 0.43153770562246213, 0.43228067177694723, 0.4318163629914175, 0.4311664027893134, 0.4317466085271318, 0.4310261728843669, 0.4316536025747509, 0.43188683843207826, 0.43170010555094135, 0.43172299655084906, 0.43137458471760803, 0.43111845786037284, 0.43230392326504247, 0.43174552706256925, 0.4309331669319859, 0.43125940874169433, 0.43197948389627167, 0.43260691358665565, 0.4309335274201735, 0.43211935331303064, 0.4315842085986526, 0.4312350757890366, 0.4320488778723699, 0.43163035108665565, 0.43221199877722405, 0.4318170839677925, 0.4316543235511259, 0.4323976501937985, 0.43158276664590256, 0.4321422443129383, 0.4319787629198967, 0.43200237489617943, 0.4320717688722776, 0.432397289705611, 0.4318170839677925, 0.4323053652177925, 0.4320488778723699, 0.4323278957295128, 0.4313981966938907, 0.4317473295035068, 0.4324216226582687, 0.4320020144079919, 0.4330014679078996, 0.4323282562177002, 0.43204815689599485, 0.43151409364617943, 0.43153698464608714, 0.4318871989202658, 0.4317702205034145, 0.4321651353128461, 0.4312812182770395, 0.43260691358665565, 0.43153770562246213, 0.43260763456303064, 0.4321651353128461, 0.4317466085271318, 0.43170082652731634, 0.4320942993839978, 0.4323268142649502, 0.4322577807770395, 0.43235114721760803, 0.4326294440983758, 0.4323275352413252, 0.4316543235511259, 0.43153698464608714, 0.43244487414636396, 0.4317702205034145, 0.43197948389627167, 0.43258330161037284, 0.43279256500323, 0.43172371752722405, 0.4322577807770395, 0.4325600501222776, 0.4319329809200812, 0.4315137331579919, 0.43172371752722405, 0.43200237489617943, 0.43188683843207826, 0.4321647748246586, 0.4316993845745663, 0.4322585017534145, 0.4318399749677002, 0.43237403821751574, 0.4314218086701735, 0.43197840243170915, 0.43177058099160204, 0.4318163629914175, 0.43197948389627167, 0.43121254527731634, 0.43251354714608714, 0.4325836620985604], "end": "2016-01-25 03:09:00.464000", "learning_rate_per_epoch": [0.0003212822484783828, 0.0003003370657097548, 0.0002807573473546654, 0.0002624540647957474, 0.0002453440392855555, 0.00022934944718144834, 0.00021439758711494505, 0.0002004204725380987, 0.00018735455523710698, 0.00017514044884592295, 0.00016372260870411992, 0.00015304912813007832, 0.00014307147648651153, 0.00013374429545365274, 0.00012502516619861126, 0.00011687446385622025, 0.00010925512469839305, 0.00010213250789092854, 9.54742354224436e-05, 8.925003203330562e-05, 8.343160152435303e-05, 7.799248851370066e-05, 7.290796202141792e-05, 6.815490633016452e-05, 6.371171912178397e-05, 5.955819142400287e-05, 5.567544576479122e-05, 5.20458233950194e-05, 4.865282608079724e-05, 4.5481025154003873e-05, 4.251600330462679e-05, 3.9744278183206916e-05, 3.715324783115648e-05, 3.473113611107692e-05, 3.2466927223140374e-05, 3.035032750631217e-05, 2.837171450664755e-05, 2.6522091502556577e-05, 2.479304930602666e-05, 2.3176728063845076e-05, 2.16657790588215e-05, 2.0253331967978738e-05, 1.8932965758722275e-05, 1.769867776602041e-05, 1.6544856407563202e-05, 1.5466255717910826e-05, 1.4457971701631323e-05, 1.3515420505427755e-05, 1.2634316590265371e-05, 1.181065363198286e-05, 1.1040687240893021e-05, 1.0320917681383435e-05, 9.648071682022419e-06, 9.01908970263321e-06, 8.431112291873433e-06, 7.88146735430928e-06, 7.367654689005576e-06, 6.88733871356817e-06, 6.438335731218103e-06, 6.018604381097248e-06, 5.6262365433212835e-06, 5.259448244032683e-06, 4.916571469948394e-06, 4.59604780189693e-06, 4.296419774618698e-06, 4.016325419797795e-06, 3.754491217478062e-06, 3.509726639094879e-06, 3.280918690506951e-06, 3.0670273645228008e-06, 2.8670801839325577e-06, 2.6801681087817997e-06, 2.50544121627172e-06, 2.342105290153995e-06, 2.1894175006309524e-06, 2.046683903245139e-06, 1.9132555735268397e-06, 1.7885256511362968e-06, 1.6719271798137925e-06, 1.5629300378350308e-06, 1.461038777961221e-06, 1.3657900126418099e-06, 1.2767507087119156e-06, 1.1935161410292494e-06, 1.115707846111036e-06, 1.0429720305182855e-06, 9.749780929269036e-07, 9.114168619817065e-07, 8.519993457412056e-07, 7.964553674355557e-07, 7.445324285981769e-07, 6.959945153539593e-07, 6.506209047074663e-07, 6.082053118916519e-07, 5.685548671863216e-07, 5.314893769536866e-07, 4.968402436134056e-07, 4.6444998247352487e-07, 4.341713406574854e-07, 4.0586661498309695e-07, 3.7940716879347747e-07, 3.5467266457089863e-07, 3.3155066603285377e-07, 3.099360412761598e-07, 2.8973053645131586e-07, 2.7084229259344283e-07, 2.5318541929664207e-07, 2.3667962523177266e-07, 2.2124989129679307e-07, 2.0682605850197433e-07, 1.933425579636605e-07, 1.807380698437555e-07, 1.6895531018690235e-07, 1.5794068985997e-07, 1.4764414402179682e-07, 1.380188479060962e-07, 1.2902106050205475e-07, 1.206098545480927e-07, 1.1274699573959879e-07, 1.0539673667153693e-07, 9.852566051904432e-08, 9.21025247180296e-08, 8.609813306748038e-08, 8.048517941006139e-08, 7.52381481561315e-08, 7.033318638605124e-08, 6.57479901633451e-08, 6.146171216414587e-08, 5.745486930663901e-08, 5.3709243275079643e-08, 5.020780236009159e-08, 4.693463040439383e-08, 4.387484509038586e-08, 4.1014533991301505e-08, 3.834069417507635e-08, 3.584116825550154e-08, 3.350459465423228e-08, 3.1320347204655263e-08, 2.927849607203825e-08, 2.7369758015538537e-08, 2.558545553199565e-08, 2.3917475999724047e-08, 2.2358236151376332e-08, 2.0900646546806456e-08, 1.9538081375003458e-08, 1.8264344703311508e-08, 1.7073645608434163e-08, 1.596057153108177e-08, 1.4920061630618875e-08, 1.3947385468782159e-08, 1.3038120805219933e-08, 1.2188133169388493e-08, 1.1393558096983725e-08, 1.0650783366372707e-08, 9.95643212320374e-09, 9.30734778137321e-09, 8.700578923992452e-09, 8.13336686888988e-09, 7.603132345934682e-09, 7.107465282985004e-09, 6.644111927300855e-09, 6.210965519670708e-09, 5.806056968538087e-09, 5.427545524128163e-09, 5.073710340752768e-09, 4.742942483204615e-09, 4.4337382654191515e-09, 4.144691700957992e-09, 3.87448872984919e-09, 3.6219012233829062e-09], "accuracy_valid": [0.14425328266189757, 0.18769119446536145, 0.2282611892884036, 0.26134371470256024, 0.2849753506212349, 0.3044874811746988, 0.3209875635353916, 0.33437558829066266, 0.34739740210843373, 0.3536538733057229, 0.36406073512801207, 0.36808905544051207, 0.37309393825301207, 0.37763112998870485, 0.3811814641378012, 0.38611575207078314, 0.3910294498305723, 0.39112063488328314, 0.39604462772966864, 0.3983742587537651, 0.4020981386483434, 0.4019554781626506, 0.40674710560993976, 0.40601468373493976, 0.40823253953313254, 0.41066365069653615, 0.4103180299322289, 0.4121593797063253, 0.41323742234563254, 0.4131256471197289, 0.4152008424322289, 0.4158214890813253, 0.4162994752447289, 0.41592296922063254, 0.4176525437688253, 0.41726574265813254, 0.4177746140813253, 0.4191173875188253, 0.4193512330572289, 0.41870999623493976, 0.4211822877447289, 0.4206940064947289, 0.41958507859563254, 0.4214264283697289, 0.42066312123493976, 0.42190441453313254, 0.42288097703313254, 0.42349132859563254, 0.42335896319653615, 0.42298245717243976, 0.42360310382153615, 0.4233795533697289, 0.42372517413403615, 0.42311482257153615, 0.4247223268072289, 0.4232574830572289, 0.42458996140813254, 0.42483410203313254, 0.4250885377447289, 0.42532238328313254, 0.4254547486822289, 0.42532238328313254, 0.42629894578313254, 0.42286038685993976, 0.42456937123493976, 0.42542386342243976, 0.4260651002447289, 0.4275402390813253, 0.42665486163403615, 0.4254650437688253, 0.42654308640813254, 0.42555622882153615, 0.42531208819653615, 0.42642101609563254, 0.42568859422063254, 0.42715343797063254, 0.42568859422063254, 0.42517972279743976, 0.42666515672063254, 0.42604451007153615, 0.42702107257153615, 0.42593273484563254, 0.42542386342243976, 0.42493558217243976, 0.42604451007153615, 0.42568859422063254, 0.42642101609563254, 0.42616658038403615, 0.42581066453313254, 0.42593273484563254, 0.4263092408697289, 0.42593273484563254, 0.42617687547063254, 0.4278961549322289, 0.42519001788403615, 0.42591214467243976, 0.42665486163403615, 0.42530179310993976, 0.4280285203313253, 0.42544445359563254, 0.4276520143072289, 0.42690929734563254, 0.42593273484563254, 0.42678722703313254, 0.4271637330572289, 0.42506794757153615, 0.42727550828313254, 0.42433552569653615, 0.4282623658697289, 0.4269298875188253, 0.42813000047063254, 0.42678722703313254, 0.42567829913403615, 0.4269195924322289, 0.42665486163403615, 0.42751964890813254, 0.42690929734563254, 0.42629894578313254, 0.4271637330572289, 0.4289947877447289, 0.42690929734563254, 0.42678722703313254, 0.42615628529743976, 0.42592243975903615, 0.42703136765813254, 0.42666515672063254, 0.42544445359563254, 0.4253326783697289, 0.4260651002447289, 0.42666515672063254, 0.4256988893072289, 0.42642101609563254, 0.42628865069653615, 0.42678722703313254, 0.4272858033697289, 0.4259430299322289, 0.42713284779743976, 0.42506794757153615, 0.42519001788403615, 0.42616658038403615, 0.42775349444653615, 0.42727550828313254, 0.42666515672063254, 0.4286388718938253, 0.42629894578313254, 0.4259018495858434, 0.42481351185993976, 0.42555622882153615, 0.42568859422063254, 0.42507824265813254, 0.42617687547063254, 0.42666515672063254, 0.4270416627447289, 0.4272858033697289, 0.42690929734563254, 0.42714314288403615, 0.42715343797063254, 0.4274078736822289, 0.42603421498493976, 0.4280182252447289, 0.42654308640813254, 0.42678722703313254, 0.4259018495858434, 0.42542386342243976, 0.42690929734563254, 0.42554593373493976, 0.42678722703313254, 0.4260651002447289, 0.42629894578313254, 0.42568859422063254], "accuracy_test": 0.4301638233418368, "start": "2016-01-23 12:52:44.526000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0], "accuracy_train_last": 0.4325836620985604, "batch_size_eval": 1024, "accuracy_train_std": [0.012822283334426797, 0.014170999730387178, 0.010470059176001282, 0.0140488600092125, 0.01405824771104765, 0.0147927044343265, 0.014466202464592877, 0.013421808652671376, 0.013544238383398115, 0.013410865583068017, 0.014111334845281149, 0.013534076379638832, 0.013838835097137651, 0.0138427700215257, 0.013314897273996013, 0.012677102685202135, 0.012187138875590474, 0.012206586152749102, 0.01182123338913599, 0.011558901683822077, 0.012507320409014935, 0.011181198226012837, 0.011611701533886696, 0.010967205885588005, 0.011489109779335831, 0.011438966930422116, 0.011142670284761735, 0.010892613264829461, 0.01090701420762671, 0.010982555961236472, 0.011064627788024249, 0.0112029974851272, 0.011777364880774032, 0.011285728209641988, 0.011290065392711513, 0.011532087796418022, 0.011513282869265836, 0.011829860618880583, 0.011429539796844246, 0.012332868965784003, 0.01214136268137527, 0.012259131403691224, 0.011995415838415591, 0.01176281384878996, 0.012775806247970724, 0.011659311592596233, 0.012227880464193373, 0.01252472083541502, 0.012080210647780055, 0.013432094924113557, 0.012563243550291014, 0.012440433693392677, 0.012857238451177803, 0.012454415244506802, 0.012859246207034731, 0.012352267079452302, 0.013047448570537452, 0.012631693272707136, 0.01256819104177242, 0.013037800324292586, 0.012716568259681779, 0.012766464646725428, 0.013335233533046194, 0.01270236010722674, 0.013005489322601156, 0.013242909675359989, 0.012732221655776716, 0.012946509471080527, 0.013550308715221742, 0.012586897835072369, 0.013164281666141035, 0.013123393924640911, 0.013016976401786826, 0.012641678375032914, 0.012972750021348121, 0.013115131988302097, 0.012677262125443022, 0.012670323237649618, 0.013163663530342729, 0.012903119638809052, 0.01302142468845079, 0.012469480433068784, 0.012761128839950049, 0.013318182431833714, 0.0132862006896021, 0.013390011053430597, 0.01368068079464859, 0.01313823832473958, 0.012882621570562483, 0.012989663949314315, 0.013158412037989637, 0.013035076034994927, 0.013377964403103152, 0.013192372947620854, 0.01238137314072564, 0.013763480401891855, 0.013048088714684844, 0.01273661311937764, 0.013365495266815937, 0.012929775788529427, 0.012852481628904249, 0.01303048386049675, 0.01277472223388647, 0.01356295818986282, 0.013057611181023073, 0.013073567642561129, 0.013183886623993134, 0.013083930914432268, 0.013311424228230001, 0.013276922995795326, 0.013626388954396424, 0.013164296427786921, 0.013175478736684254, 0.013190830738873187, 0.013384706719640952, 0.013061812461026015, 0.013327294444545588, 0.013488075166754508, 0.012539443402135761, 0.013522762774129603, 0.013162083821201043, 0.013013997333546712, 0.013372910520810046, 0.01362368615177828, 0.013148129616269098, 0.012825941067765727, 0.013190062619844912, 0.012350809025022677, 0.013041130319531882, 0.01304343071565261, 0.013155539859421151, 0.012894317783426534, 0.013185828298578199, 0.013018475724757317, 0.012993703379046095, 0.012119868983208226, 0.012915447090556581, 0.013067790674713477, 0.013375970104927803, 0.013428766389287208, 0.013068405165147774, 0.012938478525541018, 0.013061404191962642, 0.012566617292824978, 0.013291906699349978, 0.013035938270079534, 0.013512069430981821, 0.01356278776494733, 0.012970327281293426, 0.013513542159520808, 0.013099811951149315, 0.013258787681324739, 0.013124450288786902, 0.013364137084795155, 0.013067632239089728, 0.012436291815567482, 0.01351536658027268, 0.013533488580805506, 0.01350664998485499, 0.013470197689188213, 0.0125133531584409, 0.013103351911085727, 0.012594004567392469, 0.013075384783481957, 0.013002393662567965, 0.01294304461409596, 0.012887632775364567, 0.012945226859706768, 0.013701518515180849, 0.013288593716106243], "accuracy_test_std": 0.01510058725308442, "error_valid": [0.8557467173381024, 0.8123088055346386, 0.7717388107115963, 0.7386562852974398, 0.7150246493787651, 0.6955125188253012, 0.6790124364646084, 0.6656244117093373, 0.6526025978915663, 0.6463461266942772, 0.6359392648719879, 0.6319109445594879, 0.6269060617469879, 0.6223688700112951, 0.6188185358621988, 0.6138842479292168, 0.6089705501694277, 0.6088793651167168, 0.6039553722703314, 0.6016257412462349, 0.5979018613516567, 0.5980445218373494, 0.5932528943900602, 0.5939853162650602, 0.5917674604668675, 0.5893363493034638, 0.5896819700677711, 0.5878406202936747, 0.5867625776543675, 0.5868743528802711, 0.5847991575677711, 0.5841785109186747, 0.5837005247552711, 0.5840770307793675, 0.5823474562311747, 0.5827342573418675, 0.5822253859186747, 0.5808826124811747, 0.5806487669427711, 0.5812900037650602, 0.5788177122552711, 0.5793059935052711, 0.5804149214043675, 0.5785735716302711, 0.5793368787650602, 0.5780955854668675, 0.5771190229668675, 0.5765086714043675, 0.5766410368034638, 0.5770175428275602, 0.5763968961784638, 0.5766204466302711, 0.5762748258659638, 0.5768851774284638, 0.5752776731927711, 0.5767425169427711, 0.5754100385918675, 0.5751658979668675, 0.5749114622552711, 0.5746776167168675, 0.5745452513177711, 0.5746776167168675, 0.5737010542168675, 0.5771396131400602, 0.5754306287650602, 0.5745761365775602, 0.5739348997552711, 0.5724597609186747, 0.5733451383659638, 0.5745349562311747, 0.5734569135918675, 0.5744437711784638, 0.5746879118034638, 0.5735789839043675, 0.5743114057793675, 0.5728465620293675, 0.5743114057793675, 0.5748202772025602, 0.5733348432793675, 0.5739554899284638, 0.5729789274284638, 0.5740672651543675, 0.5745761365775602, 0.5750644178275602, 0.5739554899284638, 0.5743114057793675, 0.5735789839043675, 0.5738334196159638, 0.5741893354668675, 0.5740672651543675, 0.5736907591302711, 0.5740672651543675, 0.5738231245293675, 0.5721038450677711, 0.5748099821159638, 0.5740878553275602, 0.5733451383659638, 0.5746982068900602, 0.5719714796686747, 0.5745555464043675, 0.5723479856927711, 0.5730907026543675, 0.5740672651543675, 0.5732127729668675, 0.5728362669427711, 0.5749320524284638, 0.5727244917168675, 0.5756644743034638, 0.5717376341302711, 0.5730701124811747, 0.5718699995293675, 0.5732127729668675, 0.5743217008659638, 0.5730804075677711, 0.5733451383659638, 0.5724803510918675, 0.5730907026543675, 0.5737010542168675, 0.5728362669427711, 0.5710052122552711, 0.5730907026543675, 0.5732127729668675, 0.5738437147025602, 0.5740775602409638, 0.5729686323418675, 0.5733348432793675, 0.5745555464043675, 0.5746673216302711, 0.5739348997552711, 0.5733348432793675, 0.5743011106927711, 0.5735789839043675, 0.5737113493034638, 0.5732127729668675, 0.5727141966302711, 0.5740569700677711, 0.5728671522025602, 0.5749320524284638, 0.5748099821159638, 0.5738334196159638, 0.5722465055534638, 0.5727244917168675, 0.5733348432793675, 0.5713611281061747, 0.5737010542168675, 0.5740981504141567, 0.5751864881400602, 0.5744437711784638, 0.5743114057793675, 0.5749217573418675, 0.5738231245293675, 0.5733348432793675, 0.5729583372552711, 0.5727141966302711, 0.5730907026543675, 0.5728568571159638, 0.5728465620293675, 0.5725921263177711, 0.5739657850150602, 0.5719817747552711, 0.5734569135918675, 0.5732127729668675, 0.5740981504141567, 0.5745761365775602, 0.5730907026543675, 0.5744540662650602, 0.5732127729668675, 0.5739348997552711, 0.5737010542168675, 0.5743114057793675], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.7425944144386307, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0003436881267871949, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "adadelta", "nb_data_augmentation": 1, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 3.4942270830686325e-09, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.0651924984992496}, "accuracy_valid_max": 0.4289947877447289, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.42568859422063254, "loss_train": [2.3442068099975586, 2.1829135417938232, 2.0941288471221924, 2.033957004547119, 1.9894399642944336, 1.9533021450042725, 1.9220330715179443, 1.895400881767273, 1.8720306158065796, 1.8524155616760254, 1.8345941305160522, 1.8194047212600708, 1.806752324104309, 1.7938839197158813, 1.7842267751693726, 1.774725317955017, 1.7662917375564575, 1.7601786851882935, 1.7533372640609741, 1.746402621269226, 1.7403172254562378, 1.7365102767944336, 1.7309973239898682, 1.7278940677642822, 1.7233073711395264, 1.7198257446289062, 1.7170219421386719, 1.7130773067474365, 1.7107592821121216, 1.708351492881775, 1.7053526639938354, 1.703798532485962, 1.7011553049087524, 1.700102686882019, 1.6978509426116943, 1.6966973543167114, 1.6942771673202515, 1.6929017305374146, 1.691755771636963, 1.691192865371704, 1.6895090341567993, 1.688887119293213, 1.6877819299697876, 1.6868417263031006, 1.685588002204895, 1.68565034866333, 1.6845629215240479, 1.6841739416122437, 1.6833195686340332, 1.6825958490371704, 1.6821315288543701, 1.6812809705734253, 1.6817271709442139, 1.6805340051651, 1.6796015501022339, 1.6799237728118896, 1.6799054145812988, 1.6786454916000366, 1.6791666746139526, 1.679234266281128, 1.6782423257827759, 1.677888035774231, 1.6774160861968994, 1.677114725112915, 1.676851511001587, 1.6769797801971436, 1.677167296409607, 1.6777005195617676, 1.6767489910125732, 1.676727533340454, 1.6765872240066528, 1.6768871545791626, 1.676426649093628, 1.675528883934021, 1.675628662109375, 1.6759964227676392, 1.6758708953857422, 1.67545747756958, 1.6762233972549438, 1.6751649379730225, 1.6760344505310059, 1.6754846572875977, 1.6750982999801636, 1.6755541563034058, 1.6751965284347534, 1.6761722564697266, 1.6749335527420044, 1.6757614612579346, 1.6757371425628662, 1.6755023002624512, 1.6749051809310913, 1.6754225492477417, 1.6747151613235474, 1.6748610734939575, 1.6743743419647217, 1.6746571063995361, 1.674993634223938, 1.6739211082458496, 1.6747701168060303, 1.6746569871902466, 1.6751474142074585, 1.6749520301818848, 1.6739240884780884, 1.6745102405548096, 1.6742808818817139, 1.6743676662445068, 1.673880696296692, 1.6740310192108154, 1.675294041633606, 1.6747267246246338, 1.6754056215286255, 1.674570918083191, 1.6743007898330688, 1.6747852563858032, 1.6747900247573853, 1.6751619577407837, 1.6748013496398926, 1.6750696897506714, 1.674564003944397, 1.6750072240829468, 1.6746692657470703, 1.67464017868042, 1.6742496490478516, 1.6747337579727173, 1.6742852926254272, 1.674006700515747, 1.6746113300323486, 1.674945592880249, 1.6751307249069214, 1.6737101078033447, 1.6743513345718384, 1.6738865375518799, 1.6750253438949585, 1.6745246648788452, 1.6747286319732666, 1.6746242046356201, 1.674119234085083, 1.6749825477600098, 1.6756166219711304, 1.675881028175354, 1.6748993396759033, 1.6748335361480713, 1.6749078035354614, 1.674517035484314, 1.6743122339248657, 1.6738343238830566, 1.6748241186141968, 1.6755592823028564, 1.6746349334716797, 1.6749461889266968, 1.675079107284546, 1.6749886274337769, 1.6751898527145386, 1.6745595932006836, 1.6749447584152222, 1.6744897365570068, 1.673689603805542, 1.6750558614730835, 1.6746584177017212, 1.67433500289917, 1.674774408340454, 1.6743489503860474, 1.6742857694625854, 1.6744009256362915, 1.6742603778839111, 1.6753824949264526, 1.6748818159103394, 1.6746573448181152, 1.6752879619598389, 1.6750072240829468], "accuracy_train_first": 0.1520584235995755, "model": "residualv5", "loss_std": [0.11920430511236191, 0.10345491021871567, 0.10836520791053772, 0.11600688099861145, 0.12238708138465881, 0.12761367857456207, 0.13087664544582367, 0.13432246446609497, 0.13766533136367798, 0.1402333825826645, 0.14325553178787231, 0.14515988528728485, 0.14713358879089355, 0.1503252387046814, 0.15245944261550903, 0.15470746159553528, 0.15544529259204865, 0.1575680822134018, 0.15938133001327515, 0.15951353311538696, 0.16173294186592102, 0.16247041523456573, 0.162718266248703, 0.16502295434474945, 0.16494837403297424, 0.16618545353412628, 0.16655613481998444, 0.1675017774105072, 0.16773322224617004, 0.16839009523391724, 0.16854585707187653, 0.16989850997924805, 0.17002639174461365, 0.1717609316110611, 0.1713740974664688, 0.171345517039299, 0.1714172512292862, 0.17206861078739166, 0.1714254766702652, 0.17234058678150177, 0.17272132635116577, 0.17357902228832245, 0.1724952757358551, 0.17360058426856995, 0.1734151691198349, 0.1733519285917282, 0.17311865091323853, 0.17389418184757233, 0.17374996840953827, 0.17418278753757477, 0.17301306128501892, 0.17462465167045593, 0.17474661767482758, 0.17440152168273926, 0.17605087161064148, 0.17462308704853058, 0.17490141093730927, 0.17505542933940887, 0.17502962052822113, 0.1746305227279663, 0.1744256466627121, 0.17421568930149078, 0.17589721083641052, 0.1740972399711609, 0.17501990497112274, 0.17604225873947144, 0.17468160390853882, 0.17557303607463837, 0.17495261132717133, 0.17561683058738708, 0.17482848465442657, 0.17593024671077728, 0.17528609931468964, 0.1756080687046051, 0.17626698315143585, 0.17566506564617157, 0.1759137064218521, 0.1748272031545639, 0.1753760278224945, 0.1749306619167328, 0.17592264711856842, 0.175532728433609, 0.17589783668518066, 0.17556868493556976, 0.1753096878528595, 0.17506612837314606, 0.17570269107818604, 0.17576801776885986, 0.17532555758953094, 0.17582456767559052, 0.17571918666362762, 0.17551103234291077, 0.17505718767642975, 0.17541204392910004, 0.17568863928318024, 0.17578117549419403, 0.17566706240177155, 0.17485855519771576, 0.17594347894191742, 0.17621324956417084, 0.17558550834655762, 0.17584379017353058, 0.1756363809108734, 0.17549237608909607, 0.17535996437072754, 0.1747550219297409, 0.17566873133182526, 0.1755037009716034, 0.17531351745128632, 0.17527510225772858, 0.1765911877155304, 0.17606066167354584, 0.17540311813354492, 0.1756257265806198, 0.17563103139400482, 0.1760350465774536, 0.17477205395698547, 0.17519843578338623, 0.1756943017244339, 0.17594753205776215, 0.17590804398059845, 0.1762477159500122, 0.17564542591571808, 0.17525939643383026, 0.1762072741985321, 0.17502455413341522, 0.17559275031089783, 0.1764039695262909, 0.1760758012533188, 0.1759350597858429, 0.17577533423900604, 0.17541000247001648, 0.17665110528469086, 0.17511853575706482, 0.1757701337337494, 0.17528189718723297, 0.1758071631193161, 0.17561258375644684, 0.17584918439388275, 0.17683973908424377, 0.17520485818386078, 0.17516596615314484, 0.1757347732782364, 0.17569051682949066, 0.17493951320648193, 0.1745055764913559, 0.1755751371383667, 0.17630480229854584, 0.17619435489177704, 0.17560872435569763, 0.1762167364358902, 0.17585283517837524, 0.17624692618846893, 0.17537207901477814, 0.1755933165550232, 0.1764335334300995, 0.17549198865890503, 0.1748354732990265, 0.17450812458992004, 0.1759592741727829, 0.1757393330335617, 0.17543651163578033, 0.17474742233753204, 0.17597106099128723, 0.17588087916374207, 0.17552550137043, 0.17523081600666046, 0.17530937492847443, 0.17598652839660645, 0.17523272335529327]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:12 2016", "state": "available"}], "summary": "c5eb2b90a73d870f2f0aa576f84f58b1"}