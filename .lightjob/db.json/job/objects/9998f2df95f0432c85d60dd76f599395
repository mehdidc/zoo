{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 64, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 3, "nbg3": 5, "nbg2": 6, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.013053575849773971, 0.01849504066310042, 0.02561470171928355, 0.019927471502760107, 0.012015086006529568, 0.012293666092888192, 0.016420945935649602, 0.012778182299850217, 0.016093876808171124, 0.01356749214841662, 0.017348071222211915, 0.016811586857279074, 0.010730164366970725, 0.013973473576147245, 0.011865352253495063, 0.011286248558977953, 0.013726902272349205, 0.013858720887189015, 0.013155974420132631, 0.011210015479744912, 0.01812026962086089, 0.013832626354765195, 0.009721594313726859, 0.009556506134410435, 0.009981030190776062, 0.009152465986421941, 0.010190718280056861, 0.012334056001813587, 0.014252925167575167, 0.009882858429197049, 0.01509280812751494, 0.016507023952128882, 0.012518774071018497, 0.016833775089018465, 0.007642948014460659, 0.01062246844011582, 0.013309566841262882, 0.012873081815516241, 0.014189162775618956, 0.014705554467246166, 0.013142720103160747, 0.01736745034595637, 0.011022403487728134, 0.012037884567431824, 0.010691211200043859, 0.008621949974342106, 0.013459767591569494, 0.015400206219958208, 0.015663571632813074, 0.012704298608195072, 0.018672723736122165, 0.01104442111798478, 0.01458614699706386, 0.015463638031615459, 0.012308925941315635, 0.011965862085727088, 0.014270605502089775, 0.01788882794284512, 0.015735644512620363, 0.010180772329944547, 0.01570283266699427, 0.012225758195698646, 0.012830740732302347, 0.01727352918148309, 0.012717587535303531, 0.013359270680768566, 0.014454892299156611, 0.012344006402167018, 0.010807674038514419, 0.012429747038147526, 0.011238348799092074, 0.007943168927258607, 0.01194883361771482, 0.009851671374167757, 0.010908602291068652, 0.012765215966771005, 0.015056196999801057, 0.01095557613604653, 0.009363900180648326, 0.011859897355197, 0.014235465611338856, 0.008326323709423994, 0.009611801263745405, 0.01015770320551834, 0.011562720810276343, 0.011607741699539214, 0.00828765719968878, 0.011114735767381725, 0.00814502696702145, 0.008779961235063662, 0.014152256528415216, 0.012990071868504224, 0.012650740002263058, 0.00966294051768868, 0.009178857615056431, 0.015384872762289588, 0.009251844595366451, 0.008173520092665748, 0.012599189077132344, 0.014730521474478668, 0.01171375496200292, 0.01330376175202355, 0.011259187759318402, 0.0111972700676699, 0.011480919116023466, 0.014284822399196002, 0.011730795720026959, 0.015029716293305323, 0.010061764537918648, 0.010990709406480698, 0.00923524907223207, 0.008724156147043707, 0.009100786080593632, 0.01398261809491508, 0.007300319728864108, 0.010975247788504982, 0.014762583049170892, 0.012322070579038216, 0.011003394593127066, 0.011695389913857426, 0.011270247270845427, 0.01220511738335477, 0.011568539558103597, 0.011148738239968918, 0.009166945574920196, 0.011053807395133017, 0.009541638851341558, 0.008840109623319768, 0.011669781096020323, 0.009758614271639716, 0.008804836015678282, 0.008193779739826396, 0.007832801876744051, 0.009575721795271774, 0.009082352288197898, 0.011370216508216359, 0.015644140946755942, 0.013919166869735863, 0.008794691100046353, 0.011428130895958354, 0.012304307616184774, 0.012093819548170149, 0.010233956514049014, 0.009439916988824642, 0.008075801721036308, 0.011534773214446006, 0.011739506001887614, 0.007287512766413807, 0.004046847309971374, 0.009361186953718719, 0.013656765756351184, 0.009791620048853185, 0.010951798217273018, 0.011673072883856076, 0.008668632966817868, 0.00901119863443451, 0.011860560479173604, 0.010322628821088642, 0.006623063278164924, 0.00712516521343302, 0.0070093886628630475, 0.008142836289452761, 0.006389286025705915, 0.006132630186043108, 0.008539538484070951, 0.012923232869447762, 0.004054302053427819, 0.011913414279127019, 0.00822753556064393, 0.01197555054598701, 0.0107673279651817, 0.006830056968577626, 0.010698424277427036, 0.005150420076084212, 0.011955010673840276, 0.007823515485518939, 0.008334457177869114, 0.00832750732628751, 0.007230202188633522, 0.013373129137967716, 0.01020194688834432, 0.011527633123746217, 0.01592923822736773, 0.010201345152241558, 0.015339579475748776, 0.011627489792281625, 0.010873021198682923, 0.01130964547952357, 0.016533844381057716, 0.01488324801251834, 0.010569833250197906, 0.013980974521835979, 0.011283577470538021, 0.00878214913769322, 0.014474343621260618], "moving_avg_accuracy_train": [0.05298891570921003, 0.10839223349829272, 0.16769366542947117, 0.22548830644812426, 0.2803745014123927, 0.33234543983127707, 0.3801119427058459, 0.423132418764488, 0.46443343866590925, 0.5020972520759887, 0.5358575003652982, 0.5673297853220777, 0.5967892981093144, 0.623460969736875, 0.6470520302994094, 0.6677930539675121, 0.6873066178260212, 0.7052127670783951, 0.7224511319876191, 0.7378749796023493, 0.751851629361522, 0.7645632557733764, 0.7767891151582019, 0.7878459030759827, 0.7984806420008042, 0.8077914903664768, 0.8160387564622581, 0.8231661462449267, 0.8307782847350519, 0.8376639424130324, 0.8432845055649093, 0.8498703828277243, 0.8551488837487632, 0.8596626938384956, 0.8641222907798582, 0.8684244267235594, 0.8729170917073956, 0.8761255454773168, 0.8788388758559879, 0.8823784155325246, 0.8855196431699329, 0.887693705667493, 0.8893059515009375, 0.8916475768427227, 0.8931554396480222, 0.8955932056857578, 0.8981639052756816, 0.9000265641840215, 0.9019051009526811, 0.9035517864611874, 0.9044828873462609, 0.9057786440677533, 0.9069637146981228, 0.9084976331761695, 0.9099243383432332, 0.9109922783495799, 0.9127161452624533, 0.913369973848288, 0.9141819222518036, 0.9152287158089691, 0.9156385511771309, 0.9163604651393367, 0.9167612886362465, 0.9173268952204348, 0.9181264766009383, 0.918722830907668, 0.9192641279837064, 0.9198235552093298, 0.920017686774268, 0.9211318734482162, 0.9213949917916098, 0.9216995160066902, 0.9219500839704361, 0.9217830054771854, 0.9226018613986438, 0.9228273710874985, 0.9224514398515338, 0.9235429240617476, 0.9240486764426252, 0.9229068007926411, 0.9236715861468856, 0.9245177867918408, 0.9249166081091961, 0.924615349228186, 0.9246653030639037, 0.9247961838755457, 0.9241605923429191, 0.9249602175170806, 0.9253707795774342, 0.9251034469996742, 0.92497899697371, 0.9259617765514405, 0.9257791069654641, 0.9255588647178379, 0.9257279481092415, 0.9262035351389412, 0.9267104382811009, 0.9275598896484042, 0.9265105274658174, 0.9264521273931928, 0.9260066171790212, 0.9270169512160101, 0.927412466060033, 0.9259037682208717, 0.9263591957488676, 0.9270158346884235, 0.9267444679162404, 0.92714383540687, 0.9266756213187024, 0.9264030742119798, 0.9258996542492535, 0.9268950718934956, 0.926795856180475, 0.9275644338030145, 0.9276260022871002, 0.928220848446587, 0.9286329050055828, 0.928243684589696, 0.9270402368463964, 0.9271379129356585, 0.9273864008791266, 0.9278749988460774, 0.9283962614199414, 0.9280168986650366, 0.9288263487486991, 0.9292780890180244, 0.9294217332008473, 0.9294044925415692, 0.9291752785506552, 0.9290943997992719, 0.9299517765932926, 0.9303488144078467, 0.9302414431184094, 0.9305793594435655, 0.928316772192223, 0.9297587220897874, 0.9301171529273664, 0.9300815956668833, 0.9295800582681998, 0.9297983535153461, 0.9295969304007993, 0.9302877245965536, 0.9307976519857816, 0.9311265224980284, 0.9320084074102318, 0.932527736271691, 0.9316885788601458, 0.9320958755456982, 0.932376412056743, 0.9333357401547786, 0.9339780660620121, 0.9336169434547495, 0.934066027417691, 0.9338378706546041, 0.9345554334011131, 0.934089926888475, 0.9338315865390516, 0.9336246208126566, 0.9341079584672252, 0.9341174961730128, 0.9340654820438989, 0.9349138516193632, 0.9347008217372811, 0.9345905831981968, 0.9342493367439179, 0.9346373983802958, 0.9340847845054906, 0.9335734811253089, 0.9336271299212313, 0.934282169530391, 0.9345972655214734, 0.9350553101717993, 0.935095634694025, 0.9355038063782769, 0.9354527783036644, 0.9353904688483903, 0.935448214483854, 0.9361280838807989, 0.9359053460618867, 0.9359979949701407, 0.9362880648898856, 0.9369582819104947, 0.9364108530076973, 0.9365389456785038, 0.936607473764308, 0.9368344148511015, 0.937447743824417, 0.9370395976373426, 0.9353820968702399, 0.9357640195833173, 0.9365005920274404, 0.9362896478117765, 0.9366853750294913, 0.9363255278873764, 0.9362738520654623], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 238168386, "moving_var_accuracy_train": [0.025270426692339892, 0.05036913262144868, 0.07698215782109755, 0.09934592681326261, 0.11652378371083749, 0.129180211301009, 0.13679693934270384, 0.1397740976512432, 0.14114865609019725, 0.13980085604648104, 0.13607855972283542, 0.13138524623415845, 0.12605748765369484, 0.11985414149500116, 0.11287757059168751, 0.1054615240977259, 0.0983423842580943, 0.09139381746171948, 0.08492888673823962, 0.0785770537415974, 0.07247746901385288, 0.0666839911267786, 0.06136083675337848, 0.05632502610956967, 0.05171040254660472, 0.047319589367541194, 0.04319978701327868, 0.03933700547797762, 0.035924806801715475, 0.032759036656468926, 0.02976744956212016, 0.02718106861979593, 0.024713724905577016, 0.022425722748754845, 0.020362142517794064, 0.01849250362911743, 0.016824909614118592, 0.015235066233050229, 0.01377781906543958, 0.012512792228991617, 0.011350318805722615, 0.010257825854840029, 0.009255437299003159, 0.008379242452274458, 0.007561781059203464, 0.0068590872825757526, 0.006232655021752881, 0.005640615003456956, 0.0051083136066321115, 0.004621886404444224, 0.004167500303723465, 0.003765861142682753, 0.003401914560005154, 0.003082899257080277, 0.002792928720075787, 0.00252390031078261, 0.002298255733904044, 0.002072277586890532, 0.0018709831700912225, 0.0016937468438440095, 0.0015258838447205754, 0.0013779858981679658, 0.0012416332436322444, 0.0011203491165417134, 0.0010140681783439745, 0.000915862106641973, 0.0008269129186985241, 0.0007470382562155905, 0.0006726736141745796, 0.0006165789602567565, 0.0005555441455947526, 0.0005008243460134106, 0.00045130697015217144, 0.0004064275101431166, 0.0003718194843097704, 0.0003350952274566995, 0.000302857623358595, 0.0002832939010530521, 0.00025726658018461766, 0.00024327484216639757, 0.00022421142769236076, 0.00020823478470682834, 0.00018884283222473812, 0.00017077536122275114, 0.00015372028357180228, 0.00013850242329632698, 0.00012828797033381342, 0.00012121377707280573, 0.00011060945021414122, 0.00010019170555691349, 9.031192528188428e-05, 8.997343403933326e-05, 8.127640423416727e-05, 7.358532363950529e-05, 6.648409401479204e-05, 6.18713318186801e-05, 5.7996755796595195e-05, 5.869118884565758e-05, 6.273251887328227e-05, 5.648996210229703e-05, 5.262728005044859e-05, 5.655152584208985e-05, 5.2304261184462525e-05, 6.755935759502572e-05, 6.267014993483107e-05, 6.02837072138171e-05, 5.491809581784096e-05, 5.08617357692026e-05, 4.774858208351007e-05, 4.3642261203604725e-05, 4.155892001308558e-05, 4.632073458999596e-05, 4.177725495038815e-05, 4.2915933512164486e-05, 3.865845626504161e-05, 3.797718821964228e-05, 3.570758486798101e-05, 3.3500259170470845e-05, 4.318481149109988e-05, 3.895219590771185e-05, 3.5612692639381773e-05, 3.4199975135219285e-05, 3.3225409659899496e-05, 3.119811359219018e-05, 3.397518717444152e-05, 3.241429189536873e-05, 2.9358565567161526e-05, 2.6425384173436487e-05, 2.4255697238769444e-05, 2.1888999866720474e-05, 2.6315954582376257e-05, 2.5103110359811655e-05, 2.2696556667989364e-05, 2.1454587986453422e-05, 6.538283881724725e-05, 7.75575304992945e-05, 7.095803143731348e-05, 6.387360716253966e-05, 5.975010430678985e-05, 5.4203969210451367e-05, 4.914871372906984e-05, 4.852861194415368e-05, 4.601598423030223e-05, 4.238778813170169e-05, 4.514849830388007e-05, 4.306097067059341e-05, 4.509254005569603e-05, 4.207630136068429e-05, 3.857697783087857e-05, 4.300207364491526e-05, 4.2415109420353386e-05, 3.934728431560255e-05, 3.722764353598294e-05, 3.397337875926541e-05, 3.5210107539936154e-05, 3.363936360571874e-05, 3.0876084870408506e-05, 2.81739896904876e-05, 2.7459128316353693e-05, 2.4714034195203548e-05, 2.226698000233036e-05, 2.6517860431258476e-05, 2.4274509964071993e-05, 2.195643178715982e-05, 2.0808830891464974e-05, 2.0083274304972946e-05, 2.0823385726119984e-05, 2.1093927472775775e-05, 1.9010438465233518e-05, 2.0971086624822065e-05, 1.976754731470542e-05, 1.9679036698464396e-05, 1.7725767632452578e-05, 1.7452627983633232e-05, 1.5730799964857818e-05, 1.41926621823211e-05, 1.2803406989824928e-05, 1.568306786296337e-05, 1.4561270300431369e-05, 1.3182397652194037e-05, 1.26214229120418e-05, 1.5401998313264707e-05, 1.6558904114499348e-05, 1.505068329387847e-05, 1.3587879851386385e-05, 1.269261217812348e-05, 1.4808902825884944e-05, 1.482726233350686e-05, 3.8070315236668915e-05, 3.557606834188249e-05, 3.690131219666689e-05, 3.361165813609885e-05, 3.1659892600052556e-05, 2.9659313031241497e-05, 2.6717415243251813e-05], "duration": 87278.986071, "accuracy_train": [0.5298891570921004, 0.607022093600037, 0.7014065528100775, 0.7456400756160022, 0.7743502560908084, 0.8000838856012367, 0.8100104685769656, 0.8103167032922666, 0.8361426177787007, 0.8410715727667036, 0.8396997349690846, 0.8505803499330934, 0.8619249131944444, 0.8635060143849206, 0.8593715753622186, 0.8544622669804356, 0.8629286925526025, 0.86636811034976, 0.8775964161706349, 0.8766896081349206, 0.8776414771940754, 0.8789678934800664, 0.8868218496216316, 0.8873569943360096, 0.8941932923241971, 0.8915891256575305, 0.8902641513242894, 0.8873126542889442, 0.8992875311461794, 0.8996348615148578, 0.8938695739318014, 0.9091432781930602, 0.9026553920381136, 0.9002869846460871, 0.9042586632521227, 0.9071436502168696, 0.9133510765619232, 0.9050016294066077, 0.9032588492640274, 0.9142342726213547, 0.9137906919066077, 0.9072602681455334, 0.903816164001938, 0.9127222049187893, 0.906726204895718, 0.9175331000253784, 0.9213002015849945, 0.9167904943590809, 0.9188119318706165, 0.9183719560377446, 0.9128627953119232, 0.917440454561185, 0.917629350371447, 0.9223028994785898, 0.922764684846807, 0.9206037384067, 0.9282309474783131, 0.9192544311208011, 0.921489457883444, 0.9246498578234589, 0.9193270694905868, 0.9228576907991879, 0.9203687001084349, 0.9224173544781286, 0.9253227090254706, 0.9240900196682356, 0.924135801668051, 0.9248584002399409, 0.9217648708587117, 0.9311595535137505, 0.9237630568821521, 0.9244402339424143, 0.9242051956441492, 0.9202792990379292, 0.9299715646917681, 0.9248569582871908, 0.9190680587278516, 0.9333662819536729, 0.9286004478705242, 0.9126299199427832, 0.9305546543350868, 0.9321335925964378, 0.9285059999653931, 0.9219040192990956, 0.9251148875853636, 0.9259741111803249, 0.9184402685492802, 0.932156844084533, 0.9290658381206165, 0.9226974537998339, 0.9238589467400333, 0.9348067927510151, 0.9241350806916758, 0.9235766844892026, 0.9272496986318751, 0.9304838184062385, 0.9312725665605389, 0.9352049519541344, 0.917066267822536, 0.9259265267395718, 0.9219970252514765, 0.936109957548911, 0.9309720996562385, 0.9123254876684201, 0.9304580435008305, 0.932925585144426, 0.9243021669665927, 0.930738142822536, 0.9224616945251938, 0.9239501502514765, 0.9213688745847176, 0.9358538306916758, 0.925902914763289, 0.9344816324058692, 0.9281801186438722, 0.9335744638819674, 0.9323414140365448, 0.9247407008467147, 0.9162092071567, 0.928016997739018, 0.9296227923703396, 0.9322723805486341, 0.9330876245847176, 0.9246026338708934, 0.9361113995016611, 0.9333437514419527, 0.9307145308462532, 0.9292493266080657, 0.927112352632429, 0.9283664910368217, 0.9376681677394795, 0.9339221547388336, 0.9292751015134736, 0.9336206063699704, 0.9079534869301403, 0.9427362711678663, 0.9333430304655776, 0.929761580322536, 0.925066221680048, 0.931763010739664, 0.9277841223698781, 0.9365048723583426, 0.9353869984888336, 0.9340863571082503, 0.9399453716200628, 0.9372016960248246, 0.9241361621562385, 0.9357615457156699, 0.9349012406561462, 0.9419696930370985, 0.9397589992271133, 0.9303668399893872, 0.9381077830841639, 0.9317844597868217, 0.9410134981196937, 0.9299003682747323, 0.9315065233942414, 0.9317619292751015, 0.9384579973583426, 0.9342033355251015, 0.9335973548818751, 0.9425491777985419, 0.9327835527985419, 0.9335984363464378, 0.931178118655408, 0.9381299531076966, 0.9291112596322444, 0.9289717507036729, 0.934109969084533, 0.9401775260128276, 0.9374331294412146, 0.9391777120247323, 0.9354585553940569, 0.9391773515365448, 0.9349935256321521, 0.9348296837509228, 0.9359679252030271, 0.9422469084533037, 0.9339007056916758, 0.936831835144426, 0.9388986941675894, 0.9429902350959765, 0.9314839928825213, 0.9376917797157622, 0.9372242265365448, 0.9388768846322444, 0.9429677045842562, 0.9333662819536729, 0.9204645899663161, 0.9392013240010151, 0.9431297440245479, 0.9343911498708011, 0.9402469199889257, 0.9330869036083426, 0.9358087696682356], "end": "2016-01-26 03:24:42.263000", "learning_rate_per_epoch": [0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315, 0.0020255581475794315], "accuracy_valid": [0.5293395260730422, 0.6002756141754518, 0.6872352692018072, 0.7231548263365963, 0.7486984069088856, 0.7717196912650602, 0.7840193782944277, 0.7792777555534638, 0.8031638271837349, 0.807274508189006, 0.8003459149096386, 0.8141913356551205, 0.8196639095444277, 0.8204478068524097, 0.8143840008471386, 0.8080569347703314, 0.8172739787274097, 0.8134589137801205, 0.8262159967996988, 0.8254938700112951, 0.8266645684299698, 0.8262365869728916, 0.8356565912085843, 0.833031344126506, 0.8355448159826807, 0.8322386224585843, 0.8321268472326807, 0.8307223032756024, 0.8433176063629518, 0.8397157967808735, 0.8350771249058735, 0.845616352127259, 0.8431337655308735, 0.8377626717808735, 0.8418821771460843, 0.8401937829442772, 0.850255024002259, 0.8415468514683735, 0.8360330972326807, 0.8485666298004518, 0.8463075936558735, 0.843297016189759, 0.8368464090737951, 0.845860492752259, 0.8408041345067772, 0.8463281838290663, 0.8538965432040663, 0.8494108269013554, 0.8473959313817772, 0.8459207925451807, 0.8450971856174698, 0.8493284662085843, 0.8486372246799698, 0.8510183311370482, 0.8486166345067772, 0.8517301628388554, 0.8546701454254518, 0.8477327277861446, 0.8495123070406627, 0.8504682793674698, 0.846104633377259, 0.849888813064759, 0.8483621987951807, 0.8460634530308735, 0.8503873894013554, 0.8419630671121988, 0.8503873894013554, 0.8492681664156627, 0.849888813064759, 0.8539377235504518, 0.8505697595067772, 0.8423395731362951, 0.8475385918674698, 0.8456869470067772, 0.8458693171121988, 0.8490137307040663, 0.8381185876317772, 0.852208149002259, 0.8494402414344879, 0.8409570900790663, 0.8489416650978916, 0.850987445877259, 0.8503359139683735, 0.8519228280308735, 0.8519228280308735, 0.8479048028049698, 0.8413836008094879, 0.8494196512612951, 0.8509256753576807, 0.8469179452183735, 0.8473959313817772, 0.8528493858245482, 0.8461649331701807, 0.8406614740210843, 0.8489313700112951, 0.8494917168674698, 0.8473547510353916, 0.8539877282567772, 0.8440397331513554, 0.8447515648531627, 0.8466032097138554, 0.8544465949736446, 0.851231586502259, 0.8363890130835843, 0.8562673545745482, 0.8494417121611446, 0.8439882577183735, 0.851283061935241, 0.8436426369540663, 0.8392069253576807, 0.8438970726656627, 0.8493799416415663, 0.8443544686558735, 0.8490652061370482, 0.8458090173192772, 0.853062641189759, 0.8553216773343373, 0.844261812876506, 0.8329607492469879, 0.8470606057040663, 0.8507227150790663, 0.8485151543674698, 0.8509977409638554, 0.8509977409638554, 0.8503256188817772, 0.8531950065888554, 0.8522184440888554, 0.850377094314759, 0.852330219314759, 0.850499164627259, 0.8578542686370482, 0.8499594079442772, 0.8480062829442772, 0.8522890389683735, 0.8347918039344879, 0.8584234398531627, 0.8527273155120482, 0.855189311935241, 0.8425940088478916, 0.8550466514495482, 0.8513639519013554, 0.8558805534638554, 0.8531744164156627, 0.8511801110692772, 0.8634695030120482, 0.8545583701995482, 0.8478239128388554, 0.857213031814759, 0.8528788003576807, 0.8569585961031627, 0.8570100715361446, 0.8531538262424698, 0.856968891189759, 0.852086078689759, 0.8553613869540663, 0.8508550804781627, 0.8467252800263554, 0.8518213478915663, 0.8529199807040663, 0.8455134012612951, 0.8532964867281627, 0.860020649002259, 0.8509668557040663, 0.8531229409826807, 0.8462767083960843, 0.8572748023343373, 0.8495726068335843, 0.8497770378388554, 0.8537641778049698, 0.8544157097138554, 0.8570100715361446, 0.8552496117281627, 0.851719867752259, 0.8580572289156627, 0.8503256188817772, 0.8558599632906627, 0.8579351586031627, 0.8575689476656627, 0.8488607751317772, 0.8591161521084337, 0.8555952324924698, 0.862147319747741, 0.8547113257718373, 0.8521978539156627, 0.8576513083584337, 0.860072124435241, 0.8572542121611446, 0.851353656814759, 0.844325054122741, 0.855921733810241, 0.8607633659638554, 0.8501535438629518, 0.8589631965361446, 0.8521978539156627, 0.8560540992093373], "accuracy_test": 0.1122528698979592, "start": "2016-01-25 03:10:03.276000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0], "accuracy_train_last": 0.9358087696682356, "batch_size_eval": 1024, "accuracy_train_std": [0.016047101704486314, 0.018474561288983244, 0.015072191816008178, 0.019875003151008496, 0.01940686542414136, 0.018732350445283867, 0.020771771975042624, 0.016630505287922433, 0.017808302099494078, 0.017927938620860784, 0.020028025166933288, 0.01749210467953807, 0.01848383460993022, 0.01820333479651877, 0.01847233588617537, 0.01990765204959458, 0.01797588089174968, 0.019057417945958086, 0.017871838868459167, 0.017707290133602116, 0.017809469276443595, 0.017809345018275267, 0.018272181003419603, 0.019826301630866086, 0.01767784770361951, 0.01875107255423205, 0.0175267406182189, 0.018052061913282718, 0.015874567009040254, 0.017248082754690322, 0.01697971083273872, 0.015604826556210248, 0.018222262333472614, 0.016114021113475648, 0.01828294864747416, 0.017038777714450212, 0.017544440057054297, 0.017578248562987363, 0.01793031818774177, 0.017283269855727115, 0.016759881914833345, 0.016481054052954868, 0.019381514347452153, 0.01852711230962546, 0.01676686873199068, 0.017710077725636507, 0.016040250829227692, 0.0162932255197269, 0.01691664965580921, 0.017146048707889085, 0.01712521522815881, 0.01799908851472273, 0.01603232323872933, 0.017385028084650228, 0.01762292905934565, 0.0184950412459013, 0.01599854343701446, 0.017189792081158787, 0.01645483761704298, 0.017181324183997683, 0.015184298983281096, 0.017441826453249126, 0.018976793447641777, 0.017261965289625408, 0.016354311139677436, 0.018142124474227375, 0.017297651446602128, 0.016719395125334973, 0.01600744476048975, 0.015093911925760867, 0.018043338239786676, 0.01749334130857514, 0.016661287169182686, 0.016935229158914752, 0.01606465748490427, 0.017362544973008664, 0.01795429218354418, 0.01662420545503612, 0.016197867261298713, 0.0188733265687058, 0.015855811184858806, 0.01805859617113452, 0.01863736274692117, 0.017238233852001433, 0.015876840536613788, 0.016228657968338434, 0.016939427027088387, 0.01619588787431897, 0.016812279177164564, 0.015982160557518138, 0.018946143079135787, 0.016746535166080406, 0.019174483099524776, 0.019020206434313132, 0.01740640242906069, 0.015103289396482752, 0.01753702204022895, 0.016240964409536127, 0.018604483252199967, 0.017634201789479553, 0.016093342898160105, 0.014554026551714214, 0.01615250143138673, 0.01880006044321065, 0.0169667929546888, 0.01593817648430192, 0.015524374244694612, 0.015805810486206804, 0.017887458101976995, 0.015636060505421507, 0.01887381014287493, 0.016009027525386924, 0.01744313397088657, 0.015352091620668173, 0.018232497524891502, 0.016746391142360012, 0.01760323335394286, 0.01844496285942735, 0.015149169527668274, 0.016707395891103472, 0.015197697449333754, 0.016627409026924347, 0.016619638922556877, 0.01527837297134458, 0.015046660336518972, 0.016263133962621194, 0.016686575277568887, 0.017016739189925453, 0.015673505116470008, 0.016368812592089776, 0.015773699634909148, 0.017700521560400004, 0.015430960722989539, 0.01631311532412983, 0.0178052666211826, 0.015479612787542844, 0.01577083510135, 0.01579314333828468, 0.016228335097848875, 0.016275148714328398, 0.0171475683187156, 0.01570092518985785, 0.016464422842479808, 0.016099338300710454, 0.014802636794905044, 0.015599865079725158, 0.016022276098903235, 0.01528396360240668, 0.015761327757469096, 0.015194755871832062, 0.015301898318437018, 0.01576376429794933, 0.01771489823847417, 0.01701643711280009, 0.015673029801221483, 0.017587908879605717, 0.017238911118485828, 0.016757186778906235, 0.015298940048276968, 0.015135628410925094, 0.01623570185944299, 0.015971313667286942, 0.016356987321058652, 0.015128068140549603, 0.01739943725793225, 0.01630011667300737, 0.016185708360487005, 0.016340728663247206, 0.016413967492304945, 0.016081145876931366, 0.016432971206252475, 0.016497085373498588, 0.014544070418903288, 0.015417961085144507, 0.01727234330980266, 0.01537348352246809, 0.01656561439923715, 0.014415074157770947, 0.016820205797889985, 0.01459192186242767, 0.015480103262244891, 0.015059968935663515, 0.014182961022793436, 0.015405709335421012, 0.016540934585380064, 0.014571648251022399, 0.014773501159902702, 0.014715814739720496, 0.01801578990364647, 0.01557250892137494, 0.01453648918156318, 0.01505342806907692, 0.015618130838290108, 0.015965454508325767, 0.01623710723069145], "accuracy_test_std": 0.01140039925374045, "error_valid": [0.47066047392695776, 0.39972438582454817, 0.3127647307981928, 0.27684517366340367, 0.25130159309111444, 0.22828030873493976, 0.2159806217055723, 0.2207222444465362, 0.1968361728162651, 0.19272549181099397, 0.19965408509036142, 0.18580866434487953, 0.1803360904555723, 0.1795521931475903, 0.18561599915286142, 0.19194306522966864, 0.1827260212725903, 0.18654108621987953, 0.17378400320030118, 0.17450612998870485, 0.17333543157003017, 0.1737634130271084, 0.16434340879141573, 0.16696865587349397, 0.1644551840173193, 0.16776137754141573, 0.1678731527673193, 0.16927769672439763, 0.15668239363704817, 0.1602842032191265, 0.1649228750941265, 0.15438364787274095, 0.1568662344691265, 0.1622373282191265, 0.15811782285391573, 0.15980621705572284, 0.14974497599774095, 0.1584531485316265, 0.1639669027673193, 0.15143337019954817, 0.1536924063441265, 0.15670298381024095, 0.16315359092620485, 0.15413950724774095, 0.15919586549322284, 0.15367181617093373, 0.14610345679593373, 0.1505891730986446, 0.15260406861822284, 0.1540792074548193, 0.15490281438253017, 0.15067153379141573, 0.15136277532003017, 0.14898166886295183, 0.15138336549322284, 0.1482698371611446, 0.14532985457454817, 0.1522672722138554, 0.15048769295933728, 0.14953172063253017, 0.15389536662274095, 0.15011118693524095, 0.1516378012048193, 0.1539365469691265, 0.1496126105986446, 0.15803693288780118, 0.1496126105986446, 0.15073183358433728, 0.15011118693524095, 0.14606227644954817, 0.14943024049322284, 0.15766042686370485, 0.15246140813253017, 0.15431305299322284, 0.15413068288780118, 0.15098626929593373, 0.16188141236822284, 0.14779185099774095, 0.15055975856551207, 0.15904290992093373, 0.1510583349021084, 0.14901255412274095, 0.1496640860316265, 0.1480771719691265, 0.1480771719691265, 0.15209519719503017, 0.15861639919051207, 0.15058034873870485, 0.1490743246423193, 0.1530820547816265, 0.15260406861822284, 0.14715061417545183, 0.1538350668298193, 0.15933852597891573, 0.15106862998870485, 0.15050828313253017, 0.1526452489646084, 0.14601227174322284, 0.1559602668486446, 0.15524843514683728, 0.1533967902861446, 0.1455534050263554, 0.14876841349774095, 0.16361098691641573, 0.14373264542545183, 0.1505582878388554, 0.1560117422816265, 0.14871693806475905, 0.15635736304593373, 0.1607930746423193, 0.15610292733433728, 0.15062005835843373, 0.1556455313441265, 0.15093479386295183, 0.15419098268072284, 0.14693735881024095, 0.14467832266566272, 0.15573818712349397, 0.16703925075301207, 0.15293939429593373, 0.14927728492093373, 0.15148484563253017, 0.1490022590361446, 0.1490022590361446, 0.14967438111822284, 0.1468049934111446, 0.1477815559111446, 0.14962290568524095, 0.14766978068524095, 0.14950083537274095, 0.14214573136295183, 0.15004059205572284, 0.15199371705572284, 0.1477109610316265, 0.16520819606551207, 0.14157656014683728, 0.14727268448795183, 0.14481068806475905, 0.1574059911521084, 0.14495334855045183, 0.1486360480986446, 0.1441194465361446, 0.14682558358433728, 0.14881988893072284, 0.13653049698795183, 0.14544162980045183, 0.1521760871611446, 0.14278696818524095, 0.1471211996423193, 0.14304140389683728, 0.1429899284638554, 0.14684617375753017, 0.14303110881024095, 0.14791392131024095, 0.14463861304593373, 0.14914491952183728, 0.1532747199736446, 0.14817865210843373, 0.14708001929593373, 0.15448659873870485, 0.14670351327183728, 0.13997935099774095, 0.14903314429593373, 0.1468770590173193, 0.15372329160391573, 0.14272519766566272, 0.15042739316641573, 0.1502229621611446, 0.14623582219503017, 0.1455842902861446, 0.1429899284638554, 0.14475038827183728, 0.14828013224774095, 0.14194277108433728, 0.14967438111822284, 0.14414003670933728, 0.14206484139683728, 0.14243105233433728, 0.15113922486822284, 0.14088384789156627, 0.14440476750753017, 0.13785268025225905, 0.14528867422816272, 0.14780214608433728, 0.14234869164156627, 0.13992787556475905, 0.1427457878388554, 0.14864634318524095, 0.15567494587725905, 0.14407826618975905, 0.1392366340361446, 0.14984645613704817, 0.1410368034638554, 0.14780214608433728, 0.14394590079066272], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.834584832732157, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.002025558100208296, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "adam", "nb_data_augmentation": 1, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 3.406984959386438e-05, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.051603471284777186}, "accuracy_valid_max": 0.8634695030120482, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8560540992093373, "loss_train": [2.233921527862549, 1.5986137390136719, 1.3732783794403076, 1.2276008129119873, 1.121856451034546, 1.0464903116226196, 0.98897385597229, 0.9491607546806335, 0.9130820631980896, 0.8839048743247986, 0.8589491248130798, 0.8410671949386597, 0.8206526637077332, 0.8047236800193787, 0.7917534708976746, 0.7776025533676147, 0.766887366771698, 0.7585118412971497, 0.747477114200592, 0.7431691288948059, 0.7344931364059448, 0.7310578227043152, 0.7257584929466248, 0.7166087031364441, 0.7148541212081909, 0.7075632810592651, 0.7031798958778381, 0.6974682807922363, 0.696789026260376, 0.6922842264175415, 0.6863711476325989, 0.6820632219314575, 0.6806052923202515, 0.6801295876502991, 0.6744461059570312, 0.6719993352890015, 0.6671450138092041, 0.6682614684104919, 0.6635712385177612, 0.6635491251945496, 0.6571390628814697, 0.6555153131484985, 0.6554691195487976, 0.6554510593414307, 0.6506787538528442, 0.6475257873535156, 0.6477664113044739, 0.6458355784416199, 0.6444559693336487, 0.6446857452392578, 0.6420571208000183, 0.6391616463661194, 0.6379699110984802, 0.6382023692131042, 0.6354430913925171, 0.6345147490501404, 0.637075662612915, 0.6332926750183105, 0.6317877769470215, 0.6278524994850159, 0.626459538936615, 0.6318101286888123, 0.6260138750076294, 0.6253830194473267, 0.6246181726455688, 0.6204004883766174, 0.6236804723739624, 0.6204811334609985, 0.6188469529151917, 0.6186856627464294, 0.6192861199378967, 0.6210346221923828, 0.6167727112770081, 0.6171044111251831, 0.614689826965332, 0.6115716099739075, 0.6132100820541382, 0.6130492091178894, 0.6125089526176453, 0.6084657311439514, 0.6122368574142456, 0.6103388071060181, 0.608461856842041, 0.6041028499603271, 0.6057383418083191, 0.6095963716506958, 0.606605589389801, 0.6064039468765259, 0.604316234588623, 0.6006361842155457, 0.6088746786117554, 0.6011511087417603, 0.6036726832389832, 0.6032654643058777, 0.603397786617279, 0.6021116971969604, 0.6008016467094421, 0.5989329218864441, 0.6010283827781677, 0.5985412001609802, 0.5994579792022705, 0.5975755453109741, 0.5976618528366089, 0.5967220067977905, 0.5962479710578918, 0.5957662463188171, 0.597681999206543, 0.5965566635131836, 0.5939231514930725, 0.5968615412712097, 0.5958576798439026, 0.5970156192779541, 0.5939778685569763, 0.5920448899269104, 0.5915929675102234, 0.5903335809707642, 0.5880711674690247, 0.59147709608078, 0.5915812849998474, 0.5899218320846558, 0.5903623104095459, 0.5915762186050415, 0.5910142660140991, 0.5854542255401611, 0.5887966752052307, 0.584795355796814, 0.5840739011764526, 0.5895713567733765, 0.5859264135360718, 0.5862848162651062, 0.587390661239624, 0.5908077359199524, 0.5844690203666687, 0.5865706205368042, 0.5829558372497559, 0.588262140750885, 0.5841485261917114, 0.5862956047058105, 0.5830850005149841, 0.5843991637229919, 0.5848249793052673, 0.5834310054779053, 0.5844261050224304, 0.5799190402030945, 0.583690881729126, 0.5845127701759338, 0.5841498970985413, 0.581410825252533, 0.5790575742721558, 0.5821606516838074, 0.5789437890052795, 0.5790310502052307, 0.5776599645614624, 0.5741636753082275, 0.5870423316955566, 0.5792493224143982, 0.5820004343986511, 0.5795046091079712, 0.5750382542610168, 0.5770894885063171, 0.5769937634468079, 0.5785794258117676, 0.5754961967468262, 0.5765372514724731, 0.5762390494346619, 0.5751403570175171, 0.5773245692253113, 0.5752229690551758, 0.5740918517112732, 0.5757490992546082, 0.5717381238937378, 0.5759612917900085, 0.5774757266044617, 0.5743666291236877, 0.5711955428123474, 0.5706640481948853, 0.5774388313293457, 0.5747193098068237, 0.5732192397117615, 0.5704576969146729, 0.5701261162757874, 0.5780701041221619, 0.569328784942627, 0.5742029547691345, 0.5689212679862976, 0.574224054813385, 0.5711381435394287, 0.5675680041313171, 0.5671276450157166, 0.5733035802841187, 0.5688167214393616, 0.5669369697570801, 0.5680755972862244, 0.5712312459945679, 0.5707612633705139], "accuracy_train_first": 0.5298891570921004, "model": "residualv5", "loss_std": [0.3962266445159912, 0.15323951840400696, 0.1332177072763443, 0.12853236496448517, 0.12562216818332672, 0.12132658064365387, 0.11915864050388336, 0.11876711249351501, 0.1136162132024765, 0.11334383487701416, 0.11026892811059952, 0.10668700933456421, 0.10832898318767548, 0.10673412680625916, 0.1050221398472786, 0.10185892134904861, 0.10164034366607666, 0.10291798412799835, 0.09970203042030334, 0.10047554969787598, 0.09889370948076248, 0.0998862162232399, 0.10043509304523468, 0.09538139402866364, 0.09678832441568375, 0.09581834077835083, 0.09593573957681656, 0.09554191678762436, 0.09612303972244263, 0.09057454764842987, 0.09403430670499802, 0.09090683609247208, 0.0907929316163063, 0.09504857659339905, 0.09135608375072479, 0.0918964296579361, 0.08749022334814072, 0.08864974975585938, 0.08759161829948425, 0.0908537283539772, 0.08699361979961395, 0.08911889791488647, 0.08704859763383865, 0.0883411169052124, 0.08567924797534943, 0.08475644141435623, 0.08497419208288193, 0.08473426848649979, 0.0863875076174736, 0.08378088474273682, 0.0841376855969429, 0.08361970633268356, 0.0806262195110321, 0.08429290354251862, 0.083817258477211, 0.0831514298915863, 0.08307123929262161, 0.08341841399669647, 0.08351776003837585, 0.08204592764377594, 0.07946740835905075, 0.0813128724694252, 0.08037984371185303, 0.08027762174606323, 0.0824066549539566, 0.07915899902582169, 0.08223648369312286, 0.07989119738340378, 0.07926204055547714, 0.07823678851127625, 0.08290063589811325, 0.07966797053813934, 0.0816335678100586, 0.07927752286195755, 0.07910811901092529, 0.07969877868890762, 0.0807817280292511, 0.08142584562301636, 0.07877551019191742, 0.07999866455793381, 0.07789207249879837, 0.07941164821386337, 0.07868005335330963, 0.07939254492521286, 0.0772663950920105, 0.07603318244218826, 0.07911539077758789, 0.08135267347097397, 0.07819844037294388, 0.07926751673221588, 0.0801439881324768, 0.07891302555799484, 0.08026372641324997, 0.07680176943540573, 0.07628955692052841, 0.08045994490385056, 0.07836487889289856, 0.07974150776863098, 0.07739879190921783, 0.07750515639781952, 0.07624920457601547, 0.0788906142115593, 0.07763800024986267, 0.07724710553884506, 0.07727579027414322, 0.07902874052524567, 0.07947597652673721, 0.07397183030843735, 0.07627400010824203, 0.07830073684453964, 0.07671336829662323, 0.07589620351791382, 0.07725287973880768, 0.07852310687303543, 0.07640530914068222, 0.07531120628118515, 0.07572348415851593, 0.07641879469156265, 0.07646818459033966, 0.075219064950943, 0.07595941424369812, 0.0745302364230156, 0.07782546430826187, 0.07514648884534836, 0.07237844914197922, 0.07565509527921677, 0.07525846362113953, 0.0759616568684578, 0.07383215427398682, 0.07804557681083679, 0.07629404962062836, 0.07708555459976196, 0.07639574259519577, 0.0753631517291069, 0.07721948623657227, 0.07839414477348328, 0.07702820003032684, 0.07993146777153015, 0.07702617347240448, 0.07591095566749573, 0.07566814124584198, 0.073704294860363, 0.075089231133461, 0.07732059061527252, 0.07465189695358276, 0.07641561329364777, 0.07322504371404648, 0.07519680261611938, 0.07625754922628403, 0.07677548378705978, 0.07348641008138657, 0.07658380270004272, 0.07333526015281677, 0.07443010807037354, 0.07849058508872986, 0.07552601397037506, 0.07201370596885681, 0.07380902022123337, 0.0747114047408104, 0.07848476618528366, 0.0730152577161789, 0.07445009052753448, 0.07511470466852188, 0.07203381508588791, 0.07359319180250168, 0.07282887399196625, 0.07460223138332367, 0.0721881091594696, 0.07386621832847595, 0.07204122096300125, 0.0709763616323471, 0.07481705397367477, 0.07426315546035767, 0.07311461120843887, 0.07463838905096054, 0.07270839065313339, 0.07147230207920074, 0.07254718989133835, 0.0719599574804306, 0.0725228413939476, 0.06978928297758102, 0.07299231737852097, 0.07452458888292313, 0.0745612233877182, 0.07365928590297699, 0.07202105969190598, 0.07471128553152084, 0.06971859931945801, 0.0736001655459404, 0.07421140372753143, 0.07133661955595016, 0.07727345079183578, 0.0715867355465889, 0.07414786517620087, 0.07455868273973465]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:14 2016", "state": "available"}], "summary": "f2252dca8cab14d0fe4bf5e0a6271c5a"}