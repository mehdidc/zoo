{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 16, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.012535538045652871, 0.011736445900990704, 0.01450791263910141, 0.014340297696578826, 0.01814866968862463, 0.007658392158866271, 0.01228139072183513, 0.01342883197870945, 0.012507294345436262, 0.011191115837169141, 0.012937605808595931, 0.014498399734202706, 0.011520655051996922, 0.012529318405191125, 0.012127784212398303, 0.013309487532509708, 0.015684281421278923, 0.01126624502975005, 0.018202586711617365, 0.014518173284932151, 0.009763528435723073, 0.01245526503743917, 0.008270208543144055, 0.010469940996043096, 0.010468316103999115, 0.007389952754541344, 0.007556043378885417, 0.013396063999618967, 0.00929496442829257, 0.010788872138504801, 0.005375983542826227, 0.00563279611781435, 0.009158300271447277, 0.009712334536346795, 0.005745862149713375, 0.00889780328814647, 0.007465748657531069, 0.005837344856006214, 0.007847402737072394, 0.006853201011224837, 0.007906320098700157, 0.005371417324280895, 0.007710332855154143, 0.010279096940306558, 0.0049303932443751095, 0.011634827391815868, 0.010346264952952015, 0.008939743478383834, 0.010928930795529077, 0.009069118315700222, 0.00865154365094514, 0.00980121011443825, 0.007373741655876388, 0.009079802684695862, 0.012947888498290426, 0.008148959173383886, 0.009388350406833269, 0.01203546431580081, 0.008950180266586021, 0.007107921965180832, 0.011364198858078768, 0.004825514674519486, 0.009226868675766973, 0.009749243667655462, 0.011432583717368468, 0.010257832498807653, 0.007448287042263936, 0.009809526629227044, 0.009244560059049122, 0.010876997602939824, 0.009256352938831156, 0.013587509816816713, 0.008922864445292195, 0.008566326161760198, 0.011041403066926032, 0.009997032585861829, 0.00994440765979242, 0.011471421666606007, 0.008458189791745407, 0.006967326278091883, 0.010874740965099845, 0.009294102428200376, 0.013085660565534639, 0.010410753552709495, 0.0103714992090298, 0.012614080868670476, 0.013392587705678685, 0.008114092533954089, 0.012232615549332297, 0.007857793861046057, 0.00903611010809149, 0.006452615388086267, 0.00864398982620519, 0.0094795711467306, 0.010680285237982876, 0.009818014995078454, 0.011597404023403935, 0.008865453915910239, 0.009963132247387482, 0.009493024603997372, 0.010108361980090923, 0.010508297397146238, 0.011178855356936928, 0.010592516041314589, 0.010529358922427762, 0.011138016303097045, 0.010162810057737014, 0.00896438759018835, 0.01079108449468492, 0.011738897214837922, 0.009975813477617794, 0.011548199558178738, 0.010455867276212947, 0.009657791408614929, 0.011313803766784094, 0.011237289398243484, 0.010040034526488642, 0.01045297820557931, 0.009995395845117017, 0.010109112076351864, 0.011365300194538866, 0.009452188937110287, 0.010959739391615491, 0.010962738884687668, 0.010434111389522384, 0.010428166694359448, 0.011945347292849892, 0.01365043701735465, 0.009503180496871278, 0.009413943718938162, 0.010197965610999726, 0.012779526278941395, 0.01171192892277539, 0.012638813923461247, 0.011615160771220453, 0.012709397134621247, 0.01097616379296938, 0.010935703049524906, 0.011270621133626809, 0.011471021443683362, 0.012052991694812087, 0.011571555365069304, 0.009950262688351336, 0.011881042163009737, 0.011904311129020073, 0.011173935667230104, 0.011454005227251632, 0.011389744792077603, 0.011431764360415259, 0.012971523633923938, 0.012332614630254726, 0.011609537919944306, 0.011604849275104424, 0.010759736509135706, 0.010864057718564519, 0.011231567215069665, 0.010232737218102896, 0.012730457700969337, 0.010805254888010173, 0.012291786040602262, 0.012276403751759762, 0.012369129019488685, 0.012347047227192607, 0.012804788453521166, 0.010743424578902443, 0.011685553673874301, 0.011755700234820164, 0.011278657816076763, 0.009540848561641358, 0.010861200652113078, 0.010636061614888904, 0.011128980641905338, 0.011007476291577353, 0.012082574731163455, 0.010498257035573581, 0.01113063569488999, 0.012061871005108062, 0.011223883507026767, 0.011117868252400546, 0.011078867346763228, 0.009234199259459652, 0.011046401853309884, 0.010944966752444291, 0.010093156223970742, 0.011762242430275738, 0.01067839398273894, 0.01005246349867175, 0.010644949972940888, 0.010272383799572604, 0.011785428070547896, 0.012114757475011702, 0.011459449103590065, 0.010290543990279656, 0.010221774573360057, 0.010963271110298718, 0.010431633393430709, 0.009728575579447764, 0.010994378044144878, 0.010081187030172288, 0.009802082245676805, 0.009866042647053874, 0.010328328499512355, 0.010973610200610414, 0.010532267397017363], "moving_avg_accuracy_train": [0.05448888903308415, 0.11466107080853172, 0.17384886406567224, 0.23074900341238114, 0.2819197094411135, 0.32868011800743036, 0.37342842132527354, 0.41424572679102967, 0.45094660472216114, 0.4867228548719458, 0.520981129266165, 0.5496447170651392, 0.5775039204924385, 0.6014330861674472, 0.6254176809225648, 0.6475061204426653, 0.6668228497547573, 0.6845102475785156, 0.6997107951259427, 0.7160575486755946, 0.7315044099429096, 0.7459645126513226, 0.7609384171447728, 0.7735011116579164, 0.7843841793923185, 0.7966971125628135, 0.8074673987686953, 0.8168095588837508, 0.8261336116182532, 0.8347621358673267, 0.8437064778938277, 0.8515682289082009, 0.8589787704449833, 0.8659129282553668, 0.8723838600168549, 0.8784589949177165, 0.8843147719844073, 0.8895172896872217, 0.8940159770102584, 0.899218105459334, 0.9032352808457096, 0.9066880143255998, 0.9107087893049999, 0.9145461228721928, 0.9178952355815129, 0.9209419170055967, 0.9240372808086823, 0.9266184590874025, 0.928985769463269, 0.9317626680729592, 0.934071250668118, 0.9361281207621127, 0.9382487687668815, 0.9405386763759352, 0.9427135976133879, 0.9440944619199708, 0.9457372014399523, 0.9477897985197943, 0.9493651295297566, 0.9509782759875414, 0.9524741774804724, 0.9538623415026817, 0.9555279628572123, 0.956652601020319, 0.9582135825838002, 0.9594836073599808, 0.9605847048823345, 0.9616430859191103, 0.9627329207784266, 0.9635417471887254, 0.9645439503710618, 0.9658784109137359, 0.9665841687057142, 0.9675611475934945, 0.968252163636563, 0.9691577822789053, 0.9701378164760331, 0.9708198123581917, 0.9717150237557244, 0.9725788787825607, 0.9736051392293322, 0.9740381311837985, 0.9749183942928088, 0.9759129911349842, 0.9767546138215042, 0.977493473048896, 0.9782909077380539, 0.9788180809511902, 0.9792715023572708, 0.979670317076324, 0.9802245267746532, 0.9806465855924351, 0.981205511035591, 0.9814574278630026, 0.9819863863040925, 0.982592621185588, 0.9832196127872673, 0.9836490826466451, 0.9841262863236565, 0.9847139158008331, 0.9851055625017113, 0.9854743205741685, 0.9859014978917517, 0.9862697535335474, 0.9866430362897349, 0.9869045499595802, 0.9872212564219556, 0.9874807516500074, 0.9876701555766918, 0.9880800733892698, 0.9884094358420096, 0.9886570699732941, 0.9888450274104884, 0.9891025447587252, 0.9893041194864333, 0.989650586258028, 0.9899670566500824, 0.9901844506874552, 0.9904452094877574, 0.9905775858604102, 0.990817632333893, 0.9909941466302656, 0.991197187324382, 0.9914078617836197, 0.9916067333433529, 0.9917927292423602, 0.9920275188181241, 0.9922016270553593, 0.9923048460462519, 0.9924860987928172, 0.9924911521944971, 0.9925980068036281, 0.9926569375220748, 0.9928146068651054, 0.9929588344226424, 0.9930839889268067, 0.9932082537246023, 0.9933340429354753, 0.9934728659109846, 0.9935652184567909, 0.9936599614920641, 0.9937336044797626, 0.9938394467472716, 0.9938788651677826, 0.9939399183831472, 0.993957663896023, 0.9940201738826204, 0.9940810471193584, 0.9941149427419556, 0.9942035414737125, 0.9942135258680078, 0.9942318124181119, 0.994294773289396, 0.9943630638175992, 0.9944501019298868, 0.9945400619749933, 0.9946349769084464, 0.9946576573795157, 0.9946478068201356, 0.994729622120265, 0.9947846546999051, 0.9948434846168193, 0.9948662046075183, 0.9948866525991473, 0.9949841108511374, 0.9950555472362618, 0.9950733370066831, 0.9950963232464909, 0.9951053851182704, 0.9950809887195387, 0.9950915840440133, 0.9950964695384215, 0.9950869155905318, 0.9951434212040976, 0.9951617241729735, 0.9951991231842476, 0.9952048805086801, 0.9952426141840025, 0.9953137768727451, 0.9953336454652325, 0.9953399014544235, 0.9954106360113621, 0.9954231438387973, 0.9954041739489652, 0.9954382543219258, 0.9954014973421141, 0.9954707226079027, 0.9954888475197314, 0.9955121353868059, 0.9955284441695539, 0.9955500975204556, 0.9955626100898386, 0.9955831719975214, 0.9956179537561026, 0.9956004292138256, 0.995645110994824, 0.9956550976631987, 0.9956943125992598, 0.9956738024702861, 0.995673944544686, 0.9956810478580745, 0.995668839649648, 0.9956648277084927, 0.9956472660685959], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 310334223, "moving_var_accuracy_train": [0.026721351252537832, 0.056635439263841604, 0.08250064917330756, 0.1033892169750508, 0.11661626567685622, 0.12463346139277051, 0.13019181110192468, 0.13216710182109584, 0.13107298160722855, 0.12948514411952536, 0.12709929398779937, 0.12178377597860492, 0.11659061532117723, 0.11008499851817762, 0.10425384573646479, 0.09821955360671654, 0.09175582252789456, 0.08539583665108812, 0.07893576279765366, 0.07344713368240582, 0.06824987002127034, 0.06330673415218398, 0.058994021078976305, 0.054515010611953796, 0.05012948002056269, 0.0464810069278561, 0.04287689781967997, 0.039374691638249984, 0.03621966410898702, 0.03326776057453997, 0.030660995805669264, 0.028151160386210322, 0.025830289480404677, 0.02368000343321794, 0.02168885971065266, 0.01985213911616039, 0.0181755363300374, 0.01660157841106653, 0.015123564258633945, 0.013854767096376694, 0.01261452966950314, 0.011460369018901212, 0.010459831799925825, 0.00954637478008639, 0.00869268630553531, 0.007906958084280498, 0.0072024937695135205, 0.006542206724321001, 0.005938423477630073, 0.005413981622863553, 0.004920549442965232, 0.0044665709299208396, 0.004060388168569924, 0.003701542443434956, 0.003373960740593562, 0.0030537257426329592, 0.0027726405065442456, 0.0025332948488394035, 0.002302300374074001, 0.002095490510114973, 0.0019060809505924542, 0.001732815849706216, 0.0015845029152056112, 0.0014374359226662953, 0.0013156223013734202, 0.001198576737625094, 0.001089630805646186, 0.0009907492588526266, 0.000902363993152594, 0.000818015395295305, 0.0007452535567339413, 0.0006867552655201358, 0.0006225625855165636, 0.0005688967166894245, 0.0005163045735664835, 0.0004720554223380552, 0.00043349408335211143, 0.00039433074046643185, 0.00036211029743623944, 0.00033261547725912705, 0.00030883282407468543, 0.0002796368799609089, 0.000258646960234579, 0.00024168527011730816, 0.00022389170182376424, 0.0002064157482625066, 0.0001914972921875089, 0.000174848767338594, 0.00015921420934816432, 0.00014472426703454987, 0.0001330161758385943, 0.00012131776106574277, 0.00011199756381823169, 0.00010136896622780734, 9.375024289662914e-05, 8.768290519084318e-05, 8.245268088894614e-05, 7.586741204107694e-05, 7.033018098114789e-05, 6.640493850505493e-05, 6.11449288993292e-05, 5.625427865341703e-05, 5.227117493399311e-05, 4.826456740002285e-05, 4.469217080462321e-05, 4.0838458319804307e-05, 3.7657339337616446e-05, 3.4497645364289534e-05, 3.1370745454851743e-05, 2.974596442698625e-05, 2.774768461175988e-05, 2.5524820117376962e-05, 2.3290290089409487e-05, 2.1558097742255105e-05, 1.976797930568511e-05, 1.8871534389490296e-05, 1.7885762531964467e-05, 1.652252778613487e-05, 1.5482231374936258e-05, 1.4091719773773123e-05, 1.320114858127954e-05, 1.2161449394566835e-05, 1.1316334166315185e-05, 1.0584154299659505e-05, 9.881687945130225e-06, 9.204869420644923e-06, 8.780517782567318e-06, 8.175289108768942e-06, 7.453647638620105e-06, 7.003955897995224e-06, 6.303790140012546e-06, 5.776172293444036e-06, 5.229810530289443e-06, 4.930566072845847e-06, 4.624723760739269e-06, 4.3032242338789134e-06, 4.011877470231454e-06, 3.753096053357027e-06, 3.5512328147846615e-06, 3.2728704677583496e-06, 3.026369605577687e-06, 2.7725422517541716e-06, 2.596111296902086e-06, 2.3504844740920653e-06, 2.148983482640066e-06, 1.9369192634210712e-06, 1.778394822898632e-06, 1.6339052991673657e-06, 1.4808549883318985e-06, 1.4034171069189653e-06, 1.2639725893920863e-06, 1.1405849116852267e-06, 1.062203062332316e-06, 9.979551222796128e-07, 9.663403069671099e-07, 9.425415637105564e-07, 9.293670086710735e-07, 8.410599417153185e-07, 7.578272492247027e-07, 7.422882143194526e-07, 6.953166562841733e-07, 6.569336227730088e-07, 5.958860422919829e-07, 5.400605213177691e-07, 5.715374671145701e-07, 5.603121344798284e-07, 5.071292044166751e-07, 4.611715889595564e-07, 4.1579348774493026e-07, 3.795707974101706e-07, 3.4262406577567244e-07, 3.0857647169861767e-07, 2.7854032581127907e-07, 2.794222525102528e-07, 2.5449501528629336e-07, 2.416336881561692e-07, 2.1776864040212877e-07, 2.088062486419917e-07, 2.33502778199489e-07, 2.137053490864114e-07, 1.9268705078459604e-07, 2.1844874361383635e-07, 1.980118809767961e-07, 1.8144940336129863e-07, 1.7375770941538474e-07, 1.6854161855769543e-07, 1.9481669351343094e-07, 1.782916360212969e-07, 1.6534339519507132e-07, 1.5120284322805078e-07, 1.4030236735271486e-07, 1.2768121015052475e-07, 1.1871821756347653e-07, 1.177343323771326e-07, 1.087248853775599e-07, 1.1582055081849781e-07, 1.0513609764369157e-07, 1.084627887717961e-07, 1.0140249840923683e-07, 9.126243023452926e-08, 8.259030076093826e-08, 7.567263386170825e-08, 6.825023152203823e-08, 6.420090913264743e-08], "duration": 117400.242194, "accuracy_train": [0.5448888903308416, 0.65621070678756, 0.7065390033799372, 0.7428502575327611, 0.7424560636997047, 0.749523795104282, 0.7761631511858619, 0.781601475982835, 0.781254506102344, 0.8087091062200074, 0.8293055988141381, 0.8076170072559062, 0.8282367513381322, 0.8167955772425249, 0.8412790337186231, 0.8463020761235696, 0.8406734135635843, 0.8436968279923404, 0.8365157230527871, 0.8631783306224622, 0.8705261613487449, 0.8761054370270396, 0.895703557585825, 0.8865653622762089, 0.882331789001938, 0.9075135110972684, 0.9043999746216316, 0.9008889999192506, 0.9100500862287744, 0.9124188541089886, 0.9242055561323367, 0.92232398803756, 0.9256736442760245, 0.9283203485488187, 0.9306222458702473, 0.9331352090254706, 0.9370167655846253, 0.9363399490125508, 0.9345041629175894, 0.9460372615010151, 0.9393898593230897, 0.9377626156446106, 0.9468957641196014, 0.9490821249769288, 0.9480372499653931, 0.9483620498223514, 0.9518955550364526, 0.9498490635958842, 0.9502915628460686, 0.9567547555601699, 0.9548484940245479, 0.9546399516080657, 0.9573346008098007, 0.9611478448574198, 0.9622878887504615, 0.9565222406792175, 0.960521857119786, 0.9662631722383721, 0.9635431086194168, 0.9654965941076044, 0.9659372909168512, 0.9663558177025655, 0.9705185550479882, 0.9667743444882798, 0.9722624166551311, 0.9709138303456073, 0.970494582583518, 0.9711685152500923, 0.9725414345122739, 0.9708211848814139, 0.9735637790120893, 0.9778885557978036, 0.972935988833518, 0.976353957583518, 0.9744713080241787, 0.9773083500599853, 0.9789581242501846, 0.9769577752976191, 0.979771926333518, 0.9803535740240864, 0.9828414832502769, 0.9779350587739941, 0.9828407622739018, 0.9848643627145626, 0.9843292180001846, 0.9841432060954227, 0.9854678199404762, 0.9835626398694168, 0.9833522950119971, 0.9832596495478036, 0.9852124140596161, 0.9844451149524732, 0.9862358400239941, 0.9837246793097084, 0.9867470122739018, 0.9880487351190477, 0.9888625372023809, 0.9875143113810447, 0.9884211194167589, 0.9900025810954227, 0.9886303828096161, 0.9887931432262828, 0.98974609375, 0.9895840543097084, 0.9900025810954227, 0.9892581729881875, 0.9900716145833334, 0.9898162087024732, 0.9893747909168512, 0.9917693337024732, 0.9913736979166666, 0.9908857771548542, 0.9905366443452381, 0.9914202008928571, 0.9911182920358066, 0.9927687872023809, 0.9928152901785714, 0.9921409970238095, 0.9927920386904762, 0.9917689732142857, 0.9929780505952381, 0.9925827752976191, 0.9930245535714286, 0.9933039319167589, 0.9933965773809523, 0.9934666923334257, 0.994140625, 0.9937686011904762, 0.9932338169642857, 0.9941173735119048, 0.9925366328096161, 0.9935596982858066, 0.9931873139880952, 0.9942336309523809, 0.9942568824404762, 0.9942103794642857, 0.9943266369047619, 0.9944661458333334, 0.9947222726905685, 0.9943963913690477, 0.9945126488095238, 0.9943963913690477, 0.9947920271548542, 0.9942336309523809, 0.9944893973214286, 0.9941173735119048, 0.9945827637619971, 0.99462890625, 0.9944200033453304, 0.9950009300595238, 0.9943033854166666, 0.9943963913690477, 0.9948614211309523, 0.9949776785714286, 0.9952334449404762, 0.9953497023809523, 0.9954892113095238, 0.99486178161914, 0.9945591517857143, 0.9954659598214286, 0.9952799479166666, 0.9953729538690477, 0.9950706845238095, 0.9950706845238095, 0.9958612351190477, 0.9956984747023809, 0.9952334449404762, 0.9953031994047619, 0.9951869419642857, 0.9948614211309523, 0.9951869419642857, 0.9951404389880952, 0.9950009300595238, 0.9956519717261905, 0.9953264508928571, 0.9955357142857143, 0.9952566964285714, 0.9955822172619048, 0.9959542410714286, 0.9955124627976191, 0.9953962053571429, 0.9960472470238095, 0.9955357142857143, 0.9952334449404762, 0.9957449776785714, 0.9950706845238095, 0.99609375, 0.9956519717261905, 0.9957217261904762, 0.9956752232142857, 0.9957449776785714, 0.9956752232142857, 0.9957682291666666, 0.9959309895833334, 0.9954427083333334, 0.9960472470238095, 0.9957449776785714, 0.9960472470238095, 0.9954892113095238, 0.9956752232142857, 0.9957449776785714, 0.9955589657738095, 0.9956287202380952, 0.9954892113095238], "end": "2016-01-25 16:09:05.136000", "learning_rate_per_epoch": [0.0012961223255842924, 0.0012562524061650038, 0.001217608922161162, 0.0011801541550084949, 0.001143851550295949, 0.001108665717765689, 0.0010745621984824538, 0.0010415076976642013, 0.0010094699682667851, 0.0009784178109839559, 0.0009483208414167166, 0.000919149664696306, 0.0008908758172765374, 0.0008634717087261379, 0.0008369105635210872, 0.0008111664792522788, 0.0007862143102101982, 0.0007620296673849225, 0.0007385889766737819, 0.0007158693042583764, 0.0006938485312275589, 0.0006725051207467914, 0.0006518182926811278, 0.0006317677907645702, 0.0006123340572230518, 0.0005934981163591146, 0.0005752415745519102, 0.0005575466202571988, 0.0005403960240073502, 0.0005237729637883604, 0.0005076612578704953, 0.0004920451319776475, 0.0004769093939103186, 0.00046223922981880605, 0.0004480203497223556, 0.00043423884199000895, 0.0004208812606520951, 0.00040793459629639983, 0.00039538615965284407, 0.00038322372711263597, 0.0003714354243129492, 0.0003600097552407533, 0.0003489355440251529, 0.0003382019931450486, 0.0003277985961176455, 0.00031771522480994463, 0.00030794201302342117, 0.00029846944380551577, 0.0002892882621381432, 0.00028038950404152274, 0.0002717644674703479, 0.0002634047414176166, 0.00025530217681080103, 0.0002474488574080169, 0.00023983711434993893, 0.00023245951160788536, 0.00022530884598381817, 0.00021837814711034298, 0.0002116606483468786, 0.000205149786779657, 0.00019883920322172344, 0.00019272272766102105, 0.00018679440836422145, 0.00018104845366906375, 0.00017547924653626978, 0.00017008134454954416, 0.00016484949446748942, 0.0001597785740159452, 0.00015486365009564906, 0.00015009990602266043, 0.00014548269973602146, 0.00014100752014201134, 0.00013667000166606158, 0.0001324659097008407, 0.00012839114060625434, 0.00012444172170944512, 0.0001206137822009623, 0.00011690359679050744, 0.00011330754205118865, 0.00010982210369547829, 0.00010644387657521293, 0.00010316957195755094, 9.99959884211421e-05, 9.692002640804276e-05, 9.393868094775826e-05, 9.104904165724292e-05, 8.824829274090007e-05, 8.553369843866676e-05, 8.290260302601382e-05, 8.035244536586106e-05, 7.78807298047468e-05, 7.548504800070077e-05, 7.31630643713288e-05, 7.091250154189765e-05, 6.873116944916546e-05, 6.661693623755127e-05, 6.456774281105027e-05, 6.258158100536093e-05, 6.0656515415757895e-05, 5.87906688451767e-05, 5.698221502825618e-05, 5.5229393183253706e-05, 5.353048982215114e-05, 5.1883846026612446e-05, 5.028785381000489e-05, 4.874095611739904e-05, 4.7241643187589943e-05, 4.578844891511835e-05, 4.4379954488249496e-05, 4.30147883889731e-05, 4.169161547906697e-05, 4.0409144276054576e-05, 3.916612331522629e-05, 3.796133751166053e-05, 3.679361179820262e-05, 3.566180748748593e-05, 3.456481863395311e-05, 3.350157567183487e-05, 3.247103813919239e-05, 3.14722019538749e-05, 3.0504088499583304e-05, 2.956575553980656e-05, 2.865628630388528e-05, 2.7774793124990538e-05, 2.6920415621134453e-05, 2.60923206951702e-05, 2.5289697077823803e-05, 2.4511764422641136e-05, 2.3757760573062114e-05, 2.3026950657367706e-05, 2.231862163171172e-05, 2.163208046113141e-05, 2.0966657757526264e-05, 2.032170414167922e-05, 1.969659024325665e-05, 1.9090704881818965e-05, 1.8503458704799414e-05, 1.7934276911546476e-05, 1.7382602891302668e-05, 1.6847898223204538e-05, 1.6329642676282674e-05, 1.5827328752493486e-05, 1.5340465324698016e-05, 1.4868579455651343e-05, 1.4411209122044966e-05, 1.3967907761980314e-05, 1.3538242455979344e-05, 1.3121793926984537e-05, 1.2718155630864203e-05, 1.2326933756412473e-05, 1.1947746315854602e-05, 1.1580223144846968e-05, 1.1224004992982373e-05, 1.0878744433284737e-05, 1.0544104952714406e-05, 1.021975913317874e-05, 9.905390470521525e-06, 9.600691555533558e-06, 9.305365892942064e-06, 9.019124263431877e-06, 8.741688361624256e-06, 8.47278624860337e-06, 8.212155989895109e-06, 7.959542926982976e-06, 7.714700586802792e-06, 7.477389772247989e-06, 7.247378562169615e-06, 7.024442766123684e-06, 6.808364560129121e-06, 6.598933396162465e-06, 6.395944183168467e-06, 6.199199106049491e-06, 6.008506261423463e-06, 5.823679202876519e-06, 5.644537850457709e-06, 5.4709066716895904e-06, 5.302616955304984e-06, 5.139503628015518e-06, 4.981407982995734e-06, 4.828175406146329e-06, 4.679656285588862e-06, 4.535706011665752e-06, 4.3961836126982234e-06, 4.260953119228361e-06, 4.129882654524408e-06, 4.002843979833415e-06, 3.879712949128589e-06, 3.760369509109296e-06, 3.6446972444537096e-06, 3.5325831504451344e-06, 3.423917860345682e-06, 3.318595190648921e-06, 3.21651236845355e-06, 3.1175695767160505e-06, 3.021670408998034e-06, 2.928721187345218e-06, 2.8386311896611005e-06, 2.7513124223332852e-06, 2.666679620233481e-06, 2.5846502467175014e-06, 2.5051440388779156e-06, 2.4280836896650726e-06, 2.3533937110187253e-06, 2.281001343362732e-06], "accuracy_valid": [0.5389727856739458, 0.6549851750753012, 0.7028087937688253, 0.7366031508847892, 0.7344161803463856, 0.74072265625, 0.7618114057793675, 0.7620540756777108, 0.7565506165286144, 0.7886771696159638, 0.8017283979668675, 0.7742714020143072, 0.7931834760918675, 0.7875579466302711, 0.804577195500753, 0.8077716137989458, 0.7953395613704819, 0.8017489881400602, 0.7972823912838856, 0.818859422063253, 0.8193991787462349, 0.8244143566453314, 0.8334887401167168, 0.8233363140060241, 0.8230612881212349, 0.8386774637612951, 0.8370905496987951, 0.8361639919051205, 0.8376597209149097, 0.8413218302899097, 0.8512815912085843, 0.8454016260353916, 0.8487592949924698, 0.8483416086219879, 0.8460634530308735, 0.8498579278049698, 0.8530008706701807, 0.8503050287085843, 0.8488916603915663, 0.8587381753576807, 0.8490740304969879, 0.8462664133094879, 0.8536318124058735, 0.8594205925263554, 0.8527876153049698, 0.8530008706701807, 0.855381977127259, 0.8512007012424698, 0.857090961502259, 0.8549745858433735, 0.8532758965549698, 0.8550966561558735, 0.8577822030308735, 0.8585352150790663, 0.8572733316076807, 0.8567953454442772, 0.8580160485692772, 0.8634797980986446, 0.8570600762424698, 0.8601221291415663, 0.8602236092808735, 0.863194477127259, 0.866856586502259, 0.8585455101656627, 0.8643137001129518, 0.862828266189759, 0.8601118340549698, 0.8621267295745482, 0.8627370811370482, 0.8622487998870482, 0.8629915168486446, 0.8658506094691265, 0.8598882836031627, 0.8641813347138554, 0.8621973244540663, 0.8672330925263554, 0.8626753106174698, 0.866419780685241, 0.8688405967620482, 0.8678640342620482, 0.8642828148531627, 0.865321147872741, 0.8687802969691265, 0.863438617752259, 0.868250835372741, 0.8693700583584337, 0.8665918557040663, 0.8685861610504518, 0.8674978233245482, 0.8651270119540663, 0.865757953689759, 0.8634283226656627, 0.8676301887236446, 0.8665315559111446, 0.867344867752259, 0.8693185829254518, 0.869593608810241, 0.8684846809111446, 0.8689729621611446, 0.8707937217620482, 0.867955219314759, 0.8699701148343373, 0.868321430252259, 0.8678640342620482, 0.8716790992093373, 0.8700715949736446, 0.8681890648531627, 0.8685449807040663, 0.8674360528049698, 0.8696436135165663, 0.8662256447665663, 0.8708040168486446, 0.8712922980986446, 0.869786274002259, 0.871129047439759, 0.870030414627259, 0.8689214867281627, 0.873499858810241, 0.8720041298004518, 0.8734895637236446, 0.8716379188629518, 0.871617328689759, 0.8720144248870482, 0.8712614128388554, 0.871739399002259, 0.8707834266754518, 0.8704878106174698, 0.8726953713290663, 0.8701421898531627, 0.871495258377259, 0.8702642601656627, 0.870518695877259, 0.8699289344879518, 0.8712408226656627, 0.8706201760165663, 0.8716276237763554, 0.869664203689759, 0.8687685311558735, 0.8701421898531627, 0.8697656838290663, 0.8698774590549698, 0.8686567559299698, 0.872349750564759, 0.871983539627259, 0.8708746117281627, 0.8711393425263554, 0.872349750564759, 0.8724821159638554, 0.8719629494540663, 0.871617328689759, 0.870762836502259, 0.8721159050263554, 0.8734689735504518, 0.8742116905120482, 0.8720041298004518, 0.8722070900790663, 0.8718511742281627, 0.8705995858433735, 0.8696230233433735, 0.871495258377259, 0.872838031814759, 0.871495258377259, 0.872471820877259, 0.871495258377259, 0.8724821159638554, 0.8715967385165663, 0.8729498070406627, 0.8721262001129518, 0.8719938347138554, 0.8728483269013554, 0.8736013389495482, 0.8716070336031627, 0.8727365516754518, 0.8726144813629518, 0.872349750564759, 0.871739399002259, 0.8712305275790663, 0.8720953148531627, 0.8714849632906627, 0.8704981057040663, 0.8705084007906627, 0.8711084572665663, 0.8714849632906627, 0.871251117752259, 0.8713628929781627, 0.872349750564759, 0.8719732445406627, 0.8725835961031627, 0.872105609939759, 0.8724512307040663, 0.8731939476656627, 0.8722173851656627, 0.873570453689759, 0.8724615257906627, 0.8726041862763554, 0.872471820877259, 0.8723600456513554, 0.872227680252259, 0.872349750564759, 0.872227680252259, 0.872349750564759, 0.8729703972138554, 0.8723394554781627, 0.872349750564759], "accuracy_test": 0.8603754783163264, "start": "2016-01-24 07:32:24.894000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0], "accuracy_train_last": 0.9954892113095238, "batch_size_eval": 1024, "accuracy_train_std": [0.018918512168442887, 0.020397211876716777, 0.01959036245169802, 0.018684235381536202, 0.01867646639213601, 0.01921280994167372, 0.02054740655134815, 0.021401996744966463, 0.0194576109247759, 0.019425609805297114, 0.018753733530262655, 0.01957706841319077, 0.01984734964600612, 0.021609152494902018, 0.020315820864314264, 0.018259207437231476, 0.02114499205100607, 0.019321074531905364, 0.01824074905589415, 0.0187716062917423, 0.019246166102205985, 0.017043226352156018, 0.01916086437199137, 0.019754193553968742, 0.020693931908051057, 0.01766528335475681, 0.017290413597079748, 0.017243073582851987, 0.016919892556530217, 0.018121237935024383, 0.017425100944187238, 0.01648146794673172, 0.015826759686191717, 0.016952953826961423, 0.016639885375708107, 0.013958418407580265, 0.01473837019458187, 0.016521767760418575, 0.01549175668690756, 0.013360659706787337, 0.014564034207850409, 0.013115270285938497, 0.01415959342255365, 0.014191889876987154, 0.012718520997955576, 0.0128893424675745, 0.012477368421366531, 0.011681646683740173, 0.013540722694570904, 0.012895095353636494, 0.012568707318569036, 0.01248123078177674, 0.011014049633156392, 0.011346847419099328, 0.011406854054608053, 0.011626458926810728, 0.010289122195818623, 0.00994426278763149, 0.00997380336966608, 0.009889544463160139, 0.01132892754710995, 0.010088476307178407, 0.009275792197700219, 0.011169992640308223, 0.009031982182451813, 0.00981870027260167, 0.008525066029241437, 0.009512781809482061, 0.009087434865927245, 0.009133032504213538, 0.008638205790661069, 0.00734016731768932, 0.009163499863786518, 0.00836050865899824, 0.008819233629265934, 0.008156151675654488, 0.007415101327920203, 0.008010702886378201, 0.007735989682546483, 0.007191601127772928, 0.007353402050128786, 0.008296366641744494, 0.0071489266283899875, 0.005850427647397142, 0.005691602359555763, 0.006103057902944876, 0.006379599901682205, 0.006412487816542946, 0.006325926472266257, 0.006238434138379694, 0.005981244819138841, 0.006777924413652834, 0.006009074115061269, 0.006364838740734654, 0.005735302737175052, 0.005166645986730338, 0.005128047040901059, 0.005385671892863229, 0.0049691235535385255, 0.003969696231995492, 0.004868706594461015, 0.0044923145233249796, 0.0036555155170453052, 0.004218553295936378, 0.003963972138100014, 0.004305214467402899, 0.004469671482330076, 0.004242169269126394, 0.003977769139459316, 0.0034245568034810304, 0.0037089585562487893, 0.0034148481654435727, 0.003432804849583129, 0.003706625602568261, 0.0034881301044008043, 0.0030141857433966467, 0.003209463468283298, 0.0035972990338195157, 0.002736967814278676, 0.0032816738773023347, 0.003296139350198912, 0.0032602699009468524, 0.0030405266068224564, 0.00310024703630519, 0.002801394248852127, 0.0032498160308269226, 0.0028828094754145958, 0.0033556336241217367, 0.003061524417569403, 0.0029489084205299824, 0.0030653503621754885, 0.0026627770604835087, 0.0026315302445379897, 0.0024091952840380616, 0.002567895018950085, 0.0028141985987881687, 0.0027558653999309995, 0.0025003729648009794, 0.002531471418931478, 0.00279530855908382, 0.0029215566498316725, 0.002512990026213568, 0.0023022907946275255, 0.0025287529993159455, 0.002608419047511757, 0.0026828026025099653, 0.002257865560656331, 0.0022678248056652982, 0.0023858391549351756, 0.0023168798276618353, 0.0027532157646420466, 0.0025754631254690783, 0.0023926447828565597, 0.0023689147371558834, 0.002046263058844103, 0.0018797789184717018, 0.002234563574541288, 0.0022157653065220943, 0.002402903772835531, 0.002243377026419696, 0.0023238696258062576, 0.0021633882257127224, 0.0024847563257798634, 0.002565683449429636, 0.0022933087808664344, 0.0021422937449799965, 0.0022872894324436296, 0.0022374649783704257, 0.002279238906359001, 0.0022558737064220925, 0.002218659869413179, 0.00230001764585008, 0.0024870398597880475, 0.0022872894324436296, 0.0021995711107295887, 0.002276746961475753, 0.0022102370979010686, 0.002365717511528442, 0.002115757657380455, 0.002152866821813748, 0.002288116555086039, 0.0020545686415898924, 0.0022567123434804556, 0.002111793285836968, 0.0018433312849217443, 0.0021622633812430564, 0.001964716348054093, 0.002175350375967552, 0.0021411578186795645, 0.002143176826972268, 0.002474399722585654, 0.0023064726130277613, 0.0022719928385986064, 0.0022544353170900317, 0.002251915925000605, 0.0020545686415898924, 0.0022128039440062557, 0.0021517364771687637, 0.0022243788584638017, 0.0019778804239338485, 0.0019393748290259271, 0.0020901781638736146, 0.0023074100130311154, 0.002556818042018466], "accuracy_test_std": 0.006264656926391066, "error_valid": [0.4610272143260542, 0.3450148249246988, 0.2971912062311747, 0.2633968491152108, 0.26558381965361444, 0.25927734375, 0.23818859422063254, 0.23794592432228923, 0.24344938347138556, 0.2113228303840362, 0.19827160203313254, 0.22572859798569278, 0.20681652390813254, 0.21244205336972888, 0.19542280449924698, 0.1922283862010542, 0.2046604386295181, 0.19825101185993976, 0.20271760871611444, 0.18114057793674698, 0.1806008212537651, 0.17558564335466864, 0.1665112598832832, 0.17666368599397586, 0.1769387118787651, 0.16132253623870485, 0.16290945030120485, 0.16383600809487953, 0.1623402790850903, 0.1586781697100903, 0.14871840879141573, 0.1545983739646084, 0.15124070500753017, 0.15165839137801207, 0.1539365469691265, 0.15014207219503017, 0.1469991293298193, 0.14969497129141573, 0.15110833960843373, 0.1412618246423193, 0.15092596950301207, 0.15373358669051207, 0.1463681875941265, 0.1405794074736446, 0.14721238469503017, 0.1469991293298193, 0.14461802287274095, 0.14879929875753017, 0.14290903849774095, 0.1450254141566265, 0.14672410344503017, 0.1449033438441265, 0.1422177969691265, 0.14146478492093373, 0.1427266683923193, 0.14320465455572284, 0.14198395143072284, 0.1365202019013554, 0.14293992375753017, 0.13987787085843373, 0.1397763907191265, 0.13680552287274095, 0.13314341349774095, 0.14145448983433728, 0.13568629988704817, 0.13717173381024095, 0.13988816594503017, 0.13787327042545183, 0.13726291886295183, 0.13775120011295183, 0.1370084831513554, 0.1341493905308735, 0.14011171639683728, 0.1358186652861446, 0.13780267554593373, 0.1327669074736446, 0.13732468938253017, 0.13358021931475905, 0.13115940323795183, 0.13213596573795183, 0.13571718514683728, 0.13467885212725905, 0.1312197030308735, 0.13656138224774095, 0.13174916462725905, 0.13062994164156627, 0.13340814429593373, 0.13141383894954817, 0.13250217667545183, 0.13487298804593373, 0.13424204631024095, 0.13657167733433728, 0.1323698112763554, 0.1334684440888554, 0.13265513224774095, 0.13068141707454817, 0.13040639118975905, 0.1315153190888554, 0.1310270378388554, 0.12920627823795183, 0.13204478068524095, 0.13002988516566272, 0.13167856974774095, 0.13213596573795183, 0.12832090079066272, 0.1299284050263554, 0.13181093514683728, 0.13145501929593373, 0.13256394719503017, 0.13035638648343373, 0.13377435523343373, 0.1291959831513554, 0.1287077019013554, 0.13021372599774095, 0.12887095256024095, 0.12996958537274095, 0.13107851327183728, 0.12650014118975905, 0.12799587019954817, 0.1265104362763554, 0.12836208113704817, 0.12838267131024095, 0.12798557511295183, 0.1287385871611446, 0.12826060099774095, 0.12921657332454817, 0.12951218938253017, 0.12730462867093373, 0.12985781014683728, 0.12850474162274095, 0.12973573983433728, 0.12948130412274095, 0.13007106551204817, 0.12875917733433728, 0.12937982398343373, 0.1283723762236446, 0.13033579631024095, 0.1312314688441265, 0.12985781014683728, 0.13023431617093373, 0.13012254094503017, 0.13134324407003017, 0.12765024943524095, 0.12801646037274095, 0.12912538827183728, 0.1288606574736446, 0.12765024943524095, 0.1275178840361446, 0.12803705054593373, 0.12838267131024095, 0.12923716349774095, 0.1278840949736446, 0.12653102644954817, 0.12578830948795183, 0.12799587019954817, 0.12779290992093373, 0.12814882577183728, 0.1294004141566265, 0.1303769766566265, 0.12850474162274095, 0.12716196818524095, 0.12850474162274095, 0.12752817912274095, 0.12850474162274095, 0.1275178840361446, 0.12840326148343373, 0.12705019295933728, 0.12787379988704817, 0.1280061652861446, 0.1271516730986446, 0.12639866105045183, 0.12839296639683728, 0.12726344832454817, 0.12738551863704817, 0.12765024943524095, 0.12826060099774095, 0.12876947242093373, 0.12790468514683728, 0.12851503670933728, 0.12950189429593373, 0.12949159920933728, 0.12889154273343373, 0.12851503670933728, 0.12874888224774095, 0.12863710702183728, 0.12765024943524095, 0.12802675545933728, 0.12741640389683728, 0.12789439006024095, 0.12754876929593373, 0.12680605233433728, 0.12778261483433728, 0.12642954631024095, 0.12753847420933728, 0.1273958137236446, 0.12752817912274095, 0.1276399543486446, 0.12777231974774095, 0.12765024943524095, 0.12777231974774095, 0.12765024943524095, 0.1270296027861446, 0.12766054452183728, 0.12765024943524095], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.9662470721241054, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.001337257594526009, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "adam", "nb_data_augmentation": 1, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 3.3285003315341976e-05, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.03076088079936571}, "accuracy_valid_max": 0.8742116905120482, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.872349750564759, "loss_train": [2.1642751693725586, 1.5960009098052979, 1.3730690479278564, 1.2399739027023315, 1.141758918762207, 1.0672972202301025, 1.0122982263565063, 0.9661569595336914, 0.9286440014839172, 0.8931208252906799, 0.8669461011886597, 0.8365252614021301, 0.816771924495697, 0.7937130928039551, 0.7780144810676575, 0.7581438422203064, 0.7432766556739807, 0.7298758625984192, 0.7115253806114197, 0.6956319212913513, 0.6838657855987549, 0.6735022664070129, 0.661776602268219, 0.6514193415641785, 0.6374293565750122, 0.6294833421707153, 0.620106041431427, 0.6079748868942261, 0.5986222624778748, 0.5923395752906799, 0.5787734985351562, 0.5748222470283508, 0.5628922581672668, 0.5538305640220642, 0.5499743223190308, 0.5433938503265381, 0.53438401222229, 0.5287825465202332, 0.5207980871200562, 0.5164135694503784, 0.5136455297470093, 0.5064731240272522, 0.5029424428939819, 0.49583014845848083, 0.48776745796203613, 0.48249438405036926, 0.4808661639690399, 0.47416573762893677, 0.4663374125957489, 0.4659380614757538, 0.4585346281528473, 0.45539477467536926, 0.4520735442638397, 0.4479513168334961, 0.4462873637676239, 0.4413893222808838, 0.4362525939941406, 0.4333471655845642, 0.42687514424324036, 0.4233984053134918, 0.41804957389831543, 0.418455570936203, 0.4159417450428009, 0.40964996814727783, 0.4105801582336426, 0.40732917189598083, 0.40289920568466187, 0.40134698152542114, 0.39793360233306885, 0.39483320713043213, 0.3938060700893402, 0.3906153738498688, 0.38717666268348694, 0.3861347436904907, 0.3826621174812317, 0.37958306074142456, 0.37865304946899414, 0.376250684261322, 0.37539738416671753, 0.3702180087566376, 0.3709907829761505, 0.36828842759132385, 0.36828184127807617, 0.36298325657844543, 0.3643147945404053, 0.3609670102596283, 0.3588905334472656, 0.3573022782802582, 0.3543252944946289, 0.3554620146751404, 0.35433685779571533, 0.35207346081733704, 0.3507418632507324, 0.348480761051178, 0.3474692702293396, 0.34570932388305664, 0.3466079831123352, 0.3436860144138336, 0.34259432554244995, 0.3415984511375427, 0.34091392159461975, 0.3396845757961273, 0.33820897340774536, 0.3382602632045746, 0.3375125229358673, 0.33336153626441956, 0.3346126675605774, 0.33429527282714844, 0.33219385147094727, 0.33175384998321533, 0.3313768804073334, 0.3306666910648346, 0.3297058939933777, 0.3304653465747833, 0.32942652702331543, 0.32755398750305176, 0.32693466544151306, 0.3253293037414551, 0.3263022005558014, 0.32462579011917114, 0.32392555475234985, 0.3246403634548187, 0.32260972261428833, 0.3217400014400482, 0.3219146132469177, 0.322170227766037, 0.3203878402709961, 0.31881681084632874, 0.31941959261894226, 0.31842491030693054, 0.31734734773635864, 0.3181005120277405, 0.31800809502601624, 0.31888487935066223, 0.31617704033851624, 0.3150961995124817, 0.3150811493396759, 0.315304696559906, 0.3152047097682953, 0.3157200813293457, 0.31387433409690857, 0.31366464495658875, 0.31571829319000244, 0.3124247193336487, 0.3130651116371155, 0.3121768832206726, 0.31265074014663696, 0.31271371245384216, 0.31335315108299255, 0.3117944598197937, 0.310686856508255, 0.31026968359947205, 0.31018850207328796, 0.31172746419906616, 0.31164559721946716, 0.3088705539703369, 0.30848997831344604, 0.3085525333881378, 0.31244519352912903, 0.3091224730014801, 0.31022509932518005, 0.30831554532051086, 0.3083283305168152, 0.30740490555763245, 0.3063470721244812, 0.3083140552043915, 0.3073001801967621, 0.3107227385044098, 0.3064509928226471, 0.30802908539772034, 0.3067435026168823, 0.3068552613258362, 0.3064919710159302, 0.30612754821777344, 0.30688995122909546, 0.30702143907546997, 0.30463817715644836, 0.3050461709499359, 0.30576759576797485, 0.30586326122283936, 0.3059583604335785, 0.3057846128940582, 0.3044492304325104, 0.3072235882282257, 0.3058566451072693, 0.30537205934524536, 0.30584776401519775, 0.3046940267086029, 0.3043025732040405, 0.3059995472431183, 0.30486613512039185, 0.30287012457847595, 0.3016270101070404, 0.3032713234424591, 0.30497416853904724, 0.30524829030036926, 0.30460676550865173, 0.3049105405807495, 0.3049735426902771, 0.303632527589798, 0.3035900294780731, 0.30344080924987793, 0.30519309639930725, 0.3047773540019989], "accuracy_train_first": 0.5448888903308416, "model": "residualv3", "loss_std": [0.33925628662109375, 0.1543053239583969, 0.13474273681640625, 0.12525604665279388, 0.12680526077747345, 0.12188408523797989, 0.11861160397529602, 0.11530204862356186, 0.11509329825639725, 0.11128345131874084, 0.10879606753587723, 0.10841242223978043, 0.10387572646141052, 0.10271447151899338, 0.10163874179124832, 0.10089129209518433, 0.09690199792385101, 0.09568134695291519, 0.09556411951780319, 0.09440863132476807, 0.09499412029981613, 0.09373778849840164, 0.09100265055894852, 0.08757854998111725, 0.08657258003950119, 0.08619227260351181, 0.08867157995700836, 0.08217135816812515, 0.08667338639497757, 0.08017344027757645, 0.08109469711780548, 0.07870867848396301, 0.08022543787956238, 0.07590726017951965, 0.07486997544765472, 0.07384052127599716, 0.07310447841882706, 0.07116268575191498, 0.06902176886796951, 0.07036285847425461, 0.06921984255313873, 0.06877441704273224, 0.06792643666267395, 0.0685342401266098, 0.06590103358030319, 0.06610823422670364, 0.06591270118951797, 0.06235579401254654, 0.06409209221601486, 0.06319834291934967, 0.061125677078962326, 0.06110939010977745, 0.058904364705085754, 0.05971948802471161, 0.059487733989953995, 0.05859764665365219, 0.060814183205366135, 0.05761532112956047, 0.0554550364613533, 0.05570375546813011, 0.05392381176352501, 0.053891200572252274, 0.05632336065173149, 0.053600654006004333, 0.05328364297747612, 0.05472869053483009, 0.05231661722064018, 0.051384035497903824, 0.05072976276278496, 0.05264606699347496, 0.0508950874209404, 0.05279655382037163, 0.049009859561920166, 0.04916197434067726, 0.04874807596206665, 0.047218430787324905, 0.04744807630777359, 0.047511547803878784, 0.04949626326560974, 0.04752476513385773, 0.048092521727085114, 0.04776729270815849, 0.04700934886932373, 0.04443083330988884, 0.04628991335630417, 0.04564923048019409, 0.04507501423358917, 0.046924419701099396, 0.04491685703396797, 0.045875176787376404, 0.044595785439014435, 0.04559703171253204, 0.04551490396261215, 0.04292202368378639, 0.045694924890995026, 0.043745845556259155, 0.044292137026786804, 0.04330742359161377, 0.043303776532411575, 0.041437119245529175, 0.04298276826739311, 0.04254425689578056, 0.04392360523343086, 0.04447716847062111, 0.04250064492225647, 0.041279375553131104, 0.04252598434686661, 0.0424930714070797, 0.04237803444266319, 0.04225512221455574, 0.03893697261810303, 0.03796958550810814, 0.040786948055028915, 0.04114101454615593, 0.040889739990234375, 0.039622992277145386, 0.04214024171233177, 0.04046812653541565, 0.040768932551145554, 0.03982819616794586, 0.03962961211800575, 0.04042515903711319, 0.04111989587545395, 0.03868946060538292, 0.03884762153029442, 0.038771480321884155, 0.03969154134392738, 0.03870727866888046, 0.0396958664059639, 0.03877079486846924, 0.03788027912378311, 0.03957168385386467, 0.03883225843310356, 0.038457080721855164, 0.037840332835912704, 0.03738103434443474, 0.03814799338579178, 0.03871653601527214, 0.03869732469320297, 0.040458738803863525, 0.03749105706810951, 0.037695884704589844, 0.03903567045927048, 0.03914830833673477, 0.0389009565114975, 0.03730472922325134, 0.037169862538576126, 0.03788076341152191, 0.039039481431245804, 0.03873228654265404, 0.03839234262704849, 0.03732657805085182, 0.038222864270210266, 0.03635546565055847, 0.03926602005958557, 0.03712555021047592, 0.03650297224521637, 0.036231961101293564, 0.038522638380527496, 0.03496743366122246, 0.03813968971371651, 0.037995219230651855, 0.036904118955135345, 0.03750019893050194, 0.036375150084495544, 0.037843894213438034, 0.037256330251693726, 0.040090978145599365, 0.037272799760103226, 0.037956200540065765, 0.03813743591308594, 0.038115859031677246, 0.0356033518910408, 0.036523643881082535, 0.03723026439547539, 0.039009541273117065, 0.03763705492019653, 0.03557674214243889, 0.03678335249423981, 0.03774847835302353, 0.03668544068932533, 0.03643672168254852, 0.03740031644701958, 0.037278685718774796, 0.035521250218153, 0.03506869077682495, 0.03783855587244034, 0.03585617244243622, 0.0362168624997139, 0.03625214472413063, 0.03686400130391121, 0.03488975018262863, 0.0348949134349823, 0.03659331798553467, 0.03528454154729843, 0.03710635006427765, 0.03616537153720856, 0.03684159740805626, 0.0374143086373806, 0.03564145043492317, 0.03510098159313202, 0.03553725406527519, 0.03629361465573311, 0.036033745855093]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:13 2016", "state": "available"}], "summary": "423382c6ff54130c906620c3bd85877b"}