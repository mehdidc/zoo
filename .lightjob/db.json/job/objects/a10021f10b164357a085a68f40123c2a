{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 64, "f3": 16, "nonlin": "very_leaky_rectify", "nbg1": 2, "nbg3": 4, "nbg2": 2, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.01583586240271332, 0.017997793004246244, 0.013593076611982115, 0.018469040272168755, 0.01782345668203283, 0.01912499687515076, 0.018053979520153394, 0.021066146854246414, 0.019868475674673206, 0.01832682725581704, 0.01608557189476655, 0.01781510425483037, 0.01815441513210634, 0.017890873474629875, 0.018975196315622455, 0.01936233489174223, 0.01540895464718775, 0.014006072030205948, 0.013492785809905889, 0.01514228825261747, 0.013070800492736095, 0.014703586274162494, 0.014324628485635888, 0.013184756547357084, 0.013934533030738236, 0.013320538747599335, 0.015383830003790485, 0.012155690877554177, 0.01370442891281509, 0.013482930802500728, 0.013067281700068626, 0.014863258768213582, 0.014165802100746203, 0.011715565853965914, 0.013813523179057796, 0.01319617096239769, 0.012598359231409171, 0.013671596467227421, 0.011498724160500795, 0.012291334570567117, 0.011756039156062935, 0.011180483680431416, 0.012053760928048142, 0.011677203262812836, 0.011044257485079327, 0.012267430048998327, 0.011978888590537806, 0.012859169989632744, 0.011565660426979148, 0.011859790934571655, 0.013918776499756525, 0.012385316639651478, 0.014677888506518829, 0.014237032403018756, 0.010791619273135974, 0.012294219170667565, 0.014035392223077571, 0.011777476217798047, 0.013479396435724492, 0.01245322674453384, 0.012478077636347843, 0.012617484504263933, 0.012131931300280317, 0.01342017169543409, 0.011529285450737373, 0.011757973046558755, 0.010823262729321707, 0.010915429168213128, 0.011425909845095946, 0.012902322091536269, 0.012890512906273632, 0.0131921619190017, 0.012387653609421623, 0.013658564899804842, 0.012192083768293765, 0.013364834141368865, 0.01249604944193026, 0.01241866226598975, 0.013149020713644852, 0.012093652764979094, 0.014481347868725585, 0.014113457072239073, 0.013174621873282047, 0.01403599510032403, 0.014405136804519367, 0.013424082352426392, 0.013570933441678836, 0.013446611808174342, 0.01364713115879526, 0.01321661683310286, 0.013282713401845045, 0.013253416687699299, 0.013755294663265017, 0.013186608280149078, 0.01287512084062032, 0.013317584987379472, 0.012715905650027549, 0.013273438037136386, 0.013089818561157008, 0.013331262789770402, 0.012573249671017426, 0.01338760904194142, 0.01365897759242967, 0.014776003014274078, 0.014276559861812217, 0.013852426405995195, 0.011258081133768508, 0.01351490337301346, 0.01349788913503496, 0.014663035604903575, 0.013250129964487493, 0.01348726077541828, 0.01365782745386487, 0.013492135333873811, 0.013293208182465167, 0.013264453955839927, 0.01349085785676921, 0.013081010312497362, 0.013481617797470312, 0.014248457840359555], "moving_avg_accuracy_train": [0.05113831354974159, 0.10910263314991692, 0.16933687657172847, 0.2298595734487472, 0.28768893944735713, 0.34176947751743314, 0.3917296616351527, 0.43799122432799587, 0.48024508428606927, 0.5198404201554044, 0.5563550565901686, 0.5894274927743134, 0.6202132814721496, 0.6487318961370886, 0.6748497282045813, 0.6985068035914876, 0.7202958253825787, 0.741019655225504, 0.7598151892126896, 0.7771380347940047, 0.7929749534445414, 0.8075583153121582, 0.8209506970572897, 0.8336175717671661, 0.8450781047286463, 0.8556088592820829, 0.8651726049349469, 0.8740867875189254, 0.8822165807873817, 0.8897286351313549, 0.8967730800980551, 0.9031223811633234, 0.9089625624995105, 0.914650951038919, 0.9200099550029489, 0.92489823483488, 0.9295789215431141, 0.9336543558007628, 0.9375803021016851, 0.9411509642999237, 0.9446295551449867, 0.9477648651055249, 0.9507819205211906, 0.9536251896286322, 0.9561609163860532, 0.9584662859070086, 0.9608270757306211, 0.9628285536849676, 0.9648158957486412, 0.9665696263738048, 0.9684107257519282, 0.9699840098350964, 0.9714836348182718, 0.9728774751305107, 0.9741738561877347, 0.9754149678523222, 0.9766064091611746, 0.9777740013915134, 0.9787457793392944, 0.9796924230565739, 0.9805909053783158, 0.9813972143190741, 0.9822066337717181, 0.9828142035410026, 0.98345169713693, 0.984034705919684, 0.9846570700741626, 0.9851892960274791, 0.9856334582021398, 0.9861145843676677, 0.9865127206845, 0.9869175463458395, 0.9872934791362741, 0.9876829719214746, 0.9879963490960215, 0.9883388424221614, 0.9886540618621158, 0.9889168330187891, 0.9891881682431191, 0.9894114796545491, 0.9896240135712462, 0.989831606186759, 0.9900393658800064, 0.9902100735622623, 0.9903939013619977, 0.9905546960841405, 0.9907413000614591, 0.9908883173017603, 0.9910136934204214, 0.9911846245986358, 0.9913338123614097, 0.9914494801574301, 0.9915605205714582, 0.991641891802426, 0.9917522922424307, 0.9918470023408159, 0.9919345665781721, 0.9920273252846499, 0.9920922069300036, 0.9921645513036791, 0.9922226857935584, 0.9923098840665928, 0.9923860373635143, 0.9924545753307435, 0.9925325355429165, 0.9925771230969674, 0.9926126015979942, 0.9926515076953469, 0.9926818728853453, 0.9927650051277723, 0.9928165726578614, 0.9928374067980369, 0.9928608078218139, 0.9928818687432132, 0.9929101241677106, 0.9928937013711868, 0.992939374723363, 0.9929293274665121, 0.9929295855305844, 0.9929112165977733], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 965172208, "moving_var_accuracy_train": [0.023536144014405163, 0.05142129073336608, 0.0789326383854119, 0.10400634608227813, 0.123703831620261, 0.13765578983977536, 0.1463543908294858, 0.15098014139159194, 0.15195062538464107, 0.15086567844962606, 0.1477789786703305, 0.14284515511968643, 0.1370905226794482, 0.13070127285316885, 0.12377041593500371, 0.11643028928425922, 0.10906011359134711, 0.10201939634243905, 0.09499690558896622, 0.08819794384137637, 0.08163542138833221, 0.07538594923975572, 0.06946155731504615, 0.06395944901778289, 0.05874559845785515, 0.05386910973525217, 0.04930538583994104, 0.0450900111162115, 0.041175851851880885, 0.03756614531089405, 0.03425614862380446, 0.03119335637758077, 0.02838099020217862, 0.02583411105953822, 0.023509170264962787, 0.02137331075590388, 0.019433159132859243, 0.01763932569906907, 0.01601411061838169, 0.014527446213348897, 0.013183606940420213, 0.011953717763316045, 0.01084026959741522, 0.009829000250629686, 0.008903969417261426, 0.008061405033188635, 0.0073054244871912245, 0.006610935264487719, 0.0059853874943413725, 0.005414528884857965, 0.004903582818653306, 0.004435501542045129, 0.004012191263652092, 0.0036284572546310834, 0.0032807369637777367, 0.002966526490875741, 0.002682649633320124, 0.0024266541145352392, 0.0021924878744998583, 0.001981304295997055, 0.001790439300739696, 0.0016172465776372474, 0.0014614183585263906, 0.0013185987918946866, 0.0011903964954688543, 0.001074415939088884, 0.0009704603794470146, 0.000875963721690766, 0.0007901428698582836, 0.0007132119243568564, 0.0006433173446621982, 0.0005804605645406898, 0.0005236864372529357, 0.00047268313519515184, 0.0004262986689573796, 0.0003847245171676945, 0.0003471463351088516, 0.000313053139724981, 0.00028241043098814294, 0.0002546181997676027, 0.00022956291578256225, 0.00020699447645044537, 0.0001866835056166448, 0.000168277425070011, 0.00015175381650260985, 0.0001368111293363697, 0.00012344340580189314, 0.00011129359184221562, 0.00010030570519816895, 9.053809188752396e-05, 8.168459559582498e-05, 7.363654738756832e-05, 6.638386241073934e-05, 5.9805067664728476e-05, 5.393425521263476e-05, 4.8621559715996506e-05, 4.382841120537082e-05, 3.952300768348053e-05, 3.5608593566266785e-05, 3.209483758526266e-05, 2.8915770396958253e-05, 2.6092625206643964e-05, 2.3535556607667523e-05, 2.1224278023468006e-05, 1.915655037325969e-05, 1.7258787785719908e-05, 1.554423752346388e-05, 1.4003436930818425e-05, 1.2611391640609305e-05, 1.141245120412698e-05, 1.0295138975149733e-05, 9.269531630206422e-06, 8.347506938410075e-06, 7.516748306260745e-06, 6.772258796756442e-06, 6.097460291291739e-06, 5.506488758053715e-06, 4.956748408580393e-06, 4.461074167095943e-06, 4.018003509619923e-06], "duration": 78063.09209, "accuracy_train": [0.511383135497416, 0.630781509551495, 0.7114450673680325, 0.7745638453419158, 0.8081532334348468, 0.8284943201481173, 0.841371318694629, 0.8543452885635843, 0.8605298239087301, 0.8761984429794205, 0.8849867845030455, 0.8870794184316169, 0.8972853797526762, 0.9053994281215393, 0.9099102168120154, 0.9114204820736435, 0.9163970215023993, 0.9275341238118309, 0.9289749950973607, 0.9330436450258398, 0.9355072212993725, 0.9388085721207088, 0.9414821327634736, 0.9476194441560539, 0.9482229013819674, 0.9503856502630121, 0.9512463158107235, 0.9543144307747323, 0.9553847202034883, 0.9573371242271133, 0.9601730847983574, 0.9602660907507383, 0.9615241945251938, 0.9658464478935955, 0.9682409906792175, 0.9688927533222591, 0.9717051019172205, 0.9703332641196014, 0.9729138188099853, 0.9732869240840717, 0.9759368727505537, 0.9759826547503692, 0.9779354192621816, 0.9792146115956073, 0.9789824572028424, 0.9792146115956073, 0.9820741841431341, 0.9808418552740864, 0.9827019743217055, 0.9823532020002769, 0.9849806201550388, 0.9841435665836102, 0.9849802596668512, 0.9854220379406607, 0.9858412857027501, 0.9865849728336102, 0.9873293809408453, 0.9882823314645626, 0.9874917808693245, 0.9882122165120893, 0.9886772462739941, 0.9886539947858989, 0.989491408845515, 0.9882823314645626, 0.9891891395002769, 0.9892817849644703, 0.9902583474644703, 0.9899793296073275, 0.9896309177740864, 0.9904447198574198, 0.9900959475359912, 0.9905609772978959, 0.9906768742501846, 0.9911884069882798, 0.9908167436669435, 0.9914212823574198, 0.9914910368217055, 0.9912817734288483, 0.9916301852620893, 0.9914212823574198, 0.9915368188215209, 0.991699939726375, 0.9919092031192323, 0.9917464427025655, 0.9920483515596161, 0.9920018485834257, 0.9924207358573275, 0.9922114724644703, 0.9921420784883721, 0.9927230052025655, 0.992676502226375, 0.9924904903216132, 0.9925598842977114, 0.992374232881137, 0.9927458962024732, 0.9926993932262828, 0.992722644714378, 0.9928621536429494, 0.9926761417381875, 0.9928156506667589, 0.9927458962024732, 0.9930946685239018, 0.9930714170358066, 0.9930714170358066, 0.9932341774524732, 0.9929784110834257, 0.9929319081072352, 0.9930016625715209, 0.9929551595953304, 0.9935131953096161, 0.9932806804286637, 0.9930249140596161, 0.9930714170358066, 0.9930714170358066, 0.9931644229881875, 0.9927458962024732, 0.9933504348929494, 0.9928389021548542, 0.9929319081072352, 0.9927458962024732], "end": "2016-01-25 22:36:32.301000", "learning_rate_per_epoch": [0.002723046112805605, 0.0025755579117685556, 0.0024360581301152706, 0.002304114168509841, 0.002179316710680723, 0.002061278559267521, 0.0019496337044984102, 0.001844035810790956, 0.0017441574018448591, 0.001649688696488738, 0.0015603366773575544, 0.0014758242759853601, 0.001395889325067401, 0.0013202838599681854, 0.0012487734202295542, 0.0011811362346634269, 0.0011171624064445496, 0.0010566535638645291, 0.0009994221618399024, 0.0009452905505895615, 0.0008940908592194319, 0.0008456642972305417, 0.0007998606306500733, 0.0007565378327853978, 0.0007155615021474659, 0.0006768045714125037, 0.0006401468417607248, 0.0006054746336303651, 0.0005726803792640567, 0.0005416623316705227, 0.0005123243317939341, 0.00048457534285262227, 0.0004583293339237571, 0.0004335048724897206, 0.00041002497891895473, 0.00038781683542765677, 0.0003668115532491356, 0.00034694396890699863, 0.00032815246959216893, 0.00031037876033224165, 0.0002935677475761622, 0.0002776672481559217, 0.00026262798928655684, 0.00024840328842401505, 0.00023494903871323913, 0.00022222350526135415, 0.00021018722327426076, 0.0001988028670893982, 0.00018803511920850724, 0.00017785058298613876, 0.0001682176807662472, 0.00015910652291495353, 0.00015048884961288422, 0.00014233792899176478, 0.00013462848437484354, 0.00012733660696540028, 0.00012043968308717012, 0.00011391631414880976, 0.00010774627298815176, 0.0001019104165607132, 9.639064955990762e-05, 9.116985165746883e-05, 8.623182657174766e-05, 8.156125841196626e-05, 7.71436607465148e-05, 7.296533294720575e-05, 6.901331653352827e-05, 6.527535151690245e-05, 6.173984729684889e-05, 5.8395835367264226e-05, 5.5232947488548234e-05, 5.2241368393879384e-05, 4.941182123729959e-05, 4.673553121392615e-05, 4.420419645612128e-05, 4.1809966205619276e-05, 3.954541534767486e-05, 3.7403518945211545e-05, 3.5377634048927575e-05, 3.3461477869423106e-05, 3.164910594932735e-05, 2.993489761138335e-05, 2.831353594956454e-05, 2.6779991458170116e-05, 2.532950929889921e-05, 2.395758929196745e-05, 2.2659976821159944e-05, 2.143264646292664e-05, 2.027179107244592e-05, 1.9173810869688168e-05, 1.8135300706489943e-05, 1.7153039152617566e-05, 1.62239794008201e-05, 1.5345241990871727e-05, 1.4514098438667133e-05, 1.3727972145716194e-05, 1.2984424756723456e-05, 1.2281150702619925e-05, 1.1615967196121346e-05, 1.09868124127388e-05, 1.0391734576842282e-05, 9.828887414187193e-06, 9.296526513935532e-06, 8.792999324214179e-06, 8.31674515211489e-06, 7.866286068747286e-06, 7.4402250902494416e-06, 7.037240720819682e-06, 6.656083314737771e-06, 6.295570528891403e-06, 5.95458413954475e-06, 5.632066404359648e-06, 5.327017333911499e-06, 5.038490598963108e-06, 4.76559125672793e-06, 4.507473022385966e-06, 4.2633350858523045e-06, 4.0324202927877195e-06, 3.8140126434882404e-06, 3.6074345644010464e-06], "accuracy_valid": [0.5042827560240963, 0.6186979362763554, 0.6900811252823795, 0.7542518707643072, 0.7862151731927711, 0.8032653073230422, 0.8083010753953314, 0.8193697642131024, 0.8254526896649097, 0.8378435617469879, 0.8441912179969879, 0.8453913309487951, 0.8520949030496988, 0.8565306146460843, 0.8549834102033133, 0.8546686746987951, 0.8643739999058735, 0.8657064782567772, 0.8687685311558735, 0.8661844644201807, 0.8671816170933735, 0.8701421898531627, 0.8705084007906627, 0.8739572548004518, 0.878138530685241, 0.8756868293486446, 0.8734895637236446, 0.8764192512236446, 0.8734586784638554, 0.874354350997741, 0.8749544074736446, 0.877894390060241, 0.8750661826995482, 0.8793695288968373, 0.877842914627259, 0.8782503059111446, 0.882533061935241, 0.879063617752259, 0.8784738563629518, 0.8841508612575302, 0.8843950018825302, 0.8822992163968373, 0.8795621940888554, 0.8808034873870482, 0.881312358810241, 0.8814035438629518, 0.8838964255459337, 0.8780767601656627, 0.8826139519013554, 0.8810064476656627, 0.8826448371611446, 0.881016742752259, 0.8825021766754518, 0.8795210137424698, 0.8815462043486446, 0.8799489951995482, 0.8820241905120482, 0.8810476280120482, 0.8815256141754518, 0.8815462043486446, 0.8817594597138554, 0.8822580360504518, 0.8828786826995482, 0.8839876105986446, 0.8842214561370482, 0.8831022331513554, 0.8828786826995482, 0.8822580360504518, 0.8826448371611446, 0.8846185523343373, 0.8831228233245482, 0.8831228233245482, 0.8841096809111446, 0.8825124717620482, 0.884242046310241, 0.8827669074736446, 0.8836213996611446, 0.883021343185241, 0.8827669074736446, 0.884364116622741, 0.8844758918486446, 0.880772602127259, 0.8827463173004518, 0.8837125847138554, 0.8820138954254518, 0.8827566123870482, 0.8817697548004518, 0.8826242469879518, 0.8834787391754518, 0.8815153190888554, 0.8832243034638554, 0.8823801063629518, 0.8829801628388554, 0.8828786826995482, 0.8833566688629518, 0.8829904579254518, 0.8812814735504518, 0.8820036003388554, 0.881138813064759, 0.8816373894013554, 0.8817594597138554, 0.8822580360504518, 0.881627094314759, 0.8819830101656627, 0.882725727127259, 0.881382953689759, 0.8823904014495482, 0.881993305252259, 0.881749164627259, 0.8817388695406627, 0.882115375564759, 0.8820036003388554, 0.881016742752259, 0.881627094314759, 0.881749164627259, 0.8820036003388554, 0.882115375564759, 0.881871234939759, 0.8818609398531627, 0.881627094314759], "accuracy_test": 0.8768614477040817, "start": "2016-01-25 00:55:29.209000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0], "accuracy_train_last": 0.9927458962024732, "batch_size_eval": 1024, "accuracy_train_std": [0.019639665120019096, 0.018356385065449343, 0.015618848741274534, 0.016839119555322645, 0.019011091636810005, 0.018189693177983763, 0.019780758560412393, 0.018304416636546462, 0.01895545233342994, 0.017881005777636638, 0.016813789371213094, 0.016630735082153292, 0.0162609968427616, 0.01619759472557236, 0.01660862078592017, 0.015600635213620076, 0.014430710082055749, 0.015050584820344591, 0.012909311469663676, 0.013816565022690873, 0.013083328950677901, 0.013820781326887082, 0.012722578054740695, 0.012763827385870782, 0.013489282766846982, 0.013072887269151387, 0.011389047444871625, 0.012322648659689782, 0.012046546299199275, 0.0111999430863333, 0.011014823165744371, 0.01067977248705735, 0.008691048523813855, 0.010387232088837767, 0.009191291513636596, 0.009309752306957674, 0.00879699413569553, 0.00820009748685324, 0.008293084182595813, 0.0085566726047627, 0.007434230016764553, 0.006951518642919793, 0.0075187534966031444, 0.006596270017388317, 0.006206156480597862, 0.006547900065264581, 0.006541952668923128, 0.006200492165405983, 0.005985253531753686, 0.005886267090833844, 0.00557496502975794, 0.005558228651238191, 0.005570548156815333, 0.00556413038951098, 0.0051439427522125625, 0.004532747524848642, 0.004053440673349303, 0.004420445488863914, 0.004719919980260072, 0.004344611491415099, 0.004542130581300425, 0.004389046158251653, 0.0039755756720133316, 0.004067318057730493, 0.003961831328596685, 0.004633246849764928, 0.003780251884396625, 0.0037596548659207378, 0.003918833644019994, 0.0036583894172605345, 0.0037927927404913814, 0.003858515218195118, 0.003643849375835244, 0.0031582783684192565, 0.003330274773189848, 0.003236551387039196, 0.003438917361887573, 0.0032144734966131567, 0.003344332459892381, 0.0036518882554322624, 0.0034147794375878064, 0.003478158414830504, 0.0032766620553921926, 0.00346475543890117, 0.003453766513081073, 0.003220148262975012, 0.003058064660971454, 0.0028797506352053544, 0.003232215168698846, 0.0031196905231308013, 0.003008903887084287, 0.003165163151003067, 0.003287872864188523, 0.003046385258229166, 0.002892458916391394, 0.002916661850850465, 0.0031049322473263767, 0.0030554193492564924, 0.0029477323485049907, 0.0029827215740092228, 0.003052866341558098, 0.0028469179854648937, 0.0027529845871267517, 0.002818195990069808, 0.0029333232216835585, 0.0029552970240490726, 0.002936583927180923, 0.0028669884881474864, 0.0029033466237758966, 0.002703084882285719, 0.002672542512700245, 0.0028721665987233354, 0.0029442889874164418, 0.0028976471947608916, 0.0029916381916304134, 0.0029927726939963603, 0.0026557915883203714, 0.0031009985513025134, 0.0029288414260064204, 0.0027966697750681853], "accuracy_test_std": 0.007777402591639794, "error_valid": [0.49571724397590367, 0.3813020637236446, 0.3099188747176205, 0.24574812923569278, 0.21378482680722888, 0.19673469267695776, 0.19169892460466864, 0.18063023578689763, 0.1745473103350903, 0.16215643825301207, 0.15580878200301207, 0.15460866905120485, 0.14790509695030118, 0.14346938535391573, 0.14501658979668675, 0.14533132530120485, 0.1356260000941265, 0.13429352174322284, 0.1312314688441265, 0.1338155355798193, 0.1328183829066265, 0.12985781014683728, 0.12949159920933728, 0.12604274519954817, 0.12186146931475905, 0.12431317065135539, 0.1265104362763554, 0.12358074877635539, 0.1265413215361446, 0.12564564900225905, 0.1250455925263554, 0.12210560993975905, 0.12493381730045183, 0.12063047110316272, 0.12215708537274095, 0.12174969408885539, 0.11746693806475905, 0.12093638224774095, 0.12152614363704817, 0.11584913874246983, 0.11560499811746983, 0.11770078360316272, 0.12043780591114461, 0.11919651261295183, 0.11868764118975905, 0.11859645613704817, 0.11610357445406627, 0.12192323983433728, 0.11738604809864461, 0.11899355233433728, 0.11735516283885539, 0.11898325724774095, 0.11749782332454817, 0.12047898625753017, 0.11845379565135539, 0.12005100480045183, 0.11797580948795183, 0.11895237198795183, 0.11847438582454817, 0.11845379565135539, 0.11824054028614461, 0.11774196394954817, 0.11712131730045183, 0.11601238940135539, 0.11577854386295183, 0.11689776684864461, 0.11712131730045183, 0.11774196394954817, 0.11735516283885539, 0.11538144766566272, 0.11687717667545183, 0.11687717667545183, 0.11589031908885539, 0.11748752823795183, 0.11575795368975905, 0.11723309252635539, 0.11637860033885539, 0.11697865681475905, 0.11723309252635539, 0.11563588337725905, 0.11552410815135539, 0.11922739787274095, 0.11725368269954817, 0.11628741528614461, 0.11798610457454817, 0.11724338761295183, 0.11823024519954817, 0.11737575301204817, 0.11652126082454817, 0.11848468091114461, 0.11677569653614461, 0.11761989363704817, 0.11701983716114461, 0.11712131730045183, 0.11664333113704817, 0.11700954207454817, 0.11871852644954817, 0.11799639966114461, 0.11886118693524095, 0.11836261059864461, 0.11824054028614461, 0.11774196394954817, 0.11837290568524095, 0.11801698983433728, 0.11727427287274095, 0.11861704631024095, 0.11760959855045183, 0.11800669474774095, 0.11825083537274095, 0.11826113045933728, 0.11788462443524095, 0.11799639966114461, 0.11898325724774095, 0.11837290568524095, 0.11825083537274095, 0.11799639966114461, 0.11788462443524095, 0.11812876506024095, 0.11813906014683728, 0.11837290568524095], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.6666904363309907, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0028789799948130356, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "adam", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 3.3363286822255385e-08, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.05416292971823743}, "accuracy_valid_max": 0.8846185523343373, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.881627094314759, "loss_train": [1.5210355520248413, 1.0845903158187866, 0.8832938075065613, 0.7582926750183105, 0.673703134059906, 0.6107130646705627, 0.5642892718315125, 0.5233123302459717, 0.4888763725757599, 0.4603067636489868, 0.43166035413742065, 0.40705984830856323, 0.38565394282341003, 0.36667871475219727, 0.35157501697540283, 0.33344095945358276, 0.3194505274295807, 0.30603912472724915, 0.29393112659454346, 0.2797621190547943, 0.26886239647865295, 0.2596178650856018, 0.24944935739040375, 0.23981983959674835, 0.2314358800649643, 0.224344864487648, 0.2131626158952713, 0.2063111662864685, 0.19930344820022583, 0.19340384006500244, 0.18744833767414093, 0.18049095571041107, 0.17587187886238098, 0.17004022002220154, 0.164806067943573, 0.16073298454284668, 0.15756581723690033, 0.15364393591880798, 0.1484592854976654, 0.14532318711280823, 0.13997173309326172, 0.13773368299007416, 0.13282492756843567, 0.13134323060512543, 0.12926676869392395, 0.12552407383918762, 0.12345699220895767, 0.12105566263198853, 0.11999774724245071, 0.11559543758630753, 0.11461039632558823, 0.11370658874511719, 0.1104467436671257, 0.10936399549245834, 0.10843222588300705, 0.10649905353784561, 0.10660284757614136, 0.10282500833272934, 0.10302972048521042, 0.10244279354810715, 0.09944476187229156, 0.09860837459564209, 0.09877052903175354, 0.09694576263427734, 0.09594662487506866, 0.09557076543569565, 0.09494301676750183, 0.09334640204906464, 0.0937083438038826, 0.09276482462882996, 0.09230643510818481, 0.09174136817455292, 0.09056112915277481, 0.08953165262937546, 0.0886397585272789, 0.0886785089969635, 0.08847862482070923, 0.08789699524641037, 0.08703061938285828, 0.08847012370824814, 0.08831336349248886, 0.08532119542360306, 0.08551168441772461, 0.0858905166387558, 0.08532010763883591, 0.08454104512929916, 0.08493959158658981, 0.08325422555208206, 0.08424001932144165, 0.0844154953956604, 0.08283638954162598, 0.08320985734462738, 0.08318404853343964, 0.08254901319742203, 0.0831114873290062, 0.0816316083073616, 0.08344882726669312, 0.08235408365726471, 0.08270025253295898, 0.08276968449354172, 0.08274286240339279, 0.08069974184036255, 0.08255913853645325, 0.08111691474914551, 0.08181814104318619, 0.08157724887132645, 0.08061432838439941, 0.08167935162782669, 0.08154474198818207, 0.08133503794670105, 0.08165697753429413, 0.07948855310678482, 0.08184009045362473, 0.08028192818164825, 0.07942140847444534, 0.08035596460103989, 0.08119069039821625, 0.08097771555185318, 0.07985955476760864, 0.08185521513223648], "accuracy_train_first": 0.511383135497416, "model": "residualv5", "loss_std": [0.2639845907688141, 0.14926785230636597, 0.13971754908561707, 0.12998248636722565, 0.12591253221035004, 0.11968530714511871, 0.11623342335224152, 0.1115301176905632, 0.10698496550321579, 0.10432052612304688, 0.10111735761165619, 0.100179024040699, 0.09517982602119446, 0.09617061913013458, 0.09237666428089142, 0.08870071172714233, 0.0871037021279335, 0.08284291625022888, 0.08212291449308395, 0.08091439306735992, 0.0760752484202385, 0.07486831396818161, 0.07481634616851807, 0.07309500128030777, 0.07180729508399963, 0.06960872560739517, 0.06652672588825226, 0.065369613468647, 0.0642564445734024, 0.06122618541121483, 0.060889020562171936, 0.05913500860333443, 0.05870668962597847, 0.0582273006439209, 0.05553997680544853, 0.05450322851538658, 0.05249122530221939, 0.05212026834487915, 0.052950505167245865, 0.051510244607925415, 0.04958217218518257, 0.04874110221862793, 0.04557463899254799, 0.04588860645890236, 0.04644276574254036, 0.04509055241942406, 0.04457736760377884, 0.043153997510671616, 0.04388849437236786, 0.04237184673547745, 0.04085738584399223, 0.041664715856313705, 0.04015659540891647, 0.04219984635710716, 0.0396709144115448, 0.038324810564517975, 0.03995828330516815, 0.03891623765230179, 0.03754455968737602, 0.03848174959421158, 0.038180992007255554, 0.03575243055820465, 0.03769293799996376, 0.037214867770671844, 0.03842423856258392, 0.03652232512831688, 0.03646542504429817, 0.03472403436899185, 0.035692207515239716, 0.03569061681628227, 0.034654878079891205, 0.034009311348199844, 0.03391566500067711, 0.03406898304820061, 0.03351043537259102, 0.0346934013068676, 0.035354986786842346, 0.0336025096476078, 0.03346245735883713, 0.03463348001241684, 0.03467888385057449, 0.03269459307193756, 0.03288750350475311, 0.03238086402416229, 0.034377746284008026, 0.03190892934799194, 0.031796857714653015, 0.03268714249134064, 0.03232565149664879, 0.03380081057548523, 0.03147590905427933, 0.031869951635599136, 0.032936640083789825, 0.03130381554365158, 0.03213772922754288, 0.03210534155368805, 0.033056698739528656, 0.03214447200298309, 0.033032797276973724, 0.032016921788454056, 0.032054271548986435, 0.031003380194306374, 0.03206409141421318, 0.03125058487057686, 0.03380313515663147, 0.031050100922584534, 0.0302236620336771, 0.031031960621476173, 0.031259775161743164, 0.03104141540825367, 0.03180289641022682, 0.030602412298321724, 0.03228616714477539, 0.031233198940753937, 0.032165370881557465, 0.030284591019153595, 0.03182218596339226, 0.03197453171014786, 0.03182008117437363, 0.03248700499534607]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:14 2016", "state": "available"}], "summary": "c3f51ea7093f44d285ec8c4fcffef1ac"}