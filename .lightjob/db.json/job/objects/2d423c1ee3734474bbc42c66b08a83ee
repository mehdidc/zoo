{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 16, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.4385700225830078, 1.065886378288269, 0.8623698353767395, 0.7405381202697754, 0.6485874056816101, 0.5725162029266357, 0.5069357752799988, 0.4469548761844635, 0.3896436095237732, 0.3346049189567566, 0.2824440002441406, 0.23288054764270782, 0.19186349213123322, 0.15996937453746796, 0.13954439759254456, 0.12059386819601059, 0.10456027835607529, 0.09059111773967743, 0.07865302264690399, 0.07260381430387497, 0.06651537865400314, 0.058572396636009216, 0.0546347014605999, 0.05385587736964226, 0.04606364294886589, 0.044458355754613876, 0.04040728509426117, 0.03942957893013954, 0.03642500191926956, 0.03493792936205864, 0.031133050099015236, 0.030167527496814728, 0.028838250786066055, 0.02693088911473751, 0.025788428261876106, 0.027861004695296288, 0.02307569980621338, 0.0239595714956522, 0.020868053659796715, 0.023740066215395927, 0.020245321094989777, 0.0179225355386734, 0.018737521022558212, 0.019855352118611336, 0.01823459565639496, 0.016514522954821587, 0.016526468098163605, 0.0169353149831295, 0.015355589799582958, 0.014654990285634995, 0.013831879943609238, 0.016195818781852722, 0.013792183250188828, 0.011349630542099476, 0.013882867991924286, 0.012392919510602951, 0.012292720377445221, 0.0116032175719738, 0.011646907776594162, 0.011241027154028416, 0.01171958353370428, 0.010019814595580101, 0.0119805708527565, 0.008204976096749306, 0.009978662244975567, 0.010531384497880936, 0.01024303026497364, 0.008636928163468838, 0.009743791073560715, 0.008520365692675114, 0.008170168846845627, 0.009763174690306187, 0.007099595386534929, 0.009448637254536152, 0.006254139356315136, 0.00895420927554369, 0.0064924354664981365, 0.010093525983393192, 0.006001479923725128, 0.007364673539996147, 0.007945431396365166, 0.008566305041313171, 0.006447248160839081, 0.00613681273534894, 0.009053678251802921, 0.007332389242947102, 0.00644101994112134, 0.0068486537784338, 0.005562287289649248, 0.005835407413542271, 0.007009288761764765, 0.006716378033161163, 0.0053300135768949986, 0.007647369522601366, 0.0063072144985198975, 0.003390717785805464, 0.0073102666065096855, 0.006353969685733318, 0.0043927887454628944, 0.006899805273860693, 0.005053059197962284, 0.00416188919916749, 0.0063009136356413364, 0.0047219377011060715, 0.005878531839698553, 0.006313135847449303, 0.004066535271704197, 0.005500721745193005, 0.00408039428293705, 0.005864896811544895, 0.004567427560687065, 0.004546867683529854, 0.00525541789829731, 0.004026825074106455, 0.005599645897746086, 0.0048936400562524796, 0.005716136656701565, 0.00494450842961669, 0.002269116695970297, 0.0076118214055895805, 0.003066267119720578, 0.0058264341205358505, 0.002836008556187153, 0.005272838287055492, 0.0031964292284101248, 0.00562191242352128, 0.002083671046420932, 0.006219052709639072, 0.001269084750674665, 0.0009533981210552156, 0.0008941845153458416, 0.000877560640219599, 0.0008578293491154909, 0.007280596531927586, 0.0031280445400625467, 0.0036722447257488966, 0.004338425118476152, 0.004690296482294798, 0.00386684644035995, 0.004080234561115503, 0.003729822114109993, 0.003837545868009329, 0.001955161103978753, 0.004841049667447805, 0.0035928748548030853, 0.0035913027822971344, 0.004261191468685865, 0.0040567778050899506, 0.002948976354673505, 0.0038293132092803717, 0.001760908868163824, 0.005389419384300709, 0.0030087262857705355, 0.0028530156705528498, 0.000993335503153503, 0.0008579370914958417, 0.0008372603915631771, 0.000826446630526334, 0.0008059091633185744, 0.007129231933504343, 0.002300707623362541, 0.0040895771235227585, 0.0022124091628938913, 0.004086967092007399, 0.0027710264548659325, 0.0046826573088765144, 0.0037165898829698563, 0.002673283452168107, 0.004792873281985521, 0.003800539765506983, 0.0026947923470288515, 0.003937150351703167, 0.0028631992172449827, 0.0027021130081266165, 0.004069164395332336, 0.002353739459067583, 0.0028152577579021454, 0.003775394521653652, 0.0029383106157183647, 0.0025600953958928585, 0.003509738715365529, 0.002852295059710741, 0.002261091722175479, 0.0032188219483941793, 0.0025325235910713673, 0.0031397577840834856, 0.0029920260421931744, 0.0030710964929312468, 0.003041853429749608, 0.0023126506712287664, 0.0039476267993450165, 0.00212484085932374, 0.003188453149050474, 0.002642204985022545, 0.0030090513173490763, 0.003461808431893587, 0.002230575308203697, 0.0025918767787516117, 0.002703593811020255, 0.0024267593398690224, 0.002575896680355072, 0.002914228243753314, 0.0027946338523179293, 0.001353479572571814, 0.004200525116175413, 0.0021471786312758923, 0.0033124256879091263, 0.0020563178695738316, 0.002150466665625572], "moving_avg_accuracy_train": [0.06284470588235293, 0.12748729411764703, 0.19002091764705878, 0.24907529647058818, 0.3044854138823529, 0.3560462842588235, 0.4039663617152941, 0.4482638431908235, 0.48865392945997643, 0.5261179482786846, 0.5606096828625808, 0.5926922439880875, 0.6221218431186905, 0.649942599983292, 0.675560104690845, 0.6987146824570547, 0.7190361553878198, 0.7391866574960967, 0.7575126976288399, 0.7745826043365441, 0.7902419909617132, 0.8042177918655419, 0.8180360126789877, 0.8308818231757948, 0.8419348173288036, 0.8529483944194527, 0.8620700255657427, 0.8702418465385803, 0.8790553089435458, 0.8867850721668382, 0.8936453884795662, 0.9001443790433743, 0.9060193529037428, 0.9112291823192509, 0.9162097934990905, 0.9204476376785933, 0.9251605209695575, 0.9292985865196606, 0.9329640219853416, 0.9369099727279838, 0.9402542695728324, 0.9432382543802551, 0.9459191348245825, 0.9479483978127125, 0.9502476756785, 0.9527546728165324, 0.9544556761231144, 0.9564454026284501, 0.9578714506008993, 0.9594937173055152, 0.9613419926337873, 0.9624430874880556, 0.9636293669745442, 0.9650876067476779, 0.9661811990140866, 0.9667207261715014, 0.9681521829661159, 0.9690404940812691, 0.9700117387907893, 0.9705752707940632, 0.9716424495970098, 0.9720734987549559, 0.9726943841735779, 0.9733237692856318, 0.9736008041217745, 0.9744477825331265, 0.9752453572209903, 0.9761655273812442, 0.9766172099372374, 0.9771201948258667, 0.9777328812256328, 0.9781242989854225, 0.978460104380998, 0.9784211527664275, 0.978800213960373, 0.9791390160937474, 0.9797757027196667, 0.9800169559771119, 0.9800858486146948, 0.9797360872826371, 0.9798236550249616, 0.9799871718754066, 0.9796072782172778, 0.9801618445131971, 0.9802938953559951, 0.9804268587615721, 0.9805582905324737, 0.9811495203027557, 0.9810345682724801, 0.9813993467393498, 0.9817229414771795, 0.9809482943882851, 0.9812228767141624, 0.9811335302192168, 0.9814601771972951, 0.9819706300658008, 0.9821335670592207, 0.9818402103532987, 0.9821291304944394, 0.982273864503819, 0.9824300074652018, 0.9828881831892698, 0.9828746589879899, 0.9818789577950732, 0.9824228267214482, 0.982604073461068, 0.9828683719973142, 0.9834591818564064, 0.9825979695531186, 0.9832417020095715, 0.9832798847497908, 0.9837801315689294, 0.9837856478238012, 0.984056494806127, 0.9838720217961026, 0.9838048196164922, 0.9840596317724901, 0.984258374477594, 0.9846066546768935, 0.984804812738616, 0.9850890373471073, 0.985222486553573, 0.9854531790746863, 0.9854678611672177, 0.9855634279916724, 0.9856894381336816, 0.9862357884379606, 0.986499268417694, 0.9871011062818069, 0.987760407418332, 0.9884431902059105, 0.9891118123617901, 0.9897747487726699, 0.9896184503659912, 0.9894871935646862, 0.989176121267041, 0.9891032150226898, 0.9889599523439503, 0.9887463100507318, 0.987838737869188, 0.9878548640822692, 0.9878740835563952, 0.9879478516713439, 0.9880118900336213, 0.9879636422067298, 0.987793160338998, 0.9875691384227453, 0.9876639892863531, 0.9879164138871296, 0.9878024195572401, 0.9880292364250456, 0.9879486657237175, 0.9879326226807575, 0.9883087721773877, 0.9887790714302371, 0.9893317525225075, 0.9899162243290802, 0.9905034254255839, 0.9910601417065549, 0.9908576569476641, 0.9902660088999566, 0.9899805844805493, 0.9900131142677885, 0.989503567546892, 0.9894802696157321, 0.9894122426541588, 0.9890921948593312, 0.9889594459616333, 0.9889929131301758, 0.9888277394642171, 0.9885167302236778, 0.9883803513189571, 0.9886293750105908, 0.98862761398012, 0.9883742643468139, 0.9884238967356619, 0.9884591541209192, 0.988436768120592, 0.9884283854261798, 0.9886937821776794, 0.9887185216069703, 0.9885760812109791, 0.9887961201487047, 0.9886482728397165, 0.9887975632028037, 0.9887954539413468, 0.9888994379589768, 0.9887647882807262, 0.9885518388644183, 0.9883225373309177, 0.9882596953625318, 0.9880878434733374, 0.9880202355965919, 0.987914682625168, 0.9878361555391217, 0.9878666576322684, 0.9880658742219828, 0.9876945809174316, 0.9878521816492178, 0.9875351987784137, 0.9877863847829252, 0.9877206874811033, 0.9878662657918166, 0.988305521565576, 0.988432616467842, 0.9887375901151755, 0.9886214781624815, 0.9888699185815274, 0.9889264561351393], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.060519999999999984, 0.12188133333333331, 0.1807865333333333, 0.23610787999999994, 0.2867770919999999, 0.33309938279999995, 0.3753094445199999, 0.41311183340133323, 0.44688065006119987, 0.4773525850550799, 0.5045506598829053, 0.5289755938946147, 0.5508513678384865, 0.5709528977213045, 0.5884042746158408, 0.6041238471542567, 0.617711462438831, 0.6308336495282811, 0.6429369512421197, 0.6537365894512411, 0.6634162638394503, 0.6715013041221719, 0.6804178403766213, 0.6878293896722925, 0.6940731173717299, 0.7006791389678904, 0.705677891737768, 0.7106301025639912, 0.7153004256409254, 0.7194637164101662, 0.7233306781024829, 0.7271976102922346, 0.7304111825963445, 0.7334500643367101, 0.736305057903039, 0.7388478854460685, 0.741656430234795, 0.7435307872113155, 0.745217708490184, 0.7475359376411657, 0.7489823438770491, 0.7502307761560107, 0.7517143652070764, 0.7524762620197021, 0.7541886358177319, 0.755263105569292, 0.7554434616790295, 0.7566191155111266, 0.7579038706266805, 0.7581001502306791, 0.7588368018742778, 0.7590597883535167, 0.7595004761848317, 0.7602704285663486, 0.7609767190430471, 0.761185713805409, 0.7620404757582014, 0.7623964281823813, 0.7626367853641431, 0.7626531068277289, 0.763187796144956, 0.7632423498637937, 0.7641981148774144, 0.7651249700563396, 0.765105806384039, 0.7654218924123017, 0.7660797031710715, 0.7670317328539644, 0.7669018929019014, 0.7671983702783779, 0.7675451999172067, 0.7677240132588193, 0.767578278599604, 0.7675404507396436, 0.7679330723323459, 0.7683530984324447, 0.7686377885892002, 0.7685340097302801, 0.7679472754239187, 0.7680592145481936, 0.7683066264267076, 0.7684492971173702, 0.7682977007389665, 0.7682012639984032, 0.7680478042652296, 0.7681763571720399, 0.7683987214548359, 0.768785515976019, 0.7684536310450837, 0.7687149346072419, 0.7687501078131844, 0.7683417636985327, 0.7684142539953461, 0.7685328285958115, 0.7690395457362303, 0.7697889244959407, 0.7701166987130132, 0.7696916955083787, 0.7698291926242075, 0.7698329400284534, 0.7695429793589413, 0.7698953480897138, 0.7695191466140757, 0.7686072319526681, 0.7690398420907346, 0.7694158578816612, 0.7695409387601617, 0.7702001782174789, 0.769780160395731, 0.7702688110228246, 0.7703485965872089, 0.7708870702618213, 0.7714116965689726, 0.7714305269120754, 0.7708741408875346, 0.7705200601321145, 0.7706280541189031, 0.7707785820403462, 0.7710073905029783, 0.7707599847860138, 0.7709106529740791, 0.7711262543433378, 0.7716936289090041, 0.771297599351437, 0.7711278394162933, 0.7712417221413306, 0.7719842165938642, 0.7719324616011445, 0.77233921544103, 0.7728119605635937, 0.7734374311739011, 0.7739203547231777, 0.7743683192508599, 0.7739448206591073, 0.7736436719265299, 0.7736526380672103, 0.7737673742604892, 0.7733906368344403, 0.773318239817663, 0.7722664158358967, 0.7725864409189738, 0.7727277968270765, 0.7728950171443688, 0.772725515429932, 0.7726796305536054, 0.7722783341649115, 0.7723438340817538, 0.7724294506735784, 0.7723731722728872, 0.7723091883789318, 0.7722649362077053, 0.7722117759202681, 0.7728705983282412, 0.7732235384954171, 0.7737811846458754, 0.7742563995146212, 0.7746307595631591, 0.7751410169401766, 0.7758802485794922, 0.775532223721543, 0.7748323346827221, 0.7745357678811164, 0.7745221910930048, 0.7742699719837043, 0.7741496414520005, 0.7742013439734672, 0.7739412095761204, 0.7737870886185084, 0.7740883797566576, 0.7737862084476584, 0.7733409209362259, 0.77291349550927, 0.7734088126250097, 0.7736679313625088, 0.7733278048929246, 0.7731816910702988, 0.7728901886299356, 0.772921169766942, 0.7729490527902478, 0.7728808141778898, 0.7726193994267675, 0.7723307928174241, 0.772591046869015, 0.7723586088487803, 0.7725094146305689, 0.7722451398341786, 0.7722072925174274, 0.7722265632656846, 0.7718172402724495, 0.7715688495785379, 0.7716386312873508, 0.7717414348252825, 0.7716339580094208, 0.771443895541812, 0.7712595059876308, 0.7713068887222011, 0.7715495331833143, 0.7709012465316495, 0.7711844552118179, 0.7706926763573028, 0.7709567420549057, 0.7706210678494151, 0.7706389610644735, 0.7709083982913594, 0.7710708917955568, 0.7714171359493345, 0.7716087556877345, 0.7721012134522944, 0.772051092107065], "moving_var_accuracy_train": [0.03554511351695501, 0.06959858008907957, 0.0978328087256354, 0.11943630477716814, 0.1351252043037213, 0.145539394059162, 0.15165245906415306, 0.15414761494341161, 0.15341508506853702, 0.15070555091612003, 0.14634211361596153, 0.1409715188097122, 0.1346692786736329, 0.1281683014189431, 0.12125778020402236, 0.11395721242740316, 0.10627815154334522, 0.09930472100595172, 0.09239684262787881, 0.08577959380017848, 0.07940858192544936, 0.07322563083103556, 0.06762155678597433, 0.06234453473325581, 0.05720959937764824, 0.052580329362868446, 0.048071133819502315, 0.043865028359661044, 0.04017761959976861, 0.036697600795185234, 0.03345141617486284, 0.03048640646251275, 0.02774840367700159, 0.025217844212149673, 0.022919318180457397, 0.020789020272019364, 0.018910019665045675, 0.017173129977013656, 0.01557673573368994, 0.01415919690569118, 0.012843936107600247, 0.011639679984818584, 0.010540396065947722, 0.009523417633827897, 0.008618655978782013, 0.007813355692754735, 0.0070580608337202895, 0.006387885854442576, 0.0057673997843758545, 0.005214345549286419, 0.004723656089559668, 0.004262202169506567, 0.0038486472837364824, 0.0034829207244863785, 0.0031453921484440807, 0.0028334727395819665, 0.0025685670826174037, 0.0023188122440914046, 0.0020954208662542022, 0.0018887368944972083, 0.0017101130404246145, 0.0015407739667712473, 0.0013901660584216401, 0.0012547145831529529, 0.001129933859541587, 0.001023396825451095, 0.0009267822713504752, 0.0008417244623298232, 0.000759388170279338, 0.0006857262974351081, 0.0006205321293117243, 0.0005598577871446607, 0.0005048868958034726, 0.0004544118612776241, 0.00041026386164866004, 0.00037027055745400593, 0.0003368918304452269, 0.000303726475608755, 0.0002733965438074977, 0.000247157886331373, 0.00022251111068369804, 0.00020050063945874336, 0.00018174944823624802, 0.00016634239740174984, 0.0001498650944873277, 0.00013503769844359852, 0.00012168939739285969, 0.00011266643142498354, 0.00010151871400586549, 9.256441257430476e-05, 8.425039330603408e-05, 8.122605698642408e-05, 7.378201037093974e-05, 6.647565449927742e-05, 6.0788373283939025e-05, 5.705459513423721e-05, 5.158807179523602e-05, 4.72037880278975e-05, 4.323468285671869e-05, 3.9099745972286634e-05, 3.540919699456249e-05, 3.375760224223383e-05, 3.0383488154192782e-05, 3.626792712895386e-05, 3.530327509774548e-05, 3.2068601013576504e-05, 2.9490424358575468e-05, 2.968288852912177e-05, 3.338977935821643e-05, 3.378032470181202e-05, 3.0415413526486724e-05, 2.9626094094362358e-05, 2.6663758546536417e-05, 2.465760548239753e-05, 2.2498117557005163e-05, 2.028895099780397e-05, 1.8844419011621923e-05, 1.7315465075947977e-05, 1.6675610443369578e-05, 1.5361448955862877e-05, 1.4552356712925127e-05, 1.3257399257989637e-05, 1.2410630685869103e-05, 1.117150769185209e-05, 1.0136554084094076e-05, 9.265805678687338e-06, 1.1025713005689362e-05, 1.0547937002603041e-05, 1.2753022634463114e-05, 1.5389822268627537e-05, 1.8046571056886792e-05, 2.02654142371951e-05, 2.219423497730746e-05, 2.0194674206949577e-05, 1.833026191725401e-05, 1.7368129494788333e-05, 1.567915442949801e-05, 1.4295956742624964e-05, 1.3277148333427657e-05, 1.9362618882493916e-05, 1.7428697486979587e-05, 1.5689152231952757e-05, 1.4169212621805258e-05, 1.2789199566213245e-05, 1.1531230284789732e-05, 1.063968386133869e-05, 1.0027387845858762e-05, 9.10561923821723e-06, 8.768520926089892e-06, 8.00862119870337e-06, 7.670772102522497e-06, 6.962119633482748e-06, 6.268224083181217e-06, 6.914797669198997e-06, 8.213950387356032e-06, 1.0141662856399293e-05, 1.220196220486548e-05, 1.4085012133995745e-05, 1.5465908078080086e-05, 1.4288317968519628e-05, 1.600991288287358e-05, 1.5142125487332865e-05, 1.3637436622120019e-05, 1.4610433706895304e-05, 1.3154275478572706e-05, 1.1880496938223378e-05, 1.1614322563167412e-05, 1.0611490735410542e-05, 9.560422124201787e-06, 8.849920971117895e-06, 8.835469603313964e-06, 8.119315493857986e-06, 7.865499135426115e-06, 7.078977132938377e-06, 6.948753749911649e-06, 6.276048741125343e-06, 5.659631615949467e-06, 5.09817865145037e-06, 4.5889932123958e-06, 4.764012812515428e-06, 4.293119885518631e-06, 4.046411294657771e-06, 4.07752437223077e-06, 3.866501375982921e-06, 3.6804397509809268e-06, 3.3124358167378743e-06, 3.0785063183664126e-06, 2.9338305092067326e-06, 3.048574543439007e-06, 3.216929828486892e-06, 2.9307788625537145e-06, 2.9034986226753577e-06, 2.654286185390148e-06, 2.4891304348388545e-06, 2.295715920541143e-06, 2.0745177276639665e-06, 2.224251201454257e-06, 3.2425545433499432e-06, 3.1418410049510158e-06, 3.731960167905004e-06, 3.926613830876891e-06, 3.5727976669893405e-06, 3.4062553012413814e-06, 4.802140484146911e-06, 4.467304463370081e-06, 4.8576543471438675e-06, 4.493226782455248e-06, 4.599407880551368e-06, 4.16823554721207e-06], "duration": 79235.449509, "accuracy_train": [0.6284470588235294, 0.7092705882352941, 0.7528235294117647, 0.7805647058823529, 0.8031764705882353, 0.8200941176470589, 0.8352470588235295, 0.8469411764705882, 0.8521647058823529, 0.8632941176470589, 0.871035294117647, 0.8814352941176471, 0.8869882352941176, 0.9003294117647059, 0.9061176470588236, 0.9071058823529412, 0.9019294117647059, 0.9205411764705882, 0.9224470588235294, 0.9282117647058824, 0.9311764705882353, 0.93, 0.9424, 0.9464941176470588, 0.9414117647058824, 0.9520705882352941, 0.9441647058823529, 0.9437882352941176, 0.9583764705882353, 0.9563529411764706, 0.9553882352941176, 0.958635294117647, 0.9588941176470588, 0.9581176470588235, 0.9610352941176471, 0.9585882352941176, 0.9675764705882353, 0.9665411764705882, 0.9659529411764706, 0.9724235294117647, 0.9703529411764706, 0.9700941176470588, 0.9700470588235294, 0.9662117647058823, 0.9709411764705882, 0.9753176470588235, 0.969764705882353, 0.9743529411764705, 0.9707058823529412, 0.9740941176470588, 0.9779764705882353, 0.9723529411764706, 0.9743058823529411, 0.9782117647058823, 0.9760235294117647, 0.9715764705882353, 0.981035294117647, 0.9770352941176471, 0.9787529411764706, 0.9756470588235294, 0.9812470588235294, 0.9759529411764706, 0.9782823529411765, 0.9789882352941176, 0.9760941176470588, 0.9820705882352941, 0.9824235294117647, 0.9844470588235295, 0.9806823529411764, 0.9816470588235294, 0.9832470588235294, 0.9816470588235294, 0.9814823529411765, 0.9780705882352941, 0.9822117647058823, 0.9821882352941177, 0.9855058823529412, 0.9821882352941177, 0.9807058823529412, 0.9765882352941176, 0.9806117647058824, 0.9814588235294117, 0.9761882352941177, 0.9851529411764706, 0.9814823529411765, 0.9816235294117647, 0.9817411764705882, 0.9864705882352941, 0.98, 0.9846823529411765, 0.984635294117647, 0.9739764705882353, 0.9836941176470588, 0.9803294117647059, 0.9844, 0.9865647058823529, 0.9836, 0.9792, 0.9847294117647059, 0.9835764705882353, 0.983835294117647, 0.9870117647058824, 0.9827529411764706, 0.9729176470588236, 0.9873176470588235, 0.9842352941176471, 0.9852470588235294, 0.9887764705882353, 0.9748470588235294, 0.989035294117647, 0.9836235294117647, 0.9882823529411765, 0.983835294117647, 0.9864941176470589, 0.9822117647058823, 0.9832, 0.9863529411764705, 0.9860470588235294, 0.9877411764705882, 0.9865882352941177, 0.9876470588235294, 0.9864235294117647, 0.9875294117647059, 0.9856, 0.9864235294117647, 0.9868235294117647, 0.9911529411764706, 0.9888705882352942, 0.9925176470588235, 0.9936941176470588, 0.9945882352941177, 0.9951294117647059, 0.9957411764705882, 0.9882117647058823, 0.9883058823529411, 0.9863764705882353, 0.9884470588235295, 0.9876705882352941, 0.9868235294117647, 0.9796705882352941, 0.988, 0.9880470588235294, 0.9886117647058823, 0.9885882352941177, 0.9875294117647059, 0.9862588235294117, 0.9855529411764706, 0.9885176470588235, 0.9901882352941177, 0.9867764705882353, 0.9900705882352941, 0.9872235294117647, 0.9877882352941176, 0.9916941176470588, 0.9930117647058824, 0.9943058823529412, 0.9951764705882353, 0.9957882352941176, 0.9960705882352942, 0.989035294117647, 0.9849411764705882, 0.9874117647058823, 0.9903058823529411, 0.9849176470588236, 0.9892705882352941, 0.9888, 0.9862117647058823, 0.987764705882353, 0.9892941176470588, 0.9873411764705883, 0.9857176470588235, 0.9871529411764706, 0.9908705882352942, 0.9886117647058823, 0.9860941176470588, 0.9888705882352942, 0.9887764705882353, 0.9882352941176471, 0.9883529411764705, 0.9910823529411765, 0.9889411764705882, 0.9872941176470589, 0.9907764705882353, 0.9873176470588235, 0.9901411764705882, 0.9887764705882353, 0.989835294117647, 0.9875529411764706, 0.986635294117647, 0.9862588235294117, 0.9876941176470588, 0.9865411764705883, 0.9874117647058823, 0.986964705882353, 0.9871294117647059, 0.9881411764705882, 0.9898588235294118, 0.9843529411764705, 0.9892705882352941, 0.9846823529411765, 0.9900470588235294, 0.9871294117647059, 0.9891764705882353, 0.9922588235294117, 0.9895764705882353, 0.9914823529411765, 0.9875764705882353, 0.9911058823529412, 0.9894352941176471], "end": "2016-02-05 06:36:02.168000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0], "moving_var_accuracy_valid": [0.03296403359999999, 0.06355454929599999, 0.08842749764975999, 0.1071288104579056, 0.11952225081442354, 0.12688181735761908, 0.13022883941551305, 0.13006714092018173, 0.12732342363563276, 0.12294793067251074, 0.11731075507431954, 0.11094887618017485, 0.10416093393294781, 0.09738148407232147, 0.09038429066472571, 0.08356980624536783, 0.07687443522292543, 0.07073671784672769, 0.06498145527344079, 0.05953299941512794, 0.05442296433997093, 0.04956897879333292, 0.04532762248299184, 0.041289239801352136, 0.03751117304147942, 0.034152811429291906, 0.03096241804965195, 0.028086895773293017, 0.025474513454750214, 0.023083059019538405, 0.020909333652153167, 0.018952978767979092, 0.01715062431376486, 0.015518675102475718, 0.014040166486602166, 0.01269434358516425, 0.01149590054112037, 0.010377929413687214, 0.009365747802928384, 0.008477540700203694, 0.007648615449176148, 0.006897781152654916, 0.006227812365641399, 0.005610255509855062, 0.005075619975087166, 0.00457844834480161, 0.0041208962652583255, 0.0037212460961288133, 0.003363976847878412, 0.0030279258942370836, 0.002730017205609526, 0.0024574629917778834, 0.002213464544482117, 0.0019974535300621363, 0.0018021977931931973, 0.0016223711231701302, 0.0014667095728165909, 0.0013211789346894475, 0.0011895809853939236, 0.0010706252843660934, 0.0009661357899230952, 0.0008695489959049369, 0.0007908154771657944, 0.0007194654741535196, 0.0006475222319551921, 0.0005836692021550393, 0.0005291967168887154, 0.00048443428985382524, 0.0004361425865868085, 0.0003933194174409894, 0.000355070092882222, 0.000319850851494248, 0.00028805691366289254, 0.00025926410081950595, 0.0002347250561730602, 0.0002128403478786316, 0.00019228574945894986, 0.00017315410497708357, 0.00015893700879572709, 0.00014315608122404504, 0.00012939138684030883, 0.00011663544249004519, 0.00010517873139854668, 9.47445586630663e-05, 8.548205180411122e-05, 7.708257927234468e-05, 6.981933421348067e-05, 6.418389080668782e-05, 5.875683019245599e-05, 5.3495663137579945e-05, 4.81572312135684e-05, 4.484221233594822e-05, 4.040528469054225e-05, 3.649129564436775e-05, 3.5153026423479236e-05, 3.669184051067571e-05, 3.398957989600615e-05, 3.2216271421953114e-05, 2.9164793391508952e-05, 2.6248440439705296e-05, 2.438029110450939e-05, 2.3059735495894688e-05, 2.202750989875541e-05, 2.7309054056090678e-05, 2.6262512434503058e-05, 2.4908752066287748e-05, 2.2558683895157204e-05, 2.4214185464395922e-05, 2.338050165322891e-05, 2.319146640613634e-05, 2.092961139207966e-05, 2.1446235337127394e-05, 2.177870666281058e-05, 1.9604027232921824e-05, 2.0429713184369204e-05, 1.951510049816229e-05, 1.7668554758988416e-05, 1.6105627179295175e-05, 1.4966244274514018e-05, 1.4020506146143086e-05, 1.2822763657582657e-05, 1.1958842845660659e-05, 1.3660183640978998e-05, 1.3705719971082029e-05, 1.2594513894193782e-05, 1.1451785980331781e-05, 1.526828949068719e-05, 1.3765567755061272e-05, 1.3878049155909942e-05, 1.450163579848857e-05, 1.6572393577863334e-05, 1.7014090610089808e-05, 1.7118731511634904e-05, 1.702101787541979e-05, 1.613513112007456e-05, 1.45223415331754e-05, 1.3188586926291215e-05, 1.31471080273358e-05, 1.187956917694659e-05, 2.0648615456820324e-05, 1.950549839532419e-05, 1.773478199059142e-05, 1.6212967502170468e-05, 1.4850248232726805e-05, 1.3384172206333634e-05, 1.3495104109908949e-05, 1.2184205850875046e-05, 1.1031757072948518e-05, 9.957086691112864e-06, 8.998223470172816e-06, 8.116025415079878e-06, 7.329857019015535e-06, 1.0503294004342394e-05, 1.0574065458363352e-05, 1.231538197461558e-05, 1.3116306320447704e-05, 1.30659847018745e-05, 1.4102649548893295e-05, 1.761055534309202e-05, 1.693959152453801e-05, 1.9654234372039885e-05, 1.848037774516671e-05, 1.66339989332289e-05, 1.554312935177294e-05, 1.4119131348336628e-05, 1.2731276570037011e-05, 1.2067178055179792e-05, 1.1074239675839108e-05, 1.0783802857600163e-05, 1.052719007168014e-05, 1.1258999775051937e-05, 1.177733225802282e-05, 1.2807650438522505e-05, 1.2131168075778072e-05, 1.1959225406006667e-05, 1.0955446107866887e-05, 1.0624664551719591e-05, 9.570836574199537e-06, 8.620750083677628e-06, 7.800583649258924e-06, 7.635564333272106e-06, 7.62165187455521e-06, 7.469076229425117e-06, 7.20841550573884e-06, 6.692255409552916e-06, 6.65160038066144e-06, 5.999332117062679e-06, 5.402741161001964e-06, 6.370374860020442e-06, 6.288618805415451e-06, 5.703582306837511e-06, 5.228341182855005e-06, 4.809468458099179e-06, 4.653635286630974e-06, 4.494267327188179e-06, 4.065046706287557e-06, 4.188429046239165e-06, 7.55206638615561e-06, 7.518724156244537e-06, 8.943469716353946e-06, 8.676698978573873e-06, 8.823123630802038e-06, 7.943692772028002e-06, 7.802691267913304e-06, 7.260059391279103e-06, 7.6130185783779995e-06, 7.182179837840141e-06, 8.646593702933944e-06, 7.804543675869038e-06], "accuracy_test": 0.757, "start": "2016-02-04 08:35:26.718000", "learning_rate_per_epoch": [0.001378518296405673, 0.0009747596341185272, 0.0007958879577927291, 0.0006892591482028365, 0.0006164921214804053, 0.0005627777427434921, 0.0005210309755057096, 0.0004873798170592636, 0.00045950611820444465, 0.00043592575821094215, 0.00041563890408724546, 0.00039794397889636457, 0.00038233218947425485, 0.00036842451663687825, 0.0003559319011401385, 0.00034462957410141826, 0.00033433979842811823, 0.00032491987803950906, 0.0003162538050673902, 0.00030824606074020267, 0.0003008173662237823, 0.00029390110285021365, 0.0002874409547075629, 0.00028138887137174606, 0.0002757036709226668, 0.0002703497011680156, 0.0002652959665283561, 0.0002605154877528548, 0.0002559844288043678, 0.00025168186402879655, 0.0002475891960784793, 0.0002436899085296318, 0.0002399692457402125, 0.00023641393636353314, 0.0002330121205886826, 0.00022975305910222232, 0.00022662701667286456, 0.00022362520394381136, 0.00022073958825785667, 0.00021796287910547107, 0.00021528839715756476, 0.00021271000150591135, 0.0002102220751112327, 0.00020781945204362273, 0.00020549737382680178, 0.00020325144578237087, 0.0002010775642702356, 0.00019897198944818228, 0.00019693118520081043, 0.00019495193555485457, 0.00019303117005620152, 0.00019116609473712742, 0.00018935406114906073, 0.00018759258091449738, 0.00018587936938274652, 0.00018421225831843913, 0.00018258921045344323, 0.0001810083194868639, 0.00017946779553312808, 0.00017796595057006925, 0.0001765011838870123, 0.0001750719966366887, 0.00017367699183523655, 0.00017231478705070913, 0.00017098415992222726, 0.00016968387353699654, 0.0001684128219494596, 0.00016716989921405911, 0.0001659541012486443, 0.00016476445307489485, 0.00016360002337023616, 0.00016245993901975453, 0.00016134337056428194, 0.0001602495030965656, 0.00015917757991701365, 0.0001581269025336951, 0.00015709674335084856, 0.00015608646208420396, 0.0001550954330014065, 0.00015412303037010133, 0.0001531687012175098, 0.000152231878018938, 0.00015131205145735294, 0.00015040868311189115, 0.00014952130732126534, 0.0001486494584241882, 0.00014779268531128764, 0.00014695055142510682, 0.00014612264931201935, 0.00014530858607031405, 0.000144507983350195, 0.00014372047735378146, 0.00014294568973127753, 0.00014218331489246339, 0.00014143300359137356, 0.00014069443568587303, 0.00013996733468957245, 0.0001392513804603368, 0.00013854631106369197, 0.0001378518354613334, 0.00013716770627070218, 0.000136493647005409, 0.00013582945393864065, 0.0001351748505840078, 0.000134529618662782, 0.00013389353989623487, 0.00013326639600563794, 0.00013264798326417804, 0.00013203811249695718, 0.00013143656542524695, 0.0001308431674260646, 0.0001302577438764274, 0.00012968009104952216, 0.0001291100779781118, 0.00012854750093538314, 0.0001279922144021839, 0.00012744405830744654, 0.00012690290168393403, 0.00012636856990866363, 0.00012584093201439828, 0.0001253198424819857, 0.00012480518489610404, 0.00012429681373760104, 0.00012379459803923965, 0.00012329842138569802, 0.00012280816736165434, 0.00012232371955178678, 0.0001218449542648159, 0.00012137176963733509, 0.00012090405652998015, 0.00012044170580338687, 0.00011998462287010625, 0.00011953269859077409, 0.00011908584565389901, 0.00011864396947203204, 0.00011820696818176657, 0.00011777476902352646, 0.00011734727013390511, 0.00011692439875332639, 0.0001165060602943413, 0.00011609218927333131, 0.00011568269110284746, 0.00011527749302331358, 0.00011487652955111116, 0.00011447971337474883, 0.00011408698628656566, 0.00011369827552698553, 0.00011331350833643228, 0.00011293261923128739, 0.00011255555000388995, 0.00011218222789466381, 0.00011181260197190568, 0.00011144659947603941, 0.0001110841694753617, 0.00011072525376221165, 0.00011036979412892833, 0.00011001773964380845, 0.0001096690320991911, 0.00010932361328741536, 0.00010898143955273554, 0.00010864246723940596, 0.0001083066308638081, 0.00010797388677019626, 0.00010764419857878238, 0.00010731750808190554, 0.00010699377162382007, 0.00010667295282473788, 0.00010635500075295568, 0.00010603987175272778, 0.00010572752944426611, 0.00010541793017182499, 0.00010511103755561635, 0.0001048068079398945, 0.00010450520494487137, 0.00010420619219075888, 0.00010390972602181137, 0.00010361578461015597, 0.0001033243170240894, 0.00010303529415978119, 0.00010274868691340089, 0.00010246445890516043, 0.00010218257375527173, 0.00010190300963586196, 0.00010162572289118543, 0.00010135068441741168, 0.00010107786511071026, 0.00010080724314320832, 0.0001005387821351178, 0.00010027245298260823, 0.00010000823385780677, 9.974608838092536e-05, 9.948599472409114e-05, 9.922792378347367e-05, 9.897184645524248e-05, 9.871774818748236e-05, 9.846559260040522e-05, 9.821536514209583e-05, 9.796702943276614e-05, 9.77205709205009e-05, 9.747596777742729e-05, 9.723318362375721e-05, 9.699221118353307e-05, 9.67530213529244e-05, 9.651558502810076e-05, 9.627989493310452e-05, 9.604592196410522e-05, 9.581364429323003e-05, 9.558304736856371e-05, 9.535410936223343e-05], "accuracy_train_first": 0.6284470588235294, "accuracy_train_last": 0.9894352941176471, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.39480000000000004, 0.32586666666666664, 0.2890666666666667, 0.266, 0.2572, 0.25, 0.24480000000000002, 0.2466666666666667, 0.24919999999999998, 0.24839999999999995, 0.2506666666666667, 0.2512, 0.25226666666666664, 0.24813333333333332, 0.2545333333333333, 0.25439999999999996, 0.26, 0.25106666666666666, 0.24813333333333332, 0.24906666666666666, 0.24946666666666661, 0.25573333333333337, 0.2393333333333333, 0.24546666666666672, 0.24973333333333336, 0.23986666666666667, 0.2493333333333333, 0.24480000000000002, 0.2426666666666667, 0.24306666666666665, 0.24186666666666667, 0.238, 0.2406666666666667, 0.23919999999999997, 0.238, 0.23826666666666663, 0.23306666666666664, 0.23960000000000004, 0.23960000000000004, 0.23160000000000003, 0.238, 0.23853333333333337, 0.23493333333333333, 0.2406666666666667, 0.23040000000000005, 0.23506666666666665, 0.24293333333333333, 0.2328, 0.23053333333333337, 0.2401333333333333, 0.23453333333333337, 0.23893333333333333, 0.23653333333333337, 0.2328, 0.2326666666666667, 0.23693333333333333, 0.23026666666666662, 0.23440000000000005, 0.23519999999999996, 0.23719999999999997, 0.23199999999999998, 0.23626666666666662, 0.22719999999999996, 0.22653333333333336, 0.23506666666666665, 0.23173333333333335, 0.22799999999999998, 0.22440000000000004, 0.23426666666666662, 0.2301333333333333, 0.22933333333333328, 0.2306666666666667, 0.23373333333333335, 0.2328, 0.22853333333333337, 0.22786666666666666, 0.2288, 0.23240000000000005, 0.23733333333333329, 0.23093333333333332, 0.2294666666666667, 0.23026666666666662, 0.23306666666666664, 0.2326666666666667, 0.23333333333333328, 0.2306666666666667, 0.22960000000000003, 0.22773333333333334, 0.23453333333333337, 0.22893333333333332, 0.23093333333333332, 0.23533333333333328, 0.23093333333333332, 0.23040000000000005, 0.22640000000000005, 0.2234666666666667, 0.22693333333333332, 0.2341333333333333, 0.22893333333333332, 0.2301333333333333, 0.23306666666666664, 0.22693333333333332, 0.23386666666666667, 0.23960000000000004, 0.22706666666666664, 0.22719999999999996, 0.22933333333333328, 0.22386666666666666, 0.23399999999999999, 0.22533333333333339, 0.22893333333333332, 0.22426666666666661, 0.22386666666666666, 0.22840000000000005, 0.2341333333333333, 0.2326666666666667, 0.22840000000000005, 0.22786666666666666, 0.22693333333333332, 0.2314666666666667, 0.22773333333333334, 0.22693333333333332, 0.22319999999999995, 0.23226666666666662, 0.23040000000000005, 0.22773333333333334, 0.22133333333333338, 0.22853333333333337, 0.22399999999999998, 0.22293333333333332, 0.22093333333333331, 0.22173333333333334, 0.22160000000000002, 0.22986666666666666, 0.22906666666666664, 0.22626666666666662, 0.22519999999999996, 0.22999999999999998, 0.2273333333333334, 0.23719999999999997, 0.22453333333333336, 0.22599999999999998, 0.22560000000000002, 0.2288, 0.22773333333333334, 0.23133333333333328, 0.22706666666666664, 0.2268, 0.2281333333333333, 0.22826666666666662, 0.2281333333333333, 0.22826666666666662, 0.22119999999999995, 0.22360000000000002, 0.22119999999999995, 0.2214666666666667, 0.22199999999999998, 0.22026666666666672, 0.2174666666666667, 0.22760000000000002, 0.2314666666666667, 0.2281333333333333, 0.22560000000000002, 0.22799999999999998, 0.22693333333333332, 0.22533333333333339, 0.22840000000000005, 0.22760000000000002, 0.22319999999999995, 0.22893333333333332, 0.2306666666666667, 0.23093333333333332, 0.2221333333333333, 0.22399999999999998, 0.22973333333333334, 0.2281333333333333, 0.22973333333333334, 0.2268, 0.2268, 0.22773333333333334, 0.22973333333333334, 0.23026666666666662, 0.22506666666666664, 0.22973333333333334, 0.2261333333333333, 0.2301333333333333, 0.2281333333333333, 0.22760000000000002, 0.23186666666666667, 0.2306666666666667, 0.22773333333333334, 0.2273333333333334, 0.22933333333333328, 0.23026666666666662, 0.23040000000000005, 0.22826666666666662, 0.22626666666666662, 0.23493333333333333, 0.22626666666666662, 0.23373333333333335, 0.22666666666666668, 0.23240000000000005, 0.22919999999999996, 0.22666666666666668, 0.2274666666666667, 0.2254666666666667, 0.22666666666666668, 0.2234666666666667, 0.22840000000000005], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.05514595066574011, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.0013785183308668858, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 5.7347240986946016e-08, "rotation_range": [0, 0], "momentum": 0.5814381850820357}, "accuracy_valid_max": 0.7825333333333333, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7716, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.6052, 0.6741333333333334, 0.7109333333333333, 0.734, 0.7428, 0.75, 0.7552, 0.7533333333333333, 0.7508, 0.7516, 0.7493333333333333, 0.7488, 0.7477333333333334, 0.7518666666666667, 0.7454666666666667, 0.7456, 0.74, 0.7489333333333333, 0.7518666666666667, 0.7509333333333333, 0.7505333333333334, 0.7442666666666666, 0.7606666666666667, 0.7545333333333333, 0.7502666666666666, 0.7601333333333333, 0.7506666666666667, 0.7552, 0.7573333333333333, 0.7569333333333333, 0.7581333333333333, 0.762, 0.7593333333333333, 0.7608, 0.762, 0.7617333333333334, 0.7669333333333334, 0.7604, 0.7604, 0.7684, 0.762, 0.7614666666666666, 0.7650666666666667, 0.7593333333333333, 0.7696, 0.7649333333333334, 0.7570666666666667, 0.7672, 0.7694666666666666, 0.7598666666666667, 0.7654666666666666, 0.7610666666666667, 0.7634666666666666, 0.7672, 0.7673333333333333, 0.7630666666666667, 0.7697333333333334, 0.7656, 0.7648, 0.7628, 0.768, 0.7637333333333334, 0.7728, 0.7734666666666666, 0.7649333333333334, 0.7682666666666667, 0.772, 0.7756, 0.7657333333333334, 0.7698666666666667, 0.7706666666666667, 0.7693333333333333, 0.7662666666666667, 0.7672, 0.7714666666666666, 0.7721333333333333, 0.7712, 0.7676, 0.7626666666666667, 0.7690666666666667, 0.7705333333333333, 0.7697333333333334, 0.7669333333333334, 0.7673333333333333, 0.7666666666666667, 0.7693333333333333, 0.7704, 0.7722666666666667, 0.7654666666666666, 0.7710666666666667, 0.7690666666666667, 0.7646666666666667, 0.7690666666666667, 0.7696, 0.7736, 0.7765333333333333, 0.7730666666666667, 0.7658666666666667, 0.7710666666666667, 0.7698666666666667, 0.7669333333333334, 0.7730666666666667, 0.7661333333333333, 0.7604, 0.7729333333333334, 0.7728, 0.7706666666666667, 0.7761333333333333, 0.766, 0.7746666666666666, 0.7710666666666667, 0.7757333333333334, 0.7761333333333333, 0.7716, 0.7658666666666667, 0.7673333333333333, 0.7716, 0.7721333333333333, 0.7730666666666667, 0.7685333333333333, 0.7722666666666667, 0.7730666666666667, 0.7768, 0.7677333333333334, 0.7696, 0.7722666666666667, 0.7786666666666666, 0.7714666666666666, 0.776, 0.7770666666666667, 0.7790666666666667, 0.7782666666666667, 0.7784, 0.7701333333333333, 0.7709333333333334, 0.7737333333333334, 0.7748, 0.77, 0.7726666666666666, 0.7628, 0.7754666666666666, 0.774, 0.7744, 0.7712, 0.7722666666666667, 0.7686666666666667, 0.7729333333333334, 0.7732, 0.7718666666666667, 0.7717333333333334, 0.7718666666666667, 0.7717333333333334, 0.7788, 0.7764, 0.7788, 0.7785333333333333, 0.778, 0.7797333333333333, 0.7825333333333333, 0.7724, 0.7685333333333333, 0.7718666666666667, 0.7744, 0.772, 0.7730666666666667, 0.7746666666666666, 0.7716, 0.7724, 0.7768, 0.7710666666666667, 0.7693333333333333, 0.7690666666666667, 0.7778666666666667, 0.776, 0.7702666666666667, 0.7718666666666667, 0.7702666666666667, 0.7732, 0.7732, 0.7722666666666667, 0.7702666666666667, 0.7697333333333334, 0.7749333333333334, 0.7702666666666667, 0.7738666666666667, 0.7698666666666667, 0.7718666666666667, 0.7724, 0.7681333333333333, 0.7693333333333333, 0.7722666666666667, 0.7726666666666666, 0.7706666666666667, 0.7697333333333334, 0.7696, 0.7717333333333334, 0.7737333333333334, 0.7650666666666667, 0.7737333333333334, 0.7662666666666667, 0.7733333333333333, 0.7676, 0.7708, 0.7733333333333333, 0.7725333333333333, 0.7745333333333333, 0.7733333333333333, 0.7765333333333333, 0.7716], "seed": 375684552, "model": "residualv3", "loss_std": [0.36114102602005005, 0.28614693880081177, 0.2668430507183075, 0.2564411163330078, 0.24755878746509552, 0.23751960694789886, 0.22703242301940918, 0.21649323403835297, 0.20334526896476746, 0.18900848925113678, 0.17325685918331146, 0.15559834241867065, 0.1369773894548416, 0.12205909937620163, 0.11257809400558472, 0.10344909131526947, 0.09661029279232025, 0.08479515463113785, 0.07633841037750244, 0.07583243399858475, 0.07306724041700363, 0.06514652818441391, 0.06472305953502655, 0.06819828599691391, 0.058997124433517456, 0.05882653221487999, 0.05417256057262421, 0.05689501017332077, 0.052696727216243744, 0.05399326980113983, 0.04958830028772354, 0.05160270258784294, 0.05033915489912033, 0.04451007395982742, 0.0449359193444252, 0.050516724586486816, 0.04108254611492157, 0.04534439370036125, 0.04160763695836067, 0.04756079241633415, 0.04073211923241615, 0.03725571930408478, 0.0391317680478096, 0.04260381683707237, 0.04159165918827057, 0.036531392484903336, 0.03789196163415909, 0.03769517317414284, 0.03560067340731621, 0.03600730001926422, 0.03378133848309517, 0.03923596069216728, 0.03414743021130562, 0.02743157185614109, 0.03469882160425186, 0.03171813488006592, 0.030418211594223976, 0.03274798393249512, 0.0300556980073452, 0.027831124141812325, 0.0308858472853899, 0.028606386855244637, 0.03334873542189598, 0.022410694509744644, 0.02906913124024868, 0.033169008791446686, 0.02979748509824276, 0.024109438061714172, 0.028457779437303543, 0.026617664843797684, 0.023554174229502678, 0.027174118906259537, 0.0200691819190979, 0.027879709377884865, 0.019395140931010246, 0.027840057387948036, 0.020228983834385872, 0.030992357060313225, 0.019318487495183945, 0.020885556936264038, 0.026078836992383003, 0.028369054198265076, 0.023896947503089905, 0.02326803281903267, 0.030591441318392754, 0.027492860332131386, 0.024078864604234695, 0.02799384295940399, 0.019663909450173378, 0.020419122651219368, 0.025609416887164116, 0.025307659059762955, 0.01851552352309227, 0.026663847267627716, 0.020375249907374382, 0.010228835977613926, 0.025064818561077118, 0.024246331304311752, 0.01604929380118847, 0.022510753944516182, 0.019511407241225243, 0.015916844829916954, 0.022369464859366417, 0.01841864548623562, 0.020202092826366425, 0.023794740438461304, 0.013906841166317463, 0.01993451826274395, 0.016345975920557976, 0.021421024575829506, 0.016476614400744438, 0.019111447036266327, 0.019159629940986633, 0.016179047524929047, 0.020713381469249725, 0.019994480535387993, 0.024017436429858208, 0.02021176554262638, 0.007250522263348103, 0.02997402474284172, 0.011434411630034447, 0.02530183456838131, 0.014764982275664806, 0.023058971390128136, 0.013321520760655403, 0.02045985497534275, 0.006892367731779814, 0.027935631573200226, 0.0023276975844055414, 0.00031476703588850796, 3.6898643884342164e-05, 1.276422608498251e-05, 9.479620530328248e-06, 0.03971162810921669, 0.014570143073797226, 0.019131824374198914, 0.021311424672603607, 0.023718077689409256, 0.015843739733099937, 0.016630744561553, 0.017971929162740707, 0.015044696629047394, 0.006403022911399603, 0.018824009224772453, 0.015878992155194283, 0.014392269775271416, 0.020321959629654884, 0.018145080655813217, 0.011356808245182037, 0.020881274715065956, 0.005076015368103981, 0.02370869182050228, 0.016198154538869858, 0.013041877187788486, 0.000744248041883111, 9.116403816733509e-05, 1.5387435269076377e-05, 7.324977104872232e-06, 9.687686542747542e-06, 0.036798734217882156, 0.014046801254153252, 0.01897403970360756, 0.009109506383538246, 0.02122865989804268, 0.013287077657878399, 0.02458685263991356, 0.01857651397585869, 0.01180275995284319, 0.024961253628134727, 0.019930019974708557, 0.01246634591370821, 0.018159721046686172, 0.014507963322103024, 0.00990331918001175, 0.022165605798363686, 0.009516159072518349, 0.01110853161662817, 0.01760966330766678, 0.013474822044372559, 0.011996888555586338, 0.015104767866432667, 0.012751977890729904, 0.010741992853581905, 0.017026159912347794, 0.012022187002003193, 0.016730904579162598, 0.015164648182690144, 0.015192163176834583, 0.01246460247784853, 0.009884932078421116, 0.02018551714718342, 0.00920198205858469, 0.012806756421923637, 0.013323427177965641, 0.016914203763008118, 0.016079645603895187, 0.010788376443088055, 0.011487487703561783, 0.014473550021648407, 0.015790410339832306, 0.010870636440813541, 0.01399010419845581, 0.01299082301557064, 0.003421770641580224, 0.020921891555190086, 0.01466311514377594, 0.016095390543341637, 0.009361295029520988, 0.008177884854376316]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:39 2016", "state": "available"}], "summary": "3c90d19ecfa216ebfa07ba5e8c8e3288"}