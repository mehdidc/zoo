{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 32, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.007405013782802182, 0.005111534774415311, 0.009966274876418414, 0.009610932008587762, 0.010201998197532541, 0.010149972579093038, 0.00965001445770205, 0.008544295464240617, 0.009943393889271766, 0.0057866855627977, 0.0066612947878282, 0.00690715622006157, 0.007889897759217702, 0.009095626101541129, 0.008125565528558405, 0.008226946638720532, 0.011887102506304795, 0.011994995506945348, 0.011308537957763869, 0.011763075820338278, 0.006263164194631996, 0.010754016772346433, 0.012590762920353691, 0.012188587516812427, 0.012258660106956188, 0.011172478795103512, 0.010322252369826722, 0.011660845110051289, 0.005008768163086752, 0.014518790381053968, 0.01140748754834489, 0.011495562990315449, 0.016304197315015173, 0.012682246346969652, 0.007918005369943444, 0.012390128229924767, 0.014279675848449845, 0.011061568179083842, 0.010731532636634079, 0.007088965799134453, 0.008587723948706489, 0.007742642683823684, 0.008944662570272988, 0.00853970312533737, 0.009541497732981534, 0.013743139040707167, 0.007920930036092464, 0.013854986694025967, 0.006860279964859915, 0.009481280850176597, 0.0115570055351004, 0.014162073502941378, 0.013861187865423204, 0.008848342844884215, 0.007570555714896649, 0.009147851587782389, 0.012740096041591133, 0.008300763400091372, 0.012702653106694394, 0.010215776930345942, 0.005015515162901424, 0.0068115821913697664, 0.007149636584380699, 0.011437441607232663, 0.008718673539629945, 0.008753455564659292, 0.00786125272829809], "moving_avg_accuracy_train": [0.008238560988833516, 0.01719204575604466, 0.025475973822789766, 0.03352907232690798, 0.04080018456634712, 0.04787886166157656, 0.054479680535332156, 0.06053910825745416, 0.06572956300133777, 0.06916482222699764, 0.07291163466445845, 0.07576322091538987, 0.07917084772681324, 0.08088476364050141, 0.083327192649744, 0.08554441253436355, 0.09187276215163595, 0.09574107033108292, 0.09672759092232863, 0.09820855264497211, 0.10121530904529587, 0.10237527399371017, 0.10338905156157799, 0.10386003358703702, 0.10211455357066444, 0.101041527840536, 0.10100811725840193, 0.102274687696183, 0.10377034885804311, 0.10656719256558504, 0.10764548132628382, 0.10760729526222318, 0.11085506460550991, 0.11148106233238969, 0.11099850381048332, 0.11120561777282738, 0.11082028607352544, 0.10985023951674507, 0.10977933790610969, 0.11041550439466077, 0.11070609839848558, 0.11082572682690944, 0.1128054255947076, 0.11232783081924386, 0.11194166866524491, 0.11375189687070271, 0.11253054191277603, 0.11495570731359883, 0.1146676602591031, 0.11318553383152244, 0.11336228184533402, 0.11170845996271091, 0.11288707409933017, 0.11151034989353152, 0.11155914215823891, 0.10920858450849105, 0.10755618377381951, 0.10661718145056269, 0.10591906841809409, 0.10528389938890004, 0.10470552415792808, 0.10429924318108875, 0.10364873253615263, 0.10248277882734172, 0.10217671108326573, 0.1019476809921503, 0.10221181075077027], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 564922615, "moving_var_accuracy_train": [0.0006108649845005654, 0.0012712624913406474, 0.0017617474201416488, 0.0021692442377807637, 0.002428141472789382, 0.0026362963502771777, 0.002764804003486613, 0.0028187735820145382, 0.0027793636078478185, 0.002607636300590368, 0.0024732201015049307, 0.0022990819886729475, 0.002173681074179035, 0.001982750536593874, 0.0018381646181211942, 0.0016985927324598457, 0.0018891655391197457, 0.0018349232587483576, 0.001660189938766088, 0.0015139101735048952, 0.0014438844126123966, 0.0013116056394851053, 0.0011896947801506026, 0.0010727217187502917, 0.0009928698512632669, 0.0009039453240945996, 0.0008135608380881265, 0.0007466425603440622, 0.0006921113251095265, 0.0006933012051183264, 0.0006344354444695377, 0.0005710050236019799, 0.0006088365726065197, 0.0005514797737323959, 0.0004984275609027364, 0.00044897087055304353, 0.0004054101081641214, 0.0003733380102486025, 0.0003360494525692585, 0.00030608687752273136, 0.0002762381936459886, 0.00024874317292937427, 0.00025914172053743105, 0.00023528041940964036, 0.00021309446835130632, 0.00022127735691869003, 0.00021257499262608968, 0.000244250338355613, 0.00022057204447048468, 0.00021828512874943353, 0.00019673777461796723, 0.00020168013853115774, 0.00019401430622539118, 0.00019167120145233938, 0.00017252550747296294, 0.00020499904810875974, 0.00020907299698937144, 0.00019610122555816954, 0.0001808773592572751, 0.00016642058059237344, 0.00015278918370335284, 0.00013899584342229115, 0.00012890473597263887, 0.00012824929483518416, 0.00011626746252733967, 0.00010511280931833276, 9.522940915099725e-05], "duration": 23475.099276, "accuracy_train": [0.08238560988833518, 0.097773408660945, 0.10003132642349574, 0.10600695886397193, 0.10624019472129936, 0.11158695551864158, 0.11388705039913252, 0.11507395775655224, 0.11244365569629015, 0.1000821552579365, 0.10663294660160576, 0.10142749717377261, 0.10983948902962348, 0.09631000686369509, 0.10530905373292729, 0.10549939149593947, 0.14882790870708748, 0.1305558439461056, 0.10560627624354006, 0.11153720814876338, 0.12827611664820968, 0.11281495852943892, 0.11251304967238833, 0.10809887181616833, 0.08640523342331119, 0.09138429626937986, 0.10070742201919528, 0.11367382163621263, 0.11723129931478406, 0.13173878593346253, 0.1173500801725729, 0.10726362068567738, 0.14008498869509042, 0.11711504187430787, 0.10665547711332596, 0.11306964343392395, 0.10735230077980805, 0.10111982050572166, 0.1091412234103913, 0.11614100279162051, 0.11332144443290883, 0.11190238268272426, 0.1306227145048911, 0.10802947784007014, 0.10846620927925435, 0.1300439507198228, 0.10153834729143595, 0.13678219592100407, 0.11207523676864158, 0.09984639598329642, 0.11495301396963824, 0.09682406301910298, 0.12349460132890366, 0.09911983204134367, 0.11199827254060539, 0.08805356566076043, 0.09268457716177557, 0.09816616054125138, 0.0996360511258767, 0.09956737812615357, 0.0995001470791805, 0.10064271438953488, 0.09779413673172757, 0.09198919544804356, 0.09942210138658177, 0.09988641017211149, 0.10458897857834995], "end": "2016-01-21 20:58:57.757000", "learning_rate_per_epoch": [0.005461075808852911, 0.0038615637458860874, 0.0031529534608125687, 0.0027305379044264555, 0.002442267257720232, 0.0022294747177511454, 0.0020640925504267216, 0.0019307818729430437, 0.001820358564145863, 0.0017269437666982412, 0.001646576332859695, 0.0015764767304062843, 0.001514629926532507, 0.0014595339307561517, 0.0014100436819717288, 0.0013652689522132277, 0.0013245054287835956, 0.0012871879152953625, 0.001252856687642634, 0.001221133628860116, 0.001191704417578876, 0.0011643052566796541, 0.001138712977990508, 0.0011147373588755727, 0.0010922151850536466, 0.00107100501190871, 0.0010509844869375229, 0.0010320462752133608, 0.0010140963131561875, 0.0009970514802262187, 0.0009808382019400597, 0.0009653909364715219, 0.0009506512433290482, 0.0009365667356178164, 0.0009230902651324868, 0.0009101792820729315, 0.0008977953111752868, 0.0008859034860506654, 0.0008744719671085477, 0.0008634718833491206, 0.0008528767502866685, 0.0008426622953265905, 0.0008328062249347568, 0.0008232881664298475, 0.000814089085906744, 0.000805191695690155, 0.0007965797558426857, 0.0007882383652031422, 0.000780153670348227, 0.0007723127491772175, 0.0007647035527043045, 0.0007573149632662535, 0.0007501364452764392, 0.0007431582780554891, 0.0007363713230006397, 0.0007297669653780758, 0.0007233371725305915, 0.0007170743774622679, 0.0007109714788384736, 0.0007050218409858644, 0.0006992191192694008, 0.0006935573183000088, 0.0006880308501422405, 0.0006826344761066139, 0.0006773630739189684, 0.0006722119287587702, 0.0006671765586361289], "accuracy_valid": [0.08035609233810241, 0.09517807558358433, 0.09318377023719879, 0.10028443853539157, 0.09572665662650602, 0.10551287179969879, 0.1146078454442771, 0.11696836172816265, 0.10918527626129518, 0.10445541933358433, 0.10900290615587349, 0.10388624811746988, 0.11415044945406627, 0.09748711643448796, 0.10745717243975904, 0.1095529579254518, 0.1466843938253012, 0.13506712396460843, 0.10517901684864459, 0.11214731974774096, 0.12910479809864459, 0.10451718985316265, 0.1073556923004518, 0.1033273719879518, 0.08309311464608433, 0.08553452089608433, 0.08970403096762047, 0.10544227692018072, 0.10555405214608433, 0.1297445641942771, 0.11152667309864459, 0.10490252023719879, 0.13545245434864459, 0.11885089184864459, 0.10409950348268072, 0.11075307087725904, 0.1085043298192771, 0.10392742846385541, 0.10855580525225904, 0.11268560570406627, 0.11624623493975904, 0.11196347891566265, 0.1284738563629518, 0.10512754141566265, 0.10502606127635541, 0.1313638342432229, 0.09972556240587349, 0.13175063535391568, 0.11601238940135541, 0.09866663921310241, 0.11095603115587349, 0.10076242469879518, 0.13310223315135541, 0.09631788874246988, 0.1094808923192771, 0.08629782803087349, 0.08740675592996988, 0.09655173428087349, 0.10173898719879518, 0.09891077983810241, 0.10523931664156627, 0.10770131306475904, 0.0948324548192771, 0.08938929546310241, 0.09778273249246988, 0.09814894342996988, 0.10345826666039157], "accuracy_test": 0.1423168789808917, "start": "2016-01-21 14:27:42.658000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0], "accuracy_train_last": 0.10458897857834995, "batch_size_eval": 1024, "accuracy_train_std": [0.009650756302029384, 0.009646525352998259, 0.009444403903969223, 0.00982422799316509, 0.011000917805138658, 0.010745649589679763, 0.009497317574246026, 0.011407774172301495, 0.009154032770066234, 0.011327537546378216, 0.008729244336544734, 0.008903091233599529, 0.008280397692991895, 0.008488388217734245, 0.008062359746213943, 0.011257943501609366, 0.010351513620551655, 0.008442280467731267, 0.010157555014073658, 0.009275314837462259, 0.009254319763651426, 0.01064276630486689, 0.009381021845452722, 0.00890395730185265, 0.009918118121703932, 0.007744015561420332, 0.00821292092867337, 0.009765563740768686, 0.009367561656519046, 0.009702538383525817, 0.010998795671601842, 0.00934649954336806, 0.01091035645289849, 0.009491398859863187, 0.010359260656688712, 0.00962510436850326, 0.008834222868224743, 0.010299888813300672, 0.008660366200712031, 0.009808117220390314, 0.011945675540840826, 0.013269449475922727, 0.010984135199886917, 0.009788874529966273, 0.009463362841796302, 0.0074392675399403705, 0.008415193960325843, 0.011444680520681429, 0.008401579230784278, 0.00858776465138847, 0.009986217739701453, 0.009825850182795984, 0.012674168884910093, 0.010174284314762538, 0.009383884205124777, 0.01004574581488671, 0.010471652923251164, 0.01061823016542453, 0.009571940403719875, 0.008409282291259965, 0.008826064924219336, 0.009834871449999026, 0.01052006376683324, 0.007210806668744289, 0.010887975348369097, 0.010653377192697797, 0.008947315951954088], "accuracy_test_std": 0.045701867950974216, "error_valid": [0.9196439076618976, 0.9048219244164156, 0.9068162297628012, 0.8997155614646084, 0.904273343373494, 0.8944871282003012, 0.8853921545557228, 0.8830316382718374, 0.8908147237387049, 0.8955445806664156, 0.8909970938441265, 0.8961137518825302, 0.8858495505459337, 0.9025128835655121, 0.892542827560241, 0.8904470420745482, 0.8533156061746988, 0.8649328760353916, 0.8948209831513554, 0.887852680252259, 0.8708952019013554, 0.8954828101468374, 0.8926443076995482, 0.8966726280120482, 0.9169068853539156, 0.9144654791039156, 0.9102959690323795, 0.8945577230798193, 0.8944459478539156, 0.8702554358057228, 0.8884733269013554, 0.8950974797628012, 0.8645475456513554, 0.8811491081513554, 0.8959004965173193, 0.889246929122741, 0.8914956701807228, 0.8960725715361446, 0.891444194747741, 0.8873143942959337, 0.883753765060241, 0.8880365210843374, 0.8715261436370482, 0.8948724585843374, 0.8949739387236446, 0.8686361657567772, 0.9002744375941265, 0.8682493646460843, 0.8839876105986446, 0.9013333607868976, 0.8890439688441265, 0.8992375753012049, 0.8668977668486446, 0.9036821112575302, 0.8905191076807228, 0.9137021719691265, 0.9125932440700302, 0.9034482657191265, 0.8982610128012049, 0.9010892201618976, 0.8947606833584337, 0.892298686935241, 0.9051675451807228, 0.9106107045368976, 0.9022172675075302, 0.9018510565700302, 0.8965417333396084], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.7684643913651421, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.005461075705392215, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "adam", "nb_data_augmentation": 4, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 8.074265953733542e-09, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.0704859009308447}, "accuracy_valid_max": 0.1466843938253012, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.10345826666039157, "loss_train": [27232269500416.0, 5242338304.0, 2829158912.0, 1893831936.0, 1407497600.0, 1080473856.0, 824323264.0, 649807744.0, 497658496.0, 432901856.0, 322699264.0, 277981792.0, 201501072.0, 188370768.0, 126685464.0, 98216256.0, 80053840.0, 69856600.0, 45237556.0, 38540164.0, 3340937723904.0, 1851915520.0, 673966784.0, 432342752.0, 373755072.0, 303396288.0, 231574592.0, 173982944.0, 159270880.0, 134538320.0, 182933440.0, 68703496.0, 87218536.0, 38389296.0, 836344938496.0, 963638976.0, 351206656.0, 255219008.0, 196381344.0, 150928800.0, 120570360.0, 90720448.0, 71151552.0, 55315076.0, 41898700.0, 29313950.0, 23163034.0, 17408820.0, 13981551.0, 21246236672.0, 220644800.0, 43756536.0, 25476368.0, 18028160.0, 11575745.0, 8399811.0, 6458713.0, 5172268.5, 3823422.75, 3141071.0, 1992943.0, 2553979.5, 1007191.0, 39237936.0, 718543.5625, 399159.6875, 287289.46875], "accuracy_train_first": 0.08238560988833518, "model": "residualv2", "loss_std": [187751341228032.0, 2768510720.0, 1263145216.0, 849080256.0, 644166080.0, 537413632.0, 403986496.0, 312901824.0, 265658288.0, 234963328.0, 163735456.0, 159177216.0, 105182280.0, 153696032.0, 82413200.0, 71047416.0, 59677980.0, 82966904.0, 30521688.0, 29036832.0, 66528066142208.0, 1058855808.0, 315040448.0, 161104800.0, 229245824.0, 234790560.0, 142171808.0, 77312632.0, 134406496.0, 147881344.0, 314813792.0, 66163988.0, 145215760.0, 42938164.0, 10025913286656.0, 537393600.0, 101832344.0, 81550560.0, 65014276.0, 58064688.0, 63581224.0, 34846848.0, 39531056.0, 22903224.0, 19228988.0, 12242577.0, 10401177.0, 8367083.0, 6993147.0, 176995745792.0, 373880224.0, 19943422.0, 10836387.0, 12369918.0, 5406500.5, 3781164.0, 3307090.0, 2970592.25, 2578890.25, 2390727.5, 1408258.875, 4140550.5, 458357.0, 183766784.0, 340888.375, 151373.953125, 112400.3984375]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:06 2016", "state": "available"}], "summary": "d1f845c8cb4dc7c30b8aee6a2d0364eb"}