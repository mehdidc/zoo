{"content": {"hp_model": {"f0": 32, "f1": 64, "f2": 16, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.7721929550170898, 1.4134761095046997, 1.2362914085388184, 1.1082274913787842, 1.0214115381240845, 0.9604461789131165, 0.9161207675933838, 0.8818260431289673, 0.8574333786964417, 0.8366143107414246, 0.8205175995826721, 0.8067209124565125, 0.7931727170944214, 0.7821899652481079, 0.7734814286231995, 0.7679033875465393, 0.7589043974876404, 0.7534483075141907, 0.7475363612174988, 0.7420688271522522, 0.7384620904922485, 0.7328769564628601, 0.7300963401794434, 0.7277209758758545, 0.72381991147995, 0.7205179929733276, 0.7209277153015137, 0.7184173464775085, 0.7152999639511108, 0.7120686173439026, 0.7109271883964539, 0.7088307738304138, 0.7082239985466003, 0.7058095335960388, 0.7030894160270691, 0.7041059732437134, 0.7028268575668335, 0.7019811868667603, 0.6991144418716431, 0.6977088451385498, 0.6977982521057129, 0.6972723007202148, 0.6970747113227844, 0.6970142126083374, 0.6961609721183777, 0.6949540972709656, 0.6972488760948181, 0.6941686272621155, 0.6923695802688599, 0.6917905807495117, 0.6948149800300598, 0.6919984817504883, 0.6925140023231506, 0.6940214037895203, 0.6917073130607605, 0.6912197470664978, 0.6910768151283264, 0.6938064694404602, 0.6932902336120605, 0.6953380107879639, 0.691539466381073, 0.6921527981758118, 0.6918705701828003, 0.6935539245605469, 0.694047212600708, 0.6912356019020081, 0.6928021311759949, 0.6918445229530334, 0.6921792030334473, 0.6935219168663025, 0.6932761669158936, 0.693595826625824, 0.6917057633399963, 0.6905862092971802, 0.6904019117355347, 0.6920661330223083, 0.6930630207061768, 0.6915904879570007, 0.6923290491104126, 0.6940035223960876, 0.6949255466461182, 0.6949825286865234, 0.6964398622512817, 0.6920765042304993, 0.6942654848098755, 0.6928252577781677, 0.6938574910163879, 0.6951648592948914, 0.6944513320922852, 0.6944836378097534, 0.6965471506118774, 0.6942662000656128, 0.6962663531303406, 0.6956783533096313, 0.6966010332107544, 0.695702850818634, 0.6967592835426331, 0.6988875269889832, 0.696233332157135, 0.6961455345153809, 0.6987340450286865, 0.6982328295707703, 0.6981887817382812, 0.6976016759872437, 0.6999749541282654, 0.6988757252693176, 0.6989047527313232, 0.6983829736709595, 0.6998332142829895, 0.7009203433990479, 0.70168536901474, 0.7002775073051453, 0.7043029069900513, 0.7017636895179749, 0.7030866146087646, 0.7030808925628662, 0.7043145298957825, 0.7056827545166016, 0.7071744799613953, 0.7043164968490601, 0.7059457302093506], "moving_avg_accuracy_train": [0.03390551820897932, 0.07867061869866647, 0.13144313136622138, 0.18576362376785135, 0.23709729435410606, 0.28594801518286117, 0.3330730725262399, 0.3776264733589334, 0.41550884753697503, 0.4512378523008578, 0.4857721512346701, 0.5156018739226649, 0.5438549788834346, 0.5701496933658922, 0.5952773108060194, 0.6175086250950188, 0.6385071050550628, 0.6594095466582682, 0.6783773848761162, 0.6930398013984251, 0.7086167484053618, 0.7227149115758537, 0.7342643680984546, 0.7452214568342441, 0.7566708412357218, 0.7667029924446063, 0.7720470363041508, 0.7807763359383167, 0.787481865094808, 0.7923966603417409, 0.7963524048603539, 0.8020399418925448, 0.8062937338155549, 0.8116124227378938, 0.8124516808217087, 0.8158155056220591, 0.8200306303493973, 0.8222344536482061, 0.8261053386693674, 0.8296333130157936, 0.8303442287264882, 0.8328906027922852, 0.8350402890812091, 0.8385976001221007, 0.8400117174054794, 0.839666515926098, 0.8417827492195162, 0.8439175489157356, 0.8458294238518198, 0.847487548569351, 0.8469620281854687, 0.8479000646791681, 0.8471425944331044, 0.8480416840115731, 0.8484232895929739, 0.8491103519565817, 0.8521654279873909, 0.8535734576496614, 0.8534598343433978, 0.854375844351057, 0.8558439590900008, 0.8550385442683485, 0.856896226328686, 0.8574894874282832, 0.8565983585394139, 0.8559379963727187, 0.8574870971368864, 0.8575541685866805, 0.8595183611318515, 0.8615927656748126, 0.8647246107158585, 0.8644770128029566, 0.8642587168325078, 0.8647900220364847, 0.8634965054248444, 0.864312970723076, 0.8664173742379315, 0.867890449417959, 0.8660361705337489, 0.865448153246201, 0.8668437283158683, 0.8678186552143479, 0.8663316654254252, 0.8668693010700366, 0.8643195388813145, 0.865395389562443, 0.8656360998909901, 0.865397154215291, 0.8662492022154581, 0.8653096385311493, 0.8625116271916464, 0.8636428191524835, 0.8654606349612569, 0.8665805582510762, 0.8676024761535894, 0.8677294707170787, 0.8692363136682113, 0.8670520637613385, 0.8689498070437501, 0.8685513353717672, 0.8694996247420287, 0.8719824377096401, 0.8729966988413155, 0.8731933880264899, 0.870929578824247, 0.8717748664313941, 0.8721236774015345, 0.8743557829448806, 0.8719753016649736, 0.8707282310488177, 0.8691341867253128, 0.870465716892298, 0.870952634555689, 0.8681921766672114, 0.8702314947839898, 0.87134168700225, 0.8727151368593802, 0.8710985364832429, 0.871278103660177, 0.8687035186540707, 0.8695386090067331], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.033304163921310234, 0.07701538085937498, 0.12902799234045553, 0.18188623532214793, 0.2320914017936982, 0.2791682885345091, 0.3247021962209678, 0.3683928211489463, 0.40438068747155165, 0.4383790362695772, 0.47110672116859836, 0.4995972821089675, 0.5264494891390346, 0.551227709464589, 0.5743784819104494, 0.5951756440733201, 0.6142520943967863, 0.6342460185226196, 0.6515701930257793, 0.6649258572698882, 0.6791819008256554, 0.6923969350465538, 0.7030126985260852, 0.7127897007461423, 0.7233703468706998, 0.7326487877578015, 0.73747640592291, 0.7452312890975618, 0.7514223744667965, 0.7562903144485807, 0.7598229982239786, 0.7647877287103158, 0.7686640186687269, 0.7733775007909356, 0.773593226333077, 0.7761342178469832, 0.7797404990931583, 0.7812617252098215, 0.7848101514746075, 0.7876454660993606, 0.7885269449977077, 0.7908389483518526, 0.792735763465839, 0.7958183206377039, 0.7975956164918552, 0.7976204757293414, 0.7992643251820096, 0.8011751538177996, 0.8028857810847395, 0.8044965287951662, 0.8036521622955441, 0.8042645674891825, 0.8036906262711377, 0.8042056468518552, 0.8044362022720913, 0.8052612602734214, 0.8085399335213052, 0.8094834917034367, 0.8093205399822647, 0.8102542792351678, 0.8117060256339402, 0.8102851348081215, 0.8122129583943274, 0.8130089406516567, 0.8121597361572742, 0.8111188084282185, 0.8123467360379568, 0.812598408207881, 0.8143691761502706, 0.8159260991320056, 0.8192205962406425, 0.8190716740883404, 0.8186101133248377, 0.8189627220977755, 0.8178243447486907, 0.8184264288167432, 0.8207312645212888, 0.8220549577698376, 0.820371305225158, 0.819113395099856, 0.8206059347784999, 0.8213663715152583, 0.8196193592696059, 0.8199157535384285, 0.8176230027798417, 0.8190419136746738, 0.8192354491520859, 0.8192396150802508, 0.8197479707314878, 0.8184872408646642, 0.8157106647450653, 0.8170713742024112, 0.8183673429568689, 0.8196150460199922, 0.8203626493340321, 0.8200465757127524, 0.8219351080887664, 0.8195248415852361, 0.8208744623307034, 0.8204055801977836, 0.8206792399867402, 0.8229888162233824, 0.8236003849963754, 0.8239087152843885, 0.8214506609622749, 0.8224663629383365, 0.8225289439826052, 0.8246819348441338, 0.822380551177115, 0.8203571044931385, 0.8188889768751499, 0.820228941904126, 0.8209292746056562, 0.8180601270302563, 0.8198736020437217, 0.8210073002919098, 0.8221344034705502, 0.819910697451432, 0.819711002188593, 0.8178539127001554, 0.8188405764470826], "moving_var_accuracy_train": [0.010346257485174858, 0.027346859733323473, 0.04967661659921632, 0.07126539799209455, 0.08785516981560813, 0.10054718916344806, 0.11047940951365383, 0.1172965182941163, 0.11848253492499111, 0.11812333746525003, 0.11704456394437424, 0.11334841875072085, 0.10919771833496719, 0.10450065458889615, 0.09973316355396303, 0.09420792921371314, 0.08875556173803319, 0.08381221414900858, 0.0786690027140338, 0.07273698056709355, 0.06764705401289643, 0.06267117245464311, 0.057604564722885866, 0.05292462839267296, 0.048811961181960885, 0.04483656158466609, 0.0406099346691541, 0.03723474725116614, 0.03391594961746653, 0.030741751566593357, 0.027808407642202843, 0.025318699575415445, 0.0229496823293923, 0.02090931016312657, 0.01882471833399515, 0.017044084356182702, 0.015499581408767595, 0.013993334802082192, 0.01272885507949743, 0.011567988998449056, 0.01041573870893356, 0.00943252102598688, 0.00853085928365528, 0.007791663511864599, 0.007030494709898493, 0.006328517715460948, 0.0057359719340844, 0.005203391068362762, 0.004715949353467531, 0.004269098816330769, 0.003844674479762575, 0.003468126243957923, 0.0031264774701251774, 0.0028211049817436613, 0.0025403050889471015, 0.002290523072275768, 0.0021454721710344126, 0.0019487678816994738, 0.001754007285831063, 0.0015861582262551416, 0.0014469406516099625, 0.0013080848237634018, 0.0012083351851227611, 0.001090669295201143, 0.000988749361950226, 0.0008937991294760245, 0.0008260166351263286, 0.0007434554588280933, 0.0007038323841358318, 0.0006721775335929648, 0.0006932358604837811, 0.0006244640169736631, 0.0005624464934527246, 0.0005087424110854095, 0.0004729268369981742, 0.00043163369354730596, 0.00042832695157260324, 0.0004050238107894587, 0.0003954665813343593, 0.00035903180217502134, 0.0003406572899332152, 0.0003151459030563054, 0.0003035315604419204, 0.0002757798731749389, 0.00030671347082877274, 0.00028645921593865604, 0.0002583347675052138, 0.00023301514607811073, 0.00021624750362159831, 0.00020256777251128616, 0.000252770802364038, 0.0002390100793979986, 0.0002448491602898385, 0.00023165229783657192, 0.00021788591384820882, 0.00019624247103579057, 0.0001970534050466156, 0.0002202865934430191, 0.00023067080019216753, 0.0002090327372333071, 0.000196222738077734, 0.0002320797063592172, 0.00021813026651234138, 0.00019666541958118862, 0.00022312236656050815, 0.00020724073015362547, 0.00018761167897427553, 0.00021369116748657115, 0.00024332227085380455, 0.00023298670986353915, 0.0002325568346248686, 0.00022525790443270497, 0.00020486591328773462, 0.0002529604717454916, 0.00026509379000372443, 0.00024967715185672355, 0.00024168671726150775, 0.000241038616520504, 0.00021722495420774235, 0.00025515885036997454, 0.0002359193484069644], "duration": 226906.252149, "accuracy_train": [0.3390551820897933, 0.4815565231058509, 0.6063957453742156, 0.6746480553825213, 0.6991003296303987, 0.7256045026416574, 0.7571985886166482, 0.7786070808531746, 0.7564502151393503, 0.7727988951758029, 0.7965808416389812, 0.784069378114618, 0.7981329235303618, 0.8068021237080103, 0.821425867767165, 0.8175904536960132, 0.8274934246954596, 0.8475315210871169, 0.8490879288367479, 0.8250015500992063, 0.8488092714677926, 0.8495983801102805, 0.8382094768018641, 0.8438352554563492, 0.8597153008490217, 0.8569923533245662, 0.8201434310400517, 0.8593400326458103, 0.84783162750323, 0.8366298175641381, 0.83195410552787, 0.8532277751822629, 0.8445778611226468, 0.8594806230389442, 0.8200050035760429, 0.8460899288252122, 0.8579667528954411, 0.8420688633374861, 0.8609433038598191, 0.8613850821336286, 0.8367424701227391, 0.8558079693844592, 0.8543874656815246, 0.8706133994901256, 0.8527387729558878, 0.8365597026116648, 0.8608288488602805, 0.8631307461817092, 0.863036298276578, 0.8624106710271319, 0.8422323447305279, 0.8563423931224622, 0.8403253622185308, 0.8561334902177926, 0.8518577398255813, 0.8552939132290514, 0.8796611122646733, 0.866245724610096, 0.8524372245870249, 0.862619934419989, 0.8690569917404946, 0.8477898108734773, 0.8736153648717239, 0.8628288373246585, 0.8485781985395902, 0.8499947368724622, 0.8714290040143964, 0.8581578116348283, 0.8771960940383905, 0.8802624065614618, 0.8929112160852714, 0.8622486315868402, 0.8622940530984681, 0.8695717688722776, 0.8518548559200813, 0.8716611584071613, 0.8853570058716316, 0.8811481260382059, 0.8493476605758582, 0.8601559976582688, 0.8794039039428755, 0.8765929973006644, 0.8529487573251201, 0.8717080218715393, 0.8413716791828165, 0.8750780456925988, 0.8678024928479143, 0.8632466431339978, 0.8739176342169619, 0.8568535653723699, 0.8373295251361205, 0.8738235468000184, 0.8818209772402179, 0.8766598678594499, 0.8767997372762089, 0.8688724217884828, 0.8827979002284054, 0.8473938145994832, 0.8860294965854559, 0.8649650903239202, 0.8780342290743817, 0.8943277544181433, 0.8821250490263934, 0.8749635906930602, 0.8505552960040605, 0.879382454895718, 0.8752629761327981, 0.8944447328349945, 0.8505509701458103, 0.8595045955034146, 0.8547877878137689, 0.8824494883951642, 0.8753348935262089, 0.8433480556709118, 0.8885853578349945, 0.8813334169665927, 0.8850761855735512, 0.8565491330980066, 0.8728942082525839, 0.845532253599114, 0.8770544221806941], "end": "2016-02-06 12:34:38.980000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0], "moving_var_accuracy_valid": [0.009982506010477522, 0.02618028978528881, 0.04791006658449644, 0.06826500458605149, 0.08412353279137101, 0.09565727889909814, 0.10475158175197884, 0.11145625993615664, 0.1119667726446244, 0.11117308486909182, 0.10969568861182921, 0.10603152831491826, 0.1019177446848967, 0.09725161203892248, 0.0923500752185903, 0.08700776528298995, 0.081582187367184, 0.07702178164801185, 0.07202074668315403, 0.06642403592145092, 0.06161074533008122, 0.05702140496220872, 0.05233351437426802, 0.04796047088854024, 0.04417197445140423, 0.040529582193922775, 0.03668637704886326, 0.03355898326144941, 0.03054805077774685, 0.027706517256968446, 0.02504818422318424, 0.0227652027400835, 0.02062391308065027, 0.018761473996032667, 0.016885745434015192, 0.015255280631477361, 0.013846799948168259, 0.012482947113433594, 0.011347974362699836, 0.010285528007621902, 0.009263968252293793, 0.008385679662650608, 0.007579492864575373, 0.006907063006578177, 0.00624478573089901, 0.005620312719644305, 0.005082601616887215, 0.004607202849876689, 0.004172818775706606, 0.0037788874718157474, 0.0034074153177053286, 0.003070049147025553, 0.0027660089090189343, 0.002491795233904105, 0.002243094112729897, 0.0020249111878069357, 0.0019191673534237404, 0.0017352633364689726, 0.0015619759821929717, 0.001413625204905382, 0.0012912307928719807, 0.0011802780902348449, 0.0010956988152271473, 0.00099183122349028, 0.0008991384356007675, 0.000818976366874745, 0.0007506489861200911, 0.0006761541374381111, 0.0006367592956464529, 0.0005948994486212986, 0.0006330929045485138, 0.0005699832143606793, 0.0005149022379702582, 0.0004645310106940075, 0.0004297410365247904, 0.0003900294798973364, 0.00039883694053213504, 0.00037472272082520554, 0.00036276262176353777, 0.0003407274005372183, 0.0003267037327144348, 0.0002992377357184978, 0.000296782428224781, 0.0002678948314656205, 0.0002884157026880626, 0.00027769390556651546, 0.0002502616188390182, 0.00022523561314973368, 0.0002050378810480614, 0.00019883905111716172, 0.0002483395205367901, 0.00024016934052890592, 0.00023126822158879055, 0.00022215226583345799, 0.00020496723568658366, 0.00018536963492454518, 0.0001989316622493656, 0.0002313229575867887, 0.0002245839472374728, 0.00020410420660486775, 0.00018436779306520677, 0.00021393829529444953, 0.00019591061304190705, 0.0001771751598362723, 0.0002138359233067948, 0.00020173718551369556, 0.00018159871444624193, 0.00020515716985005034, 0.0002323087539104332, 0.0002459269068654517, 0.00024073280450321306, 0.00023281908056279894, 0.00021395136554201193, 0.0002666442992726218, 0.00026957809396552744, 0.00025418773003047684, 0.00024020221120513957, 0.0002606858062197913, 0.00023497612937981515, 0.00024251754875442323, 0.0002270273420244852], "accuracy_test": 0.3356465242346939, "start": "2016-02-03 21:32:52.728000", "learning_rate_per_epoch": [0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405, 0.0072189136408269405], "accuracy_train_first": 0.3390551820897933, "accuracy_train_last": 0.8770544221806941, "batch_size_eval": 1024, "accuracy_train_std": [0.015606740482385523, 0.017490129951711283, 0.015817399187948746, 0.017490491079606506, 0.019576818635241928, 0.018350707633990503, 0.016868916000424988, 0.01879653167460761, 0.020142777429246455, 0.019219380225959166, 0.016807989813797306, 0.019245035522815097, 0.020430063878421886, 0.0220350870407859, 0.02133106051600023, 0.02113037036620478, 0.022290959964568608, 0.020620326953662158, 0.021631380147267402, 0.024104148573636155, 0.022094549183812016, 0.023493594522145073, 0.022610352778626018, 0.022893392298092324, 0.019418226331768297, 0.021182835486673143, 0.02320403297914992, 0.022415182583162226, 0.022392733994998595, 0.0203688952505507, 0.020580101251113606, 0.02080709925719364, 0.023338639929238285, 0.02163294204574565, 0.021769255998399337, 0.02182018394998334, 0.02146018660425048, 0.02137454481695154, 0.021719939745991908, 0.021411110496141375, 0.023334090901133378, 0.021266672025884996, 0.022196080233173758, 0.02097255094364509, 0.02105358285106338, 0.020847071252624503, 0.02145969055705106, 0.020403573166378912, 0.02196567580245517, 0.019903698687053602, 0.020419755099735377, 0.020004845959392958, 0.020357227287397205, 0.020620286985591166, 0.01926504288941186, 0.020393200900825855, 0.01840642147219526, 0.020050769894144137, 0.021191197236672888, 0.020032830105921492, 0.022875617602004808, 0.01926675831122504, 0.019554901855025637, 0.019126698354881733, 0.01991220605902179, 0.021019371201414307, 0.019072771511895244, 0.018628161079989446, 0.019565370570187663, 0.01947682635858865, 0.018097171829272913, 0.02031077722410067, 0.020401434375573776, 0.019151799893747407, 0.02192069399201849, 0.020448201929115176, 0.018405779551310394, 0.018636154124178878, 0.018681441950726765, 0.018144230709556935, 0.020335638837373365, 0.01839487677326228, 0.022303719991196725, 0.0199595318234127, 0.019382684295933227, 0.019813655133349897, 0.019942929820137522, 0.019649434300947625, 0.020339129036568718, 0.022389904223306008, 0.01770789048167916, 0.020635563690676803, 0.021013447151540125, 0.020640403156176858, 0.021404847883199653, 0.01911520088218939, 0.01732748941791448, 0.019587415925090855, 0.018765886298248883, 0.019292219475394685, 0.018253828921215246, 0.019893487524400323, 0.018587053146013333, 0.020223862471307814, 0.018733175193642152, 0.016091664604317327, 0.021276550017401384, 0.01914015894193025, 0.021460318630720465, 0.01838558810205702, 0.017871463951086543, 0.019773085604600643, 0.01937044019043089, 0.019071496495524597, 0.020612334571229567, 0.018257420858508986, 0.019145078097123756, 0.021735853950012395, 0.019897748404551552, 0.02014560550083225, 0.02028780796293241], "accuracy_test_std": 0.017428429969695215, "error_valid": [0.6669583607868976, 0.5295836666980421, 0.4028585043298193, 0.3423895778426205, 0.31606209996234935, 0.2971397307981928, 0.26549263460090367, 0.23839155449924698, 0.271728515625, 0.2556358245481928, 0.23434411474021077, 0.24398766942771077, 0.23188064759036142, 0.22576830760542166, 0.21726456607680722, 0.21764989646084332, 0.2140598526920181, 0.18580866434487953, 0.1925122364457832, 0.21487316453313254, 0.19251370717243976, 0.18866775696536142, 0.20144543015813254, 0.19921727927334332, 0.1814038380082832, 0.1838452442582832, 0.21907503059111444, 0.1849747623305723, 0.1928578572100903, 0.19989822571536142, 0.20838284779743976, 0.19052969691265065, 0.1964493717055723, 0.18420116010918675, 0.22446524378765065, 0.20099685852786142, 0.1878029696912651, 0.20504723974021077, 0.1832540121423193, 0.18683670227786142, 0.20353974491716864, 0.18835302146084332, 0.1901929005082832, 0.17643866481551207, 0.1864087208207832, 0.2021557911332832, 0.18594102974397586, 0.1816273884600903, 0.18171857351280118, 0.18100674181099397, 0.2039471362010542, 0.1902237857680723, 0.2014748446912651, 0.19115916792168675, 0.1934887989457832, 0.1873132177146084, 0.16195200724774095, 0.18202448465737953, 0.1921460255082832, 0.18134206748870485, 0.1752282567771084, 0.20250288262424698, 0.1704366293298193, 0.17982721903237953, 0.19548310429216864, 0.1982495411332832, 0.17660191547439763, 0.18513654226280118, 0.16969391236822284, 0.17006159403237953, 0.1511289297816265, 0.18226862528237953, 0.18554393354668675, 0.1778637989457832, 0.1924210513930723, 0.1761548145707832, 0.15852521413780118, 0.16603180299322284, 0.19478156767695776, 0.19220779602786142, 0.16596120811370485, 0.17178969785391573, 0.1961037509412651, 0.17741669804216864, 0.20301175404743976, 0.16818788827183728, 0.17902273155120485, 0.1807228915662651, 0.17567682840737953, 0.19285932793674698, 0.20927852033132532, 0.17068224068147586, 0.16996893825301207, 0.16915562641189763, 0.1729089208396084, 0.1827980868787651, 0.1610681005271084, 0.2021675569465362, 0.1669789509600903, 0.18381435899849397, 0.17685782191265065, 0.15622499764683728, 0.17089549604668675, 0.17331631212349397, 0.20067182793674698, 0.1683923192771084, 0.17690782661897586, 0.1559411474021084, 0.1983319018260542, 0.19785391566265065, 0.19432417168674698, 0.1677113728350903, 0.1727677310805723, 0.20776220114834332, 0.1638051228350903, 0.16878941547439763, 0.16772166792168675, 0.20010265672063254, 0.18208625517695776, 0.1988598926957832, 0.1722794498305723], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.03315934890815247, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.007218913498931528, "optimization": "rmsprop", "nb_data_augmentation": 4, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.7881998424139823e-06, "rotation_range": [0, 0], "momentum": 0.6206752070737898}, "accuracy_valid_max": 0.8488710702183735, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8277205501694277, "accuracy_valid_std": [0.010024395967260983, 0.012082910122706712, 0.012539214683929054, 0.014441870705741395, 0.01180491483336135, 0.01979579111069587, 0.011503480128789897, 0.011679544687313463, 0.016771234858980074, 0.01115382961714806, 0.005741342957701149, 0.007692451003015713, 0.014959366324816788, 0.009561419204872528, 0.016285287932290275, 0.006375469606606292, 0.012203672481054162, 0.007910063473139533, 0.012272370698088548, 0.0041990232529511085, 0.013067686913054039, 0.00998495943178816, 0.012148456798393452, 0.007945932979087727, 0.006170654310458272, 0.004187728549455846, 0.011630597065302773, 0.008740496596372858, 0.01057454378671038, 0.009214911309239888, 0.005536413513902115, 0.009420429370580996, 0.010700475022747218, 0.009418860420819856, 0.013737127068529939, 0.011444265252707971, 0.010648411606188256, 0.015864757213273673, 0.01681323231804835, 0.01146727139776781, 0.010340548072237618, 0.013020940388799465, 0.011577149861345486, 0.009911637005517483, 0.00932968852440817, 0.013677770478961498, 0.009444216458178127, 0.010087046617234613, 0.012326785340803401, 0.010122401088999611, 0.010497138912366108, 0.01163255645900728, 0.00708999781706006, 0.01446072418310583, 0.012066065217176784, 0.01684632272173196, 0.014859423217310356, 0.013275338908652328, 0.010631125943243362, 0.013673578427581944, 0.010566972779826867, 0.011295141560896423, 0.013448357681320295, 0.012371999415139376, 0.011051393583366972, 0.013618734356813931, 0.01448190925408004, 0.01950124666820095, 0.01667852267491788, 0.012424360965649785, 0.008902610100703303, 0.01164881312208712, 0.014625576440850266, 0.007912752160836086, 0.01087567070529376, 0.010068753664261569, 0.00993791658787637, 0.011955311016202723, 0.014146957475866355, 0.011934750901264087, 0.010835271093267646, 0.016346792480810873, 0.013535964439505327, 0.011028236160885687, 0.014894265966332839, 0.018306175223565892, 0.011701689851752472, 0.012442365594901984, 0.010524939933852034, 0.013400815808674818, 0.010357017094367444, 0.013305039635215283, 0.012436010998562296, 0.014712205540123329, 0.011601363989729078, 0.014482655070563356, 0.011966906513758683, 0.014873194431052207, 0.011750624723168531, 0.015247575595202236, 0.017898630054860903, 0.014346826213951454, 0.010751411825768456, 0.012967571659364487, 0.014058701881849708, 0.016294472607670956, 0.011857505442000489, 0.0127666633919641, 0.012125174258158091, 0.013776773820788117, 0.013204965950497914, 0.011398716353750947, 0.013653070988446817, 0.012493463354960812, 0.016967582090255555, 0.01336710795670417, 0.01080308627955448, 0.015129118492418166, 0.016629477040199347, 0.014565333109013213, 0.011320351032289238], "accuracy_valid": [0.3330416392131024, 0.47041633330195787, 0.5971414956701807, 0.6576104221573795, 0.6839379000376506, 0.7028602692018072, 0.7345073653990963, 0.761608445500753, 0.728271484375, 0.7443641754518072, 0.7656558852597892, 0.7560123305722892, 0.7681193524096386, 0.7742316923945783, 0.7827354339231928, 0.7823501035391567, 0.7859401473079819, 0.8141913356551205, 0.8074877635542168, 0.7851268354668675, 0.8074862928275602, 0.8113322430346386, 0.7985545698418675, 0.8007827207266567, 0.8185961619917168, 0.8161547557417168, 0.7809249694088856, 0.8150252376694277, 0.8071421427899097, 0.8001017742846386, 0.7916171522025602, 0.8094703030873494, 0.8035506282944277, 0.8157988398908133, 0.7755347562123494, 0.7990031414721386, 0.8121970303087349, 0.7949527602597892, 0.8167459878576807, 0.8131632977221386, 0.7964602550828314, 0.8116469785391567, 0.8098070994917168, 0.8235613351844879, 0.8135912791792168, 0.7978442088667168, 0.8140589702560241, 0.8183726115399097, 0.8182814264871988, 0.818993258189006, 0.7960528637989458, 0.8097762142319277, 0.7985251553087349, 0.8088408320783133, 0.8065112010542168, 0.8126867822853916, 0.838047992752259, 0.8179755153426205, 0.8078539744917168, 0.8186579325112951, 0.8247717432228916, 0.797497117375753, 0.8295633706701807, 0.8201727809676205, 0.8045168957078314, 0.8017504588667168, 0.8233980845256024, 0.8148634577371988, 0.8303060876317772, 0.8299384059676205, 0.8488710702183735, 0.8177313747176205, 0.8144560664533133, 0.8221362010542168, 0.8075789486069277, 0.8238451854292168, 0.8414747858621988, 0.8339681970067772, 0.8052184323230422, 0.8077922039721386, 0.8340387918862951, 0.8282103021460843, 0.8038962490587349, 0.8225833019578314, 0.7969882459525602, 0.8318121117281627, 0.8209772684487951, 0.8192771084337349, 0.8243231715926205, 0.807140672063253, 0.7907214796686747, 0.8293177593185241, 0.8300310617469879, 0.8308443735881024, 0.8270910791603916, 0.8172019131212349, 0.8389318994728916, 0.7978324430534638, 0.8330210490399097, 0.816185641001506, 0.8231421780873494, 0.8437750023531627, 0.8291045039533133, 0.826683687876506, 0.799328172063253, 0.8316076807228916, 0.8230921733810241, 0.8440588525978916, 0.8016680981739458, 0.8021460843373494, 0.805675828313253, 0.8322886271649097, 0.8272322689194277, 0.7922377988516567, 0.8361948771649097, 0.8312105845256024, 0.8322783320783133, 0.7998973432793675, 0.8179137448230422, 0.8011401073042168, 0.8277205501694277], "seed": 666348855, "model": "residualv3", "loss_std": [0.32096558809280396, 0.2666687071323395, 0.2749165892601013, 0.2728317677974701, 0.27196455001831055, 0.2700033485889435, 0.26785901188850403, 0.26224833726882935, 0.25886768102645874, 0.25853440165519714, 0.2550993859767914, 0.25248563289642334, 0.25098687410354614, 0.2475409358739853, 0.244585782289505, 0.24659468233585358, 0.24211597442626953, 0.24232815206050873, 0.23820559680461884, 0.23555998504161835, 0.2362847477197647, 0.23338450491428375, 0.23174871504306793, 0.2321311980485916, 0.23149697482585907, 0.22896689176559448, 0.22755812108516693, 0.22933471202850342, 0.22742228209972382, 0.22682787477970123, 0.2240251749753952, 0.2238510698080063, 0.22133252024650574, 0.21924665570259094, 0.22105593979358673, 0.21720585227012634, 0.21874119341373444, 0.21762223541736603, 0.21633706986904144, 0.2154875099658966, 0.21505111455917358, 0.21280209720134735, 0.2122240960597992, 0.21200333535671234, 0.21496734023094177, 0.20926688611507416, 0.2116851657629013, 0.2098686248064041, 0.20907720923423767, 0.20956014096736908, 0.20839674770832062, 0.21086136996746063, 0.20728763937950134, 0.20842741429805756, 0.20516522228717804, 0.20390579104423523, 0.2022416889667511, 0.2022998332977295, 0.206477090716362, 0.2071187049150467, 0.20476242899894714, 0.20406389236450195, 0.20440757274627686, 0.19995692372322083, 0.20480382442474365, 0.20056387782096863, 0.20214895904064178, 0.2040856033563614, 0.20222312211990356, 0.2020915448665619, 0.19861117005348206, 0.20083855092525482, 0.20156697928905487, 0.20038694143295288, 0.1997966170310974, 0.19922903180122375, 0.19305486977100372, 0.19614438712596893, 0.1992311179637909, 0.19543851912021637, 0.19492459297180176, 0.19314394891262054, 0.19717299938201904, 0.19806648790836334, 0.19468918442726135, 0.19145037233829498, 0.1923978179693222, 0.19523383677005768, 0.193523108959198, 0.19182465970516205, 0.1968512088060379, 0.19387900829315186, 0.19204525649547577, 0.19042614102363586, 0.19544576108455658, 0.19261771440505981, 0.19160328805446625, 0.19525521993637085, 0.19110915064811707, 0.19216957688331604, 0.19132208824157715, 0.19272279739379883, 0.18940795958042145, 0.18958592414855957, 0.18911853432655334, 0.19070585072040558, 0.18888452649116516, 0.18999165296554565, 0.19194893538951874, 0.18920202553272247, 0.19097383320331573, 0.18952693045139313, 0.18770186603069305, 0.18991713225841522, 0.1932317614555359, 0.19160623848438263, 0.18958796560764313, 0.18885141611099243, 0.1889801323413849, 0.18847419321537018, 0.18719951808452606]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:42 2016", "state": "available"}], "summary": "a1f8c46ab9b0b25999fc9ed47341e3ac"}