{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 64, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.01059877876820966, 0.019851829839671608, 0.014869463344009473, 0.01266414696043494, 0.013436395740218675, 0.014267927867378799, 0.013819265020287682, 0.014230951080667794, 0.016262108689470013, 0.014572019979679794, 0.013359334150434709, 0.013515220664983358, 0.015376262280461522, 0.014908271961712672, 0.016884223285796952, 0.0157791415821199, 0.015257422891192588, 0.016186498472357452, 0.014344188141061587, 0.016544033017076493, 0.01796408507375698, 0.013183044759163725, 0.01400474258499858, 0.01589807270893231, 0.014841562514703624, 0.013521730988074215, 0.015747560821801822, 0.01247221080252781, 0.014571498954093896, 0.014265924769944573, 0.013296630015400816, 0.015216452075772338, 0.012881244963136359, 0.012021826477709144, 0.01304021786811203, 0.011617055179909954, 0.013276224031533999, 0.013766171204207742, 0.01376649857437666, 0.01208954631129055, 0.012866904972245142, 0.013098404806112672, 0.014582361822679626, 0.013378076245384582, 0.013404427050989959, 0.013785444028491196, 0.015439377425255994, 0.010818473246246334, 0.015481963899546973, 0.015970448891927638, 0.01236124363117125, 0.012535577818912885, 0.014030807459072729, 0.013480100960246107, 0.01474778391212299, 0.01628974799989239, 0.012077711895138503, 0.013748273491282303, 0.012893700473480835, 0.014089198366517645, 0.014592301675726723, 0.01506953921875009, 0.01502795428189704, 0.013903347729541767, 0.01368691572835889, 0.015927748605428105, 0.014459092994657103, 0.01398800499626591, 0.014675312304677729, 0.012249004064249364, 0.014243675297935158, 0.011280522160857811, 0.012743106342355141, 0.013947435472749493, 0.012547266121040398, 0.012515236348484695, 0.012294122755527097, 0.014476799299676294, 0.012823918412321663, 0.015006571818856854, 0.012920718556121664, 0.012842666692536397, 0.014091554233036525, 0.014283047244075288, 0.01335959774061752, 0.01467865656340619, 0.012073472282003438, 0.01309953231595241, 0.013540615316622331, 0.012840819012394316, 0.011725606586128064, 0.012750277525144193, 0.011694255018954363, 0.01311127787869561, 0.013783199443636837, 0.01397015033144448, 0.012902795182982641, 0.012140323852256803, 0.01252254303367323, 0.013016288118327576, 0.012910818917954706, 0.012619231615256124, 0.014107978475048423, 0.012236451351387236, 0.013267593663646517, 0.011754435725793804, 0.011974782430201718, 0.013258894951556507, 0.012921097981702935, 0.013444046713548603, 0.012835194580277121, 0.013604505858312655, 0.012177493089941269, 0.012823698715065092, 0.014172683887121564, 0.013568287351521059, 0.012310427015916389, 0.013480556903819748, 0.01335445517389663, 0.014186774099447656, 0.014256953167646887, 0.014741835960745747, 0.012812208826821013, 0.014045960321568093, 0.013002302936791636, 0.011687639974299236, 0.013294161672509085, 0.012241992188387918, 0.013462295949325421, 0.011348322062823688, 0.01369147140930721, 0.013565905429086509, 0.012268725165319464, 0.013570286950656426, 0.014040431123043788, 0.013138336014258149, 0.014071698372950971, 0.014092697445849386, 0.013557115162593303, 0.013767993049543271, 0.013258903516327622, 0.013340393855519946, 0.012852280882928488, 0.014810455419302588, 0.013865861084077105, 0.014075280408404821, 0.012766558599993691, 0.014402184408834272, 0.01483185059747411, 0.013029704498427585, 0.013671945799104428, 0.01332937727699219, 0.012634036008220747, 0.012973761929374942, 0.013555784211444183, 0.014375861213006326, 0.015604642355137541, 0.012634876264816132, 0.013785455325812667, 0.014567487991977964, 0.012476823928108637, 0.011922413053324164, 0.01418403405120219, 0.014817969141712512, 0.012345818495018842, 0.012372091114840514, 0.012980774111151437, 0.013937304127530192, 0.014019440395765984, 0.013001227304757262, 0.012817549375641512, 0.013154711242792005, 0.013979811032530924, 0.013743580434740185, 0.013660899552782291, 0.01505778189196482, 0.014259094658574766, 0.013114178235974562, 0.013918812164984154, 0.014313817170370778, 0.01529785789461099, 0.013496708441431022, 0.013790032254493389, 0.013859374447031659, 0.014505973099928418, 0.012500710698347422, 0.013017503247318834, 0.014038585004966271, 0.013524198980697443, 0.013867000441549622, 0.01416468677784624, 0.013433138731431666, 0.014613005583458002, 0.013550563735516757, 0.013165236262741124, 0.012969454317693438, 0.01299842563541337, 0.014825091540706733, 0.013883551125300032], "moving_avg_accuracy_train": [0.05757023391357511, 0.11844067409849113, 0.1833436045430163, 0.24579197930004726, 0.3044900047169879, 0.3590758598481407, 0.409623543047066, 0.45608128653559493, 0.4989300918002248, 0.5380846043360107, 0.5742185955681535, 0.6071949528925673, 0.637354872141655, 0.665017271601539, 0.6904760450297018, 0.7136004575590775, 0.7347100839319535, 0.7538831338282561, 0.7714689777682433, 0.7875542206856327, 0.802493716022016, 0.8160321596306856, 0.8285097276284883, 0.8400278212300728, 0.8506589561428721, 0.8603526076977432, 0.8691070849828323, 0.8770210638691929, 0.8843551973597653, 0.8911211833108504, 0.8973707175441341, 0.9031371684802891, 0.9085477192644584, 0.913484644285687, 0.9179930891179156, 0.9220437861181302, 0.9258590050861437, 0.9294438728787936, 0.93282131646716, 0.9358493539038235, 0.938635077514687, 0.9412444632144457, 0.9437742719513713, 0.9460533889145952, 0.9482627043005444, 0.9503301792562411, 0.9521653661282821, 0.9538938002726521, 0.9554702452442334, 0.9569146223555612, 0.9582633898807563, 0.9596307044260417, 0.9608055560430075, 0.9618513328030479, 0.9629575814037417, 0.9639298455098145, 0.964895636106489, 0.965734656757791, 0.9666711369511057, 0.9674395283143653, 0.968205449254385, 0.9688575757194504, 0.9694492479820845, 0.9701165395517701, 0.9708356845537729, 0.9713620433662989, 0.9719403619451822, 0.9725654082649682, 0.973004825212327, 0.9734351776970929, 0.9739248014810011, 0.9742886969269855, 0.9747417608640858, 0.9752099722765237, 0.9755686195786794, 0.9759517839220295, 0.9763269669120248, 0.9766924252422786, 0.9770771052621169, 0.9773187576811803, 0.9776129387202328, 0.9778846771018086, 0.9781617576797415, 0.9784553440760808, 0.9787218969815957, 0.9789687339941688, 0.9791676718662082, 0.9793978692248532, 0.979623611989291, 0.9798360450237045, 0.980018006257076, 0.9801492192837771, 0.9802696001077987, 0.9804848996946564, 0.9806624293299804, 0.9808709620291344, 0.9810075242333822, 0.9812211110207768, 0.9813947739877744, 0.981548709460444, 0.981661674748942, 0.9818796369978849, 0.9819781467719337, 0.9822016281507112, 0.98235858356423, 0.9824138129304445, 0.9825518750147995, 0.9826250136657282, 0.9827443168741831, 0.9828585931105834, 0.9829661280697816, 0.9830606204330692, 0.9831874441409619, 0.9833388599566554, 0.9833751167431511, 0.9835449316307592, 0.9836396363093685, 0.9838015643820123, 0.9839171087616867, 0.9840025335617363, 0.9840840661793998, 0.9842341393971926, 0.9842948005313014, 0.9844122106186751, 0.9845224578972931, 0.9846379925385346, 0.9847256976739853, 0.984848810123272, 0.9848967962609539, 0.9849748970658294, 0.9850312368973602, 0.9851075193826426, 0.9850831676670159, 0.9851496067777137, 0.9851768498940084, 0.9852083441451023, 0.985292528591334, 0.9853705836929333, 0.985440797235554, 0.9855342163584364, 0.98556023694643, 0.9856115212125197, 0.9857065051770005, 0.98580132738909, 0.9858447786525804, 0.9859071362778169, 0.9859493072476727, 0.9860407034943433, 0.986081143486594, 0.9861175755284386, 0.9861061865387177, 0.9861586794170073, 0.9862036339074771, 0.9862463820488908, 0.9863127932106961, 0.9863864781003592, 0.9864364824105705, 0.986483847487389, 0.986563678437478, 0.9866145639044538, 0.986662758071179, 0.9867805015343176, 0.986828377979723, 0.9868505404413022, 0.9868704866567235, 0.9870139602374981, 0.9870640674494903, 0.9871231148331403, 0.9871506808415206, 0.9871382878681104, 0.987196888656327, 0.9872263418288081, 0.9872784263209458, 0.9873253023638696, 0.9873581541584443, 0.9873575659366751, 0.9873964919692073, 0.9874641135306383, 0.9874807951085454, 0.9874748461405571, 0.9875508722777011, 0.9875635282785209, 0.9875586786864108, 0.9876751496939694, 0.9877218809293528, 0.987712821816207, 0.9877510994929288, 0.9878436781222166, 0.9878805319612038, 0.9879671788389114, 0.9880474861776578, 0.9880500443670626, 0.9880546358375176, 0.9880750442025938, 0.9880957008311533, 0.9881259535897231, 0.9881485668236356, 0.9882153856615286, 0.9882568853763373], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 921721763, "moving_var_accuracy_train": [0.029828986495773792, 0.06019298224094537, 0.09208519743943269, 0.11797487328364067, 0.1371865096459069, 0.15028439890490902, 0.15825157350542834, 0.16185131352529908, 0.16219036318622473, 0.15976900953483741, 0.1555430964826348, 0.14977574811585814, 0.14298475986627573, 0.13557315897455186, 0.12784918537729545, 0.11987691293302463, 0.11189976857014394, 0.10401824429406455, 0.09639977702839236, 0.08908841468295596, 0.0821882699028127, 0.07561904801063755, 0.06945835053783189, 0.06370651380598247, 0.05835305169119148, 0.053363448446277634, 0.04871687145446614, 0.04440886386534337, 0.04045208310532686, 0.0368188818878067, 0.0334885038022229, 0.030438921029592364, 0.027658495464725808, 0.025112004976240325, 0.022783739151863536, 0.02065303855236512, 0.01871873775909362, 0.01696252547700127, 0.015368937056034525, 0.013914564446891594, 0.012592950306527536, 0.011394935319445729, 0.010313041177709988, 0.009328486427127483, 0.008439567454686058, 0.007634080783449353, 0.006900983902802226, 0.00623777287384482, 0.0056363621951961524, 0.0050915020028340875, 0.004598724367083867, 0.0041556778719672215, 0.003752532571667483, 0.003387122155787299, 0.0030594240139074007, 0.0027619892899442794, 0.002494185124239476, 0.0022511022126953297, 0.002033884947798033, 0.0018358102806024185, 0.0016575089665194231, 0.001495585490205429, 0.0013491776257822216, 0.0012182673655547608, 0.0011010951548044347, 0.0009934791217197053, 0.0008971412809558683, 0.0008109432989771832, 0.0007315867543621005, 0.0006600949082761875, 0.0005962430004964868, 0.000537810479507312, 0.000485876833936488, 0.00043926214788347266, 0.00039649358408121735, 0.0003581655598992308, 0.0003236158643931439, 0.0002924563160741964, 0.0002645424929257413, 0.00023861380665791954, 0.00021553130834576977, 0.0001946427532433859, 0.00017586944073905568, 0.00015905823341418935, 0.0001437918641357161, 0.00012996103431912894, 0.00011732111737960003, 0.00010606592305698406, 9.591796891255029e-05, 8.673232216828627e-05, 7.835707896550843e-05, 7.067632279434202e-05, 6.373911440003713e-05, 5.7782388168943017e-05, 5.228780029481303e-05, 4.7450393244880006e-05, 4.287319704105337e-05, 3.899645117869365e-05, 3.5368235495782045e-05, 3.2044677113917914e-05, 2.8955059810174777e-05, 2.648712170683598e-05, 2.3925747116400537e-05, 2.1982667744702983e-05, 2.0006115986728307e-05, 1.803295693408758e-05, 1.6401211492906753e-05, 1.48092337039531e-05, 1.345640963348642e-05, 1.2228300193990443e-05, 1.110954408163925e-05, 1.0078948933952447e-05, 9.215812316510283e-06, 8.500571828038129e-06, 7.662345636337323e-06, 7.1556449371838015e-06, 6.520801228819666e-06, 6.10470741232896e-06, 5.614391204165073e-06, 5.118628651920022e-06, 4.666593896415707e-06, 4.402632243062209e-06, 3.995486977478191e-06, 3.7200044372843717e-06, 3.4573941555398097e-06, 3.231789019927122e-06, 2.977839834994272e-06, 2.8164659280189237e-06, 2.5555433599037843e-06, 2.3548866454131385e-06, 2.1479655704240555e-06, 1.9855401714294415e-06, 1.7923232087721958e-06, 1.652818286767837e-06, 1.4942161445600738e-06, 1.353721520771699e-06, 1.2821325575806504e-06, 1.2087526917937529e-06, 1.1322468967204729e-06, 1.0975663997294752e-06, 9.939033987523063e-07, 9.181837424123534e-07, 9.075629497474835e-07, 8.977279219227275e-07, 8.249472404206761e-07, 7.774487772049567e-07, 7.157094157716815e-07, 7.193179393436516e-07, 6.621046821684868e-07, 6.078398570082574e-07, 5.482232530892032e-07, 5.182004482203707e-07, 4.845685593189418e-07, 4.5255833573590314e-07, 4.4699648387324234e-07, 4.5116200216794327e-07, 4.2854968130861986e-07, 4.058857676960017e-07, 4.2265401625552925e-07, 4.036925913740037e-07, 3.842274315936022e-07, 4.7057639644119365e-07, 4.4414814301902516e-07, 4.0415390104636943e-07, 3.673191745284221e-07, 5.158492724983632e-07, 4.86860939491179e-07, 4.6955418718535306e-07, 4.294377318290337e-07, 3.8787623075561595e-07, 3.799950790965644e-07, 3.4980297550966524e-07, 3.3923782684982944e-07, 3.250903147666625e-07, 3.0229444695097074e-07, 2.720681162995207e-07, 2.5849842874784534e-07, 2.7380266600643926e-07, 2.4892687477900316e-07, 2.2435269928222067e-07, 2.5393719111533384e-07, 2.2998504121455217e-07, 2.0719820398580712e-07, 3.085678440027227e-07, 2.9736533484660264e-07, 2.6836740914081815e-07, 2.5471729304375054e-07, 3.063827871466048e-07, 2.8796835746484424e-07, 3.267406544666418e-07, 3.5211000692868535e-07, 3.169579052330928e-07, 2.854518491182409e-07, 2.606551764921749e-07, 2.3842992557391325e-07, 2.2282399762626066e-07, 2.0514382299547443e-07, 2.2481225457225722e-07, 2.1783106607790685e-07], "duration": 210461.301075, "accuracy_train": [0.5757023391357512, 0.6662746357627354, 0.7674699785437431, 0.807827352113326, 0.8327722334694537, 0.8503485560285161, 0.8645526918373938, 0.8742009779323551, 0.8845693391818937, 0.8904752171580842, 0.8994245166574382, 0.9039821688122923, 0.908794145383444, 0.9139788667404946, 0.9196050058831673, 0.9217201703234589, 0.9246967212878369, 0.9264405828949798, 0.9297415732281286, 0.9323214069421374, 0.9369491740494648, 0.9378781521087117, 0.9408078396087117, 0.9436906636443337, 0.9463391703580657, 0.9475954716915835, 0.9478973805486341, 0.9482468738464378, 0.9503623987749169, 0.9520150568706165, 0.9536165256436876, 0.9550352269056847, 0.9572426763219823, 0.9579169694767442, 0.9585690926079733, 0.9585000591200628, 0.9601959757982651, 0.9617076830126431, 0.9632183087624585, 0.9631016908337948, 0.9637065900124585, 0.9647289345122739, 0.9665425505837025, 0.9665654415836102, 0.9681465427740864, 0.9689374538575121, 0.9686820479766519, 0.9694497075719823, 0.9696582499884644, 0.9699140163575121, 0.9704022976075121, 0.9719365353336102, 0.9713792205956996, 0.971263323643411, 0.9729138188099853, 0.9726802224644703, 0.9735877514765596, 0.9732858426195091, 0.9750994586909376, 0.9743550505837025, 0.9750987377145626, 0.9747267139050388, 0.9747742983457919, 0.9761221636789406, 0.9773079895717978, 0.976099272679033, 0.9771452291551311, 0.9781908251430418, 0.9769595777385567, 0.9773083500599853, 0.9783314155361758, 0.9775637559408453, 0.9788193362979882, 0.9794238749884644, 0.9787964452980805, 0.9794002630121816, 0.9797036138219823, 0.9799815502145626, 0.9805392254406607, 0.9794936294527501, 0.9802605680717055, 0.9803303225359912, 0.980655482881137, 0.9810976216431341, 0.9811208731312293, 0.9811902671073275, 0.9809581127145626, 0.9814696454526578, 0.9816552968692323, 0.9817479423334257, 0.9816556573574198, 0.9813301365240864, 0.9813530275239941, 0.982422595976375, 0.9822601960478959, 0.9827477563215209, 0.9822365840716132, 0.9831433921073275, 0.982957740690753, 0.9829341287144703, 0.9826783623454227, 0.9838412972383721, 0.9828647347383721, 0.9842129605597084, 0.9837711822858989, 0.982910877226375, 0.9837944337739941, 0.9832832615240864, 0.9838180457502769, 0.9838870792381875, 0.9839339427025655, 0.9839110517026578, 0.9843288575119971, 0.9847016022978959, 0.9837014278216132, 0.9850732656192323, 0.9844919784168512, 0.9852589170358066, 0.984957008178756, 0.9847713567621816, 0.9848178597383721, 0.9855847983573275, 0.9848407507382798, 0.9854689014050388, 0.9855146834048542, 0.9856778043097084, 0.9855150438930418, 0.9859568221668512, 0.9853286715000923, 0.9856778043097084, 0.985538295381137, 0.9857940617501846, 0.984864002226375, 0.9857475587739941, 0.9854220379406607, 0.9854917924049464, 0.9860501886074198, 0.9860730796073275, 0.98607271911914, 0.986374988464378, 0.9857944222383721, 0.9860730796073275, 0.9865613608573275, 0.9866547272978959, 0.9862358400239941, 0.9864683549049464, 0.986328845976375, 0.986863269714378, 0.9864451034168512, 0.9864454639050388, 0.9860036856312293, 0.9866311153216132, 0.9866082243217055, 0.9866311153216132, 0.9869104936669435, 0.9870496421073275, 0.9868865212024732, 0.986910133178756, 0.9872821569882798, 0.9870725331072352, 0.9870965055717055, 0.9878401927025655, 0.9872592659883721, 0.987050002595515, 0.987050002595515, 0.9883052224644703, 0.9875150323574198, 0.9876545412859912, 0.9873987749169435, 0.9870267511074198, 0.9877242957502769, 0.987491420381137, 0.9877471867501846, 0.9877471867501846, 0.9876538203096161, 0.987352271940753, 0.9877468262619971, 0.988072707583518, 0.9876309293097084, 0.9874213054286637, 0.9882351075119971, 0.9876774322858989, 0.9875150323574198, 0.9887233887619971, 0.9881424620478036, 0.9876312897978959, 0.9880955985834257, 0.9886768857858066, 0.9882122165120893, 0.9887470007382798, 0.988770252226375, 0.9880730680717055, 0.9880959590716132, 0.9882587194882798, 0.9882816104881875, 0.9883982284168512, 0.9883520859288483, 0.9888167552025655, 0.9886303828096161], "end": "2016-01-31 21:15:56.292000", "learning_rate_per_epoch": [0.001205874839797616, 0.000602937419898808, 0.00040195827023126185, 0.000301468709949404, 0.00024117495922837406, 0.00020097913511563092, 0.0001722678280202672, 0.000150734354974702, 0.00013398609007708728, 0.00012058747961418703, 0.00010962498345179483, 0.00010048956755781546, 9.27595974644646e-05, 8.61339140101336e-05, 8.039165550144389e-05, 7.5367177487351e-05, 7.09338128217496e-05, 6.699304503854364e-05, 6.346709415083751e-05, 6.0293739807093516e-05, 5.742260691476986e-05, 5.4812491725897416e-05, 5.2429339120863006e-05, 5.024478377890773e-05, 4.823498966288753e-05, 4.63797987322323e-05, 4.466202881303616e-05, 4.30669570050668e-05, 4.1581890400266275e-05, 4.0195827750721946e-05, 3.889918662025593e-05, 3.76835887436755e-05, 3.6541659937938675e-05, 3.54669064108748e-05, 3.445356560405344e-05, 3.349652251927182e-05, 3.259121149312705e-05, 3.1733547075418755e-05, 3.09198658214882e-05, 3.0146869903546758e-05, 2.9411579816951416e-05, 2.871130345738493e-05, 2.8043599741067737e-05, 2.7406245862948708e-05, 2.6797217287821695e-05, 2.6214669560431503e-05, 2.5656911020632833e-05, 2.5122391889453866e-05, 2.4609689717181027e-05, 2.4117494831443764e-05, 2.364460306125693e-05, 2.318989936611615e-05, 2.275235419801902e-05, 2.233101440651808e-05, 2.1924995962763205e-05, 2.15334785025334e-05, 2.115569805027917e-05, 2.0790945200133137e-05, 2.0438556020963006e-05, 2.0097913875360973e-05, 1.976843850570731e-05, 1.9449593310127966e-05, 1.9140868971589953e-05, 1.884179437183775e-05, 1.8551920220488682e-05, 1.8270829968969338e-05, 1.799813071556855e-05, 1.77334532054374e-05, 1.7476446373621002e-05, 1.722678280202672e-05, 1.6984151443466544e-05, 1.674826125963591e-05, 1.651883212616667e-05, 1.6295605746563524e-05, 1.607833110028878e-05, 1.5866773537709378e-05, 1.566071114211809e-05, 1.54599329107441e-05, 1.526423693576362e-05, 1.5073434951773379e-05, 1.488734324084362e-05, 1.4705789908475708e-05, 1.4528612155118026e-05, 1.4355651728692465e-05, 1.4186762200552039e-05, 1.4021799870533869e-05, 1.3860630133422092e-05, 1.3703122931474354e-05, 1.3549154573411215e-05, 1.3398608643910848e-05, 1.325137145613553e-05, 1.3107334780215751e-05, 1.2966395843250211e-05, 1.2828455510316417e-05, 1.2693419193965383e-05, 1.2561195944726933e-05, 1.2431698451109696e-05, 1.2304844858590513e-05, 1.2180553312646225e-05, 1.2058747415721882e-05, 1.1939354408241343e-05, 1.1822301530628465e-05, 1.170752238977002e-05, 1.1594949683058076e-05, 1.1484521564852912e-05, 1.137617709900951e-05, 1.1269858077866957e-05, 1.116550720325904e-05, 1.1063071724493057e-05, 1.0962497981381603e-05, 1.0863736861210782e-05, 1.07667392512667e-05, 1.0671457857824862e-05, 1.0577849025139585e-05, 1.048586727847578e-05, 1.0395472600066569e-05, 1.0306622243660968e-05, 1.0219278010481503e-05, 1.01334017017507e-05, 1.0048956937680487e-05, 9.965907338482793e-06, 9.884219252853654e-06, 9.803859938983805e-06, 9.724796655063983e-06, 9.646998478274327e-06, 9.570434485794976e-06, 9.495077392784879e-06, 9.420897185918875e-06, 9.347866580355912e-06, 9.275960110244341e-06, 9.20515140023781e-06, 9.135414984484669e-06, 9.066728125617374e-06, 8.999065357784275e-06, 8.932405762607232e-06, 8.8667266027187e-06, 8.802006050245836e-06, 8.738223186810501e-06, 8.675358003529254e-06, 8.61339140101336e-06, 8.552303370379377e-06, 8.492075721733272e-06, 8.43269117467571e-06, 8.374130629817955e-06, 8.316377716255374e-06, 8.259416063083336e-06, 8.20323020889191e-06, 8.147802873281762e-06, 8.093119504337665e-06, 8.03916555014439e-06, 7.985925549292006e-06, 7.933386768854689e-06, 7.88153465691721e-06, 7.830355571059044e-06, 7.779837687849067e-06, 7.72996645537205e-06, 7.680730959691573e-06, 7.63211846788181e-06, 7.58411806600634e-06, 7.5367174758866895e-06, 7.489905328839086e-06, 7.44367162042181e-06, 7.3980049819510896e-06, 7.352894954237854e-06, 7.308331987587735e-06, 7.264306077559013e-06, 7.220807219709968e-06, 7.177825864346232e-06, 7.135353826015489e-06, 7.0933811002760194e-06, 7.051899501675507e-06, 7.010899935266934e-06, 6.970374670345336e-06, 6.930315066711046e-06, 6.890712938911747e-06, 6.851561465737177e-06, 6.8128520069876686e-06, 6.7745772867056075e-06, 6.736730483680731e-06, 6.699304321955424e-06, 6.662291525572073e-06, 6.625685728067765e-06, 6.5894796534848865e-06, 6.553667390107876e-06, 6.518242116726469e-06, 6.483197921625106e-06, 6.4485284383408725e-06, 6.414227755158208e-06, 6.3802899603615515e-06, 6.3467095969826914e-06, 6.313480753306067e-06, 6.280597972363466e-06, 6.24805579718668e-06, 6.215849225554848e-06, 6.1839732552471105e-06, 6.152422429295257e-06, 6.121191745478427e-06, 6.0902766563231125e-06, 6.0596721596084535e-06], "accuracy_valid": [0.5546492611069277, 0.6480256965361446, 0.7458599044615963, 0.7760627470820783, 0.7938144178275602, 0.8123602809676205, 0.8199492305158133, 0.8296442606362951, 0.8339373117469879, 0.8356462961219879, 0.8377626717808735, 0.8430116952183735, 0.8398584572665663, 0.8462369987763554, 0.8507844855986446, 0.8490549110504518, 0.8510080360504518, 0.849888813064759, 0.8507638954254518, 0.8507638954254518, 0.8549348762236446, 0.8524419945406627, 0.8568674110504518, 0.8602044898343373, 0.8584646201995482, 0.8565012001129518, 0.8571321418486446, 0.8544554193335843, 0.8571218467620482, 0.8590646766754518, 0.8569585961031627, 0.8576910179781627, 0.8586675804781627, 0.859776508377259, 0.8590234963290663, 0.856846820877259, 0.8602750847138554, 0.8600103539156627, 0.860753070877259, 0.8591455666415663, 0.8595014824924698, 0.8608854362763554, 0.8628488563629518, 0.863194477127259, 0.861851703689759, 0.8616178581513554, 0.8608957313629518, 0.8617090432040663, 0.862828266189759, 0.8629709266754518, 0.8622076195406627, 0.8612310570406627, 0.8625944206513554, 0.862584125564759, 0.8645681358245482, 0.8648019813629518, 0.8648931664156627, 0.8610986916415663, 0.865513813064759, 0.8644357704254518, 0.8649446418486446, 0.8659212043486446, 0.862950336502259, 0.8650358269013554, 0.8656461784638554, 0.8634489128388554, 0.862462055252259, 0.8633268425263554, 0.864415180252259, 0.864415180252259, 0.863926899002259, 0.8656461784638554, 0.8646696159638554, 0.8635503929781627, 0.8628179711031627, 0.8645475456513554, 0.863926899002259, 0.8639166039156627, 0.8632959572665663, 0.8630518166415663, 0.8631738869540663, 0.8642622246799698, 0.863560688064759, 0.864903461502259, 0.863438617752259, 0.8645269554781627, 0.8660123894013554, 0.864659320877259, 0.8642828148531627, 0.8647608010165663, 0.8656255882906627, 0.8641298592808735, 0.863438617752259, 0.8630621117281627, 0.8640386742281627, 0.864537250564759, 0.8651373070406627, 0.8634180275790663, 0.8656461784638554, 0.8650152367281627, 0.8640592644013554, 0.8635503929781627, 0.8652593773531627, 0.865391742752259, 0.8654020378388554, 0.8671316123870482, 0.865635883377259, 0.8679861045745482, 0.8668668815888554, 0.865147602127259, 0.8675993034638554, 0.865513813064759, 0.8675993034638554, 0.8657682487763554, 0.8659212043486446, 0.8665212608245482, 0.8658903190888554, 0.8665006706513554, 0.8642828148531627, 0.8665212608245482, 0.8646387307040663, 0.8671316123870482, 0.8667242211031627, 0.8678537391754518, 0.865147602127259, 0.8663786003388554, 0.8667654014495482, 0.8655241081513554, 0.866002094314759, 0.8660226844879518, 0.8651681923004518, 0.8670095420745482, 0.8671213173004518, 0.8676301887236446, 0.8672433876129518, 0.866490375564759, 0.8657785438629518, 0.8649137565888554, 0.8670095420745482, 0.8666330360504518, 0.8666227409638554, 0.865635883377259, 0.8632856621799698, 0.8666536262236446, 0.8659006141754518, 0.8689935523343373, 0.8662359398531627, 0.8677213737763554, 0.869959819747741, 0.8684743858245482, 0.8672536826995482, 0.8665006706513554, 0.8678537391754518, 0.8675993034638554, 0.8676198936370482, 0.868739116622741, 0.8671213173004518, 0.8681184699736446, 0.8684743858245482, 0.8684743858245482, 0.8665109657379518, 0.8687185264495482, 0.8672433876129518, 0.8690950324736446, 0.865513813064759, 0.8684640907379518, 0.8676198936370482, 0.8677419639495482, 0.8667448112763554, 0.8681081748870482, 0.8684743858245482, 0.8689626670745482, 0.867466938064759, 0.8679758094879518, 0.8657682487763554, 0.8667756965361446, 0.867833149002259, 0.8689729621611446, 0.868565570877259, 0.8679861045745482, 0.867955219314759, 0.8661344597138554, 0.8677316688629518, 0.8679655144013554, 0.8681184699736446, 0.8691862175263554, 0.868199359939759, 0.8686979362763554, 0.866612445877259, 0.8694509483245482, 0.8694509483245482, 0.866978656814759, 0.8679655144013554, 0.868321430252259, 0.8681081748870482, 0.867100727127259, 0.8671110222138554, 0.8674772331513554, 0.8684537956513554], "accuracy_test": 0.8585200095663265, "start": "2016-01-29 10:48:14.991000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0], "accuracy_train_last": 0.9886303828096161, "batch_size_eval": 1024, "accuracy_train_std": [0.01764726458651838, 0.015514924615445307, 0.018697292447910088, 0.018606025227535197, 0.017211157494293954, 0.016167650640231998, 0.016572964037928153, 0.015679956936568352, 0.015171528632566352, 0.014619712849883486, 0.013615842387725662, 0.01253439705616492, 0.013577948628153402, 0.012550395653238122, 0.012392122502563543, 0.0125193661462436, 0.01202167903570857, 0.01157092378748788, 0.010888637589148289, 0.011956263138372471, 0.01146965392572202, 0.011628873278734714, 0.010990852781215405, 0.010691471753574565, 0.011411250934018834, 0.010740502567356843, 0.010390148059313311, 0.010810586337644677, 0.00989251135264717, 0.009722739903887568, 0.010307192588025323, 0.00965337859712666, 0.009752820727266286, 0.010136884348847384, 0.009486240236703705, 0.008391947070332322, 0.008865510738698465, 0.009211448787421542, 0.008449378120660034, 0.008936868373349036, 0.007905711313324557, 0.008041507519937765, 0.008097060365909873, 0.008154161763242145, 0.007552979861816052, 0.007721041261259835, 0.0078069954615383255, 0.00716383791127701, 0.007140794476239349, 0.007127519041225041, 0.006963864343504109, 0.006967921517261179, 0.00658205985077679, 0.006445341892803518, 0.006405299677491099, 0.006541379169509968, 0.006386476651990472, 0.0065376912289182845, 0.0055996118383270736, 0.006210031950157523, 0.005670945699670381, 0.006001531270539153, 0.005774306958457342, 0.005533407832894128, 0.005551973244054249, 0.004690033793438095, 0.005495493306011976, 0.005341357805154396, 0.005242154785486869, 0.005028692597224721, 0.0047517091858804705, 0.005325107215422784, 0.004935260516353809, 0.0045815020724072906, 0.0047945744233236585, 0.004968156155044395, 0.0044157022159067505, 0.0053059131574943914, 0.005082805494298678, 0.004872608752196363, 0.004571541979466883, 0.004953131545400591, 0.005055218834723021, 0.0049541099864786435, 0.004440984419023219, 0.004385651735434247, 0.0045600094346683545, 0.004760811678622257, 0.0047172193078969605, 0.004797179624746397, 0.00440424199177808, 0.004706709119641592, 0.005607001609465857, 0.004728949391921771, 0.004543070861727709, 0.004783730949503613, 0.004834576567270398, 0.004632101564319049, 0.004691838782013191, 0.004932119126175908, 0.005076541776473643, 0.004519068808084476, 0.004345118064719463, 0.004231561906093661, 0.004425907636064973, 0.003979212297899148, 0.004216274190950862, 0.003996587261736215, 0.003996104338202453, 0.004705074841052327, 0.004091302240925323, 0.0039738295785504375, 0.004481105570600691, 0.004367930272909473, 0.004137072511369377, 0.0039790242955978425, 0.004063358715648626, 0.00406582465362586, 0.0043119284321828, 0.0037972304887157696, 0.003910012093742031, 0.0038779181792956284, 0.003995878662033345, 0.0038923703868168026, 0.004230603181390376, 0.00396394042021688, 0.0039019056785095365, 0.003624479536723514, 0.0042869824278825965, 0.0038417638317084155, 0.004052181027809275, 0.004238186465381942, 0.004442616074880185, 0.003814423192123086, 0.0036085159318624206, 0.004042839311600428, 0.0036828816398342713, 0.0036376333082172112, 0.004193590000155663, 0.004032881681523352, 0.0035824261266201324, 0.0036563117034942917, 0.0036670675006493397, 0.003429735748655218, 0.003958925802739174, 0.0036407489998337562, 0.003919657902136539, 0.003693396573318902, 0.003594500770048281, 0.003524720021722847, 0.003824332413873672, 0.003968469807469906, 0.0034863455419769716, 0.003558182731416618, 0.0032487262511032874, 0.003351535941905481, 0.003574298551394772, 0.003324355890157284, 0.0037080505772297706, 0.0038357658990416064, 0.003526667089776606, 0.003228111783274376, 0.0032338484927614884, 0.0032626287916425337, 0.0031420892040794476, 0.0031008990900688836, 0.0028973631010427168, 0.0032327966488780286, 0.0030817709891263756, 0.003179640258491155, 0.0034325499519733582, 0.0035049630616805828, 0.0036877844545912, 0.0034851881122217967, 0.003635187821966106, 0.003119449615857955, 0.0034386070896463995, 0.003293057190754353, 0.003641112735966783, 0.0036562682534411104, 0.003566591463255862, 0.0035353342127768246, 0.0032796634276944466, 0.003574491426046715, 0.003209484416818554, 0.0032034893478036426, 0.0036126917731769907, 0.0034573255232472322, 0.0035251941946398413, 0.0032421977317001996, 0.0036755428677775134, 0.0032516661815926214, 0.0036508074160788505, 0.003414561471515043, 0.0035284876773060848, 0.003249021423789022, 0.003122307920742534, 0.0032419320865348772, 0.003280484878403615], "accuracy_test_std": 0.007471526312320184, "error_valid": [0.4453507388930723, 0.3519743034638554, 0.25414009553840367, 0.22393725291792166, 0.20618558217243976, 0.18763971903237953, 0.18005076948418675, 0.17035573936370485, 0.16606268825301207, 0.16435370387801207, 0.1622373282191265, 0.1569883047816265, 0.16014154273343373, 0.1537630012236446, 0.1492155144013554, 0.15094508894954817, 0.14899196394954817, 0.15011118693524095, 0.14923610457454817, 0.14923610457454817, 0.1450651237763554, 0.14755800545933728, 0.14313258894954817, 0.13979551016566272, 0.14153537980045183, 0.14349879988704817, 0.1428678581513554, 0.14554458066641573, 0.14287815323795183, 0.14093532332454817, 0.14304140389683728, 0.14230898202183728, 0.14133241952183728, 0.14022349162274095, 0.14097650367093373, 0.14315317912274095, 0.1397249152861446, 0.13998964608433728, 0.13924692912274095, 0.14085443335843373, 0.14049851750753017, 0.1391145637236446, 0.13715114363704817, 0.13680552287274095, 0.13814829631024095, 0.1383821418486446, 0.13910426863704817, 0.13829095679593373, 0.13717173381024095, 0.13702907332454817, 0.13779238045933728, 0.13876894295933728, 0.1374055793486446, 0.13741587443524095, 0.13543186417545183, 0.13519801863704817, 0.13510683358433728, 0.13890130835843373, 0.13448618693524095, 0.13556422957454817, 0.1350553581513554, 0.1340787956513554, 0.13704966349774095, 0.1349641730986446, 0.1343538215361446, 0.1365510871611446, 0.13753794474774095, 0.1366731574736446, 0.13558481974774095, 0.13558481974774095, 0.13607310099774095, 0.1343538215361446, 0.1353303840361446, 0.13644960702183728, 0.13718202889683728, 0.1354524543486446, 0.13607310099774095, 0.13608339608433728, 0.13670404273343373, 0.13694818335843373, 0.13682611304593373, 0.13573777532003017, 0.13643931193524095, 0.13509653849774095, 0.13656138224774095, 0.13547304452183728, 0.1339876105986446, 0.13534067912274095, 0.13571718514683728, 0.13523919898343373, 0.13437441170933728, 0.1358701407191265, 0.13656138224774095, 0.13693788827183728, 0.13596132577183728, 0.13546274943524095, 0.13486269295933728, 0.13658197242093373, 0.1343538215361446, 0.13498476327183728, 0.1359407355986446, 0.13644960702183728, 0.13474062264683728, 0.13460825724774095, 0.1345979621611446, 0.13286838761295183, 0.13436411662274095, 0.13201389542545183, 0.1331331184111446, 0.13485239787274095, 0.1324006965361446, 0.13448618693524095, 0.1324006965361446, 0.1342317512236446, 0.1340787956513554, 0.13347873917545183, 0.1341096809111446, 0.1334993293486446, 0.13571718514683728, 0.13347873917545183, 0.13536126929593373, 0.13286838761295183, 0.13327577889683728, 0.13214626082454817, 0.13485239787274095, 0.1336213996611446, 0.13323459855045183, 0.1344758918486446, 0.13399790568524095, 0.13397731551204817, 0.13483180769954817, 0.13299045792545183, 0.13287868269954817, 0.1323698112763554, 0.13275661238704817, 0.13350962443524095, 0.13422145613704817, 0.1350862434111446, 0.13299045792545183, 0.13336696394954817, 0.1333772590361446, 0.13436411662274095, 0.13671433782003017, 0.1333463737763554, 0.13409938582454817, 0.13100644766566272, 0.13376406014683728, 0.1322786262236446, 0.13004018025225905, 0.13152561417545183, 0.13274631730045183, 0.1334993293486446, 0.13214626082454817, 0.1324006965361446, 0.13238010636295183, 0.13126088337725905, 0.13287868269954817, 0.1318815300263554, 0.13152561417545183, 0.13152561417545183, 0.13348903426204817, 0.13128147355045183, 0.13275661238704817, 0.1309049675263554, 0.13448618693524095, 0.13153590926204817, 0.13238010636295183, 0.13225803605045183, 0.1332551887236446, 0.13189182511295183, 0.13152561417545183, 0.13103733292545183, 0.13253306193524095, 0.13202419051204817, 0.1342317512236446, 0.1332243034638554, 0.13216685099774095, 0.1310270378388554, 0.13143442912274095, 0.13201389542545183, 0.13204478068524095, 0.1338655402861446, 0.13226833113704817, 0.1320344855986446, 0.1318815300263554, 0.1308137824736446, 0.13180064006024095, 0.1313020637236446, 0.13338755412274095, 0.13054905167545183, 0.13054905167545183, 0.13302134318524095, 0.1320344855986446, 0.13167856974774095, 0.13189182511295183, 0.13289927287274095, 0.1328889777861446, 0.1325227668486446, 0.1315462043486446], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.5452613327648224, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0012058747829239676, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "l2_decay": 2.996350016826854e-08, "optimization": "rmsprop", "nb_data_augmentation": 4, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.08960369781980765}, "accuracy_valid_max": 0.869959819747741, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop', 'santa_sss'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8684537956513554, "loss_train": [1.5144630670547485, 1.0430461168289185, 0.7934553623199463, 0.6828304529190063, 0.6127357482910156, 0.5646560192108154, 0.5256593823432922, 0.4934651851654053, 0.46814340353012085, 0.446093887090683, 0.4263300895690918, 0.40938708186149597, 0.3953590989112854, 0.3794822692871094, 0.36799463629722595, 0.35525718331336975, 0.34424421191215515, 0.3355577290058136, 0.327321857213974, 0.31749850511550903, 0.309628427028656, 0.30184146761894226, 0.2967565357685089, 0.2882999777793884, 0.28282955288887024, 0.27520766854286194, 0.2714104652404785, 0.26564517617225647, 0.261933296918869, 0.25691094994544983, 0.25134938955307007, 0.24751944839954376, 0.2437397539615631, 0.23966190218925476, 0.23450517654418945, 0.2330102175474167, 0.22856244444847107, 0.22361958026885986, 0.22020970284938812, 0.21776778995990753, 0.21367360651493073, 0.21243661642074585, 0.2094680666923523, 0.20705614984035492, 0.20489513874053955, 0.20192736387252808, 0.1998787224292755, 0.19599153101444244, 0.19374440610408783, 0.19396494328975677, 0.18882931768894196, 0.18733060359954834, 0.18487869203090668, 0.18236535787582397, 0.18127964437007904, 0.17879490554332733, 0.17633351683616638, 0.17532549798488617, 0.17266838252544403, 0.1717814803123474, 0.1697515994310379, 0.16784875094890594, 0.16610851883888245, 0.16492491960525513, 0.16245926916599274, 0.16222625970840454, 0.15921854972839355, 0.1593853384256363, 0.15774212777614594, 0.1564883589744568, 0.15496322512626648, 0.15359444916248322, 0.1515793800354004, 0.1514531970024109, 0.149523064494133, 0.14955560863018036, 0.14773324131965637, 0.14694246649742126, 0.14321477711200714, 0.14328019320964813, 0.1430680751800537, 0.14198528230190277, 0.13910485804080963, 0.1391124725341797, 0.13959909975528717, 0.13708987832069397, 0.13651792705059052, 0.13552799820899963, 0.13392195105552673, 0.13227248191833496, 0.13327397406101227, 0.1301260143518448, 0.13096016645431519, 0.1299680769443512, 0.12910941243171692, 0.1282324492931366, 0.1269836723804474, 0.127119779586792, 0.1248956099152565, 0.12537875771522522, 0.12372507154941559, 0.12334196269512177, 0.1213935986161232, 0.12264207005500793, 0.12022598087787628, 0.11981858313083649, 0.12006986886262894, 0.11869782954454422, 0.11883188784122467, 0.11734414100646973, 0.11593472957611084, 0.11593884229660034, 0.11669266223907471, 0.1155427098274231, 0.11391772329807281, 0.11376936733722687, 0.11320747435092926, 0.11226394772529602, 0.11125367134809494, 0.11096104234457016, 0.11130833625793457, 0.10928278416395187, 0.1086261048913002, 0.10870098322629929, 0.10673271864652634, 0.10741619765758514, 0.10620248317718506, 0.10542988032102585, 0.10572180151939392, 0.10642240196466446, 0.10613816231489182, 0.10451734066009521, 0.10354915261268616, 0.10344560444355011, 0.10400643199682236, 0.10146772861480713, 0.10251402109861374, 0.10051701962947845, 0.10035368800163269, 0.10154742747545242, 0.09925293922424316, 0.0990704745054245, 0.09882137179374695, 0.09782075881958008, 0.09690680354833603, 0.09672945737838745, 0.09651145339012146, 0.09612598270177841, 0.09525541216135025, 0.09648767113685608, 0.09481216967105865, 0.09468317776918411, 0.09393471479415894, 0.09402186423540115, 0.09353259950876236, 0.09418295323848724, 0.0921638160943985, 0.09336956590414047, 0.09188045561313629, 0.09171240031719208, 0.09059468656778336, 0.09162981063127518, 0.09144581109285355, 0.09060461074113846, 0.0903521403670311, 0.08981367200613022, 0.08876447379589081, 0.08870911598205566, 0.08860216289758682, 0.08813309669494629, 0.08757777512073517, 0.08790035545825958, 0.0859844759106636, 0.0865444615483284, 0.08621492981910706, 0.08633781969547272, 0.08668309450149536, 0.08456782251596451, 0.08472125232219696, 0.08461260795593262, 0.08563335984945297, 0.08428698778152466, 0.08278928697109222, 0.0830528512597084, 0.08399324864149094, 0.0838741585612297, 0.08185221254825592, 0.0836583599448204, 0.08199954032897949, 0.08142191916704178, 0.0803481712937355, 0.0804080069065094, 0.08190999180078506, 0.0803418830037117, 0.08006930351257324, 0.07993567734956741, 0.08047068119049072, 0.07979222387075424, 0.07861340790987015], "accuracy_train_first": 0.5757023391357512, "model": "residualv3", "loss_std": [0.40459516644477844, 0.27063313126564026, 0.2511659562587738, 0.24117042124271393, 0.23253390192985535, 0.22502635419368744, 0.219687357544899, 0.21288235485553741, 0.209544837474823, 0.20414616167545319, 0.1986086368560791, 0.19651417434215546, 0.19250844419002533, 0.1883622407913208, 0.18551981449127197, 0.18006695806980133, 0.17708256840705872, 0.17554134130477905, 0.17345105111598969, 0.1701657474040985, 0.16740724444389343, 0.16495488584041595, 0.1628112643957138, 0.16178151965141296, 0.1604825258255005, 0.1570720374584198, 0.1558878868818283, 0.15304382145404816, 0.15165705978870392, 0.14896103739738464, 0.14612041413784027, 0.1467818170785904, 0.14202101528644562, 0.14375077188014984, 0.13982313871383667, 0.1394749879837036, 0.137977734208107, 0.1345701366662979, 0.13412681221961975, 0.13267816603183746, 0.13144800066947937, 0.1320495456457138, 0.1301548033952713, 0.12843936681747437, 0.12765702605247498, 0.12556329369544983, 0.1247325912117958, 0.1247686892747879, 0.12109120190143585, 0.12122480571269989, 0.12054891139268875, 0.11894448101520538, 0.11706800013780594, 0.11530213803052902, 0.11623780429363251, 0.11712196469306946, 0.11371618509292603, 0.11439821869134903, 0.11318521201610565, 0.11062496900558472, 0.10884856432676315, 0.10815592110157013, 0.10887538641691208, 0.10783147811889648, 0.10615599900484085, 0.10699242353439331, 0.10452210903167725, 0.10368524491786957, 0.10467129945755005, 0.10294906049966812, 0.10256071388721466, 0.10081090778112411, 0.09977319091558456, 0.10251424461603165, 0.09908744692802429, 0.09958589822053909, 0.0979054868221283, 0.09864877164363861, 0.09426531940698624, 0.09710617363452911, 0.09797704219818115, 0.09516069293022156, 0.09388798475265503, 0.09488599002361298, 0.09373971819877625, 0.09272684901952744, 0.09228038042783737, 0.09301421046257019, 0.09249770641326904, 0.09002204239368439, 0.09016019105911255, 0.08944279700517654, 0.08887414634227753, 0.08859799802303314, 0.08898545056581497, 0.08819424360990524, 0.08820989727973938, 0.08826268464326859, 0.0860692635178566, 0.08652466535568237, 0.08454953879117966, 0.08645369112491608, 0.0841076597571373, 0.08505532890558243, 0.08181089162826538, 0.08503144979476929, 0.08487069606781006, 0.08329703658819199, 0.08383925259113312, 0.0820138156414032, 0.07990014553070068, 0.08002472668886185, 0.08052776753902435, 0.08208721876144409, 0.07992030680179596, 0.07968200743198395, 0.07834748178720474, 0.07865563035011292, 0.07831989228725433, 0.07901732623577118, 0.07815760374069214, 0.07544074952602386, 0.07489204406738281, 0.07698538154363632, 0.07578069716691971, 0.07696317136287689, 0.07587995380163193, 0.07437749207019806, 0.07474134862422943, 0.07617132365703583, 0.07624330371618271, 0.07438981533050537, 0.07416766881942749, 0.07545725256204605, 0.07539009302854538, 0.07219550758600235, 0.07452911138534546, 0.07241395860910416, 0.07330505549907684, 0.07411207258701324, 0.0730733573436737, 0.06949695944786072, 0.07152717560529709, 0.06857454776763916, 0.0699043795466423, 0.07012408971786499, 0.06912611424922943, 0.07020710408687592, 0.06805974245071411, 0.07047782838344574, 0.06967895478010178, 0.06826838850975037, 0.06801418215036392, 0.06875912100076675, 0.06916234642267227, 0.06788889318704605, 0.06696955114603043, 0.06650947034358978, 0.06762993335723877, 0.06622756272554398, 0.06539265811443329, 0.06723761558532715, 0.06731602549552917, 0.06521528959274292, 0.06579884141683578, 0.06535555422306061, 0.06367984414100647, 0.0649813562631607, 0.06395018100738525, 0.06441152840852737, 0.06402221322059631, 0.06527987867593765, 0.0640932023525238, 0.06340579688549042, 0.06319382041692734, 0.06364313513040543, 0.06499875336885452, 0.06128817796707153, 0.06186241656541824, 0.06138131394982338, 0.06329343467950821, 0.06151663884520531, 0.061601366847753525, 0.06177658960223198, 0.06282614916563034, 0.06115884333848953, 0.061364125460386276, 0.06033548340201378, 0.0618676133453846, 0.060818083584308624, 0.058368194848299026, 0.05945571884512901, 0.06171935424208641, 0.05855374410748482, 0.05933884158730507, 0.05919096618890762, 0.06069276109337807, 0.0588543675839901, 0.05928537994623184]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:26 2016", "state": "available"}], "summary": "65d035ae8d0b3bb0c6048324e438bf38"}