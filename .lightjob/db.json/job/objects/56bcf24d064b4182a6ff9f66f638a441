{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 32, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6513757705688477, 1.233371615409851, 1.0211546421051025, 0.8844205141067505, 0.788613498210907, 0.7204439043998718, 0.6704210042953491, 0.6245251297950745, 0.5891252756118774, 0.5596861243247986, 0.5279300808906555, 0.4979695975780487, 0.4794412851333618, 0.447832852602005, 0.42560502886772156, 0.41133052110671997, 0.4058189392089844, 0.38881054520606995, 0.37806424498558044, 0.36899885535240173, 0.3545718193054199, 0.3386528193950653, 0.3304321765899658, 0.3189460337162018, 0.3010061979293823, 0.28985944390296936, 0.27618804574012756, 0.2693863809108734, 0.2576487064361572, 0.24776414036750793, 0.23666109144687653, 0.22148139774799347, 0.2161814123392105, 0.21272869408130646, 0.19568249583244324, 0.19722537696361542, 0.19267375767230988, 0.1879006177186966, 0.18739056587219238, 0.1716575026512146, 0.16843536496162415, 0.15290555357933044, 0.15890562534332275, 0.1438225358724594, 0.14949379861354828, 0.13792504370212555, 0.13480480015277863, 0.13080361485481262, 0.12396316975355148, 0.12218265235424042, 0.10543861985206604, 0.11067862808704376, 0.09899424761533737, 0.09508290141820908, 0.09270142018795013, 0.08885256201028824, 0.08370095491409302, 0.07302344590425491, 0.07039929181337357, 0.06799305230379105, 0.06393568217754364, 0.05395494028925896, 0.05376942455768585, 0.048012956976890564, 0.04242657497525215, 0.039867695420980453, 0.03946048393845558, 0.03559456765651703, 0.033679455518722534, 0.03726475313305855, 0.032378003001213074, 0.028685312718153, 0.025714535266160965, 0.02377338893711567, 0.02370067872107029, 0.02382204681634903, 0.026534853503108025, 0.028441177681088448, 0.02261788211762905, 0.021618833765387535, 0.023626452311873436, 0.025122735649347305, 0.021145951002836227, 0.01943385973572731, 0.019187968224287033, 0.019129037857055664, 0.018275363370776176, 0.01844751089811325, 0.01951415278017521, 0.020841084420681, 0.02268473617732525, 0.021147949621081352, 0.02187942899763584, 0.022397704422473907, 0.0198875293135643, 0.019226692616939545, 0.020587101578712463, 0.02023722045123577, 0.023626120761036873, 0.023521313443779945, 0.021790433675050735, 0.021648352965712547, 0.023043105378746986, 0.020907392725348473, 0.021899349987506866, 0.02481495589017868, 0.023558732122182846, 0.020157599821686745, 0.01975620724260807, 0.01930222474038601, 0.01658332720398903, 0.01572611555457115, 0.01328566949814558, 0.012409832328557968, 0.010988200083374977, 0.010379892773926258, 0.010259740054607391, 0.010239104740321636, 0.010225730948150158, 0.010214882902801037, 0.010205430909991264, 0.010196871124207973, 0.010188939049839973, 0.010181465186178684, 0.010174334980547428, 0.010167472064495087, 0.01016082800924778, 0.010154357179999352, 0.010148035362362862, 0.01014183834195137, 0.010135745629668236, 0.010129742324352264, 0.010123815387487411, 0.01011795923113823, 0.010112163610756397, 0.010106422007083893, 0.010100730694830418, 0.01009508315473795, 0.01008947566151619, 0.010083905421197414, 0.01007836777716875, 0.010072859935462475, 0.010067380964756012, 0.010061928071081638, 0.01005649846047163, 0.010051090270280838, 0.010045705363154411, 0.010040339082479477, 0.01003499049693346, 0.010029658675193787, 0.010024343617260456, 0.010019042529165745, 0.010013756342232227, 0.010008484125137329, 0.01000322587788105, 0.009997980669140816, 0.009992744773626328, 0.009987521916627884, 0.009982309304177761, 0.009977106936275959, 0.009971914812922478, 0.009966731071472168, 0.009961556643247604, 0.009956390596926212, 0.009951234795153141, 0.009946084581315517, 0.00994094368070364, 0.00993581023067236, 0.009930683299899101, 0.00992556381970644, 0.009920449927449226, 0.009915344417095184, 0.00991024449467659, 0.009905150160193443, 0.009900063276290894, 0.009894981980323792, 0.009889906272292137, 0.009884837083518505, 0.00987977348268032, 0.009874715469777584, 0.009869663044810295, 0.00986461527645588, 0.009859573096036911, 0.00985453650355339, 0.009849505499005318, 0.009844477288424969, 0.009839455597102642, 0.009834437631070614, 0.009829424321651459, 0.009824415668845177, 0.009819412603974342, 0.009814413264393806, 0.009809417650103569, 0.009804426692426205, 0.009799439460039139, 0.00979445781558752, 0.009789478033781052, 0.009784504771232605, 0.009779534302651882, 0.009774568490684032, 0.009769604541361332, 0.009764646179974079, 0.009759691543877125, 0.009754740633070469, 0.009749792516231537, 0.009744848124682903, 0.009739908389747143, 0.009734971448779106, 0.009730039164423943, 0.009725110605359077], "moving_avg_accuracy_train": [0.05025411764705882, 0.10339576470588234, 0.16118089411764702, 0.21662751058823523, 0.2691741712941176, 0.3183955776941176, 0.3644266081599999, 0.4071510061675293, 0.4464711996684234, 0.48292055028981634, 0.516400259966717, 0.5466331751465159, 0.5743769164553937, 0.6003651071627955, 0.6237144787994571, 0.6450183250371585, 0.6649517866510897, 0.683188372691863, 0.700255417775618, 0.7157828171745267, 0.7295363001629565, 0.7421309054407784, 0.7526707560731711, 0.7639542687011481, 0.7757235477133863, 0.7862947223538124, 0.7948299560007841, 0.8036881368712939, 0.8113687349488704, 0.8180977438069245, 0.824123263543879, 0.8318097607189029, 0.8390711375881891, 0.845498141476429, 0.8513389155640803, 0.8574991416547311, 0.862953933371611, 0.8677714812109205, 0.8717849213251225, 0.874545252722022, 0.8782201392145257, 0.8815863605871908, 0.8845524304108245, 0.8869748344285656, 0.8903173509857091, 0.8939703217694911, 0.8969662307690126, 0.8992437253391701, 0.9014228822170178, 0.9045441234070807, 0.9073038287134314, 0.9107993281950294, 0.9144064541990559, 0.918104632308562, 0.9208871102541764, 0.9237183992287588, 0.926372441658824, 0.9292457857282358, 0.9307965012730592, 0.9325286158516357, 0.9345863425017662, 0.9373630023692366, 0.9406902315440776, 0.9439294436837874, 0.946343558138938, 0.9494221435015148, 0.9519152232690103, 0.9548907597656388, 0.957187566142016, 0.9589111624689909, 0.96136122269268, 0.9631780415998826, 0.9653708256751885, 0.9674431548723755, 0.9695976629145497, 0.9714026025054476, 0.973008224607844, 0.974554460970589, 0.9759248972264711, 0.9770688780920593, 0.978119637341677, 0.979246497136921, 0.9804583180114642, 0.9816148391514942, 0.9825710022951682, 0.9834339020656514, 0.9842316883296744, 0.9850108724378834, 0.9855709616646833, 0.9861762184393914, 0.9864080083601582, 0.98679544281826, 0.9873394279481987, 0.9876925439769083, 0.9882338778145117, 0.988443431209531, 0.9883661469121073, 0.9884754145738377, 0.9884678731164539, 0.9887999093342202, 0.9889222713419746, 0.9893547500901301, 0.9895416280222935, 0.9896274652200642, 0.9894576598745283, 0.9894765997694284, 0.989615998616015, 0.989981457577943, 0.9903433118201487, 0.9906760394616632, 0.9910649061037321, 0.9914266507874765, 0.9918604562969642, 0.9923073518437384, 0.9928766166593646, 0.993438366758134, 0.9939721771411442, 0.9944596653093827, 0.9949007576019737, 0.9953071524300117, 0.9956729077752458, 0.9960020875859564, 0.996298349415596, 0.9965696909446247, 0.996816251261927, 0.9970405084886754, 0.9972470458751019, 0.9974399883464153, 0.9976159895117739, 0.9977743905605965, 0.9979169515045369, 0.9980452563540831, 0.9981630836598513, 0.9982691282350427, 0.9983645683527149, 0.99845046445862, 0.9985277709539344, 0.9986020526820704, 0.9986689062373927, 0.9987314273783593, 0.9987876964052292, 0.9988383385294122, 0.9988839164411768, 0.998924936561765, 0.9989618546702943, 0.9989950809679707, 0.9990249846358795, 0.9990518979369974, 0.9990784728491799, 0.9991023902701444, 0.9991239159490122, 0.9991432890599934, 0.9991607248598764, 0.999176417079771, 0.9991905400776762, 0.9992056037169674, 0.999221513933506, 0.9992358331283907, 0.9992510733449634, 0.9992647895398789, 0.9992771341153027, 0.9992882442331842, 0.9992982433392775, 0.9993072425347616, 0.9993153418106971, 0.9993226311590392, 0.9993291915725471, 0.9993350959447042, 0.9993404098796455, 0.9993451924210927, 0.9993518496495716, 0.9993578411552027, 0.999365586451447, 0.999372557218067, 0.9993788309080249, 0.9993844772289872, 0.9993895589178531, 0.9993941324378325, 0.9993982486058139, 0.9994019531569972, 0.9994052872530621, 0.9994129938218735, 0.9994222826749803, 0.9994306426427765, 0.9994381666137929, 0.9994472911288842, 0.9994555031924663, 0.9994628940496904, 0.999469545821192, 0.9994755324155433, 0.9994809203504595, 0.9994857694918842, 0.9994901337191664, 0.9994940615237203, 0.9994975965478189, 0.9995007780695075, 0.9995036414390274, 0.9995062184715953, 0.9995085378009063, 0.9995106251972863, 0.9995125038540282, 0.999514194645096, 0.999515716357057, 0.999517085897822, 0.9995183184845103, 0.9995217807537063, 0.9995248967959828, 0.9995277012340316, 0.9995302252282755, 0.999532496823095], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05034666666666665, 0.10281866666666664, 0.15917679999999995, 0.21232578666666663, 0.2626398746666666, 0.30906922053333324, 0.35153563181333325, 0.38996873529866655, 0.42527852843546654, 0.4574440089252532, 0.48656627469939456, 0.5130163138961218, 0.5369013491731762, 0.5587712142558586, 0.5781340928302727, 0.5962273502139122, 0.6126179485258543, 0.6271428203399355, 0.6412418716392753, 0.6529576844753477, 0.6631552493611463, 0.6717463910916984, 0.6780650853158618, 0.6853385767842757, 0.6928447191058481, 0.6992002471952632, 0.7038802224757369, 0.7092522002281632, 0.7141536468720135, 0.7179916155181455, 0.721112453966331, 0.7252278752363646, 0.7296117543793949, 0.7329305789414554, 0.7356375210473098, 0.7386204356092455, 0.7404117253816543, 0.7421972195101556, 0.7436708308924733, 0.7439304144698926, 0.7445507063562367, 0.7452823023872797, 0.7454340721485517, 0.7456506649336965, 0.7464855984403269, 0.747623705262961, 0.7490480014033315, 0.748929867929665, 0.7494102144700318, 0.7505758596896952, 0.7506649403873924, 0.7523051130153198, 0.7541946017137878, 0.756495141542409, 0.7574189607215014, 0.758583731316018, 0.7593786915177495, 0.7597208223659746, 0.7589487401293771, 0.758480532783106, 0.7581258128381287, 0.7586332315543158, 0.760436575065551, 0.7621395842256625, 0.7629256258030962, 0.76428639655612, 0.7650577569005079, 0.7661119812104571, 0.7669407830894114, 0.7670333714471369, 0.7684767009690898, 0.7689090308721809, 0.7692847944516295, 0.7698229816731332, 0.7709740168391532, 0.7722899484885712, 0.7728342869730475, 0.7738575249424093, 0.7744051057815017, 0.7752979285366848, 0.775874802349683, 0.7768473221147146, 0.7777625899032432, 0.7783463309129189, 0.7786983644882937, 0.7787485280394643, 0.7792870085688511, 0.7795849743786327, 0.7801064769407695, 0.7806824959133593, 0.7802142463220233, 0.7805794883564876, 0.7807082061875055, 0.781597385568755, 0.7821576470118795, 0.7821152156440249, 0.7824903607462891, 0.7821879913383268, 0.7818891922044942, 0.7819402729840448, 0.782612912352307, 0.783138287783743, 0.7831444590053687, 0.7828566797714985, 0.7826510117943486, 0.7829725772815804, 0.7828086528867557, 0.7830211209314135, 0.7828656755049388, 0.7834191079544449, 0.7840505304923338, 0.784152144109767, 0.7840702630321237, 0.7847299033955779, 0.7853235797226867, 0.7861245550837515, 0.7867787662420429, 0.787394222951172, 0.7878948006560548, 0.788331987257116, 0.7887787885314044, 0.7891542430115972, 0.7895188187104375, 0.7898336035060605, 0.7901302431554544, 0.790357218839909, 0.7905481636225847, 0.7906666805936596, 0.7908133458676271, 0.790958677947531, 0.7910628101527779, 0.7911565291375002, 0.7912408762237502, 0.7913167886013752, 0.7913851097412377, 0.7914199321004473, 0.7914779388904025, 0.7915434783346956, 0.7916157971678928, 0.7916942174511035, 0.7917781290393264, 0.791853649468727, 0.7919216178551877, 0.7919827894030022, 0.7920511771293686, 0.7921393927497651, 0.7922321201414553, 0.7923155747939764, 0.7923906839812455, 0.7924582822497875, 0.7925457873581421, 0.7926378752889945, 0.7927074210934284, 0.7927700123174188, 0.7928396777523437, 0.7929023766437759, 0.7929721389793983, 0.7930349250814585, 0.7930914325733126, 0.793168955982648, 0.7932253937177165, 0.7932761876792782, 0.7932952355780171, 0.7933123786868821, 0.7933011408181938, 0.7933043600697078, 0.7933072573960704, 0.7932831983231301, 0.7932615451574837, 0.793242057308402, 0.7932111849108952, 0.7931967330864723, 0.7931837264444918, 0.7931853538000425, 0.7931734850867049, 0.7931628032447011, 0.7931398562535643, 0.7931192039615412, 0.7931006168987205, 0.7930572218755152, 0.7930181663546303, 0.792983016385834, 0.7929513814139173, 0.7929362432725257, 0.7929092856119397, 0.7928983570507459, 0.7928885213456713, 0.7928796692111042, 0.7928583689566604, 0.7928391987276611, 0.7928219455215617, 0.7927797509694056, 0.792741775872465, 0.7926942649518851, 0.7926515051233632, 0.7926130212776936, 0.7925783858165909, 0.7925472139015985, 0.7925324925114386, 0.7925059099269615, 0.7924553189342654, 0.7923964537075054, 0.7923434750034215, 0.7922824608364127, 0.7922275480861048, 0.7921781266108276, 0.7921336472830782, 0.7921069492214371, 0.79208292096596, 0.7920746288693641], "moving_var_accuracy_train": [0.02272928706435986, 0.04587267022704498, 0.07133749383455003, 0.09187268995342358, 0.1075357849201333, 0.11858692806006563, 0.12579793714581797, 0.12964651109718822, 0.1305965585399992, 0.12949389913249043, 0.12663252785968743, 0.12219553751613892, 0.11690342040085043, 0.11129155286696396, 0.10506913598271002, 0.09864690716511562, 0.09235830247583068, 0.08611562986205039, 0.08012562312686364, 0.07428296200301637, 0.06855709045153195, 0.06312899814531603, 0.05781589439296278, 0.05318016386869795, 0.04910879083803933, 0.04520365935374089, 0.041338945339042386, 0.037911257120150144, 0.03465105568959857, 0.03159346616254465, 0.028760881539194053, 0.0264165335346695, 0.024249428527542787, 0.022196243085603567, 0.02028365055453001, 0.018596820968468424, 0.01700493164569245, 0.015513317385779523, 0.014106954961154149, 0.012764834329825118, 0.01160989401343768, 0.010550887629061997, 0.009574976997943841, 0.00867029166917597, 0.007903814254671375, 0.007233530589128723, 0.006590956766816575, 0.005978543923788793, 0.005423428053694352, 0.004968764567423826, 0.004540431871082548, 0.004196355333606964, 0.0038938220223265836, 0.003627528512060602, 0.003334455312515016, 0.00307315555658184, 0.002829235471908938, 0.0026206168799890552, 0.0023801976602987646, 0.0021691798824887407, 0.001990370044939784, 0.0018607216006223946, 0.0017742835263973758, 0.0016912876313320338, 0.0015746104056219377, 0.0015024485555717925, 0.0014081427205584755, 0.001347012805487535, 0.0012597894007138886, 0.001160547519327751, 0.0010985179232923092, 0.0010183736094371975, 0.0009598109665017123, 0.0009024808045651656, 0.0008540098682427891, 0.0007979291437596286, 0.0007413384304050007, 0.0006887222093697751, 0.0006367528482157276, 0.0005848557933816418, 0.0005363070690493899, 0.0004941046791276885, 0.0004579107997027273, 0.0004241575900584824, 0.00038997006266852115, 0.0003576744205267683, 0.0003276351447816668, 0.00030033578117386967, 0.0002731255025342784, 0.0002491099741508215, 0.00022468251584206083, 0.0002035652133917767, 0.00018587197044694892, 0.00016840699176983841, 0.00015420367350646325, 0.00013917851978409443, 0.00012531442356933947, 0.00011289043600950588, 0.00010160190427077053, 9.243394629287089e-05, 8.332530381205911e-05, 7.667611423930834e-05, 6.932281306914475e-05, 6.245684398292033e-05, 5.647066428298119e-05, 5.082682633125252e-05, 4.591903204399428e-05, 4.2529171115275675e-05, 3.9454700437168504e-05, 3.6505599544302134e-05, 3.421599497769794e-05, 3.197212842588442e-05, 3.0468600563852525e-05, 2.9219181175006525e-05, 2.921382493029484e-05, 2.913251099847253e-05, 2.8783841623710013e-05, 2.8044259888891994e-05, 2.69908955952526e-05, 2.577821684203111e-05, 2.4404387910934062e-05, 2.2939183249856595e-05, 2.1435204570183876e-05, 1.9954320141545864e-05, 1.8506016038004873e-05, 1.710803616794467e-05, 1.57811517790775e-05, 1.4538077776298228e-05, 1.3363057690536227e-05, 1.2252569951895533e-05, 1.121022556134045e-05, 1.0237362214960324e-05, 9.338575459325541e-06, 8.505926980740598e-06, 7.737313627218096e-06, 7.029985533583039e-06, 6.3807736281850034e-06, 5.792356241580303e-06, 5.253345198155456e-06, 4.76319071594981e-06, 4.315367474818848e-06, 3.90691235001282e-06, 3.5349172293789837e-06, 3.19656935907872e-06, 2.8891789438073344e-06, 2.6101969311421563e-06, 2.3572253022175415e-06, 2.128021703989365e-06, 1.9215755672080516e-06, 1.7345663977175223e-06, 1.5652799516022947e-06, 1.4121298133038502e-06, 1.2736528960315103e-06, 1.1485038183153774e-06, 1.035448568112327e-06, 9.339459303593554e-07, 8.428295522361517e-07, 7.603919510918487e-07, 6.864431337933082e-07, 6.194920264405989e-07, 5.589143206781028e-07, 5.041338010843597e-07, 4.546202600799185e-07, 4.098871037461624e-07, 3.694887778076772e-07, 3.3301811142017586e-07, 3.001036515067039e-07, 2.704070408511552e-07, 2.436204779070884e-07, 2.194642844406294e-07, 1.9791672421575739e-07, 1.7844813505172655e-07, 1.6114322807178224e-07, 1.4546622955003998e-07, 1.3127383926623978e-07, 1.1843338380329099e-07, 1.0682245747853906e-07, 9.632846549570294e-08, 8.684810449579706e-08, 7.828680734144572e-08, 7.05581723764366e-08, 6.403687596440189e-08, 5.840973349631184e-08, 5.319776170064347e-08, 4.8387476789288793e-08, 4.429804009122182e-08, 4.047517797659813e-08, 3.691928311348183e-08, 3.362556937911319e-08, 3.058556624855521e-08, 2.7788278207656228e-08, 2.5221077939895604e-08, 2.2870388463839894e-08, 2.0722198454982168e-08, 1.8762446167880262e-08, 1.6977300273393387e-08, 1.535336021111813e-08, 1.3877794061708131e-08, 1.2538428251615722e-08, 1.1323800439277654e-08, 1.022318455573693e-08, 9.226595070076662e-09, 8.324776028698778e-09, 7.509179202988981e-09, 6.771934712189736e-09, 6.202627012842155e-09, 5.6697517867737625e-09, 5.173560463021144e-09, 4.713539339207946e-09, 4.288626692503159e-09], "duration": 38121.469339, "accuracy_train": [0.5025411764705883, 0.5816705882352942, 0.6812470588235294, 0.7156470588235294, 0.7420941176470588, 0.7613882352941177, 0.7787058823529411, 0.7916705882352941, 0.8003529411764706, 0.8109647058823529, 0.8177176470588235, 0.8187294117647059, 0.8240705882352941, 0.8342588235294117, 0.8338588235294118, 0.8367529411764706, 0.8443529411764706, 0.8473176470588235, 0.8538588235294118, 0.8555294117647059, 0.8533176470588235, 0.8554823529411765, 0.8475294117647059, 0.8655058823529411, 0.8816470588235295, 0.8814352941176471, 0.8716470588235294, 0.8834117647058823, 0.8804941176470589, 0.8786588235294117, 0.8783529411764706, 0.9009882352941176, 0.9044235294117647, 0.9033411764705882, 0.9039058823529412, 0.9129411764705883, 0.9120470588235294, 0.9111294117647059, 0.9079058823529412, 0.8993882352941176, 0.9112941176470588, 0.9118823529411765, 0.9112470588235294, 0.9087764705882353, 0.9204, 0.9268470588235294, 0.9239294117647059, 0.9197411764705883, 0.9210352941176471, 0.932635294117647, 0.9321411764705883, 0.9422588235294118, 0.9468705882352941, 0.9513882352941176, 0.9459294117647059, 0.9492, 0.9502588235294117, 0.9551058823529411, 0.9447529411764706, 0.9481176470588235, 0.9531058823529411, 0.9623529411764706, 0.970635294117647, 0.9730823529411765, 0.9680705882352941, 0.9771294117647059, 0.9743529411764705, 0.9816705882352941, 0.9778588235294118, 0.9744235294117647, 0.9834117647058823, 0.9795294117647059, 0.9851058823529412, 0.9860941176470588, 0.9889882352941176, 0.9876470588235294, 0.9874588235294117, 0.9884705882352941, 0.9882588235294117, 0.9873647058823529, 0.9875764705882353, 0.9893882352941177, 0.9913647058823529, 0.9920235294117647, 0.9911764705882353, 0.9912, 0.9914117647058823, 0.9920235294117647, 0.9906117647058823, 0.9916235294117647, 0.9884941176470589, 0.9902823529411765, 0.9922352941176471, 0.9908705882352942, 0.9931058823529412, 0.9903294117647059, 0.9876705882352941, 0.9894588235294117, 0.9884, 0.9917882352941176, 0.9900235294117647, 0.9932470588235294, 0.9912235294117647, 0.9904, 0.9879294117647058, 0.9896470588235294, 0.9908705882352942, 0.9932705882352941, 0.9936, 0.9936705882352941, 0.9945647058823529, 0.9946823529411765, 0.995764705882353, 0.9963294117647059, 0.998, 0.9984941176470589, 0.9987764705882353, 0.9988470588235294, 0.9988705882352941, 0.998964705882353, 0.998964705882353, 0.998964705882353, 0.998964705882353, 0.9990117647058824, 0.999035294117647, 0.9990588235294118, 0.9991058823529412, 0.9991764705882353, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992235294117647, 0.9992235294117647, 0.9992235294117647, 0.9992235294117647, 0.9992235294117647, 0.9992705882352941, 0.9992705882352941, 0.9992941176470588, 0.9992941176470588, 0.9992941176470588, 0.9992941176470588, 0.9992941176470588, 0.9992941176470588, 0.9992941176470588, 0.9992941176470588, 0.9992941176470588, 0.9993176470588235, 0.9993176470588235, 0.9993176470588235, 0.9993176470588235, 0.9993176470588235, 0.9993176470588235, 0.9993176470588235, 0.9993411764705883, 0.9993647058823529, 0.9993647058823529, 0.9993882352941177, 0.9993882352941177, 0.9993882352941177, 0.9993882352941177, 0.9993882352941177, 0.9993882352941177, 0.9993882352941177, 0.9993882352941177, 0.9993882352941177, 0.9993882352941177, 0.9993882352941177, 0.9993882352941177, 0.9994117647058823, 0.9994117647058823, 0.9994352941176471, 0.9994352941176471, 0.9994352941176471, 0.9994352941176471, 0.9994352941176471, 0.9994352941176471, 0.9994352941176471, 0.9994352941176471, 0.9994352941176471, 0.9994823529411765, 0.9995058823529411, 0.9995058823529411, 0.9995058823529411, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995294117647059, 0.9995529411764705, 0.9995529411764705, 0.9995529411764705, 0.9995529411764705, 0.9995529411764705], "end": "2016-02-07 09:52:51.484000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0], "moving_var_accuracy_valid": [0.02281308159999999, 0.04531157049599999, 0.06936656618175999, 0.08785324261682559, 0.10185148541658873, 0.11106749429338886, 0.11619130964706897, 0.11786610967399117, 0.11730053212886454, 0.11488204213222772, 0.11102679519338246, 0.10622055683561987, 0.10073295534373312, 0.09496427879797238, 0.0888421405183627, 0.08290422013128201, 0.07703166353536477, 0.07122724429276772, 0.0658935690913637, 0.06053955461591625, 0.05542151212072527, 0.050543630354762635, 0.04584860038957267, 0.041739873453885204, 0.03807296566146199, 0.0346292037309739, 0.03136340287550911, 0.028486785892711267, 0.025854324916262748, 0.023401462454594704, 0.021148972902712285, 0.01918650584250966, 0.01744082082532495, 0.01579587011105608, 0.014282230920030503, 0.012934087841581725, 0.01166955752886218, 0.010531293679522174, 0.009497708086124825, 0.008548543730215335, 0.00769715221541218, 0.006932254088644703, 0.006239235986324162, 0.005615734599602937, 0.005060435165287089, 0.004566049233015914, 0.0041277018851735925, 0.003715057296314638, 0.003345628161872756, 0.0030232939046886, 0.00272103593255606, 0.0024731438355450755, 0.0022579609598653134, 0.0020797972154064353, 0.0018794984707467231, 0.0017037588385127042, 0.0015390706101624672, 0.0013862170308019852, 0.0012529603265424107, 0.0011296372569600892, 0.0010178059674183623, 0.0009183426344583591, 0.0008557768013881461, 0.0007963012830441466, 0.0007222319069928229, 0.000666673989674103, 0.0006053615617347422, 0.0005548279056224588, 0.0005055273280512363, 0.0004550517486819895, 0.0004282953747942596, 0.0003871480196207939, 0.00034970400206747455, 0.00031734041122923604, 0.00029753030768704485, 0.0002833623618718005, 0.00025769286515575716, 0.00024134672211767672, 0.0002199106528839793, 0.00020509377984513714, 0.00018757945242573108, 0.00017733365942355382, 0.00016713972960365973, 0.00015349253874068832, 0.00013925863361033985, 0.00012535541768610032, 0.00011542952744224955, 0.00010468562731221359, 9.666474888182879e-05, 8.998445470469576e-05, 8.295932835230249e-05, 7.586401121072911e-05, 6.842672460985381e-05, 6.869981189722073e-05, 6.465486666936618e-05, 5.820558379123169e-05, 5.365163004188376e-05, 4.910931236753833e-05, 4.5001909432196954e-05, 4.0525201703332716e-05, 4.054467501062477e-05, 3.89743816051711e-05, 3.507728620044117e-05, 3.231490956741935e-05, 2.946411246210159e-05, 2.7448340479099117e-05, 2.4945347296157032e-05, 2.285709659654757e-05, 2.0788856462399883e-05, 2.1466558101656985e-05, 2.2908152083677108e-05, 2.0710264820540324e-05, 1.8699578936370647e-05, 2.074574972461665e-05, 2.184323898448033e-05, 2.543296884732621e-05, 2.6741602119291797e-05, 2.7476524554670197e-05, 2.6984074446834766e-05, 2.6005856119478223e-05, 2.520195291588203e-05, 2.39504522245661e-05, 2.2751645963773356e-05, 2.1368286575394236e-05, 2.002341365218789e-05, 1.8484733938971638e-05, 1.6964399735354357e-05, 1.539437621371374e-05, 1.4048534915633677e-05, 1.2833774145113205e-05, 1.164798837612813e-05, 1.0562238771391568e-05, 9.57004477288217e-06, 8.664904497284056e-06, 7.840424050924633e-06, 7.067295016140448e-06, 6.3908486036546465e-06, 5.7904225121133965e-06, 5.25845038361697e-06, 4.787953012624925e-06, 4.3725281031053676e-06, 3.986605310106615e-06, 3.629522093118504e-06, 3.3002475081649242e-06, 3.0123146874065834e-06, 2.781121179803347e-06, 2.5803943843499553e-06, 2.385037057161782e-06, 2.1973058615555288e-06, 2.0187010085890166e-06, 1.8857452036234395e-06, 1.7734923663392785e-06, 1.6396726999345506e-06, 1.5109643818267523e-06, 1.4035473990534055e-06, 1.2985730180296198e-06, 1.212516767470094e-06, 1.126743942230271e-06, 1.0428074177280675e-06, 9.926155869101319e-07, 9.220209896761084e-07, 8.5303912948866e-07, 7.710006185570812e-07, 6.965455323353743e-07, 6.280275863357166e-07, 5.653180999249355e-07, 5.088618404329019e-07, 4.631852073063648e-07, 4.2108642281829784e-07, 3.8239576689294883e-07, 3.5273413455405047e-07, 3.193404181609843e-07, 2.889289309653805e-07, 2.6005987244364184e-07, 2.3532168240588488e-07, 2.1281642990264835e-07, 1.9627386653246346e-07, 1.804851343714815e-07, 1.6554593107306806e-07, 1.6593949031672816e-07, 1.6307354468934073e-07, 1.5788587297787953e-07, 1.5110422871364782e-07, 1.3805627576544985e-07, 1.3079108736728698e-07, 1.1878687967849417e-07, 1.0777886155947438e-07, 9.770621798107956e-08, 9.20189037372897e-08, 8.612449248255848e-08, 8.019110132069303e-08, 8.819541327356374e-08, 9.235484383500826e-08, 1.0343494762059177e-07, 1.0954707927548867e-07, 1.1192142874567474e-07, 1.1152582236327284e-07, 1.091184346856011e-07, 1.001570651711885e-07, 9.650106283145586e-08, 1.0988599342614207e-07, 1.3008342837693636e-07, 1.4233577331689796e-07, 1.6160675316723488e-07, 1.7258476916795881e-07, 1.7730863221829564e-07, 1.7738346436984228e-07, 1.660601963914107e-07, 1.5465039030369642e-07, 1.3980418106694344e-07], "accuracy_test": 0.7661, "start": "2016-02-06 23:17:30.015000", "learning_rate_per_epoch": [0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359, 0.002716148504987359], "accuracy_train_first": 0.5025411764705883, "accuracy_train_last": 0.9995529411764705, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.4965333333333334, 0.4249333333333334, 0.3336, 0.30933333333333335, 0.2845333333333333, 0.2730666666666667, 0.26626666666666665, 0.26413333333333333, 0.25693333333333335, 0.25306666666666666, 0.2513333333333333, 0.24893333333333334, 0.24813333333333332, 0.24439999999999995, 0.24760000000000004, 0.24093333333333333, 0.23986666666666667, 0.2421333333333333, 0.23186666666666667, 0.24160000000000004, 0.24506666666666665, 0.25093333333333334, 0.2650666666666667, 0.24919999999999998, 0.23960000000000004, 0.24360000000000004, 0.254, 0.24239999999999995, 0.24173333333333336, 0.2474666666666666, 0.2508, 0.23773333333333335, 0.23093333333333332, 0.23719999999999997, 0.24, 0.23453333333333337, 0.24346666666666672, 0.24173333333333336, 0.24306666666666665, 0.25373333333333337, 0.24986666666666668, 0.24813333333333332, 0.2532, 0.25239999999999996, 0.246, 0.2421333333333333, 0.2381333333333333, 0.2521333333333333, 0.24626666666666663, 0.23893333333333333, 0.24853333333333338, 0.23293333333333333, 0.2288, 0.2228, 0.23426666666666662, 0.23093333333333332, 0.2334666666666667, 0.23719999999999997, 0.248, 0.24573333333333336, 0.24506666666666665, 0.2368, 0.22333333333333338, 0.22253333333333336, 0.22999999999999998, 0.2234666666666667, 0.22799999999999998, 0.22440000000000004, 0.22560000000000002, 0.2321333333333333, 0.21853333333333336, 0.22719999999999996, 0.2273333333333334, 0.22533333333333339, 0.21866666666666668, 0.21586666666666665, 0.2222666666666666, 0.2169333333333333, 0.22066666666666668, 0.21666666666666667, 0.2189333333333333, 0.21440000000000003, 0.21399999999999997, 0.21640000000000004, 0.2181333333333333, 0.2208, 0.21586666666666665, 0.21773333333333333, 0.21519999999999995, 0.2141333333333333, 0.22399999999999998, 0.2161333333333333, 0.2181333333333333, 0.21040000000000003, 0.2128, 0.21826666666666672, 0.2141333333333333, 0.22053333333333336, 0.2208, 0.21760000000000002, 0.21133333333333337, 0.21213333333333328, 0.2168, 0.21973333333333334, 0.21919999999999995, 0.2141333333333333, 0.21866666666666668, 0.21506666666666663, 0.21853333333333336, 0.2116, 0.2102666666666667, 0.2149333333333333, 0.21666666666666667, 0.20933333333333337, 0.20933333333333337, 0.20666666666666667, 0.20733333333333337, 0.20706666666666662, 0.2076, 0.20773333333333333, 0.20720000000000005, 0.2074666666666667, 0.20720000000000005, 0.20733333333333337, 0.20720000000000005, 0.2076, 0.20773333333333333, 0.2082666666666667, 0.20786666666666664, 0.20773333333333333, 0.20799999999999996, 0.20799999999999996, 0.20799999999999996, 0.20799999999999996, 0.20799999999999996, 0.2082666666666667, 0.20799999999999996, 0.20786666666666664, 0.20773333333333333, 0.2076, 0.2074666666666667, 0.2074666666666667, 0.2074666666666667, 0.2074666666666667, 0.20733333333333337, 0.20706666666666662, 0.2069333333333333, 0.2069333333333333, 0.2069333333333333, 0.2069333333333333, 0.20666666666666667, 0.20653333333333335, 0.20666666666666667, 0.20666666666666667, 0.20653333333333335, 0.20653333333333335, 0.20640000000000003, 0.20640000000000003, 0.20640000000000003, 0.20613333333333328, 0.2062666666666667, 0.2062666666666667, 0.20653333333333335, 0.20653333333333335, 0.20679999999999998, 0.20666666666666667, 0.20666666666666667, 0.2069333333333333, 0.2069333333333333, 0.2069333333333333, 0.20706666666666662, 0.2069333333333333, 0.2069333333333333, 0.20679999999999998, 0.2069333333333333, 0.2069333333333333, 0.20706666666666662, 0.20706666666666662, 0.20706666666666662, 0.20733333333333337, 0.20733333333333337, 0.20733333333333337, 0.20733333333333337, 0.20720000000000005, 0.20733333333333337, 0.20720000000000005, 0.20720000000000005, 0.20720000000000005, 0.20733333333333337, 0.20733333333333337, 0.20733333333333337, 0.2076, 0.2076, 0.20773333333333333, 0.20773333333333333, 0.20773333333333333, 0.20773333333333333, 0.20773333333333333, 0.2076, 0.20773333333333333, 0.20799999999999996, 0.20813333333333328, 0.20813333333333328, 0.2082666666666667, 0.2082666666666667, 0.2082666666666667, 0.2082666666666667, 0.20813333333333328, 0.20813333333333328, 0.20799999999999996], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.0043921028656316195, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.0027161484289568443, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 7.467647635655047e-07, "rotation_range": [0, 0], "momentum": 0.9895068574853654}, "accuracy_valid_max": 0.7938666666666667, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.792, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.5034666666666666, 0.5750666666666666, 0.6664, 0.6906666666666667, 0.7154666666666667, 0.7269333333333333, 0.7337333333333333, 0.7358666666666667, 0.7430666666666667, 0.7469333333333333, 0.7486666666666667, 0.7510666666666667, 0.7518666666666667, 0.7556, 0.7524, 0.7590666666666667, 0.7601333333333333, 0.7578666666666667, 0.7681333333333333, 0.7584, 0.7549333333333333, 0.7490666666666667, 0.7349333333333333, 0.7508, 0.7604, 0.7564, 0.746, 0.7576, 0.7582666666666666, 0.7525333333333334, 0.7492, 0.7622666666666666, 0.7690666666666667, 0.7628, 0.76, 0.7654666666666666, 0.7565333333333333, 0.7582666666666666, 0.7569333333333333, 0.7462666666666666, 0.7501333333333333, 0.7518666666666667, 0.7468, 0.7476, 0.754, 0.7578666666666667, 0.7618666666666667, 0.7478666666666667, 0.7537333333333334, 0.7610666666666667, 0.7514666666666666, 0.7670666666666667, 0.7712, 0.7772, 0.7657333333333334, 0.7690666666666667, 0.7665333333333333, 0.7628, 0.752, 0.7542666666666666, 0.7549333333333333, 0.7632, 0.7766666666666666, 0.7774666666666666, 0.77, 0.7765333333333333, 0.772, 0.7756, 0.7744, 0.7678666666666667, 0.7814666666666666, 0.7728, 0.7726666666666666, 0.7746666666666666, 0.7813333333333333, 0.7841333333333333, 0.7777333333333334, 0.7830666666666667, 0.7793333333333333, 0.7833333333333333, 0.7810666666666667, 0.7856, 0.786, 0.7836, 0.7818666666666667, 0.7792, 0.7841333333333333, 0.7822666666666667, 0.7848, 0.7858666666666667, 0.776, 0.7838666666666667, 0.7818666666666667, 0.7896, 0.7872, 0.7817333333333333, 0.7858666666666667, 0.7794666666666666, 0.7792, 0.7824, 0.7886666666666666, 0.7878666666666667, 0.7832, 0.7802666666666667, 0.7808, 0.7858666666666667, 0.7813333333333333, 0.7849333333333334, 0.7814666666666666, 0.7884, 0.7897333333333333, 0.7850666666666667, 0.7833333333333333, 0.7906666666666666, 0.7906666666666666, 0.7933333333333333, 0.7926666666666666, 0.7929333333333334, 0.7924, 0.7922666666666667, 0.7928, 0.7925333333333333, 0.7928, 0.7926666666666666, 0.7928, 0.7924, 0.7922666666666667, 0.7917333333333333, 0.7921333333333334, 0.7922666666666667, 0.792, 0.792, 0.792, 0.792, 0.792, 0.7917333333333333, 0.792, 0.7921333333333334, 0.7922666666666667, 0.7924, 0.7925333333333333, 0.7925333333333333, 0.7925333333333333, 0.7925333333333333, 0.7926666666666666, 0.7929333333333334, 0.7930666666666667, 0.7930666666666667, 0.7930666666666667, 0.7930666666666667, 0.7933333333333333, 0.7934666666666667, 0.7933333333333333, 0.7933333333333333, 0.7934666666666667, 0.7934666666666667, 0.7936, 0.7936, 0.7936, 0.7938666666666667, 0.7937333333333333, 0.7937333333333333, 0.7934666666666667, 0.7934666666666667, 0.7932, 0.7933333333333333, 0.7933333333333333, 0.7930666666666667, 0.7930666666666667, 0.7930666666666667, 0.7929333333333334, 0.7930666666666667, 0.7930666666666667, 0.7932, 0.7930666666666667, 0.7930666666666667, 0.7929333333333334, 0.7929333333333334, 0.7929333333333334, 0.7926666666666666, 0.7926666666666666, 0.7926666666666666, 0.7926666666666666, 0.7928, 0.7926666666666666, 0.7928, 0.7928, 0.7928, 0.7926666666666666, 0.7926666666666666, 0.7926666666666666, 0.7924, 0.7924, 0.7922666666666667, 0.7922666666666667, 0.7922666666666667, 0.7922666666666667, 0.7922666666666667, 0.7924, 0.7922666666666667, 0.792, 0.7918666666666667, 0.7918666666666667, 0.7917333333333333, 0.7917333333333333, 0.7917333333333333, 0.7917333333333333, 0.7918666666666667, 0.7918666666666667, 0.792], "seed": 904457746, "model": "residualv3", "loss_std": [0.33441779017448425, 0.15594618022441864, 0.14547255635261536, 0.13603432476520538, 0.1320948600769043, 0.13101507723331451, 0.12919704616069794, 0.12402553111314774, 0.12305247783660889, 0.12378299981355667, 0.12038839608430862, 0.1155843660235405, 0.1146174818277359, 0.11235719919204712, 0.10656975209712982, 0.10232207179069519, 0.10443868488073349, 0.10297922044992447, 0.10470960289239883, 0.10352773219347, 0.10025399923324585, 0.09892676770687103, 0.09889192134141922, 0.09187488257884979, 0.09365956485271454, 0.0913180559873581, 0.0877794548869133, 0.09135302156209946, 0.08758286386728287, 0.08286792039871216, 0.07682785391807556, 0.07765264064073563, 0.07468673586845398, 0.07908973097801208, 0.06982454657554626, 0.07487963140010834, 0.0745496079325676, 0.07215557247400284, 0.0722828283905983, 0.0698118731379509, 0.0697692483663559, 0.06534198671579361, 0.06761566549539566, 0.06445406377315521, 0.0682612806558609, 0.06218545883893967, 0.062427569180727005, 0.06066227704286575, 0.05802500247955322, 0.06315568089485168, 0.05200100317597389, 0.0566912516951561, 0.05213633179664612, 0.05113804712891579, 0.05382724851369858, 0.05125441402196884, 0.0494222529232502, 0.043170422315597534, 0.040457189083099365, 0.042799223214387894, 0.039526429027318954, 0.03488190472126007, 0.035928528755903244, 0.03441938757896423, 0.02805020660161972, 0.028940923511981964, 0.028069237247109413, 0.025667130947113037, 0.026351723819971085, 0.02736443653702736, 0.024322621524333954, 0.022176124155521393, 0.018397022038698196, 0.01666991412639618, 0.018436314538121223, 0.017555201426148415, 0.021404283121228218, 0.024753091856837273, 0.01667051948606968, 0.016472293063998222, 0.020209049805998802, 0.020072493702173233, 0.01551078725606203, 0.014663858339190483, 0.014373473823070526, 0.014695340767502785, 0.01481340080499649, 0.013828339986503124, 0.015435869805514812, 0.018399620428681374, 0.019386643543839455, 0.017827337607741356, 0.0190985519438982, 0.018340235576033592, 0.017277518287301064, 0.013929265551269054, 0.016561049968004227, 0.01630420610308647, 0.020905815064907074, 0.01942373812198639, 0.019808225333690643, 0.01894734986126423, 0.0206289142370224, 0.01765400730073452, 0.018877176567912102, 0.022082172334194183, 0.01989653706550598, 0.015496407635509968, 0.0167158804833889, 0.01620604656636715, 0.011100199073553085, 0.012040874920785427, 0.006797949783504009, 0.005566395819187164, 0.0023281187750399113, 0.00037804796011187136, 6.54710311209783e-05, 4.872729550697841e-05, 4.148928928771056e-05, 3.6716675822390243e-05, 3.321599433547817e-05, 3.048947655770462e-05, 2.8296113669057377e-05, 2.6483518013264984e-05, 2.4951503291958943e-05, 2.363131716265343e-05, 2.248147393402178e-05, 2.146825681847986e-05, 2.05691085284343e-05, 1.976385829038918e-05, 1.9036830053664744e-05, 1.8375689251115546e-05, 1.777112811396364e-05, 1.7217014828929678e-05, 1.6705746020306833e-05, 1.623279786144849e-05, 1.5794246792211197e-05, 1.5385838196380064e-05, 1.5005648492660839e-05, 1.4649637705588248e-05, 1.4315163753053639e-05, 1.4000225746713113e-05, 1.3703512195206713e-05, 1.3423063137452118e-05, 1.3157527973817196e-05, 1.2905869880341925e-05, 1.2667992450587917e-05, 1.2441342732927296e-05, 1.2225589671288617e-05, 1.2019006135233212e-05, 1.1822171472886112e-05, 1.1634061593213119e-05, 1.1453809747763444e-05, 1.1281228580628522e-05, 1.1116533642052673e-05, 1.0958146049233619e-05, 1.080596757674357e-05, 1.065973538061371e-05, 1.0519055649638176e-05, 1.038357731886208e-05, 1.0253233085677493e-05, 1.0127479981747456e-05, 1.0005919648392592e-05, 9.888178283290472e-06, 9.774670616025105e-06, 9.664921890362166e-06, 9.558962119626813e-06, 9.456368388782721e-06, 9.357158887723926e-06, 9.260736078431364e-06, 9.167264579446055e-06, 9.076719834411051e-06, 8.98852613318013e-06, 8.902746230887715e-06, 8.819412869343068e-06, 8.738348697079346e-06, 8.659569175506476e-06, 8.583388080296572e-06, 8.509254257660359e-06, 8.437067663180642e-06, 8.367029295186512e-06, 8.298811735585332e-06, 8.231751053244807e-06, 8.166881343640853e-06, 8.10378696769476e-06, 8.042342415137682e-06, 7.98210567154456e-06, 7.9234914664994e-06, 7.866038686188404e-06, 7.810078386683017e-06, 7.755381375318393e-06, 7.701747563260142e-06, 7.64933975005988e-06, 7.598039701406378e-06, 7.547951554442989e-06, 7.499224011553451e-06, 7.451257260981947e-06, 7.404089956253301e-06, 7.3583364610385615e-06, 7.313440164580243e-06, 7.269753496075282e-06, 7.226734851428773e-06, 7.1846575337986e-06, 7.1430426942242775e-06, 7.102487870724872e-06, 7.062815711833537e-06, 7.023764737823512e-06, 6.985387699387502e-06, 6.94806158207939e-06, 6.911309810675448e-06]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:44 2016", "state": "available"}], "summary": "39df09a86b8be374d44b985e4f3ba26b"}