{"content": {"hp_model": {"f0": 16, "f1": 32, "f2": 64, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.594624400138855, 1.3057200908660889, 1.1530673503875732, 1.038162112236023, 0.9440111517906189, 0.8664257526397705, 0.8027955293655396, 0.7498606443405151, 0.7057846188545227, 0.6666938662528992, 0.6332554221153259, 0.6024532318115234, 0.5755293965339661, 0.5489034652709961, 0.5274176001548767, 0.504755437374115, 0.4836476743221283, 0.4657023847103119, 0.4455883204936981, 0.4301776587963104, 0.41345682740211487, 0.395311564207077, 0.38546139001846313, 0.3688535988330841, 0.3587864935398102, 0.3463532626628876, 0.3357938528060913, 0.3234502971172333, 0.3139146566390991, 0.3065134584903717, 0.29815277457237244, 0.28841090202331543, 0.2799927294254303, 0.2739674746990204, 0.26737743616104126, 0.26036536693573, 0.25604134798049927, 0.24988098442554474, 0.24172654747962952, 0.2378336638212204, 0.23417045176029205, 0.22761152684688568, 0.22715461254119873, 0.2182919681072235, 0.2146729975938797, 0.21079441905021667, 0.20799122750759125, 0.20450732111930847, 0.19902074337005615, 0.19656524062156677, 0.19134074449539185, 0.18741820752620697, 0.1857948899269104, 0.18208523094654083, 0.18152037262916565, 0.17674587666988373, 0.17725224792957306, 0.1703076958656311, 0.16707506775856018, 0.16285699605941772, 0.16011890769004822, 0.15793952345848083, 0.1599314659833908, 0.15574787557125092, 0.15325912833213806, 0.14629299938678741, 0.1482768952846527, 0.14692328870296478, 0.14579695463180542, 0.14139224588871002, 0.13979844748973846, 0.14071278274059296, 0.13866126537322998, 0.13617436587810516, 0.12877090275287628, 0.13350848853588104, 0.12852728366851807, 0.12690427899360657, 0.1296994835138321, 0.12294097244739532, 0.12326386570930481, 0.12463454902172089, 0.11966709047555923, 0.11897018551826477, 0.11710416525602341, 0.11793004721403122, 0.11697858572006226, 0.11464237421751022, 0.11159202456474304, 0.11029091477394104, 0.11188703775405884, 0.10670116543769836, 0.10851118713617325, 0.10874525457620621, 0.10513817518949509, 0.10304540395736694, 0.07945884019136429, 0.0682104229927063, 0.06524334102869034, 0.06329356133937836, 0.061948638409376144, 0.059621136635541916, 0.05814802274107933, 0.05832013487815857, 0.05819420889019966, 0.05690986290574074, 0.05650630220770836, 0.05511659383773804, 0.05544150248169899, 0.055458832532167435, 0.054977692663669586, 0.053055062890052795, 0.053771622478961945, 0.05209299549460411, 0.05046587809920311, 0.051823507994413376, 0.05148075893521309, 0.05159514397382736, 0.05281444638967514, 0.051025912165641785, 0.05173313990235329, 0.0514494925737381, 0.051467712968587875, 0.05146776884794235, 0.05134929344058037, 0.052238620817661285, 0.05170073360204697, 0.051655445247888565, 0.051189690828323364, 0.05112555995583534, 0.05186449736356735, 0.050827186554670334, 0.050935015082359314, 0.05095196142792702, 0.05121999979019165, 0.05053728073835373, 0.050708476454019547, 0.05172434449195862, 0.05173230171203613, 0.05116841569542885, 0.050913434475660324, 0.05100357159972191, 0.05197318643331528, 0.0511125810444355, 0.05038919299840927, 0.05188881605863571, 0.05080672726035118, 0.051279518753290176, 0.05060841515660286, 0.051453474909067154, 0.05111448094248772, 0.05075857415795326, 0.051777828484773636, 0.0511413998901844, 0.052463263273239136, 0.051626648753881454, 0.05088169500231743, 0.05216571316123009, 0.05129491165280342, 0.05121774598956108, 0.05167268216609955], "moving_avg_accuracy_train": [0.04822318977251752, 0.09987439149593944, 0.15176521836932444, 0.2048271998431155, 0.2561632437015503, 0.30570158678825204, 0.35184798273636486, 0.3943353392015619, 0.43577500500339716, 0.47099692382868497, 0.5061467029701335, 0.5381577096699364, 0.568527394013943, 0.5970203952283203, 0.6222290772009662, 0.6467093283667592, 0.6687532883064767, 0.6878792118117926, 0.7072638714664845, 0.7243103198045628, 0.7370811936530564, 0.7523408199676602, 0.7638498930211896, 0.7774649073239525, 0.7899114435964483, 0.8016668558536362, 0.8098798335434664, 0.8186875651381322, 0.8273745588434959, 0.8360043661616656, 0.8431271986254179, 0.8474918331833264, 0.851885178242624, 0.8586289347815952, 0.8630731808440872, 0.866757034403966, 0.873540975752929, 0.8764941979623814, 0.8808050624615474, 0.8825924228751381, 0.8861935555818564, 0.8889649670560166, 0.8946487647683274, 0.8993898337510738, 0.9024503319939527, 0.9044584797423242, 0.907593400588459, 0.910730869343982, 0.9130871642156008, 0.9148848862572652, 0.9183441376589935, 0.9210507791717948, 0.922147218477299, 0.924838311880815, 0.9262071116797214, 0.9299033647974635, 0.9315608601975621, 0.932243382174299, 0.9339157028081518, 0.9332307174929605, 0.9357063541865862, 0.9367742861013532, 0.9388698090531411, 0.9413114902752264, 0.9427674611489588, 0.9416624379080478, 0.9421091487649451, 0.9432202507766012, 0.9447642553108828, 0.9457446692500788, 0.9468176319000987, 0.9475648784922963, 0.946851755930073, 0.9471609314871672, 0.948734117131363, 0.9511798448896646, 0.9521487791495445, 0.9533347150727223, 0.9534324703500108, 0.9555549553079037, 0.9567048681092931, 0.9570399558876956, 0.959175969152516, 0.9592266723480064, 0.9600117746430139, 0.9611856495215974, 0.9618515479611411, 0.9624740359471883, 0.9622833962644296, 0.9632370844273, 0.9642255400095793, 0.9648174949371928, 0.9656525958149206, 0.96658318701439, 0.9678857488558174, 0.9687255582333402, 0.971404248689768, 0.9739220269457912, 0.9762461921452689, 0.9783448802224086, 0.9802081228549296, 0.9819292551003983, 0.9834782380725013, 0.9848653473009654, 0.9861207210530117, 0.9872296310905676, 0.9882741531005586, 0.9892165480583599, 0.9900740401644379, 0.9908364824646699, 0.9915389205277267, 0.9921432129987635, 0.9926870762226967, 0.9932137555051889, 0.9936854417106223, 0.9941169347419411, 0.9945099287677469, 0.9948729239862103, 0.9951903190875893, 0.9954666740835922, 0.995720043877614, 0.995941101245805, 0.9961423780259864, 0.9963165516817211, 0.9964849337159299, 0.9966434529931465, 0.9967837951938318, 0.9968938271327819, 0.9970114570683132, 0.99712429945672, 0.9972095815646194, 0.9972840103129194, 0.9973486710375798, 0.9974184914338218, 0.9974859800880587, 0.9975536953233004, 0.9976076635885894, 0.9976562350273496, 0.9976673972389003, 0.9976960444197721, 0.9977357777754139, 0.9977436360097772, 0.99777163475999, 0.99780380908161, 0.9978118396317823, 0.9978283677221755, 0.9978525435987675, 0.9978696515900812, 0.9978920242286922, 0.9979005338593945, 0.9979128428246455, 0.9979239208933715, 0.9978966887742724, 0.9978954313551784, 0.9979012751244225, 0.9979018842191232, 0.9979163832972108, 0.9979201318722516, 0.9979328061850263, 0.997927937024857, 0.9979375056735618], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.048279691029743964, 0.09907842796969124, 0.14949920433687874, 0.2006978625929499, 0.24973590412921212, 0.296963815034062, 0.3404374083518154, 0.380779344992161, 0.41971365870454125, 0.45238328845720455, 0.4842670711043305, 0.5128770262679938, 0.5399240492624293, 0.5650171759156292, 0.5870566739678916, 0.6078993758293856, 0.6263557202494592, 0.6423265170592272, 0.658308620859705, 0.6717372773166562, 0.6816552170360448, 0.6932302885647446, 0.7019684301713724, 0.7120362002942955, 0.7214145762174262, 0.7299761553520842, 0.736348951149707, 0.7424018501800677, 0.7491241381006903, 0.7553134750436334, 0.7600924802783514, 0.7625176731390705, 0.765601755081564, 0.7704079140519468, 0.7739094089795834, 0.7764168702841854, 0.7806521540294866, 0.7830071546468693, 0.7863311803342908, 0.7877603034529701, 0.7902051527198719, 0.7919731234230354, 0.7961136546424789, 0.8000751548597069, 0.8025743753018236, 0.8041928790367016, 0.8069039151635283, 0.8092716349988321, 0.8110699044808767, 0.8118409143151685, 0.8140536425843293, 0.8157490407505952, 0.8164967376262133, 0.8181890254599775, 0.8195533931041152, 0.8226388517511585, 0.8239223886863589, 0.824364416572241, 0.8257114486537217, 0.8256358621938464, 0.8275972900934376, 0.828440870807362, 0.8302571046980113, 0.8318133254687975, 0.832570040032234, 0.8324189459970076, 0.8325118064330749, 0.8330776320962433, 0.8338513118459563, 0.8350034018115866, 0.836073815348425, 0.8369130582017602, 0.8364487031534215, 0.8372444272469648, 0.837981904467675, 0.8400189985182719, 0.8411667303964899, 0.8422526352465246, 0.8421638198581674, 0.8436952141336458, 0.8441345570839861, 0.8442552339998344, 0.8460576791145347, 0.8464215259903553, 0.846657361907886, 0.8479653055928504, 0.8481667748453123, 0.848570912261007, 0.8479326298639727, 0.8484365124912803, 0.8491840051145167, 0.8496827615119507, 0.8503503393234815, 0.8513204588173382, 0.8524966831257399, 0.8531617185499882, 0.8555739505447635, 0.8578904142064016, 0.8599162553629451, 0.8616397971365151, 0.8631278905591587, 0.8646899897280169, 0.8657551116136489, 0.8668093185433985, 0.8677713413200828, 0.8685537716176679, 0.8693423785955848, 0.8700866869521409, 0.8707087658567009, 0.871277755376076, 0.8720258975719022, 0.8724906765082361, 0.8727879367470962, 0.8730900330385011, 0.8733497126695154, 0.8735468032436783, 0.873773012885425, 0.8740498437504969, 0.8742867844978117, 0.874463410076645, 0.8746844377625046, 0.8748335050461187, 0.8749310445076213, 0.8750066229917236, 0.8751234717524158, 0.8753262918870386, 0.8754976524856088, 0.8755531912656624, 0.8755421410114607, 0.875642059063929, 0.8756099149986505, 0.8755809853398998, 0.8755793627095243, 0.8756267304671863, 0.8756693614490821, 0.8758053855827884, 0.875842358084374, 0.875912254429551, 0.8758897119214604, 0.8759060447579288, 0.8759329513420004, 0.8758717180489148, 0.8759885360312974, 0.8761414708317822, 0.8760593855897184, 0.876046544028111, 0.8760960217789143, 0.8760551025358874, 0.8761169609758227, 0.8760129126568549, 0.8760179549284435, 0.8760458775267136, 0.8759367305214067, 0.8759605685291306, 0.876018643829832, 0.8759732553504632, 0.8759812338440314, 0.8760006215194928, 0.8759936563649079, 0.8759751806945316, 0.8760317947786929], "moving_var_accuracy_train": [0.02092928428652615, 0.042846975613136155, 0.06279619927425507, 0.08185674424815409, 0.097389574414655, 0.10973704389517143, 0.11792874823665428, 0.12238245254759533, 0.125599420408746, 0.12420473045948788, 0.12290382017677262, 0.11983577910850873, 0.11615306074204933, 0.1118444147316669, 0.10637927207968227, 0.10113488914597683, 0.09539482575979402, 0.08914755173319533, 0.08361468182922996, 0.07786844625479195, 0.07154945859899997, 0.06649021849645215, 0.0610333255097702, 0.05659831048897313, 0.052332725826718815, 0.04834316070007494, 0.04411592165287028, 0.04040251471017634, 0.03704143797589195, 0.03400755634744138, 0.03106341339345739, 0.02812852236752846, 0.025489383458066096, 0.023349749382571748, 0.021192536351890355, 0.019195419710157008, 0.01769007448117683, 0.015999560725824775, 0.01456685662781383, 0.013138922880265082, 0.011941744003181147, 0.010816696096894996, 0.01002577649511573, 0.009225498461496602, 0.008387248460798927, 0.007584817531132637, 0.006914785336423152, 0.006311900194507776, 0.0057306793047551515, 0.005186697615131409, 0.0047757256359614985, 0.004364086246874722, 0.00393849723454314, 0.00360982536444685, 0.0032657053440075384, 0.0030620953936005522, 0.0027806114732526295, 0.002506742852165927, 0.0022812384736710244, 0.002057337470242172, 0.0019067627165673888, 0.001726350752081851, 0.001593236624846895, 0.0014875692270747611, 0.0013578909650337, 0.0012330915557969113, 0.0011115783555242496, 0.001011531449094581, 0.0009318338542020609, 0.0008473013722113823, 0.0007729324746252817, 0.0007006646243887118, 0.0006351750560486087, 0.0005725178561696885, 0.0005375402881926535, 0.0005376205177829263, 0.0004923079684043563, 0.0004557351676888708, 0.00041024765576812354, 0.00040976737175964895, 0.00038069132964087715, 0.0003436327510499023, 0.0003503324499523074, 0.00031532234228337313, 0.0002893375785776689, 0.0002728056607950255, 0.0002495158813016055, 0.00022805171480640293, 0.00020557363472354363, 0.00019320196125918214, 0.00018267516507651456, 0.00016756134429579624, 0.00015708175115005147, 0.00014916757585981807, 0.00014952082443052013, 0.0001409162601026442, 0.00019140307714460053, 0.00022931563554866937, 0.00025499976686397255, 0.00026914021498373325, 0.0002734712514541555, 0.0002727847921662675, 0.00026710044718042695, 0.0002577070505675982, 0.00024612001482678006, 0.00023257514658663354, 0.00021913686799216814, 0.00020521615550135483, 0.0001913121743590942, 0.0001774128212738338, 0.0001641123122383296, 0.0001509876055294631, 0.00013855092983363964, 0.00012719235644973457, 0.00011647551169232738, 0.0001065036366477842, 9.724327172187791e-05, 8.870483430733577e-05, 8.07410077300165e-05, 7.335425571135713e-05, 6.659659641292493e-05, 6.0376734011916135e-05, 5.4703671690886276e-05, 4.950633268296556e-05, 4.481087199966778e-05, 4.055594005094425e-05, 3.667760944548871e-05, 3.3118811749242033e-05, 2.9931461789915794e-05, 2.705291625251632e-05, 2.4413081968614626e-05, 2.2021630518914665e-05, 1.9857096550845725e-05, 1.7915260885343687e-05, 1.616472726286565e-05, 1.4589522714333694e-05, 1.3156783605825065e-05, 1.1862337907211595e-05, 1.067722547119077e-05, 9.616888872818848e-06, 8.66940864149201e-06, 7.803023543968598e-06, 7.029776559693025e-06, 6.3361155864691154e-06, 5.703084435446838e-06, 5.135234591850565e-06, 4.62697138974642e-06, 4.1669084010728924e-06, 3.754722375591338e-06, 3.379901862364396e-06, 3.0432752717579153e-06, 2.7400522570423904e-06, 2.4727213261337763e-06, 2.225463423445398e-06, 2.003224427851659e-06, 1.802905324033682e-06, 1.6245068010188493e-06, 1.462182587250495e-06, 1.317410072364277e-06, 1.185882443614638e-06, 1.0681182305954926e-06], "duration": 99242.412355, "accuracy_train": [0.48223189772517533, 0.5647352070067369, 0.6187826602297896, 0.6823850331072352, 0.718187638427464, 0.7515466745685677, 0.7671655462693798, 0.7767215473883352, 0.8087319972199151, 0.7879941932562754, 0.822494715243171, 0.8262567699681617, 0.8418545531100037, 0.853457406157715, 0.8491072149547803, 0.8670315888588963, 0.867148927763935, 0.8600125233596345, 0.8817258083587117, 0.8777283548472684, 0.8520190582894979, 0.8896774567990956, 0.8674315505029531, 0.9000000360488187, 0.901930270048911, 0.9074655661683279, 0.883796632751938, 0.8979571494901256, 0.9055575021917681, 0.9136726320251938, 0.9072326907991879, 0.8867735442045036, 0.8914252837763011, 0.9193227436323367, 0.9030713954065154, 0.8999117164428755, 0.9345964478935955, 0.903073197847453, 0.9196028429540422, 0.898678666597453, 0.918603749942322, 0.9139076703234589, 0.9458029441791252, 0.9420594545957919, 0.9299948161798633, 0.922531809477667, 0.9358076882036729, 0.9389680881436876, 0.9342938180601699, 0.9310643846322444, 0.9494774002745479, 0.9454105527870063, 0.9320151722268365, 0.9490581525124585, 0.9385263098698781, 0.9631696428571429, 0.9464783187984496, 0.9383860799649317, 0.9489665885128276, 0.9270658496562385, 0.9579870844292175, 0.9463856733342562, 0.9577295156192323, 0.9632866212739941, 0.9558711990125508, 0.9317172287398486, 0.946129546477021, 0.9532201688815062, 0.9586602961194168, 0.9545683947028424, 0.9564742957502769, 0.9542900978220746, 0.9404336528700628, 0.9499435115010151, 0.9628927879291252, 0.973191394714378, 0.9608691874884644, 0.9640081383813216, 0.9543122678456073, 0.9746573199289406, 0.9670540833217978, 0.9600557458933187, 0.9784000885358989, 0.9596830011074198, 0.9670776952980805, 0.9717505234288483, 0.9678446339170359, 0.9680764278216132, 0.9605676391196014, 0.9718202778931341, 0.9731216402500923, 0.9701450892857143, 0.9731685037144703, 0.9749585078096161, 0.9796088054286637, 0.9762838426310447, 0.9955124627976191, 0.99658203125, 0.9971636789405685, 0.9972330729166666, 0.9969773065476191, 0.9974194453096161, 0.9974190848214286, 0.9973493303571429, 0.9974190848214286, 0.9972098214285714, 0.9976748511904762, 0.9976981026785714, 0.99779146911914, 0.9976984631667589, 0.9978608630952381, 0.9975818452380952, 0.9975818452380952, 0.9979538690476191, 0.9979306175595238, 0.9980003720238095, 0.998046875, 0.9981398809523809, 0.998046875, 0.9979538690476191, 0.9980003720238095, 0.9979306175595238, 0.9979538690476191, 0.9978841145833334, 0.9980003720238095, 0.9980701264880952, 0.998046875, 0.9978841145833334, 0.9980701264880952, 0.9981398809523809, 0.9979771205357143, 0.9979538690476191, 0.9979306175595238, 0.998046875, 0.9980933779761905, 0.9981631324404762, 0.9980933779761905, 0.9980933779761905, 0.9977678571428571, 0.9979538690476191, 0.9980933779761905, 0.9978143601190477, 0.9980236235119048, 0.9980933779761905, 0.9978841145833334, 0.9979771205357143, 0.9980701264880952, 0.9980236235119048, 0.9980933779761905, 0.9979771205357143, 0.9980236235119048, 0.9980236235119048, 0.9976515997023809, 0.9978841145833334, 0.9979538690476191, 0.9979073660714286, 0.998046875, 0.9979538690476191, 0.998046875, 0.9978841145833334, 0.9980236235119048], "end": "2016-02-04 17:55:18.061000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0], "moving_var_accuracy_valid": [0.020978357093347864, 0.04210512645625876, 0.06077490601586229, 0.07828913887927372, 0.09210279065075598, 0.1029667917016083, 0.10967969237506421, 0.1133589698046007, 0.11566599988242701, 0.11370514226776927, 0.11148380840399486, 0.1077021933737968, 0.10351584711217085, 0.09883124744805552, 0.09331977797281107, 0.08789756416351409, 0.08217353759133408, 0.07625178098885071, 0.0709254516669689, 0.06545586582842136, 0.059795569000076064, 0.055021852628119094, 0.05020686343394453, 0.04609841704778226, 0.04228016075760405, 0.03871185041735475, 0.03520617811212307, 0.032015298580956426, 0.02922047111685054, 0.026643195031304998, 0.024184425547475654, 0.021818917036433235, 0.019722629385642045, 0.017958258923515154, 0.016272777231718025, 0.014702085768292905, 0.01339331584709254, 0.01210389851355414, 0.010992950983134468, 0.009912037420816115, 0.008974629270175335, 0.008105297826822998, 0.007449064033153371, 0.006845398985577913, 0.0062170740123847655, 0.005618942600204612, 0.0051231957921127926, 0.004661331087867932, 0.004224301937251612, 0.0038072218490076237, 0.0034705651616451544, 0.003149378019960238, 0.002839471673524497, 0.0025812990491828045, 0.002339922635879855, 0.002191610867855398, 0.0019872769846460718, 0.001790307784048542, 0.001627607464500532, 0.0014648981378667272, 0.0013530331187277095, 0.001224134462643084, 0.0011314093662886649, 0.0010400648374466364, 0.0009412119060766251, 0.0008472961801362916, 0.000762644169667942, 0.0006892611808310477, 0.0006257222859439869, 0.0005750958589497419, 0.0005278983393133882, 0.0004814474624839175, 0.0004352433467337842, 0.00039741760355781495, 0.00036257069706163074, 0.0003636613968942657, 0.00033915085338333934, 0.00031584847213496613, 0.00028433461848035105, 0.0002770076724750294, 0.00025104410527964944, 0.00022607076101385258, 0.0002327029604360298, 0.00021062412533382673, 0.00019006228002042217, 0.00018645250216572465, 0.00016817256068634034, 0.0001528252480745856, 0.00014120936303240323, 0.00012937350604808412, 0.00012146486243941165, 0.00011155719769130132, 0.00010441241913220594, 0.00010244136371023178, 0.00010464875995228454, 9.816433299660204e-05, 0.00014071766846649925, 0.00017493993668105812, 0.0001943822345368645, 0.00020167937729034387, 0.00020144123787194785, 0.00020325849840488103, 0.00019314301024566408, 0.00018383087947768578, 0.0001737771819356505, 0.00016190923827729732, 0.00015131542314014118, 0.00014116983519287858, 0.00013053569114507795, 0.00012039586368899846, 0.00011339372802667844, 0.00010399853036094744, 9.439395017131727e-05, 8.577591467771056e-05, 7.780522480681327e-05, 7.037430457594677e-05, 6.379741133652433e-05, 5.810738815358035e-05, 5.2801917597865046e-05, 4.78024951939623e-05, 4.34619248158141e-05, 3.9315721829629196e-05, 3.5469775165618227e-05, 3.197420661438927e-05, 2.889966864882785e-05, 2.6379925847021082e-05, 2.4006213355000114e-05, 2.1633353024308656e-05, 1.9471116694939106e-05, 1.7613857580326894e-05, 1.586177099068782e-05, 1.4283126218017864e-05, 1.2854837292580098e-05, 1.158954690351545e-05, 1.0446948818720528e-05, 9.568777021403285e-06, 8.624202012124445e-06, 7.805751302533999e-06, 7.029749654319748e-06, 6.329175542811712e-06, 5.70277366692815e-06, 5.1662419458742775e-06, 4.772435720358346e-06, 4.50569362711621e-06, 4.1157661470865775e-06, 3.7056736837185646e-06, 3.357138745767746e-06, 3.036494331240075e-06, 2.767283097437196e-06, 2.587989261813717e-06, 2.329419156157305e-06, 2.103494283988962e-06, 2.0003624744972477e-06, 1.8054404825576744e-06, 1.6552510992659315e-06, 1.5082670158739929e-06, 1.358013221523145e-06, 1.225594837008972e-06, 1.1034719737135878e-06, 9.961969299049224e-07, 9.254236276432376e-07], "accuracy_test": 0.8677515146683673, "start": "2016-02-03 14:21:15.648000", "learning_rate_per_epoch": [0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.008721459656953812, 0.0008721459889784455, 0.0008721459889784455, 0.0008721459889784455, 0.0008721459889784455, 0.0008721459889784455, 0.0008721459889784455, 0.0008721459889784455, 0.0008721459889784455, 0.0008721459889784455, 0.0008721459889784455, 0.0008721459889784455, 0.0008721459889784455, 0.0008721459889784455, 0.0008721459889784455, 0.0008721459889784455, 0.0008721459889784455, 0.0008721459889784455, 8.721459744265303e-05, 8.721459380467422e-06, 8.721459607841098e-07, 8.72145946573255e-08, 8.721459110461183e-09, 8.721459110461183e-10, 8.721458971683305e-11, 8.721459318628e-12, 8.721459427048217e-13, 8.721459155997674e-14, 8.721459494810853e-15, 8.721459388931734e-16, 8.721459521280632e-17, 8.721459852152877e-18, 8.721459438562571e-19, 8.721459567809542e-20, 8.721459567809542e-21, 8.72145936586115e-22, 8.72145936586115e-23, 8.721459208088969e-24, 8.721458813658517e-25, 8.721458813658517e-26, 8.721458967732912e-27, 8.721459160325907e-28, 8.72145940106715e-29, 8.72145940106715e-30, 8.721459024908957e-31, 8.721459260007827e-32, 8.721459113071034e-33, 8.721459113071034e-34, 8.721459227865404e-35, 8.721459227865404e-36, 8.721459227865404e-37, 8.721459115761527e-38, 8.721458835501834e-39, 8.72145743420337e-40, 8.721401382264796e-41, 8.721681641957661e-42, 8.716076448100362e-43, 8.688050478813866e-44, 8.407790785948902e-45, 1.401298464324817e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.48223189772517533, "accuracy_train_last": 0.9980236235119048, "batch_size_eval": 1024, "accuracy_train_std": [0.019269542795196982, 0.019481412852233614, 0.019544291635503204, 0.019712758824147643, 0.017357968995566426, 0.017960383235358635, 0.019266695664793193, 0.017825161309665476, 0.017842891265284655, 0.01888373470319688, 0.018586604219533162, 0.021437570142605483, 0.020481337717350025, 0.019136893840807384, 0.02072330100641129, 0.020178401917674525, 0.019855709530050687, 0.020038874258426043, 0.020371390574265857, 0.01859538619720798, 0.01811079118627216, 0.018902170517548056, 0.01786199726788466, 0.017159097510956258, 0.01749381284628095, 0.016120320274005014, 0.019135596213544416, 0.016650649653434737, 0.016747573620378684, 0.015806568667289165, 0.015330285698272439, 0.01765026160408332, 0.016559470206747175, 0.015340804720076165, 0.015680339391348237, 0.016133611222127118, 0.013606645506490742, 0.015124640107009615, 0.013811723157592945, 0.015611371180615745, 0.012878769098089504, 0.013162543908675927, 0.011053270825202632, 0.012356458914767655, 0.011861256641266828, 0.013232054313732694, 0.011773406406094037, 0.011361567411614282, 0.012542496891738397, 0.01398440357276757, 0.010484383767997577, 0.010924307966407972, 0.013464348763999323, 0.010408650518743576, 0.013341080667408852, 0.010068511860048934, 0.009761866396923854, 0.012859061987577277, 0.010995130406213088, 0.012807688618265254, 0.01015688428725521, 0.01153169639936539, 0.009385032360691525, 0.008695824753823196, 0.00955318801547783, 0.011386265928343355, 0.009966115802258117, 0.009420408994599661, 0.009933448550138032, 0.010731766479088608, 0.010924408912419154, 0.009725560877984483, 0.011014921988898495, 0.008765946272688466, 0.00891073709090169, 0.007744182286491811, 0.008991811539522247, 0.008237936384302865, 0.009844230950534107, 0.006471005563012682, 0.00851789428267089, 0.008841116147934914, 0.006255827162497272, 0.009424684575032514, 0.007742803663660369, 0.007365059614462823, 0.008275276743612179, 0.009476222864288671, 0.008222027422854673, 0.008029321318172612, 0.007673598762534172, 0.007549826570943776, 0.00771883522877387, 0.007388845892851863, 0.0065526966950748685, 0.006913986789585576, 0.002512990026213568, 0.002165386506453462, 0.0016482645435124618, 0.0016495451916422941, 0.002030748285053762, 0.001442688240052301, 0.001634399346241627, 0.0016546174548602301, 0.001702447219864721, 0.0019481361668204438, 0.0014287845513148453, 0.0015340696803577648, 0.0015078282469580186, 0.0015634745582164377, 0.0015402246244121272, 0.0015694323875266188, 0.001368483907139338, 0.001426512430262088, 0.0014006971244260634, 0.0014287845513148453, 0.0015367105321668813, 0.0013103611238472258, 0.0013810679320049757, 0.001426512430262088, 0.0012777745223245987, 0.0014168152656791409, 0.0013779326999654997, 0.0014282168599271034, 0.0014287845513148453, 0.0013220674270674757, 0.0013974125001224628, 0.001396057761558601, 0.0014372730153018594, 0.0013943140206998652, 0.001482453323037117, 0.0013779326999654997, 0.0014168152656791409, 0.0013810679320049757, 0.0013300176382138292, 0.001282209419546339, 0.0014287845513148453, 0.0013469818041257663, 0.0013525892459308952, 0.001426512430262088, 0.0014756911879299137, 0.0013613542753259345, 0.0014213868783089587, 0.0014445893384495041, 0.0012947968556850683, 0.0013875119743150758, 0.0013220674270674757, 0.0014213868783089587, 0.0013966385247099337, 0.0013204306979632333, 0.0014372730153018594, 0.0014372730153018594, 0.0013655177511184917, 0.0013294077714771956, 0.001426512430262088, 0.0014227174982122726, 0.0014135680946401287, 0.0014734913986007125, 0.0013810679320049757, 0.001490454846590155, 0.0014372730153018594], "accuracy_test_std": 0.007264521066333665, "error_valid": [0.5172030897025602, 0.4437329395707832, 0.39671380835843373, 0.3385142131024097, 0.3089217220444277, 0.2779849868222892, 0.26830025178840367, 0.2561432252447289, 0.2298775178840362, 0.2535900437688253, 0.2287788850715362, 0.2296333772590362, 0.21665274378765065, 0.2091446842055723, 0.21458784356174698, 0.20451630741716864, 0.20753717996987953, 0.21393631165286142, 0.19785244493599397, 0.2074048145707832, 0.22908332548945776, 0.20259406767695776, 0.21938829536897586, 0.19735386859939763, 0.19418004047439763, 0.19296963243599397, 0.20629588667168675, 0.20312205854668675, 0.19037527061370485, 0.18898249246987953, 0.19689647260918675, 0.21565559111445776, 0.20664150743599397, 0.1863366552146084, 0.19457713667168675, 0.20101597797439763, 0.18123029226280118, 0.19579783979668675, 0.18375258847891573, 0.19937758847891573, 0.18779120387801207, 0.19211514024849397, 0.16662156438253017, 0.16427134318524095, 0.1749326407191265, 0.18124058734939763, 0.16869675969503017, 0.16941888648343373, 0.17274567018072284, 0.18121999717620485, 0.16603180299322284, 0.16899237575301207, 0.17677399049322284, 0.1665803840361446, 0.1681672980986446, 0.14959202042545183, 0.16452577889683728, 0.1716573324548193, 0.16216526261295183, 0.17504441594503017, 0.15474985881024095, 0.1639669027673193, 0.1533967902861446, 0.1541806875941265, 0.16061952889683728, 0.16894090032003017, 0.1666524496423193, 0.16182993693524095, 0.1591855704066265, 0.15462778849774095, 0.15429246282003017, 0.15553375611822284, 0.1677304922816265, 0.1555940559111446, 0.15538080054593373, 0.1416471550263554, 0.14850368269954817, 0.14797422110316272, 0.15863551863704817, 0.14252223738704817, 0.15191135636295183, 0.15465867375753017, 0.13772031485316272, 0.15030385212725905, 0.15122011483433728, 0.14026320124246983, 0.15002000188253017, 0.14779185099774095, 0.15781191170933728, 0.14702854386295183, 0.1440885612763554, 0.1458284309111446, 0.14364146037274095, 0.13994846573795183, 0.1369172980986446, 0.14085296263177716, 0.12271596150225905, 0.12126141283885539, 0.12185117422816272, 0.12284832690135539, 0.12347926863704817, 0.12125111775225905, 0.12465879141566272, 0.12370281908885539, 0.12357045368975905, 0.12440435570406627, 0.12356015860316272, 0.12321453783885539, 0.12369252400225905, 0.12360133894954817, 0.12124082266566272, 0.12332631306475905, 0.12453672110316272, 0.12419110033885539, 0.12431317065135539, 0.12467938158885539, 0.12419110033885539, 0.12345867846385539, 0.12358074877635539, 0.12394695971385539, 0.12332631306475905, 0.12382488940135539, 0.12419110033885539, 0.12431317065135539, 0.12382488940135539, 0.12284832690135539, 0.12296010212725905, 0.12394695971385539, 0.12455731127635539, 0.12345867846385539, 0.12467938158885539, 0.12467938158885539, 0.12443524096385539, 0.12394695971385539, 0.12394695971385539, 0.12297039721385539, 0.12382488940135539, 0.12345867846385539, 0.12431317065135539, 0.12394695971385539, 0.12382488940135539, 0.12467938158885539, 0.12296010212725905, 0.12248211596385539, 0.12467938158885539, 0.12406903002635539, 0.12345867846385539, 0.12431317065135539, 0.12332631306475905, 0.12492352221385539, 0.12393666462725905, 0.12370281908885539, 0.1250455925263554, 0.12382488940135539, 0.12345867846385539, 0.12443524096385539, 0.12394695971385539, 0.12382488940135539, 0.12406903002635539, 0.12419110033885539, 0.12345867846385539], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.09149836824206377, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.008721459888636788, "optimization": "nesterov_momentum", "nb_data_augmentation": 1, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.706916162734635e-06, "rotation_range": [0, 0], "momentum": 0.6215278786402457}, "accuracy_valid_max": 0.8787591773343373, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            print(X_train.min(), X_train.max())\n            print(X_valid.min(), X_valid.max())\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8765413215361446, "accuracy_valid_std": [0.01997038109145016, 0.019840888859602227, 0.020378067573238512, 0.012746527876261817, 0.017273506641393474, 0.017632295986542076, 0.013197940049354534, 0.016706289007636623, 0.0062971857848946825, 0.01525059487938794, 0.012575606929369952, 0.010640301505576131, 0.010161128175430785, 0.011337972781825764, 0.006405898484308086, 0.009851750086256383, 0.011935125787308373, 0.012314759469155261, 0.013381813238467164, 0.015028357652138957, 0.01391081503919114, 0.01265042496574276, 0.015573516997405999, 0.01358038127275454, 0.017884971531297717, 0.016552103720392782, 0.015513634140468377, 0.012952130803442933, 0.013935865681446528, 0.009925617675044707, 0.01289479178478375, 0.01160979214466482, 0.017745656379832185, 0.016295778185191706, 0.009137070663202015, 0.014464840939574784, 0.02008873712767745, 0.01191492333636722, 0.015113879820447214, 0.02029325844341025, 0.01920119869710708, 0.013937554071110183, 0.012195935155918687, 0.015618222721933857, 0.015009693479545649, 0.010791673892007185, 0.01360854458480824, 0.015000660654673965, 0.015231056575653643, 0.012547281032860923, 0.014870465878359383, 0.006749117034389455, 0.016999143948721346, 0.016529970008526754, 0.017452012540218786, 0.013991712036806558, 0.014954967630069128, 0.011668325237386533, 0.017603730612315384, 0.0172079382554234, 0.012406020733149453, 0.012643081408791876, 0.014340090219799753, 0.00819909296395533, 0.012622247349581292, 0.013147489442207541, 0.01184374112850331, 0.013241238537603239, 0.010855647752913346, 0.009660616576561752, 0.008378975267551023, 0.010422995612684042, 0.011785762558084792, 0.013245383212968307, 0.01001282708180356, 0.010425052402050864, 0.011956997135649322, 0.01468056444793427, 0.015089164386703127, 0.007570989420965936, 0.011785552598292956, 0.007149969973506133, 0.012305294841821003, 0.015849814699487742, 0.011541499677233238, 0.015767469184436896, 0.008710721527998375, 0.009623227726497619, 0.011498892706752539, 0.012954413859007603, 0.011144268663695018, 0.010313194645935676, 0.009464232768177235, 0.009199199490920932, 0.010048620064626975, 0.018105686628926726, 0.009358551524509137, 0.008500559361267292, 0.008202267616690085, 0.009261704563780396, 0.008221259927271046, 0.00959669947525269, 0.008210963527233037, 0.006012893940695192, 0.007995850529743897, 0.010453466445990059, 0.007721524908173376, 0.0068417135473203975, 0.008215009779685698, 0.006559782345463435, 0.00746921126382141, 0.00889663111462635, 0.00777989643266074, 0.006973330982633911, 0.00709645333002074, 0.006805721456273259, 0.006869995161479565, 0.006582530887953001, 0.006454808518496771, 0.006731400066977124, 0.00700780309533172, 0.006737758058549491, 0.006568387753020225, 0.006839837072521465, 0.007379518254019566, 0.006866976162205421, 0.006690972770095129, 0.006991997250301452, 0.007443371728948593, 0.00683136684699901, 0.00696158112444955, 0.0069957450575685925, 0.007756828224476053, 0.006819372387339428, 0.006478725704509935, 0.00675568008003925, 0.00659469749289049, 0.007054574014538193, 0.006426487676740933, 0.007193681150665618, 0.006773051071249332, 0.006482770250174695, 0.007105713370844055, 0.006714736841277118, 0.006892745275714605, 0.0064882097852291, 0.006564395959397465, 0.0073115724296133975, 0.007323882660574958, 0.006934242706156198, 0.006756763323850508, 0.006812260450350981, 0.0065726639355448944, 0.007065927869606249, 0.006918068075249405, 0.0063732299988075265, 0.006784320349084668, 0.006702279201685459, 0.006705064441626298, 0.007208689555028978, 0.006883518630437626], "accuracy_valid": [0.48279691029743976, 0.5562670604292168, 0.6032861916415663, 0.6614857868975903, 0.6910782779555723, 0.7220150131777108, 0.7316997482115963, 0.7438567747552711, 0.7701224821159638, 0.7464099562311747, 0.7712211149284638, 0.7703666227409638, 0.7833472562123494, 0.7908553157944277, 0.785412156438253, 0.7954836925828314, 0.7924628200301205, 0.7860636883471386, 0.802147555064006, 0.7925951854292168, 0.7709166745105422, 0.7974059323230422, 0.7806117046310241, 0.8026461314006024, 0.8058199595256024, 0.807030367564006, 0.7937041133283133, 0.7968779414533133, 0.8096247293862951, 0.8110175075301205, 0.8031035273908133, 0.7843444088855422, 0.793358492564006, 0.8136633447853916, 0.8054228633283133, 0.7989840220256024, 0.8187697077371988, 0.8042021602033133, 0.8162474115210843, 0.8006224115210843, 0.8122087961219879, 0.807884859751506, 0.8333784356174698, 0.835728656814759, 0.8250673592808735, 0.8187594126506024, 0.8313032403049698, 0.8305811135165663, 0.8272543298192772, 0.8187800028237951, 0.8339681970067772, 0.8310076242469879, 0.8232260095067772, 0.8334196159638554, 0.8318327019013554, 0.8504079795745482, 0.8354742211031627, 0.8283426675451807, 0.8378347373870482, 0.8249555840549698, 0.845250141189759, 0.8360330972326807, 0.8466032097138554, 0.8458193124058735, 0.8393804711031627, 0.8310590996799698, 0.8333475503576807, 0.838170063064759, 0.8408144295933735, 0.845372211502259, 0.8457075371799698, 0.8444662438817772, 0.8322695077183735, 0.8444059440888554, 0.8446191994540663, 0.8583528449736446, 0.8514963173004518, 0.8520257788968373, 0.8413644813629518, 0.8574777626129518, 0.8480886436370482, 0.8453413262424698, 0.8622796851468373, 0.849696147872741, 0.8487798851656627, 0.8597367987575302, 0.8499799981174698, 0.852208149002259, 0.8421880882906627, 0.8529714561370482, 0.8559114387236446, 0.8541715690888554, 0.856358539627259, 0.8600515342620482, 0.8630827019013554, 0.8591470373682228, 0.877284038497741, 0.8787385871611446, 0.8781488257718373, 0.8771516730986446, 0.8765207313629518, 0.878748882247741, 0.8753412085843373, 0.8762971809111446, 0.876429546310241, 0.8755956442959337, 0.8764398413968373, 0.8767854621611446, 0.876307475997741, 0.8763986610504518, 0.8787591773343373, 0.876673686935241, 0.8754632788968373, 0.8758088996611446, 0.8756868293486446, 0.8753206184111446, 0.8758088996611446, 0.8765413215361446, 0.8764192512236446, 0.8760530402861446, 0.876673686935241, 0.8761751105986446, 0.8758088996611446, 0.8756868293486446, 0.8761751105986446, 0.8771516730986446, 0.877039897872741, 0.8760530402861446, 0.8754426887236446, 0.8765413215361446, 0.8753206184111446, 0.8753206184111446, 0.8755647590361446, 0.8760530402861446, 0.8760530402861446, 0.8770296027861446, 0.8761751105986446, 0.8765413215361446, 0.8756868293486446, 0.8760530402861446, 0.8761751105986446, 0.8753206184111446, 0.877039897872741, 0.8775178840361446, 0.8753206184111446, 0.8759309699736446, 0.8765413215361446, 0.8756868293486446, 0.876673686935241, 0.8750764777861446, 0.876063335372741, 0.8762971809111446, 0.8749544074736446, 0.8761751105986446, 0.8765413215361446, 0.8755647590361446, 0.8760530402861446, 0.8761751105986446, 0.8759309699736446, 0.8758088996611446, 0.8765413215361446], "seed": 285296137, "model": "residualv3", "loss_std": [0.2352532148361206, 0.18769405782222748, 0.18497753143310547, 0.18309137225151062, 0.17862793803215027, 0.17791514098644257, 0.17147982120513916, 0.1687561273574829, 0.1642536073923111, 0.16340933740139008, 0.1588520109653473, 0.1573919802904129, 0.15289166569709778, 0.15038371086120605, 0.14806734025478363, 0.1429031640291214, 0.14221683144569397, 0.13746890425682068, 0.1310684084892273, 0.12866328656673431, 0.1279670149087906, 0.12166576832532883, 0.12167664617300034, 0.119303859770298, 0.11479060351848602, 0.11044464260339737, 0.10986602306365967, 0.10758986324071884, 0.10782712697982788, 0.10621792078018188, 0.10335196554660797, 0.10011482238769531, 0.09754971414804459, 0.09644874185323715, 0.0947306826710701, 0.09475371241569519, 0.09036095440387726, 0.0917111486196518, 0.08723168075084686, 0.08830098062753677, 0.0887412428855896, 0.08614428341388702, 0.08576691895723343, 0.08288204669952393, 0.0845208466053009, 0.08123419433832169, 0.08264182507991791, 0.08087847381830215, 0.07923052459955215, 0.07865805923938751, 0.07839296013116837, 0.07525418698787689, 0.07264526933431625, 0.07441043853759766, 0.07772227376699448, 0.0738622322678566, 0.07508658617734909, 0.07042258232831955, 0.07085245847702026, 0.07195764034986496, 0.06802172213792801, 0.0694032832980156, 0.06886106729507446, 0.06841553002595901, 0.06633927673101425, 0.06521160155534744, 0.06674175709486008, 0.06557084619998932, 0.06636554002761841, 0.06806204468011856, 0.06406264752149582, 0.06580018252134323, 0.06311295926570892, 0.06185527145862579, 0.060636524111032486, 0.06314678490161896, 0.0623064860701561, 0.059402961283922195, 0.06408245116472244, 0.05833917856216431, 0.05770665779709816, 0.059565313160419464, 0.057622067630290985, 0.05738627538084984, 0.05800830200314522, 0.058555010706186295, 0.0593406967818737, 0.0560736358165741, 0.056143805384635925, 0.05548489838838577, 0.05651483312249184, 0.05457550659775734, 0.05376369506120682, 0.05600563809275627, 0.053690530359745026, 0.052430301904678345, 0.04492523893713951, 0.038372982293367386, 0.03640438988804817, 0.03482827916741371, 0.03499044477939606, 0.033746615052223206, 0.033230312168598175, 0.03283386304974556, 0.03325694426894188, 0.03201402723789215, 0.033153798431158066, 0.03339584544301033, 0.032014355063438416, 0.03299420326948166, 0.03194108605384827, 0.03110801801085472, 0.031088892370462418, 0.030860500410199165, 0.02972540631890297, 0.03050546906888485, 0.030963115394115448, 0.0306047685444355, 0.031489159911870956, 0.028691092506051064, 0.030692333355545998, 0.029967335984110832, 0.030573153868317604, 0.02963784523308277, 0.02946895733475685, 0.03136496618390083, 0.03002109006047249, 0.030106818303465843, 0.028464537113904953, 0.030114592984318733, 0.03167775645852089, 0.03096647746860981, 0.029284022748470306, 0.029093341901898384, 0.030879413709044456, 0.029071561992168427, 0.029528141021728516, 0.030257664620876312, 0.030545683577656746, 0.030835848301649094, 0.03046315349638462, 0.029669176787137985, 0.030518578365445137, 0.029535159468650818, 0.02973649837076664, 0.03045349381864071, 0.02843751199543476, 0.02949408069252968, 0.030165070667862892, 0.03074377216398716, 0.030870815739035606, 0.029417671263217926, 0.03085711970925331, 0.03079460933804512, 0.03042292222380638, 0.03076111152768135, 0.030078651383519173, 0.031357765197753906, 0.028666095808148384, 0.03079937770962715, 0.02989746257662773]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:38 2016", "state": "available"}], "summary": "a16b07d8582eb6f7a822e03f07bee3f1"}