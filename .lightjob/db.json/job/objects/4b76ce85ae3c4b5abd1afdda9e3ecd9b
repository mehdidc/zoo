{"content": {"hp_model": {"f0": 32, "f1": 16, "f2": 32, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.02024696394533095, 0.017599698170292413, 0.026791941180003807, 0.023115462422260126, 0.025037134840627737, 0.01901795860588563, 0.01622675828082598, 0.01565622388938533, 0.01556453754105287, 0.01731205485423576, 0.013157604245293704, 0.012127083707456625, 0.013444232140071366, 0.01145877022342974, 0.012746482312760075, 0.015234722391035082, 0.015059059452911885, 0.014266380919305939, 0.014932456791148022, 0.01550194873019966, 0.013846678945428853, 0.01620657754826649, 0.014702606565955867, 0.013782841004325575, 0.014704997942500122, 0.016639151161635557, 0.01471963422183232, 0.013809596011174003, 0.01663668669422565, 0.015183715831564882, 0.0162373924521613, 0.015875059707046404, 0.012958039002753333, 0.01650835006249926, 0.015354418696212728, 0.013966181094329008, 0.015672726556572064, 0.015776052547256267, 0.015332772249892514, 0.014350740210643857, 0.015770558460409288, 0.015950692234870015, 0.016754283040981387, 0.01591154523828813, 0.01583952562386428, 0.01697326403112618, 0.015767157500402546, 0.01641772818955163, 0.016052074202620194, 0.015998478784924927, 0.01543700642273865, 0.014794888456267755, 0.015562103323372654, 0.015644119308270966, 0.01711592930323843, 0.015505980925294066, 0.0164176188366263, 0.015166400769897469, 0.01578734415284983, 0.01683971797365972, 0.016693025563086675, 0.015478394371046915, 0.015683770314321208, 0.014998629370751788, 0.01582040743902569, 0.01619358123173711, 0.016088979288440504, 0.016869010871315746, 0.016567018620774226, 0.016961836652177632, 0.015820634400025884, 0.015838644722640422, 0.015988465787312316, 0.01602215613867102, 0.01654084045785731, 0.0160899859597347, 0.016246926061672384, 0.016346651359008983, 0.016658008475889465, 0.016460143562519854, 0.016854154729895127, 0.015907999492879778, 0.01746339364323706, 0.016026519039585112, 0.015933033930275262, 0.015501482124073217, 0.01614935174037209, 0.015587693293273568, 0.01652626579500056, 0.016109921693506297, 0.015947467225226903, 0.01637351518860152, 0.016337813304243158, 0.017293632308822464, 0.016395342490130218, 0.016788043679914596, 0.017392497243352418, 0.016153103286257357, 0.01631072006663073, 0.016062852300356624, 0.01606673578567105, 0.015624092328745042, 0.01568180703155473, 0.017438860377412765, 0.016733087942893958, 0.01658859131205494, 0.01610379122138854, 0.015931276038653616, 0.016399825978586465, 0.016002787383945078, 0.016417769559022636, 0.01648536321966755, 0.016272012056054736, 0.016607109305976357, 0.01631579900543484, 0.015704064156466135, 0.015151525592228472, 0.016436987910114546, 0.016769891422141005, 0.016595213435294475, 0.015977862238043993, 0.016378295136855098, 0.016395972113780435, 0.016465405386420044, 0.016056877163638572, 0.016380794187415525, 0.016567432760906692, 0.016350006261259487, 0.016718833540244966, 0.016459009187364464, 0.01677811132274774, 0.01659122641135547, 0.016853763613785194, 0.014956033853680831, 0.016715160633856252, 0.016129086293636068, 0.016085101173457902, 0.015863964022330757, 0.01602001019200633, 0.016747330212986973, 0.01600592833954952, 0.016880655303084316, 0.016725962260077047, 0.01599789523959902, 0.01704875062945899, 0.016410616361225445, 0.016197837933901154, 0.015967538360654066, 0.016215075007895128, 0.015744875539227477, 0.017100728976513598, 0.016085101173457902, 0.016246754050151163, 0.0156747990267922, 0.015990441942550664, 0.017178325837150284, 0.016763895203690364, 0.016697429119751202, 0.016407383274195116], "moving_avg_accuracy_train": [0.04303489958241047, 0.08800589109796048, 0.13040014008570963, 0.1720186428268618, 0.21077508952724133, 0.2500121029132492, 0.2874269168986408, 0.3227996448747051, 0.35573224585188945, 0.3859808838659069, 0.4147594977045377, 0.4414248996782699, 0.46616072548079174, 0.4889089608530706, 0.5100959770797329, 0.5294481400825846, 0.5472554954922386, 0.563628598582365, 0.5785759438563267, 0.5924401419909967, 0.6052108169645622, 0.6171693821050385, 0.6281552329195439, 0.6382565204895312, 0.6476451181060451, 0.6560646650752024, 0.6639792236807314, 0.6711301200649654, 0.6776868345488712, 0.6837018819736906, 0.6892362602964859, 0.6944124051405454, 0.699140617866847, 0.7034309226014801, 0.7072783180674302, 0.7108943977593952, 0.7141814576143157, 0.7171490039325261, 0.7198059528725146, 0.7222668532363337, 0.7245374671351994, 0.7265647436025119, 0.72851477836117, 0.730232715409466, 0.7318508302195714, 0.733272184218886, 0.7345911105921247, 0.7358035767696693, 0.7369366490080309, 0.7379890021547084, 0.7389686360212327, 0.7399013876772768, 0.74066188120583, 0.7413533008279565, 0.7419244252140609, 0.7426011975782214, 0.743121937051204, 0.7436486952483077, 0.7441204524768914, 0.7445520094290454, 0.7449334352395555, 0.7452650927249669, 0.745558934164218, 0.7458467510940956, 0.7460963415404722, 0.7463535250255446, 0.7465478598787949, 0.7467134246026633, 0.7468577465077071, 0.7470039843615507, 0.7471588138692864, 0.7472492962524298, 0.74741679695203, 0.747504768563813, 0.7476117727024945, 0.7477174491201833, 0.7477824391080358, 0.747864073438742, 0.7478934025578153, 0.747863959144734, 0.7480025816872559, 0.7480389863207635, 0.7480670280956638, 0.7481109389811879, 0.7481806496638645, 0.7481829714580447, 0.7482617549347024, 0.7483000719315421, 0.7483648202120405, 0.7483323768120989, 0.7483706070676276, 0.7484376024297555, 0.748400205956852, 0.7484549047860006, 0.7485065309786816, 0.7485367185104277, 0.74851734826399, 0.7485347922743388, 0.7485388300907865, 0.748509912042256, 0.7485280636259596, 0.7485374967025017, 0.748555251017809, 0.7485549538599188, 0.7485198452344934, 0.7484719353811253, 0.7484892343333228, 0.7484793348998521, 0.7484750396585287, 0.748496678480605, 0.7484324841121495, 0.7483909491733874, 0.7484303336880347, 0.7484145904285888, 0.7484190587343825, 0.7484417174488919, 0.7484923732752928, 0.7484751124035591, 0.7484386873285317, 0.7483803281241023, 0.7483929090067825, 0.7484157854476048, 0.7483991358145737, 0.7483981741353405, 0.7484019228728308, 0.7484424991175245, 0.7485069555722819, 0.7484812249756019, 0.7484976310171706, 0.7485030958593442, 0.748512592417282, 0.7485374874587302, 0.7485202933686341, 0.7485653446542329, 0.748508198512453, 0.7485637598789079, 0.7485928387694316, 0.7486097091756648, 0.7485667277722179, 0.7485979071198577, 0.7486165958398578, 0.7485939602557337, 0.7485130622633367, 0.7485262845761319, 0.748512571971924, 0.7485909114317083, 0.7486080106205327, 0.7486419289833133, 0.7485841719526913, 0.7485531169644174, 0.7485692732047142, 0.7485582732328954, 0.7485646492999252, 0.7485913140995376, 0.7486292993608648, 0.7485982737829363, 0.7485913852485426, 0.7485921970628356, 0.7486277328302048], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 111425713, "moving_var_accuracy_train": [0.01666802323861338, 0.03320273161577708, 0.04605790957931681, 0.05704101655512276, 0.06485547434716471, 0.07222581588753207, 0.07760204904883194, 0.08110291310416654, 0.08275362765785187, 0.08271308580739421, 0.08189565475691207, 0.0801054822430074, 0.077601683721901, 0.07449885526268431, 0.07108897664571553, 0.06735063489714448, 0.06346948856760165, 0.059535246254040664, 0.05559252980528783, 0.051763220734015535, 0.04805470991413806, 0.04453630444469537, 0.04116887426329295, 0.03797031093210788, 0.034966591725740376, 0.03210793149365897, 0.02946090048557623, 0.026975028308901063, 0.024664440021422103, 0.022523623178985333, 0.02054692495186523, 0.01873336473569885, 0.017061232222395417, 0.015520769432600015, 0.01410191455618274, 0.012809407391612241, 0.011625709514859492, 0.010542395543730057, 0.009551690388384403, 0.008651025624951765, 0.007832324249756087, 0.007086080473654749, 0.006411696146329043, 0.005797088301013314, 0.005240944130760126, 0.00473503194240642, 0.0042771848491680005, 0.003862697032336408, 0.003487982003378879, 0.0031491508273488902, 0.0028428728872259743, 0.002566415829370051, 0.002314979400095788, 0.0020877840099309633, 0.0018819412565174941, 0.0016978693183617676, 0.0015305229129140905, 0.0013799678894066247, 0.0012439740944104518, 0.0011212528575959785, 0.0010104369426766895, 0.000910383218597685, 0.0008201219818607074, 0.0007388553309407539, 0.0006655304563649813, 0.000599572700833429, 0.0005399553250667763, 0.0004862064976602052, 0.00043777330720466375, 0.0003941884460732683, 0.0003549853510541325, 0.00031956049950365294, 0.0002878569579125865, 0.00025914091316164514, 0.00023332987081673526, 0.00021009739128236178, 0.0001891256654408152, 0.00017027307637228233, 0.0001532535105100847, 0.0001379359616902411, 0.00012431531140487387, 0.00011189570794045397, 0.00010071321421666465, 9.065924628780574e-05, 8.163705787253844e-05, 7.347340060183853e-05, 6.618192206740314e-05, 5.957694359088425e-05, 5.365698029024335e-05, 4.830075542901695e-05, 4.34838338580554e-05, 3.917584587916979e-05, 3.5270847756923464e-05, 3.177069063842326e-05, 2.8617608948517412e-05, 2.5764049637321965e-05, 2.31910215316133e-05, 2.0874658019925437e-05, 1.8787338953587875e-05, 1.691613134000639e-05, 1.5227483525924264e-05, 1.370553601972928e-05, 1.2337819359164608e-05, 1.1104038217973453e-05, 1.000472793638936e-05, 9.024913329198228e-06, 8.125115280002603e-06, 7.313485741049711e-06, 6.582303208826971e-06, 5.928287035531903e-06, 5.3725465844512824e-06, 4.85081828624783e-06, 4.379696717569022e-06, 3.943957697773912e-06, 3.54974161980652e-06, 3.199388213914804e-06, 2.9025435072586e-06, 2.6149705957698358e-06, 2.3654146110096024e-06, 2.1595253205833247e-06, 1.944997296006112e-06, 1.7552075503077686e-06, 1.5821816877976135e-06, 1.423971842460382e-06, 1.2817011355092857e-06, 1.168348906659313e-06, 1.0889057270323357e-06, 9.859737267786909e-07, 8.897987779003879e-07, 8.010876806101982e-07, 7.217905740631832e-07, 6.551893844552556e-07, 5.923311766177886e-07, 5.513646239629134e-07, 5.256192952495508e-07, 5.008409547056793e-07, 4.583670961019474e-07, 4.150918819500376e-07, 3.9020930313540617e-07, 3.599377382950477e-07, 3.2708737876275504e-07, 2.989899679042772e-07, 3.279913376785182e-07, 2.9676566991154265e-07, 2.687814225478436e-07, 2.971369189267789e-07, 2.700546673601618e-07, 2.5340329862760523e-07, 2.580858400411096e-07, 2.4095696670724817e-07, 2.192104869413067e-07, 1.9837843266730106e-07, 1.7890647747748355e-07, 1.6741493357511384e-07, 1.6365936092043378e-07, 1.5595670320056282e-07, 1.4078810003534115e-07, 1.2671522141382423e-07, 1.254088161350219e-07], "duration": 55303.425676, "accuracy_train": [0.4303489958241048, 0.49274481473791065, 0.5119483809754521, 0.5465851674972315, 0.5595831098306571, 0.60314522338732, 0.624160242767165, 0.6411541966592839, 0.6521256546465486, 0.6582186259920635, 0.6737670222522149, 0.6814135174418604, 0.6887831577034883, 0.6936430792035806, 0.7007791231196936, 0.7036176071082503, 0.7075216941791251, 0.7109865263935031, 0.7131020513219822, 0.717217925203027, 0.7201468917266519, 0.7247964683693245, 0.7270278902500923, 0.7291681086194168, 0.7321424966546696, 0.7318405877976191, 0.735210251130491, 0.7354881875230712, 0.7366972649040237, 0.7378373087970653, 0.7390456652016427, 0.74099770873708, 0.7416945324035622, 0.7420436652131783, 0.7419048772609819, 0.74343911498708, 0.7437649963086009, 0.7438569207964194, 0.7437184933324105, 0.7444149565107051, 0.7449729922249908, 0.7448102318083242, 0.7460650911890919, 0.7456941488441308, 0.7464138635105205, 0.7460643702127169, 0.7464614479512736, 0.746715772367571, 0.7471342991532853, 0.7474601804748062, 0.747785340819952, 0.7482961525816721, 0.7475063229628092, 0.7475760774270949, 0.7470645446889996, 0.7486921488556663, 0.7478085923080473, 0.7483895190222407, 0.7483662675341455, 0.7484360219984312, 0.7483662675341455, 0.7482500100936692, 0.7482035071174787, 0.7484371034629937, 0.7483426555578626, 0.748668176391196, 0.7482968735580473, 0.7482035071174787, 0.7481566436531008, 0.7483201250461425, 0.7485522794389073, 0.7480636377007198, 0.7489243032484312, 0.7482965130698597, 0.7485748099506275, 0.7486685368793835, 0.748367348998708, 0.7485987824150978, 0.7481573646294758, 0.7475989684270026, 0.749250184569952, 0.748366628022333, 0.7483194040697674, 0.7485061369509044, 0.7488080458079549, 0.7482038676056663, 0.7489708062246216, 0.7486449249031008, 0.7489475547365264, 0.7480403862126246, 0.7487146793673864, 0.7490405606889073, 0.7480636377007198, 0.7489471942483389, 0.7489711667128092, 0.7488084062961425, 0.7483430160460502, 0.7486917883674787, 0.748575170438815, 0.7482496496054817, 0.7486914278792912, 0.7486223943913806, 0.748715039855574, 0.7485522794389073, 0.7482038676056663, 0.7480407467008121, 0.7486449249031008, 0.7483902399986158, 0.7484363824866187, 0.7486914278792912, 0.7478547347960502, 0.7480171347245294, 0.7487847943198597, 0.7482729010935769, 0.7484592734865264, 0.7486456458794758, 0.7489482757129015, 0.7483197645579549, 0.7481108616532853, 0.7478550952842378, 0.7485061369509044, 0.7486216734150055, 0.7482492891172942, 0.7483895190222407, 0.7484356615102437, 0.7488076853197674, 0.7490870636650978, 0.7482496496054817, 0.7486452853912883, 0.7485522794389073, 0.7485980614387228, 0.7487615428317644, 0.7483655465577703, 0.7489708062246216, 0.7479938832364341, 0.7490638121770026, 0.7488545487841455, 0.7487615428317644, 0.748179895141196, 0.7488785212486158, 0.7487847943198597, 0.7483902399986158, 0.7477849803317644, 0.7486452853912883, 0.7483891585340532, 0.7492959665697674, 0.748761903319952, 0.7489471942483389, 0.7480643586770949, 0.748273622069952, 0.7487146793673864, 0.7484592734865264, 0.748622033903193, 0.7488312972960502, 0.7489711667128092, 0.7483190435815799, 0.7485293884389996, 0.7485995033914729, 0.7489475547365264], "end": "2016-01-30 02:08:57.351000", "learning_rate_per_epoch": [0.0023348454851657152, 0.0021085350308567286, 0.0019041603663936257, 0.0017195951659232378, 0.0015529193915426731, 0.0014023990370333195, 0.0012664681999012828, 0.0011437127832323313, 0.0010328557109460235, 0.0009327437728643417, 0.0008423354011029005, 0.00076069007627666, 0.0006869584321975708, 0.0006203733501024544, 0.0005602422170341015, 0.000505939417053014, 0.0004569000448100269, 0.00041261393926106393, 0.00037262035766616464, 0.00033650326076895, 0.0003038868890143931, 0.00027443194994702935, 0.00024783198023214936, 0.00022381028975360096, 0.00020211694936733693, 0.00018252628797199577, 0.0001648345059948042, 0.00014885753626003861, 0.0001344291667919606, 0.00012139930186094716, 0.00010963239037664607, 9.900601435219869e-05, 8.940962288761511e-05, 8.074338256847113e-05, 7.291714427992702e-05, 6.584948278032243e-05, 5.946686724200845e-05, 5.370290091377683e-05, 4.8497622628929093e-05, 4.379687743494287e-05, 3.9551763620693237e-05, 3.5718116123462096e-05, 3.225605541956611e-05, 2.9129561880836263e-05, 2.6306110157747753e-05, 2.3756329028401524e-05, 2.1453690351336263e-05, 1.937424167408608e-05, 1.7496347936685197e-05, 1.5800473192939535e-05, 1.4268975064624101e-05, 1.2885921023553237e-05, 1.1636922863544896e-05, 1.0508986633794848e-05, 9.49037894315552e-06, 8.570501449867152e-06, 7.739785360172391e-06, 6.989588655414991e-06, 6.312106506811688e-06, 5.7002907851710916e-06, 5.147776846570196e-06, 4.648816684493795e-06, 4.198219357931521e-06, 3.7912971038167598e-06, 3.42381667906011e-06, 3.091955250056344e-06, 2.792260147543857e-06, 2.521613851058646e-06, 2.277200565004023e-06, 2.0564775695675053e-06, 1.8571486180007923e-06, 1.6771401760706794e-06, 1.5145794804993784e-06, 1.3677753258889425e-06, 1.2352005569482571e-06, 1.115475811275246e-06, 1.0073557632495067e-06, 9.097154247683648e-07, 8.215391176236153e-07, 7.419095027216827e-07, 6.699982009195082e-07, 6.050570391380461e-07, 5.464104901875544e-07, 4.934483968099812e-07, 4.456197473245993e-07, 4.0242701970782946e-07, 3.6342083831186756e-07, 3.281954263911757e-07, 2.963843144243583e-07, 2.676565600268077e-07, 2.417133089238632e-07, 2.1828468277362845e-07, 1.9712692278517352e-07, 1.780199312406694e-07, 1.6076492670435982e-07, 1.451824118703371e-07, 1.3111026930801017e-07, 1.1840209879210306e-07, 1.0692569674120023e-07, 9.65616706594119e-08, 8.720220279201385e-08, 7.874992746792486e-08, 7.111690791816727e-08, 6.422373388659253e-08, 5.799869740030772e-08, 5.2377039594375674e-08, 4.73002721435023e-08, 4.271558395885222e-08, 3.857527630657387e-08, 3.4836276086025464e-08, 3.14596881878515e-08, 2.841038515555283e-08, 2.5656641255977775e-08, 2.316981095873416e-08, 2.0924023402812963e-08, 1.8895914166705552e-08, 1.706438368387353e-08, 1.541037875085749e-08, 1.3916691798954162e-08, 1.2567784146710892e-08, 1.1349622575096419e-08, 1.024953366624004e-08, 9.256073241203921e-09, 8.358906455896431e-09, 7.54870033148336e-09, 6.817024722494125e-09, 6.156268828050315e-09, 5.559558147183452e-09, 5.020684756829041e-09, 4.534042918891146e-09, 4.0945700163774745e-09, 3.6976941508726213e-09, 3.339286402948005e-09, 3.015618199597725e-09, 2.7233224564326974e-09, 2.4593580505438695e-09, 2.220978956302133e-09, 2.0057053795596858e-09, 1.811297667408951e-09, 1.6357334375882715e-09, 1.4771861511775342e-09, 1.3340064608513558e-09, 1.2047047803775968e-09, 1.0879359635396213e-09, 9.824852043038845e-10, 8.872555468109056e-10, 8.012562280335089e-10, 7.235925747473004e-10, 6.534566776572603e-10, 5.901188426804538e-10, 5.329202079629169e-10, 4.812656939634508e-10, 4.346178972269854e-10, 3.9249153926945723e-10, 3.544483873074711e-10, 3.2009264683274807e-10, 2.8906690929808576e-10, 2.610484328702256e-10, 2.3574570073847667e-10], "accuracy_valid": [0.42952277861445787, 0.4837631777108434, 0.4906903002635542, 0.5301925475338856, 0.5329178040286144, 0.5836314006024097, 0.6058187829442772, 0.6186773461031627, 0.6290739128388554, 0.6317594597138554, 0.6442209266754518, 0.6524614081325302, 0.6626035391566265, 0.6640992681664157, 0.6703351491905121, 0.6718102880271084, 0.6731427663780121, 0.6776696630271084, 0.6787888860128012, 0.682837796498494, 0.6858998493975903, 0.6867131612387049, 0.6872117375753012, 0.6915253788591867, 0.6900296498493976, 0.6910062123493976, 0.6950963031814759, 0.6954316288591867, 0.6940785603350903, 0.6969170627823795, 0.6982598362198795, 0.697608304310994, 0.7004776920180723, 0.7010777484939759, 0.6998570453689759, 0.7029396884412651, 0.7004571018448795, 0.7026852527296686, 0.7010777484939759, 0.7025528873305723, 0.7019322406814759, 0.7030411685805723, 0.7053502094314759, 0.7041295063064759, 0.7042515766189759, 0.7041398013930723, 0.7060929263930723, 0.7032750141189759, 0.7042618717055723, 0.7064591373305723, 0.7049942935805723, 0.7047501529555723, 0.7057267154555723, 0.7049942935805723, 0.7042618717055723, 0.7067032779555723, 0.7054722797439759, 0.7057267154555723, 0.7063370670180723, 0.7070694888930723, 0.7057267154555723, 0.7056046451430723, 0.7060929263930723, 0.7058487857680723, 0.7057267154555723, 0.7069474185805723, 0.7060929263930723, 0.7058487857680723, 0.7059708560805723, 0.7060929263930723, 0.7071915592055723, 0.7059708560805723, 0.7059708560805723, 0.7060929263930723, 0.7055943500564759, 0.7062047016189759, 0.7059502659073795, 0.7057267154555723, 0.7056046451430723, 0.7063370670180723, 0.7060826313064759, 0.7052384342055723, 0.7063267719314759, 0.7065915027296686, 0.7057267154555723, 0.7056046451430723, 0.7069474185805723, 0.7062149967055723, 0.7064591373305723, 0.7058487857680723, 0.7053605045180723, 0.7067032779555723, 0.7065812076430723, 0.7063370670180723, 0.7067032779555723, 0.7062149967055723, 0.7057267154555723, 0.7063370670180723, 0.7054825748305723, 0.7059708560805723, 0.7060929263930723, 0.7069474185805723, 0.7067032779555723, 0.7058487857680723, 0.7063370670180723, 0.7059708560805723, 0.7060929263930723, 0.7064591373305723, 0.7073136295180723, 0.7062149967055723, 0.7058487857680723, 0.7056046451430723, 0.7065812076430723, 0.7057267154555723, 0.7058487857680723, 0.7063370670180723, 0.7069474185805723, 0.7059708560805723, 0.7065812076430723, 0.7062149967055723, 0.7057267154555723, 0.7062149967055723, 0.7058487857680723, 0.7062149967055723, 0.7063370670180723, 0.7067032779555723, 0.7058384906814759, 0.7068253482680723, 0.7063370670180723, 0.7068253482680723, 0.7060929263930723, 0.7065812076430723, 0.7063370670180723, 0.7059708560805723, 0.7056046451430723, 0.7063267719314759, 0.7059708560805723, 0.7065812076430723, 0.7052384342055723, 0.7063370670180723, 0.7064591373305723, 0.7059708560805723, 0.7063370670180723, 0.7067032779555723, 0.7062149967055723, 0.7065812076430723, 0.7067032779555723, 0.7063370670180723, 0.7071915592055723, 0.7057267154555723, 0.7065812076430723, 0.7059708560805723, 0.7062149967055723, 0.7064591373305723, 0.7067032779555723, 0.7060929263930723, 0.7060929263930723, 0.7063370670180723, 0.7062149967055723], "accuracy_test": 0.7105090082908163, "start": "2016-01-29 10:47:13.926000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0], "accuracy_train_last": 0.7489475547365264, "batch_size_eval": 1024, "accuracy_train_std": [0.01684698044970966, 0.0181122715448273, 0.01662800770301314, 0.015935343705674745, 0.015759492423551038, 0.015106143382200288, 0.015227180504316746, 0.015176430475033335, 0.014523193498352266, 0.01464463967470225, 0.015030438563204688, 0.01527699322306574, 0.013895426037345471, 0.013989873689133783, 0.013209001532640317, 0.014452809196743173, 0.01348489079362569, 0.01317760671222462, 0.01202276967165123, 0.012624057123071086, 0.012730570878160673, 0.012781160995151034, 0.013192756672740051, 0.012833170446663645, 0.012700625827666701, 0.013188431796309932, 0.014317037012761763, 0.013944441836077397, 0.0131974275161212, 0.014019438134430904, 0.013422415592487217, 0.014311645634420488, 0.014554067826702788, 0.014028628707711693, 0.013881052970145417, 0.014395766807656469, 0.014730169497609942, 0.014785443041784745, 0.014182279414205828, 0.014456382222042622, 0.014613230261530894, 0.01420809739693985, 0.014777460644950922, 0.01436322001873648, 0.014856018667011691, 0.014632152751285448, 0.014341132423839009, 0.014393681140518097, 0.014454567840143366, 0.014791135363194315, 0.01479893928106379, 0.014471535326675599, 0.014769868807454957, 0.014539081979459498, 0.014672208924158641, 0.014249955739948625, 0.014257790821510845, 0.014694034152825053, 0.014381401668507597, 0.01468669319395058, 0.014398758892366356, 0.014776747274138477, 0.014392774706996845, 0.014098137395244884, 0.014271578118312535, 0.014543326680220065, 0.013890550986593359, 0.014436880879247304, 0.014811329195325153, 0.014632854613693227, 0.014713276774231234, 0.014398172315501723, 0.014689602258063712, 0.014532768279388883, 0.01485870770757428, 0.014552850950579293, 0.014491424106580648, 0.014435831572574385, 0.01471313193853103, 0.014630800212820814, 0.01425678139380293, 0.014752460019515993, 0.014979212465450807, 0.014500355411278665, 0.014664376769822489, 0.014386538243067135, 0.014725351353722426, 0.014618270182798291, 0.014423784410416956, 0.014510500986964712, 0.015017538183932345, 0.014622533319144966, 0.01468393532056105, 0.014564624678966211, 0.014456179666321775, 0.014483706915745964, 0.014810480235045097, 0.014338847618848172, 0.014575071368743902, 0.014860855803671096, 0.01455402060951699, 0.014464060226576378, 0.01453986239855897, 0.014212450402316078, 0.01421825228270043, 0.014986406152598048, 0.01439601223380544, 0.014198971556673141, 0.014204194809791288, 0.014586747074610279, 0.014207355761213968, 0.014539392634246164, 0.014305820641862187, 0.014684524823282745, 0.014522026089170848, 0.014355171174450818, 0.014346455322286315, 0.014744086522200514, 0.014391986457529985, 0.014350559429083382, 0.014762625601407581, 0.014788300239382273, 0.014554913256827296, 0.015082936407388387, 0.014574786699504235, 0.014564785241533306, 0.014572628712015744, 0.014643831239767044, 0.014512422028196194, 0.014759502204197559, 0.01464467119395232, 0.01416584706264618, 0.014544247934566509, 0.014514124016463317, 0.01482466608714409, 0.01464808722300899, 0.014700554978358697, 0.014637271902420855, 0.014886832233033614, 0.014123395719301475, 0.01439443071981476, 0.01421655151423013, 0.014096329024694038, 0.014794443907539355, 0.014747855919660201, 0.014860930677735175, 0.014445265342251844, 0.01462374741375206, 0.014273299657230962, 0.01471102778985101, 0.014517823794663997, 0.0145782067481311, 0.014746943625416032, 0.01436986636869057, 0.014849782775085368, 0.014715911820289493, 0.014292186383857482, 0.014164341356145728, 0.014475641187416384], "accuracy_test_std": 0.013798627105164676, "error_valid": [0.5704772213855421, 0.5162368222891567, 0.5093096997364458, 0.46980745246611444, 0.46708219597138556, 0.4163685993975903, 0.39418121705572284, 0.3813226538968373, 0.3709260871611446, 0.3682405402861446, 0.35577907332454817, 0.3475385918674698, 0.3373964608433735, 0.33590073183358427, 0.32966485080948793, 0.3281897119728916, 0.32685723362198793, 0.3223303369728916, 0.3212111139871988, 0.31716220350150603, 0.3141001506024097, 0.31328683876129515, 0.3127882624246988, 0.30847462114081325, 0.30997035015060237, 0.30899378765060237, 0.30490369681852414, 0.30456837114081325, 0.3059214396649097, 0.3030829372176205, 0.3017401637801205, 0.30239169568900603, 0.2995223079819277, 0.29892225150602414, 0.30014295463102414, 0.2970603115587349, 0.2995428981551205, 0.29731474727033136, 0.29892225150602414, 0.2974471126694277, 0.29806775931852414, 0.2969588314194277, 0.29464979056852414, 0.29587049369352414, 0.29574842338102414, 0.2958601986069277, 0.2939070736069277, 0.29672498588102414, 0.2957381282944277, 0.2935408626694277, 0.2950057064194277, 0.2952498470444277, 0.2942732845444277, 0.2950057064194277, 0.2957381282944277, 0.2932967220444277, 0.29452772025602414, 0.2942732845444277, 0.2936629329819277, 0.2929305111069277, 0.2942732845444277, 0.2943953548569277, 0.2939070736069277, 0.2941512142319277, 0.2942732845444277, 0.2930525814194277, 0.2939070736069277, 0.2941512142319277, 0.2940291439194277, 0.2939070736069277, 0.2928084407944277, 0.2940291439194277, 0.2940291439194277, 0.2939070736069277, 0.29440564994352414, 0.29379529838102414, 0.2940497340926205, 0.2942732845444277, 0.2943953548569277, 0.2936629329819277, 0.29391736869352414, 0.2947615657944277, 0.29367322806852414, 0.29340849727033136, 0.2942732845444277, 0.2943953548569277, 0.2930525814194277, 0.2937850032944277, 0.2935408626694277, 0.2941512142319277, 0.2946394954819277, 0.2932967220444277, 0.2934187923569277, 0.2936629329819277, 0.2932967220444277, 0.2937850032944277, 0.2942732845444277, 0.2936629329819277, 0.2945174251694277, 0.2940291439194277, 0.2939070736069277, 0.2930525814194277, 0.2932967220444277, 0.2941512142319277, 0.2936629329819277, 0.2940291439194277, 0.2939070736069277, 0.2935408626694277, 0.2926863704819277, 0.2937850032944277, 0.2941512142319277, 0.2943953548569277, 0.2934187923569277, 0.2942732845444277, 0.2941512142319277, 0.2936629329819277, 0.2930525814194277, 0.2940291439194277, 0.2934187923569277, 0.2937850032944277, 0.2942732845444277, 0.2937850032944277, 0.2941512142319277, 0.2937850032944277, 0.2936629329819277, 0.2932967220444277, 0.29416150931852414, 0.2931746517319277, 0.2936629329819277, 0.2931746517319277, 0.2939070736069277, 0.2934187923569277, 0.2936629329819277, 0.2940291439194277, 0.2943953548569277, 0.29367322806852414, 0.2940291439194277, 0.2934187923569277, 0.2947615657944277, 0.2936629329819277, 0.2935408626694277, 0.2940291439194277, 0.2936629329819277, 0.2932967220444277, 0.2937850032944277, 0.2934187923569277, 0.2932967220444277, 0.2936629329819277, 0.2928084407944277, 0.2942732845444277, 0.2934187923569277, 0.2940291439194277, 0.2937850032944277, 0.2935408626694277, 0.2932967220444277, 0.2939070736069277, 0.2939070736069277, 0.2936629329819277, 0.2937850032944277], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.7403184074779152, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0025854457406224215, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "l2_decay": 4.600571494271316e-08, "optimization": "nesterov_momentum", "nb_data_augmentation": 1, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.09692734000610052}, "accuracy_valid_max": 0.7073136295180723, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop', 'santa_sss'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.7062149967055723, "loss_train": [1.731384038925171, 1.5034668445587158, 1.4009428024291992, 1.324953317642212, 1.2634100914001465, 1.2105793952941895, 1.1677134037017822, 1.1316771507263184, 1.0974688529968262, 1.0696778297424316, 1.044896125793457, 1.0221922397613525, 1.0037212371826172, 0.9881054162979126, 0.9729154706001282, 0.9615166187286377, 0.9477559328079224, 0.9350232481956482, 0.9241876602172852, 0.9194677472114563, 0.9118198156356812, 0.9045147895812988, 0.8978322148323059, 0.8902429938316345, 0.8848305940628052, 0.8802787661552429, 0.8766748309135437, 0.8705165982246399, 0.8687560558319092, 0.8666450381278992, 0.8636656403541565, 0.8605744242668152, 0.8572705388069153, 0.8572874069213867, 0.8548861145973206, 0.8517717719078064, 0.8514330983161926, 0.8477869629859924, 0.846541166305542, 0.8480398058891296, 0.8445833921432495, 0.844841480255127, 0.8456946611404419, 0.8459358215332031, 0.841355562210083, 0.8400906324386597, 0.841468095779419, 0.8424553275108337, 0.837946355342865, 0.83926922082901, 0.8375715613365173, 0.8405914306640625, 0.8382909893989563, 0.8368817567825317, 0.8369637131690979, 0.8395575881004333, 0.8363444805145264, 0.8396981954574585, 0.8387978076934814, 0.838378369808197, 0.8375455141067505, 0.8367446064949036, 0.8375584483146667, 0.8352506160736084, 0.8363626599311829, 0.8371748924255371, 0.8351535201072693, 0.8354560136795044, 0.8349476456642151, 0.8350699543952942, 0.83749920129776, 0.83688884973526, 0.8357982039451599, 0.8355352878570557, 0.8354622721672058, 0.8356831073760986, 0.8359946012496948, 0.8365699052810669, 0.8344372510910034, 0.8376980423927307, 0.8350077271461487, 0.8356156945228577, 0.8351904153823853, 0.8354235887527466, 0.8341881036758423, 0.8354988098144531, 0.835168182849884, 0.8371282815933228, 0.8341242671012878, 0.8365216255187988, 0.836587131023407, 0.8355007767677307, 0.8356450796127319, 0.8368183970451355, 0.8371269702911377, 0.8347878456115723, 0.8404067158699036, 0.8368253111839294, 0.8361651301383972, 0.835544764995575, 0.8376442790031433, 0.8342278003692627, 0.833379328250885, 0.8360270857810974, 0.8366460800170898, 0.8352904319763184, 0.8340018391609192, 0.8372260928153992, 0.8366893529891968, 0.8344843983650208, 0.8358442783355713, 0.8356651067733765, 0.8352871537208557, 0.8364640474319458, 0.8369269371032715, 0.8378528356552124, 0.836625874042511, 0.8358972072601318, 0.8341905474662781, 0.8368157744407654, 0.8348786234855652, 0.8353465795516968, 0.836340069770813, 0.8361908197402954, 0.8354852199554443, 0.8368927836418152, 0.8358291983604431, 0.8356086611747742, 0.837200403213501, 0.8351269960403442, 0.8355401754379272, 0.8360733389854431, 0.8340579867362976, 0.8352867364883423, 0.8337684869766235, 0.836055338382721, 0.8359639048576355, 0.8361862301826477, 0.836428701877594, 0.8352797627449036, 0.838215708732605, 0.8358607888221741, 0.835892915725708, 0.8355663418769836, 0.8375983834266663, 0.8360425233840942, 0.8348227143287659, 0.8335047364234924, 0.836568295955658, 0.837157666683197, 0.8344860672950745, 0.8352275490760803, 0.8363281488418579, 0.8364061117172241, 0.8348569273948669, 0.8363161087036133, 0.8354839086532593, 0.8365561366081238, 0.8352671265602112], "accuracy_train_first": 0.4303489958241048, "model": "residualv3", "loss_std": [0.21237029135227203, 0.11970715969800949, 0.12168816477060318, 0.12169750034809113, 0.12419730424880981, 0.12393040955066681, 0.1216660812497139, 0.12293536961078644, 0.12296216189861298, 0.12184060364961624, 0.12005981057882309, 0.1207832470536232, 0.12383468449115753, 0.12342552095651627, 0.12008614838123322, 0.12177103012800217, 0.12188737094402313, 0.12048549205064774, 0.12031753361225128, 0.12003371119499207, 0.12037952989339828, 0.12138761579990387, 0.1215219795703888, 0.1192939504981041, 0.12112293392419815, 0.11912256479263306, 0.11853837221860886, 0.11992702633142471, 0.11662440747022629, 0.11937536299228668, 0.11685846745967865, 0.11816868185997009, 0.11703628301620483, 0.11911091208457947, 0.11976679414510727, 0.11923825740814209, 0.11791855841875076, 0.11915391683578491, 0.11763051152229309, 0.11861421912908554, 0.11485587060451508, 0.11867733299732208, 0.11984054744243622, 0.11929085105657578, 0.12096709758043289, 0.11663877964019775, 0.11486994475126266, 0.11701864004135132, 0.11763228476047516, 0.11746169626712799, 0.11490883678197861, 0.11785785853862762, 0.11953990906476974, 0.11769270151853561, 0.11503689736127853, 0.11896941810846329, 0.11888313293457031, 0.11686016619205475, 0.1177382692694664, 0.11766763031482697, 0.11459246277809143, 0.11714906245470047, 0.11900894343852997, 0.11770380288362503, 0.11616310477256775, 0.11656719446182251, 0.11665916442871094, 0.1195874884724617, 0.11633982509374619, 0.11937592178583145, 0.11744175106287003, 0.11624238640069962, 0.11741209030151367, 0.11743041127920151, 0.11919578909873962, 0.11408460140228271, 0.11442634463310242, 0.11857056617736816, 0.1183319166302681, 0.1166427806019783, 0.11943347007036209, 0.11570647358894348, 0.1184786781668663, 0.11632891744375229, 0.11713971942663193, 0.11782245337963104, 0.11648043245077133, 0.11708588898181915, 0.11816225200891495, 0.1166691780090332, 0.11847759038209915, 0.1159730777144432, 0.1173681989312172, 0.11842265725135803, 0.11823610216379166, 0.11768040060997009, 0.1155540943145752, 0.11818058788776398, 0.11792956292629242, 0.11724035441875458, 0.1177646666765213, 0.11830917745828629, 0.11584261804819107, 0.11787983775138855, 0.11693944782018661, 0.11714556813240051, 0.11700236797332764, 0.11796408891677856, 0.1187647134065628, 0.1153620034456253, 0.11808346211910248, 0.11616784334182739, 0.11695680022239685, 0.11941265314817429, 0.11983533203601837, 0.11772435158491135, 0.11883910745382309, 0.11699429154396057, 0.11521460860967636, 0.11598458141088486, 0.11811544746160507, 0.11691952496767044, 0.11580958217382431, 0.11609117686748505, 0.11653191596269608, 0.11984682828187943, 0.11674880236387253, 0.11824805289506912, 0.11825158447027206, 0.11736756563186646, 0.11845126003026962, 0.11622516065835953, 0.11587375402450562, 0.11832444369792938, 0.11751972138881683, 0.11746462434530258, 0.1188770979642868, 0.11875083297491074, 0.11815478652715683, 0.11925727874040604, 0.11870651692152023, 0.11691755801439285, 0.11788985133171082, 0.11833229660987854, 0.11772949248552322, 0.11760039627552032, 0.11583218723535538, 0.1157471165060997, 0.11679250001907349, 0.11812349408864975, 0.11759337037801743, 0.11967088282108307, 0.11779717355966568, 0.11897595226764679, 0.11748489737510681, 0.11796490103006363, 0.11641903221607208, 0.1173132136464119, 0.1185050681233406]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:22 2016", "state": "available"}], "summary": "0249efbdd09b858e4c18122f61a6f004"}