{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 32, "f3": 64, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.6890473365783691, 1.2171071767807007, 1.0449461936950684, 0.9220476746559143, 0.8301100134849548, 0.7752232551574707, 0.7392080426216125, 0.7075797915458679, 0.6831060647964478, 0.6609426736831665, 0.6449903249740601, 0.6337987184524536, 0.6207598447799683, 0.6149500012397766, 0.6049007773399353, 0.5992616415023804, 0.5902044177055359, 0.5851301550865173, 0.5814895629882812, 0.5758598446846008, 0.5766200423240662, 0.5651470422744751, 0.564731240272522, 0.5628915429115295, 0.555242121219635, 0.5603655576705933, 0.5527036786079407, 0.5556448101997375, 0.550915539264679, 0.5499264001846313, 0.550574004650116, 0.542996883392334, 0.5447302460670471, 0.547858476638794, 0.545699954032898, 0.5410739779472351, 0.5426003932952881, 0.533532440662384, 0.5355843305587769, 0.5381726026535034, 0.5309693813323975, 0.5311006903648376, 0.5418609976768494, 0.5257022976875305, 0.5241369605064392, 0.5323283076286316, 0.5261471271514893, 0.5299303531646729, 0.5281157493591309, 0.5234094262123108, 0.5283059477806091, 0.5305632948875427, 0.51789790391922, 0.5205785632133484, 0.5246785879135132, 0.5237553119659424, 0.5197771191596985, 0.521259069442749, 0.5102627873420715, 0.5119778513908386, 0.5233665704727173, 0.5131080746650696, 0.5144429206848145, 0.5223737955093384, 0.5103214979171753, 0.5110767483711243, 0.5150207877159119, 0.5119301676750183, 0.506677508354187, 0.5169564485549927, 0.5119823813438416, 0.5086764693260193, 0.5131264328956604, 0.5087194442749023, 0.5116754174232483, 0.5086791515350342, 0.5032781362533569, 0.5150681734085083, 0.5066023468971252, 0.5108370780944824, 0.5088164210319519, 0.5046447515487671, 0.5086142420768738, 0.50156170129776, 0.5095255374908447, 0.5056855082511902, 0.5082632303237915, 0.4990634024143219, 0.5028500556945801, 0.4085472822189331, 0.3462425768375397, 0.3193974196910858, 0.2985615134239197, 0.2787010967731476, 0.25725501775741577, 0.23298227787017822, 0.20604021847248077, 0.19835181534290314, 0.18448565900325775, 0.17335481941699982, 0.164425328373909, 0.1636011153459549, 0.1504894495010376, 0.1432204395532608, 0.1331472098827362, 0.1493741124868393, 0.12874683737754822, 0.12290070205926895, 0.1164361834526062, 0.1334543526172638, 0.11772885173559189, 0.10848318040370941, 0.10586528480052948, 0.10540115088224411, 0.10532965511083603, 0.10531847923994064, 0.10531765222549438, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172, 0.105317622423172], "moving_avg_accuracy_train": [0.053301296748108136, 0.1132795502981958, 0.17409004003581807, 0.231548484525684, 0.2861393304022038, 0.33617561062679513, 0.3824100403240824, 0.4255857080051754, 0.4664478348502374, 0.5033539212953078, 0.5371760826910631, 0.5674185344937084, 0.5953710555540737, 0.6208143178119738, 0.6440710383702005, 0.6651252476618719, 0.6839717655255757, 0.7019428543326989, 0.7176661519270683, 0.7323142329725951, 0.7450883518207305, 0.7571268545054902, 0.7697399213216909, 0.7795386262467846, 0.7887459407746532, 0.7981553183830037, 0.805707757746023, 0.8137974475202856, 0.8216153137409407, 0.8270054763729373, 0.8308896852763412, 0.8369495356452095, 0.842122274264151, 0.847084442371143, 0.8522875739865111, 0.8565936101867433, 0.860387662558619, 0.8643395272147633, 0.8675218564469598, 0.8693560199797739, 0.8713693822271361, 0.8727931444473903, 0.8751973970765254, 0.8778751544272894, 0.8797596524120245, 0.8818252550637253, 0.8850028731181686, 0.8842098484897681, 0.8861489408717806, 0.8887638017679913, 0.889664030666266, 0.8901398838807855, 0.8905910788225796, 0.8925316803868425, 0.8937735563565562, 0.8953979108768622, 0.8959509851511638, 0.8976740333230168, 0.8973579705741868, 0.8977406515644795, 0.8981617583176383, 0.8992452744847671, 0.9002598944673074, 0.9024403306504216, 0.9029077606771568, 0.9035097372107239, 0.9043258476016394, 0.9053695556963112, 0.9042047054065342, 0.9033216780529424, 0.9029196512417881, 0.9021880924022161, 0.9034039396311713, 0.9038447632241172, 0.9048645722410834, 0.9052522664277817, 0.9056290569327056, 0.9047057567788407, 0.903944829495198, 0.9055406414826938, 0.9055656150905151, 0.9073943354255483, 0.9082379713389735, 0.909360255265892, 0.9092843942084334, 0.9091975901638821, 0.9101099078190056, 0.908036688124222, 0.9083814661844927, 0.9151137193041848, 0.921867858459499, 0.9283534847409486, 0.9344137626799675, 0.9399307918429415, 0.9449286701729516, 0.9494011840330558, 0.9532729506369024, 0.9568436071351354, 0.9603524558335359, 0.9634987939180487, 0.9641798076429382, 0.9666923665727197, 0.9692326153690284, 0.9715165141368967, 0.9731954570672915, 0.9747180233022382, 0.9763697119684615, 0.9780375933752052, 0.9794736185234267, 0.9799266263877692, 0.9810458290013916, 0.9822112214726995, 0.9833228176659149, 0.9843302296862374, 0.9852345753557181, 0.9860484864582508, 0.9867810064505302, 0.9874402744435817, 0.988033615637328, 0.9885676227116997, 0.9890482290786343, 0.9894807748088754, 0.9898700659660924, 0.9902204280075877, 0.9905357538449334, 0.9908195470985446, 0.9910749610267946, 0.9913048335622197, 0.9915117188441022, 0.9916979155977964, 0.9918654926761212, 0.9920163120466137, 0.9921520494800569, 0.9922742131701557, 0.9923841604912447, 0.9924831130802247, 0.9925721704103068, 0.9926523220073806, 0.9927244584447471, 0.9927893812383769, 0.9928478117526437], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05182502470820782, 0.11038719820689005, 0.16908195330148718, 0.22416965170251316, 0.2763641205495811, 0.32355578054846335, 0.36584311006138803, 0.40576232291631853, 0.44243630240932524, 0.4751559451823234, 0.5050167507958682, 0.5317398439297603, 0.5561456611651728, 0.5777660112760652, 0.5974471414643472, 0.6150523543698703, 0.6300740273477928, 0.6445589180053328, 0.65728705528989, 0.6687291423060816, 0.6786394841466331, 0.6872169949281295, 0.6963130606367472, 0.7036491456216418, 0.7100338074902005, 0.716549193213319, 0.7216398793607371, 0.7258907304683382, 0.7309617607253447, 0.7335785929359427, 0.7355298803855713, 0.7388731002254028, 0.741959211230724, 0.7450704190139016, 0.7483312846802825, 0.7505652625281428, 0.7522471351834611, 0.7544536799011842, 0.7561068917774062, 0.7569732533809156, 0.7581761068831854, 0.758417419387638, 0.759596897093076, 0.7603542807553798, 0.7613369837980647, 0.7613679538576408, 0.7635514709718767, 0.7634068789857583, 0.7637445843477246, 0.7649244839701811, 0.7655000243250455, 0.7658613782554927, 0.766269986994326, 0.7673986888314145, 0.7678732930751555, 0.7691408101061189, 0.7694281127551456, 0.7701108427070407, 0.7697750631708999, 0.7695512513191713, 0.7694180623694831, 0.7694496771603963, 0.7697874242881066, 0.7710253081300038, 0.7713468289924552, 0.77196990564705, 0.772092251019845, 0.773003907883222, 0.7720808526088305, 0.7715929292455378, 0.7711211480867972, 0.7703497476982981, 0.7711803367462394, 0.7714954732523385, 0.7716754098785353, 0.7717142530209529, 0.7718203950193093, 0.7709434783524688, 0.7699639413229449, 0.7710743101801083, 0.7721082042279861, 0.7726836754561665, 0.7727253782701131, 0.7734598881652404, 0.7724250521630989, 0.7722648017472409, 0.7719649734926974, 0.7708101918343163, 0.7707992204200865, 0.7742248165444936, 0.7775906737925292, 0.780337124579692, 0.7829411486145691, 0.7851738774560489, 0.7871324462710615, 0.788885010190642, 0.7902425911557646, 0.7910615819931248, 0.7922879845054087, 0.7935891182837835, 0.7933501530386131, 0.7942978408126885, 0.7951222277122179, 0.7957807857203636, 0.7963867244676044, 0.7969055962603018, 0.7973013977035488, 0.797755275252471, 0.7980650792878414, 0.7976745752182441, 0.7981908502830162, 0.7985110724836302, 0.7991909269728426, 0.7998027960131335, 0.8003534781493955, 0.8008490920720313, 0.8012951446024035, 0.8016965918797385, 0.80205789442934, 0.8023830667239813, 0.8026757217891585, 0.8029391113478179, 0.8031761619506115, 0.8033895074931257, 0.8035815184813884, 0.8037543283708248, 0.8039098572713177, 0.8040498332817612, 0.8041758116911604, 0.8042891922596196, 0.804391234771233, 0.804483073031685, 0.8045657274660918, 0.804640116457058, 0.8047070665489275, 0.80476732163161, 0.8048215512060244, 0.8048703578229972, 0.8049142837782728, 0.8049538171380208, 0.8049893971617941], "moving_var_accuracy_train": [0.025569254115268954, 0.05538884679400949, 0.08313120307377372, 0.10453133835515142, 0.12089964860126176, 0.13134234778956005, 0.13744671541550463, 0.1404792883913278, 0.14145878024491249, 0.1395714351706403, 0.13590973906690093, 0.13055021817952872, 0.1245272872642473, 0.11790079488674116, 0.11097859085817807, 0.10387024933243777, 0.0966799455194762, 0.08991859126375014, 0.08315173092254506, 0.07676765433513746, 0.07055949191274033, 0.064807872643485, 0.059758890469725996, 0.05464713298663491, 0.04994539145530732, 0.04574767979256533, 0.041686265876297555, 0.03810662701446208, 0.034846035603212415, 0.03162291672168554, 0.028596408758764527, 0.026067263961325735, 0.023701352588572172, 0.0215528253406134, 0.019641196014013654, 0.017843953942431684, 0.016189112048793336, 0.014710755952258351, 0.013330825331111355, 0.01202802020078617, 0.010861700828559483, 0.009793774635441945, 0.008866421048240078, 0.008044312403282199, 0.007271843156844214, 0.006583059269992215, 0.006015628651492308, 0.005419725778894325, 0.004911593914398702, 0.004481972000517616, 0.004041068509089455, 0.003638999584716427, 0.0032769318181242895, 0.0029831320461928368, 0.002698699144890924, 0.002452575978870578, 0.0022100714013595685, 0.002015784316246345, 0.001815104945572492, 0.001634912453678225, 0.0014730171863884072, 0.0013362815333094306, 0.001211918463359218, 0.0011335153345609994, 0.001022130218573943, 0.0009231785784392377, 0.0008368550461267548, 0.0007629734807960304, 0.0006988880184947679, 0.0006360168524100128, 0.0005738697971809939, 0.0005212994224846972, 0.0004824740405936499, 0.0004359755654951653, 0.00040173810282542017, 0.0003629170535844745, 0.0003279030879874345, 0.00030278512775583306, 0.00027771770795917574, 0.0002728654802581729, 0.0002455845453621442, 0.0002511240533997991, 0.00023241714204960679, 0.00022051111875822139, 0.00019851180098274778, 0.00017872843536382728, 0.00016834650336209337, 0.00019019601215142893, 0.00017224625813388217, 0.0005629307209289323, 0.0009172032104001831, 0.0012040530237238216, 0.0014141904396348655, 0.0015467098927373392, 0.0016168479936778596, 0.0016351936163694954, 0.0016065894444444954, 0.0015606767904554068, 0.001515417284104265, 0.0014529705457723384, 0.0013118475084364953, 0.0012374793289734677, 0.0011718071716004504, 0.0011015721966772384, 0.0010167846212812209, 0.000935970030611298, 0.0008669257066013399, 0.0008052695914238593, 0.0007433021463183933, 0.000670818876812959, 0.000615010519544718, 0.0005657327240998758, 0.0005202802665608283, 0.000477386150712958, 0.0004370081054508392, 0.0003992693564511886, 0.00036417169065787073, 0.00033166623017204244, 0.00030166809110460476, 0.0002740677539934552, 0.00024873982091355143, 0.00022554970110094406, 0.0002043586594366353, 0.00018502757553405814, 0.00016741969143393225, 0.00015140256978769576, 0.0001368494392816231, 0.0001236400677963453, 0.00011166127469544722, 0.00010080717030567901, 9.09791919697301e-05, 8.208599111539845e-05, 7.40432138613981e-05, 6.677320817986524e-05, 6.020468308261035e-05, 5.4272339308141925e-05, 4.8916486249699736e-05, 4.4082656131351077e-05, 3.972122350857923e-05, 3.5787035879915536e-05, 3.223905941690131e-05], "duration": 95481.900408, "accuracy_train": [0.5330129674810815, 0.6530838322489848, 0.7213844476744187, 0.7486744849344776, 0.7774569432908823, 0.7865021326481173, 0.7985199075996677, 0.8141667171350129, 0.8342069764557956, 0.8355086993009413, 0.8415755352528608, 0.8396006007175157, 0.8469437450973607, 0.849803678133075, 0.8533815233942414, 0.854613131286914, 0.853590426298911, 0.863682653596807, 0.8591758302763934, 0.8641469623823367, 0.8600554214539498, 0.8654733786683279, 0.8832575226674971, 0.8677269705726283, 0.8716117715254706, 0.882839716858158, 0.8736797120131967, 0.886604655488649, 0.8919761097268365, 0.8755169400609081, 0.8658475654069768, 0.8914881889650241, 0.8886769218346253, 0.8917439553340717, 0.8991157585248246, 0.8953479359888336, 0.8945341339055003, 0.8999063091200628, 0.8961628195367294, 0.8858634917751015, 0.889489642453396, 0.8856070044296788, 0.8968356707387413, 0.9019749705841639, 0.89672013427464, 0.900415678929033, 0.913601435608158, 0.8770726268341639, 0.903600772309893, 0.9122975498338871, 0.8977660907507383, 0.8944225628114618, 0.8946518332987264, 0.9099970944652085, 0.9049504400839794, 0.9100171015596161, 0.9009286536198781, 0.9131814668696937, 0.8945134058347176, 0.9011847804771133, 0.9019517190960686, 0.9089969199889257, 0.9093914743101699, 0.9220642562984496, 0.907114630917774, 0.9089275260128276, 0.9116708411198781, 0.9147629285483574, 0.8937210527985419, 0.8953744318706165, 0.899301409941399, 0.8956040628460686, 0.9143465646917681, 0.9078121755606312, 0.9140428533937799, 0.9087415141080657, 0.909020171477021, 0.8963960553940569, 0.8970964839424143, 0.919902949370155, 0.9057903775609081, 0.9238528184408453, 0.9158306945598007, 0.919460810608158, 0.9086016446913067, 0.9084163537629198, 0.9183207667151162, 0.8893777108711702, 0.9114844687269288, 0.9757039973814139, 0.9826551108573275, 0.9867241212739941, 0.988956264131137, 0.9895840543097084, 0.9899095751430418, 0.9896538087739941, 0.9881188500715209, 0.9889795156192323, 0.99193209411914, 0.9918158366786637, 0.9703089311669435, 0.989305396940753, 0.9920948545358066, 0.9920716030477114, 0.9883059434408453, 0.9884211194167589, 0.9912349099644703, 0.9930485260358989, 0.9923978448574198, 0.9840036971668512, 0.9911186525239941, 0.9926997537144703, 0.9933271834048542, 0.99339693786914, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447, 0.9933736863810447], "end": "2016-02-04 16:36:11.873000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0], "moving_var_accuracy_valid": [0.02417249867405717, 0.05262100229065929, 0.07836457054212599, 0.097840004124015, 0.11257436691566143, 0.12136040518514621, 0.12531832880264368, 0.12712838791699452, 0.12652037607197636, 0.12350351367351226, 0.11917817171317019, 0.11368746790163722, 0.10767951634582873, 0.10111852056150393, 0.0944927904747465, 0.08783300312031102, 0.08108055873978076, 0.0748608113820507, 0.06883277955245642, 0.06312779379478571, 0.05769894829387641, 0.05259121668534894, 0.048076740719193475, 0.04375342993342449, 0.03974496210466449, 0.03615251815428718, 0.032770502107922095, 0.029656079513380815, 0.026921909692850018, 0.02429134902093083, 0.02189648182323745, 0.019807427710990685, 0.0179124016701261, 0.016208278027944435, 0.014683149429197628, 0.01325975039950044, 0.011959233620208762, 0.010807129814509694, 0.009751014818627857, 0.00878266857861739, 0.007917423429686958, 0.007126205172241508, 0.006426105163935986, 0.005788657317649711, 0.00521848293331566, 0.0046966432722854055, 0.004269888667950316, 0.0038430879627373315, 0.003459805570667106, 0.0031263544816720512, 0.0028167002538055435, 0.0025362054183924363, 0.002284087526466251, 0.002067144484353048, 0.0018624572786113368, 0.001690670945564243, 0.0015223467363170585, 0.0013743071443702846, 0.0012378911610052745, 0.0011145528706095146, 0.0010032572372154346, 0.0009029405089489315, 0.0008136731161545278, 0.000746097012193346, 0.000672417691958935, 0.0006086699434205515, 0.0005479376645906955, 0.0005006239622605065, 0.0004582298453906932, 0.0004145494837276455, 0.0003750977325105639, 0.00034294348629389514, 0.0003148580411635454, 0.00028426603620447706, 0.00025613082728905337, 0.00023053132366756385, 0.00020757958641514352, 0.0001937424733388741, 0.00018300366113486492, 0.00017579956601200246, 0.00016784004153093507, 0.00015403654158801177, 0.00013864853955143018, 0.00012963922867064568, 0.00012631327576553502, 0.00011391306995102572, 0.00010333083579592581, 0.00010499943832313575, 9.450057783819401e-05, 0.00019066289932234568, 0.00027355756451749347, 0.00031408873540250595, 0.0003437083324302179, 0.00035420320190337914, 0.00035330680794129905, 0.0003456194497771105, 0.0003276447394911669, 0.00030091697946717125, 0.0002843618496196804, 0.00027116220664076323, 0.0002445599254722816, 0.00022818694197924194, 0.00021148477162235811, 0.00019423958231095756, 0.00017812007996853102, 0.000162731123406991, 0.00014786794010857908, 0.00013493518956246206, 0.0001223054774692017, 0.00011144737057763073, 0.00010270149300241529, 9.335422402206872e-05, 8.817862075838065e-05, 8.27302121847428e-05, 7.718644830305072e-05, 7.167850191553921e-05, 6.630131746264806e-05, 6.112162496470005e-05, 5.618431825936663e-05, 5.151751962425059e-05, 4.713659054639023e-05, 4.304729802824861e-05, 3.924830511998663e-05, 3.573312149258389e-05, 3.24916233198482e-05, 2.951123030884677e-05, 2.6777810427958633e-05, 2.427636893665997e-05, 2.199156707970671e-05, 1.990810675147336e-05, 1.8011010143913254e-05, 1.628581752426758e-05, 1.4718721571584798e-05, 1.3296652912218932e-05, 1.2007328454209047e-05, 1.0839271683689872e-05, 9.781812135991287e-06, 8.82506969513518e-06, 7.959928131543513e-06, 7.178001297185861e-06, 6.4715946102926e-06], "accuracy_test": 0.10010762117346939, "start": "2016-02-03 14:04:49.973000", "learning_rate_per_epoch": [0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.008568351157009602, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 0.000856835104059428, 8.56835104059428e-05, 8.56835140439216e-06, 8.568351290705323e-07, 8.568351006488228e-08, 8.568351361759596e-09, 8.568351583804201e-10, 8.568351583804201e-11, 8.568351757276549e-12, 8.568351648856332e-13, 8.56835151333106e-14, 8.56835151333106e-15, 8.568351301572823e-16, 8.568351036875027e-17, 8.56835120231115e-18, 8.568351409106303e-19, 8.568351667600244e-20, 8.568351990717671e-21, 8.568352192666063e-22, 8.568352192666063e-23, 8.568352350438244e-24, 8.568352744868696e-25, 8.56835262160918e-26, 8.568352929757971e-27, 8.568352544571982e-28, 8.568352785313225e-29, 8.568352484386671e-30, 8.568352296307575e-31, 8.568352296307575e-32, 8.568352443244369e-33, 8.568352810586354e-34], "accuracy_train_first": 0.5330129674810815, "accuracy_train_last": 0.9933736863810447, "batch_size_eval": 1024, "accuracy_train_std": [0.022755571898985905, 0.023774307026765754, 0.024559925782533566, 0.02657631987398784, 0.025712149747006273, 0.02556235361166752, 0.026918634713529233, 0.025766579577322136, 0.027837246579274003, 0.02851622086833717, 0.02916333864955245, 0.029057455111119346, 0.03059990538956832, 0.029631907383500314, 0.03019348031867069, 0.03174882416481343, 0.032517569804819936, 0.030673513242549192, 0.028763373342453567, 0.030383946861359867, 0.03030966733887782, 0.02912371804618671, 0.028755050846849234, 0.030575907707274617, 0.03012676328850238, 0.03067372545787991, 0.029082053130426246, 0.03012749547701977, 0.02968450462731332, 0.029559166098059354, 0.026576718914242042, 0.02903920636313683, 0.0274189562049961, 0.028025605233597515, 0.025059700510782164, 0.026271624378726117, 0.026815842959781795, 0.027059092487908634, 0.027327189337383706, 0.027818376720174873, 0.026826899598982986, 0.0270598681402075, 0.026298007889726333, 0.025544385565832863, 0.02583044062958728, 0.028050934459413663, 0.02418286109743004, 0.028580706688696098, 0.025767818135706652, 0.023478619464515297, 0.025282630024631214, 0.023270054264444468, 0.026164746783703712, 0.024137305066377502, 0.02717107429530574, 0.025440628266064504, 0.025474421378663643, 0.023844685427060752, 0.02314371651628653, 0.025558053798686666, 0.024301794851233932, 0.022375593323992807, 0.02527058880821585, 0.02253952367984607, 0.024184608807940346, 0.02222253714660997, 0.023935337618955433, 0.02443649904862829, 0.02447601979181964, 0.024768523654101673, 0.025422889002035338, 0.025273403307918378, 0.022356842960912778, 0.024300493830934612, 0.02528761419560431, 0.024607310032988883, 0.02322574399679674, 0.025079267789826006, 0.023012839423465815, 0.02279140715802632, 0.02295463429506026, 0.02307116747153802, 0.025446875060481003, 0.023786164917329974, 0.02418758157512245, 0.02275899749220536, 0.02297467282996515, 0.02304409759403749, 0.025038595802800327, 0.010462398326507668, 0.009139221652130176, 0.007202635188562985, 0.005780231936777156, 0.005482492027890136, 0.005552411324449968, 0.005285928977299066, 0.005594012801926765, 0.004238091039993173, 0.0040610416291874864, 0.0038949067292657507, 0.01057000571709449, 0.004465946768523052, 0.0034223116339509164, 0.003542246414078618, 0.005005601551136455, 0.00485821730533609, 0.003353538571798292, 0.0029857267869152194, 0.003387069533470466, 0.006268779174068534, 0.003455746169270608, 0.0025335687886213773, 0.0022551911167360008, 0.0021404268982729235, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135, 0.0021586652512437135], "accuracy_test_std": 0.007611098693520995, "error_valid": [0.48174975291792166, 0.3625532403049698, 0.3026652508471386, 0.280041062688253, 0.2538856598268072, 0.25171927946159633, 0.2535709243222892, 0.23496476138930722, 0.22749788215361444, 0.23036726986069278, 0.22623599868222888, 0.22775231786521077, 0.22420198371611444, 0.22765083772590367, 0.22542268684111444, 0.22650072948042166, 0.23473091585090367, 0.22507706607680722, 0.22815970914909633, 0.22829207454819278, 0.23216743928840367, 0.23558540803840367, 0.22182234798569278, 0.23032608951430722, 0.23250423569277112, 0.22481233527861444, 0.2325439453125, 0.23585160956325302, 0.22339896696159633, 0.24286991716867468, 0.24690853256777112, 0.23103792121611444, 0.23026578972138556, 0.2269287109375, 0.22232092432228923, 0.22932893684111444, 0.23261601091867468, 0.22568741763930722, 0.22901420133659633, 0.2352294921875, 0.23099821159638556, 0.23941076807228923, 0.2297878035579819, 0.23282926628388556, 0.22981868881777112, 0.23835331560617468, 0.216796875, 0.23789444888930722, 0.23321606739457834, 0.22445641942771077, 0.22932011248117468, 0.2308864363704819, 0.23005253435617468, 0.22244299463478923, 0.22785526873117468, 0.21945153661521077, 0.22798616340361444, 0.22374458772590367, 0.23324695265436746, 0.23246305534638556, 0.23178063817771077, 0.23026578972138556, 0.2271728515625, 0.21783373729292166, 0.2257594832454819, 0.22242240446159633, 0.226806640625, 0.21879118034638556, 0.23622664486069278, 0.23279838102409633, 0.23312488234186746, 0.23659285579819278, 0.22134436182228923, 0.22566829819277112, 0.22670516048569278, 0.22793615869728923, 0.2272243269954819, 0.23694877164909633, 0.23885189194277112, 0.21893237010542166, 0.21858674934111444, 0.22213708349021077, 0.22689929640436746, 0.21992952277861444, 0.23688847185617468, 0.2291774519954819, 0.23073348079819278, 0.23958284309111444, 0.2292995223079819, 0.19494481833584332, 0.19211661097515065, 0.19494481833584332, 0.1936226350715362, 0.19473156297063254, 0.19524043439382532, 0.19534191453313254, 0.19753918015813254, 0.20156750047063254, 0.1966743928840362, 0.19470067771084332, 0.20880053416792166, 0.19717296922063254, 0.1974582901920181, 0.19829219220632532, 0.19815982680722888, 0.19842455760542166, 0.19913638930722888, 0.19815982680722888, 0.19914668439382532, 0.20583996140813254, 0.1971626741340362, 0.19860692771084332, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698, 0.19469038262424698], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.0696421113608051, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.008568350915925935, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 5.914280573641859e-06, "rotation_range": [0, 0], "momentum": 0.6435981369054247}, "accuracy_valid_max": 0.8078833890248494, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            print(X_train.min(), X_train.max())\n            print(X_valid.min(), X_valid.max())\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.805309617375753, "accuracy_valid_std": [0.01667130672096728, 0.011258001206600346, 0.012645509208753361, 0.013510889806764806, 0.012683092022143004, 0.01563154871868885, 0.016321493354349313, 0.014781045457248778, 0.011116727712467266, 0.012861893268339445, 0.013163509036956655, 0.010877817983024363, 0.013940732332470694, 0.0159030167793591, 0.015671162724505215, 0.016202374740495278, 0.01214707789828269, 0.01319013230103232, 0.016209765818605268, 0.01723243739576245, 0.016861384283660272, 0.013396733994713346, 0.016255677606847114, 0.012965383596091345, 0.020318230084700656, 0.014153093228586914, 0.013051251802293624, 0.026150445056955303, 0.01661810065399462, 0.020296539412097064, 0.017942822535609304, 0.01375445326486648, 0.017951753672469557, 0.013810139829851693, 0.017410645783592266, 0.01473290244733772, 0.02005249454652999, 0.014852631270608535, 0.0175853756808858, 0.008875947230119868, 0.01689111784424085, 0.01661228773113891, 0.020255746622543334, 0.0164209301287312, 0.0206014297889739, 0.020070880269279213, 0.017462455802976236, 0.011574167562907123, 0.019940173408241394, 0.014135996658839207, 0.020320041439020952, 0.0201714900147067, 0.019903990136110897, 0.01811387019875028, 0.019072560601951047, 0.010677054052287308, 0.011010797628879535, 0.014447078430836932, 0.01862259756403111, 0.01985912701279933, 0.016798083350721672, 0.02033079824143216, 0.01221618309278247, 0.01114546654815716, 0.018608129087520763, 0.01661870563363179, 0.012292183471849712, 0.01938970415763602, 0.01606767728091594, 0.01217456688625854, 0.0199112914952408, 0.01785463517992378, 0.01685578307426326, 0.022705127473074887, 0.013005974937129596, 0.018669764268579327, 0.020699885996904956, 0.01625559810122831, 0.0157930942716817, 0.012186547218577752, 0.01087123012853884, 0.012696197682340515, 0.020300542649992856, 0.008492086770544909, 0.015529492885454138, 0.01790304461874767, 0.015648677592681393, 0.008822245639459473, 0.016341847330524115, 0.011766505577216776, 0.010561192065819442, 0.010446299261900422, 0.012638379813318296, 0.01322747050808122, 0.014339930254737437, 0.012842561678597124, 0.014109386879994676, 0.01292953395408507, 0.011109321668093006, 0.011867355272403318, 0.011983716079657813, 0.012106599389116752, 0.016146446062259474, 0.014065315891363717, 0.012952187250130337, 0.014650697165145277, 0.012995140999885827, 0.0131531123047997, 0.01392972593861036, 0.01148169276590871, 0.011637804553921747, 0.01035512392780132, 0.00939658103120252, 0.00972083423028063, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428, 0.009866896223322428], "accuracy_valid": [0.5182502470820783, 0.6374467596950302, 0.6973347491528614, 0.719958937311747, 0.7461143401731928, 0.7482807205384037, 0.7464290756777108, 0.7650352386106928, 0.7725021178463856, 0.7696327301393072, 0.7737640013177711, 0.7722476821347892, 0.7757980162838856, 0.7723491622740963, 0.7745773131588856, 0.7734992705195783, 0.7652690841490963, 0.7749229339231928, 0.7718402908509037, 0.7717079254518072, 0.7678325607115963, 0.7644145919615963, 0.7781776520143072, 0.7696739104856928, 0.7674957643072289, 0.7751876647213856, 0.7674560546875, 0.764148390436747, 0.7766010330384037, 0.7571300828313253, 0.7530914674322289, 0.7689620787838856, 0.7697342102786144, 0.7730712890625, 0.7776790756777108, 0.7706710631588856, 0.7673839890813253, 0.7743125823606928, 0.7709857986634037, 0.7647705078125, 0.7690017884036144, 0.7605892319277108, 0.7702121964420181, 0.7671707337161144, 0.7701813111822289, 0.7616466843938253, 0.783203125, 0.7621055511106928, 0.7667839326054217, 0.7755435805722892, 0.7706798875188253, 0.7691135636295181, 0.7699474656438253, 0.7775570053652108, 0.7721447312688253, 0.7805484633847892, 0.7720138365963856, 0.7762554122740963, 0.7667530473456325, 0.7675369446536144, 0.7682193618222892, 0.7697342102786144, 0.7728271484375, 0.7821662627070783, 0.7742405167545181, 0.7775775955384037, 0.773193359375, 0.7812088196536144, 0.7637733551393072, 0.7672016189759037, 0.7668751176581325, 0.7634071442018072, 0.7786556381777108, 0.7743317018072289, 0.7732948395143072, 0.7720638413027108, 0.7727756730045181, 0.7630512283509037, 0.7611481080572289, 0.7810676298945783, 0.7814132506588856, 0.7778629165097892, 0.7731007035956325, 0.7800704772213856, 0.7631115281438253, 0.7708225480045181, 0.7692665192018072, 0.7604171569088856, 0.7707004776920181, 0.8050551816641567, 0.8078833890248494, 0.8050551816641567, 0.8063773649284638, 0.8052684370293675, 0.8047595656061747, 0.8046580854668675, 0.8024608198418675, 0.7984324995293675, 0.8033256071159638, 0.8052993222891567, 0.7911994658320783, 0.8028270307793675, 0.8025417098079819, 0.8017078077936747, 0.8018401731927711, 0.8015754423945783, 0.8008636106927711, 0.8018401731927711, 0.8008533156061747, 0.7941600385918675, 0.8028373258659638, 0.8013930722891567, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753, 0.805309617375753], "seed": 161316109, "model": "residualv4", "loss_std": [0.276416152715683, 0.20783287286758423, 0.19995932281017303, 0.19613896310329437, 0.19141525030136108, 0.1881735771894455, 0.18159766495227814, 0.1765938401222229, 0.17182818055152893, 0.16577988862991333, 0.1633903831243515, 0.1607067584991455, 0.1575942486524582, 0.1495835781097412, 0.14829158782958984, 0.14731217920780182, 0.14534297585487366, 0.14575417339801788, 0.14052216708660126, 0.13971702754497528, 0.13610166311264038, 0.1360539048910141, 0.13119642436504364, 0.13114812970161438, 0.1326345056295395, 0.1321004182100296, 0.12507855892181396, 0.1269993633031845, 0.125534325838089, 0.12463006377220154, 0.12561067938804626, 0.11901449412107468, 0.12079732865095139, 0.12448110431432724, 0.11822624504566193, 0.11799407005310059, 0.1209072396159172, 0.11703835427761078, 0.11630862951278687, 0.1165180429816246, 0.11123122274875641, 0.11713620275259018, 0.1194194033741951, 0.11278121918439865, 0.10794682055711746, 0.11240680515766144, 0.1107555404305458, 0.11283925920724869, 0.10857849568128586, 0.11166136711835861, 0.11017205566167831, 0.112987220287323, 0.11015845835208893, 0.10931823402643204, 0.1127602756023407, 0.10740825533866882, 0.1133253276348114, 0.11336012929677963, 0.1045168936252594, 0.10638254135847092, 0.11282496154308319, 0.10739047080278397, 0.10419897735118866, 0.11179207265377045, 0.10998839884996414, 0.10505110025405884, 0.10614074766635895, 0.10486704856157303, 0.10522941499948502, 0.11079301685094833, 0.1048600897192955, 0.1045633926987648, 0.1086253970861435, 0.10821325331926346, 0.10720895230770111, 0.10557975620031357, 0.10340365022420883, 0.10603632777929306, 0.10681519657373428, 0.10695671290159225, 0.10747550427913666, 0.10202927142381668, 0.10436096042394638, 0.1029466912150383, 0.10783275216817856, 0.10557472705841064, 0.104175865650177, 0.10179079324007034, 0.10337071120738983, 0.0847318172454834, 0.03675121068954468, 0.0217384435236454, 0.014100363478064537, 0.009943191893398762, 0.008170392364263535, 0.00808236375451088, 0.008261534385383129, 0.01672545075416565, 0.004641224164515734, 0.0037701185792684555, 0.020824840292334557, 0.016588764265179634, 0.0034787338227033615, 0.0024685957469046116, 0.0036322795785963535, 0.033509038388729095, 0.005319363437592983, 0.001962207490578294, 0.0022836728021502495, 0.04711989313364029, 0.013882623054087162, 0.0029435378964990377, 0.0009551462135277689, 0.0005947260069660842, 0.000561730470508337, 0.0005564549355767667, 0.0005558349075727165, 0.0005558045813813806, 0.0005558040575124323, 0.0005558038246817887, 0.0005558038828894496, 0.000555804290343076, 0.0005558039410971105, 0.0005558041739277542, 0.0005558039993047714, 0.0005558038828894496, 0.0005558035336434841, 0.0005558038246817887, 0.0005558040575124323, 0.0005558039410971105, 0.0005558037664741278, 0.0005558041739277542, 0.0005558041739277542, 0.0005558039993047714, 0.0005558038246817887, 0.0005558037664741278, 0.0005558038828894496, 0.0005558038246817887, 0.0005558038246817887, 0.0005558038246817887, 0.0005558038246817887]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:37 2016", "state": "available"}], "summary": "bb30d54405350ffff50e1341d12640b9"}