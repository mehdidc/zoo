{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 32, "f3": 32, "nonlin": "rectify", "nbg1": 2, "nbg3": 7, "nbg2": 8, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.8132622241973877, 1.4596315622329712, 1.2980232238769531, 1.2050074338912964, 1.1415925025939941, 1.0909557342529297, 1.0494178533554077, 1.0145142078399658, 0.985246479511261, 0.9589966535568237, 0.9360923171043396, 0.9145336747169495, 0.8938757181167603, 0.878829300403595, 0.8617717027664185, 0.845504641532898, 0.8321717977523804, 0.8212184309959412, 0.8057849407196045, 0.7974716424942017, 0.784522294998169, 0.7750510573387146, 0.7644301652908325, 0.756601870059967, 0.747470498085022, 0.7408300042152405, 0.7293714880943298, 0.724811851978302, 0.7163808345794678, 0.7074078917503357, 0.7027469873428345, 0.6964050531387329, 0.6903884410858154, 0.6840877532958984, 0.6788901686668396, 0.6691718101501465, 0.6671810150146484, 0.6617007255554199, 0.6559486985206604, 0.6512792110443115, 0.6472716331481934, 0.6428651809692383, 0.635728657245636, 0.6313649415969849, 0.6264176368713379, 0.6227060556411743, 0.619489312171936, 0.6142452359199524, 0.61053866147995, 0.6055848002433777, 0.6019682288169861, 0.5988864302635193, 0.5949998497962952, 0.5933965444564819, 0.5879712104797363, 0.5851101875305176, 0.5830376744270325, 0.5754728317260742, 0.5735756754875183, 0.5713528394699097, 0.567240834236145, 0.5676645040512085, 0.5631161332130432, 0.5597715377807617, 0.5581199526786804, 0.5533217787742615, 0.5505918264389038, 0.5484591722488403, 0.544736921787262, 0.5427427291870117, 0.5423287153244019, 0.5361547470092773, 0.5362287759780884, 0.5321764945983887, 0.5321243405342102, 0.529104471206665, 0.525721549987793, 0.5220891237258911, 0.5182362198829651, 0.5210268497467041, 0.5160326361656189, 0.5142353177070618, 0.5109530687332153, 0.5100995302200317, 0.5053725838661194, 0.505682110786438, 0.5048665404319763, 0.5042259097099304, 0.500078558921814, 0.496966689825058, 0.4947303235530853, 0.4920605719089508, 0.493270605802536, 0.4908417761325836, 0.4867015779018402, 0.4852662682533264, 0.48369506001472473, 0.48109278082847595, 0.48106223344802856, 0.48047682642936707, 0.4763631820678711, 0.4762807786464691, 0.47456344962120056, 0.4719364643096924, 0.47129419445991516, 0.46957212686538696, 0.46695366501808167, 0.46522998809814453, 0.462594598531723, 0.4621432423591614, 0.4614173173904419, 0.45755159854888916, 0.4576478898525238, 0.4562023878097534, 0.4570302367210388, 0.45498958230018616, 0.45387905836105347, 0.45253172516822815, 0.44707897305488586, 0.4486999809741974, 0.4479489028453827, 0.44405072927474976, 0.4421441853046417, 0.4438818395137787, 0.4411929249763489, 0.44029057025909424, 0.4385356605052948, 0.4379703104496002, 0.4358653724193573, 0.4355333745479584, 0.43313392996788025, 0.43197396397590637, 0.43358492851257324, 0.42806383967399597, 0.4269535541534424, 0.4252637028694153, 0.42736509442329407, 0.42557844519615173, 0.42312610149383545, 0.4228803515434265, 0.42272523045539856, 0.4199784994125366, 0.41712674498558044, 0.4167705476284027, 0.4173714518547058, 0.4151920974254608, 0.4171510934829712, 0.4123786687850952, 0.4139353632926941, 0.4104880690574646, 0.4115529954433441, 0.4099477529525757, 0.4076096713542938, 0.4082450866699219, 0.40608054399490356, 0.4049345850944519, 0.4032897353172302, 0.40369799733161926, 0.40140724182128906, 0.4009248912334442, 0.4007745087146759, 0.3996661901473999, 0.39904388785362244, 0.39734870195388794, 0.39357778429985046, 0.3958696126937866, 0.39234137535095215, 0.3942299783229828, 0.39162737131118774, 0.39147675037384033, 0.3925246596336365, 0.39028027653694153, 0.38874417543411255, 0.38702237606048584, 0.38687756657600403, 0.38837987184524536, 0.3875716030597687, 0.3844010829925537, 0.3871987760066986, 0.3841264247894287, 0.3839436173439026, 0.3812766373157501, 0.3813372552394867, 0.37883055210113525, 0.3782421052455902, 0.3806411623954773, 0.3786454498767853, 0.37546801567077637, 0.3764449954032898, 0.37616783380508423, 0.37600669264793396, 0.3732184171676636, 0.37323862314224243, 0.3703860938549042, 0.3708976209163666, 0.37065133452415466, 0.3715441823005676, 0.3697488605976105, 0.3688819706439972, 0.3675871789455414, 0.36823350191116333, 0.36698344349861145, 0.3657636344432831, 0.3633359372615814, 0.3630889654159546, 0.361724853515625, 0.3640000820159912, 0.36394616961479187, 0.3602210283279419, 0.3602723777294159, 0.36026427149772644, 0.35798248648643494, 0.3583807647228241, 0.35645440220832825, 0.3565792739391327, 0.35583817958831787, 0.3579990267753601, 0.3522005081176758, 0.35419371724128723, 0.35446879267692566, 0.3540898859500885, 0.3534393012523651, 0.35201701521873474, 0.3507818877696991, 0.35033610463142395, 0.3466044068336487, 0.3499574065208435, 0.3488328754901886, 0.3479722738265991, 0.3456716239452362, 0.34531235694885254, 0.3454907536506653, 0.3445669114589691, 0.3410570025444031, 0.3426232635974884, 0.34139564633369446, 0.342946857213974, 0.3392610549926758, 0.34200233221054077, 0.34126171469688416, 0.3429235816001892, 0.3377538025379181, 0.33669036626815796, 0.33986175060272217, 0.3375983238220215, 0.3385167121887207, 0.3395113945007324, 0.33782458305358887, 0.33681023120880127, 0.33674612641334534, 0.335411936044693, 0.3350631892681122, 0.3345921039581299, 0.33201584219932556, 0.33368629217147827, 0.33271002769470215], "moving_avg_accuracy_train": [0.04919999999999999, 0.10010352941176467, 0.15161552941176465, 0.20160927058823525, 0.24864599058823522, 0.2927108032941176, 0.33426325237647053, 0.37271928007999994, 0.408802646189647, 0.44217179333538814, 0.47313814341361404, 0.5020219761310761, 0.5286244844003214, 0.5532585065485245, 0.5759679500113191, 0.5967523314807753, 0.6161829806856389, 0.6342117414406044, 0.6511646849436028, 0.6668411576257132, 0.6813358653925536, 0.6947458082650629, 0.7069418156738507, 0.7184758694005833, 0.7292141648134661, 0.7391939248027077, 0.7484886499694957, 0.7571150790901933, 0.7651800417694092, 0.7725843905336448, 0.7796506573626333, 0.7864173563322523, 0.7926720912872623, 0.7984048821585361, 0.8037973351191532, 0.8088270133719437, 0.8135654885053376, 0.8178724690665685, 0.8220475751010882, 0.8260992881792146, 0.8298917123024697, 0.8334201881310462, 0.8367464046120592, 0.839895293562618, 0.8430422347945915, 0.8458391877857206, 0.8487305631247957, 0.8513939774005514, 0.8539039914252022, 0.8562924158120937, 0.8585008212897078, 0.8607189744548548, 0.8629764887740752, 0.8650247222496088, 0.8670163676717068, 0.8687123779633595, 0.8703658460493765, 0.8719433790914977, 0.8735866882411714, 0.8751362547111718, 0.8766390998282899, 0.878081072198402, 0.8795294355667971, 0.8809059037748233, 0.8823282545738115, 0.8836766055870185, 0.8850877685577285, 0.8863625211137204, 0.88762509253176, 0.8887967009256429, 0.8898605602448433, 0.8909968571615354, 0.8921395243865583, 0.8930761601831966, 0.8940203088707593, 0.8949665132778011, 0.8959169207735505, 0.8968993463432542, 0.8979647058265758, 0.8989399999498006, 0.8999189411312911, 0.900682341135809, 0.9015952834928164, 0.9025181080847111, 0.9033086502174165, 0.9042295499015572, 0.905084241970225, 0.9061405236555554, 0.9069076477605881, 0.9077486476904116, 0.9085926064507822, 0.9092698163939392, 0.9100887171074864, 0.9108233748085024, 0.9113786843864756, 0.9120525806537104, 0.9127649696471629, 0.9136178844471525, 0.9144349195318491, 0.9150737805198406, 0.9155405201149153, 0.9162005857504826, 0.9169522918813167, 0.9177676509284791, 0.918400297600337, 0.918993209016774, 0.9196021234092142, 0.9202019110682927, 0.92082407290264, 0.9214169597300231, 0.9220846755217267, 0.9227232667930835, 0.9234438812902458, 0.9240336108082801, 0.9246490732568637, 0.9252947541664714, 0.925913514043942, 0.9265409861689595, 0.9270327699050048, 0.927503610561563, 0.9279932495054067, 0.9284739245548661, 0.9290218262170266, 0.9294631730070886, 0.9298015615887327, 0.9301766995475065, 0.9307684413574617, 0.9313127736923038, 0.9317085551466029, 0.9322388761025308, 0.9326408708452189, 0.9331579602312853, 0.9336680465610979, 0.9341718301402822, 0.9345334706556656, 0.9349860059430403, 0.9355156406428539, 0.9359829001079802, 0.9364387277442411, 0.9368583843815816, 0.9373937224140118, 0.9377155266431988, 0.9380475033906436, 0.9384286354045204, 0.9389010659817154, 0.9392203711482497, 0.939559510504013, 0.9398576771006705, 0.9403095564494269, 0.9406621302162489, 0.9410241524887416, 0.9414605607692792, 0.9417921517511748, 0.9420952895172338, 0.9424316429184515, 0.9428331845089594, 0.9431051601757106, 0.9433522912169631, 0.9436523562129138, 0.9440377088269165, 0.9442951144148132, 0.9445644265027436, 0.9449550426759987, 0.94508542076134, 0.945412172802853, 0.9456591908166853, 0.9459685658526638, 0.9463034739732796, 0.9466072442230106, 0.9469418139183566, 0.9471793972324034, 0.947430869273869, 0.9475677823464821, 0.9477662982294809, 0.9479520213477093, 0.9482321133305854, 0.9484418431739975, 0.9486635412095389, 0.9488395400297615, 0.949068527203256, 0.9493122627182244, 0.9496163305640491, 0.9498146975076442, 0.9500449924627622, 0.9502334343929566, 0.9503747968360139, 0.9504902583288831, 0.9508529972018771, 0.9511088739522777, 0.95115798655705, 0.9514186584895803, 0.9516791455817988, 0.9516994663177365, 0.9519389314506688, 0.952250920658543, 0.9524399462397475, 0.9525747751451845, 0.952811415277725, 0.953007920808776, 0.953180069904369, 0.9533561805609909, 0.9535923272107742, 0.9537672121367556, 0.9539975497466094, 0.9541954418307721, 0.9543147211771067, 0.9544408961182196, 0.9545662182711036, 0.9547072435028168, 0.9547847544466527, 0.9549109848843403, 0.9550504746312004, 0.9552207212857274, 0.9553457079806841, 0.9555734901237921, 0.9557149646408246, 0.9559905270002715, 0.9562691213590679, 0.9564986798113965, 0.9565617530067274, 0.9568114600589959, 0.9569938434648609, 0.9571438708830807, 0.9572224249712432, 0.9572695942388247, 0.9574108701090599, 0.9575944889805068, 0.9577362165530443, 0.9579367125447986, 0.9580630412903188, 0.9581132077495222, 0.9581654163863347, 0.9583065218065248, 0.9584170460964605, 0.9585188708985791, 0.9587469838087211, 0.9587852266043195, 0.9589019980615346, 0.9589223864906753, 0.9589877949004313, 0.959183133057447, 0.9593754079869964, 0.9595790436588849, 0.9596729039988787, 0.9597267900695791, 0.959843522827327, 0.9599062293681236, 0.960094429960723, 0.9601743987293566, 0.9603334294446563, 0.9604365570884259, 0.9605999602031128, 0.9607658465357427, 0.9608680854115802, 0.9610683356939516, 0.96131914918338], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.04805333333333332, 0.09803466666666664, 0.1479245333333333, 0.19579874666666663, 0.24085887199999995, 0.28297298479999994, 0.3221956863199999, 0.35838945102133324, 0.39144383925253323, 0.4222327886606132, 0.45024950979455186, 0.47601122548176333, 0.500050102933587, 0.5223117593068949, 0.5430005833762054, 0.5611671917052515, 0.5782638058680597, 0.5940107586145871, 0.6087830160864617, 0.6222380478111489, 0.6347475763633673, 0.6460328187270306, 0.6563495368543275, 0.6662345831688947, 0.6750244581853386, 0.6831486790334714, 0.6905938111301242, 0.6972144300171118, 0.7036796536820673, 0.709365021647194, 0.7146951861491413, 0.7194790008675604, 0.7238777674474711, 0.7281033240360574, 0.7317729916324517, 0.7353690258025399, 0.7386854565556192, 0.741843577566724, 0.744752553143385, 0.7474506311623798, 0.7497322347128085, 0.7517056779081942, 0.7536417767840414, 0.755904265772304, 0.7581538391950736, 0.7598051219422329, 0.7613046097480096, 0.7625341487732087, 0.7638940672292212, 0.7654246605062991, 0.7666288611223359, 0.7676059750101023, 0.7686853775090922, 0.769776839758183, 0.7705991557823647, 0.771365906870795, 0.7721226495170488, 0.7729237178986772, 0.7737513461088095, 0.7744162114979285, 0.775027923681469, 0.7754584646466555, 0.7762059515153232, 0.7766253563637909, 0.7769361540607452, 0.7773492053213373, 0.7776542847892036, 0.7781421896436166, 0.7786346373459215, 0.7790245069446626, 0.7793753895835297, 0.77993118395851, 0.7803380655626591, 0.7806509256730598, 0.7809058331057538, 0.7811752497951784, 0.7816443914823272, 0.7819332856674279, 0.7824199571006851, 0.7827912947239499, 0.7830588319182216, 0.783419615393066, 0.7837176538537594, 0.7837992218017167, 0.7841926329548783, 0.7845600363260572, 0.7848506993601181, 0.7852856294241063, 0.7855970664816957, 0.7856640265001927, 0.7861642905168401, 0.7862011947984895, 0.7865277419853072, 0.7867416344534431, 0.7867608043414321, 0.7870580572406223, 0.7872989181832267, 0.7875156930315707, 0.7875374570617468, 0.7876103780222388, 0.7878226735533482, 0.7878537395313467, 0.788188365578212, 0.7883561956870575, 0.7884005761183517, 0.7886671851731832, 0.7886671333225316, 0.7888937533236118, 0.789017711324584, 0.7891426068587923, 0.7892816795062464, 0.7894335115556217, 0.7896901604000595, 0.7898678110267201, 0.7901876965907147, 0.7903155935983099, 0.7905107009051456, 0.7907129641479643, 0.7908683343998346, 0.7908481676265178, 0.7909900175305327, 0.7911710157774794, 0.7913339141997315, 0.7913071894464251, 0.7915898038351159, 0.791777490118271, 0.7920397411064439, 0.7922624336624662, 0.7925428569628862, 0.7927285712665976, 0.7926957141399378, 0.7929061427259441, 0.7930821951200163, 0.793227308941348, 0.7933045780472132, 0.7936541202424919, 0.793982041551576, 0.7941438373964184, 0.7944094536567765, 0.7945818416244322, 0.7945769907953224, 0.7947859583824568, 0.7949873625442111, 0.7950619596231233, 0.7950490969941443, 0.7950108539613966, 0.7950564352319236, 0.7952174583753979, 0.795215712537858, 0.7954141412840723, 0.7953393938223318, 0.7953654544400985, 0.7954822423294221, 0.7954673514298133, 0.7954672829534987, 0.7956272213248154, 0.7957444991923339, 0.7959167159397672, 0.7959383776791238, 0.795891206577878, 0.7960620859200902, 0.7960425439947478, 0.7961849562619397, 0.7962997939690791, 0.7964164812388379, 0.7965214997816208, 0.796442683136792, 0.7965450814897794, 0.7968105733408015, 0.7971295160067213, 0.7970965644060491, 0.7972669079654442, 0.7973935505022332, 0.7976541954520098, 0.7976354425734754, 0.7975918983161279, 0.7978060418178484, 0.7976921043027303, 0.7979228938724573, 0.7978106044852116, 0.7978295440366904, 0.7977799229663547, 0.7979619306697192, 0.7978724042694139, 0.7977918305091392, 0.7978393141248918, 0.7977487160457359, 0.7977071777744957, 0.7976564599970462, 0.7977574806640082, 0.7977950659309407, 0.79802889267118, 0.798106003404062, 0.7980020697303225, 0.7983218627572902, 0.7985563431482279, 0.7987940421667384, 0.7989279712833979, 0.7989018408217248, 0.7991449900728856, 0.7990971577322638, 0.798934108625704, 0.7989606977631336, 0.7991712946534869, 0.7993874985214715, 0.7996887486693244, 0.799559873802392, 0.7997238864221528, 0.7996981644466041, 0.7997550146686104, 0.7998995132017493, 0.7997895618815744, 0.7997572723600835, 0.7995815451240752, 0.7996633906116677, 0.7997637182171675, 0.7998273463954508, 0.7999112784225724, 0.7999201505803151, 0.8000081355222837, 0.7999806553033887, 0.7999559231063832, 0.7999603307957448, 0.7999109643828369, 0.7998398679445532, 0.7999092144834312, 0.8000249597017548, 0.8001157970649126, 0.8000908840250881, 0.800188462289246, 0.8001962827269881, 0.8003499877876226, 0.8005283223421937, 0.8005688234413078, 0.800711941097177, 0.8005607469874594, 0.8006513389553802, 0.8006662050598421, 0.8006662512205245, 0.800519626098472, 0.8004543301552915, 0.8002088971397624, 0.8003346740924528, 0.8003678733498742, 0.8001310860148868, 0.800197977413398, 0.8003248463387249, 0.8002923617048524, 0.8002764588677005, 0.8003554796475971, 0.8002799316828375, 0.8002519385145538, 0.8004000779964318, 0.8005200701967886, 0.8005080631771098, 0.8006972568593987], "moving_var_accuracy_train": [0.021785759999999994, 0.04292770775916954, 0.06251631227925258, 0.0787590484627067, 0.09079522087166164, 0.09919106825373591, 0.10481141565103613, 0.10764006868654412, 0.10859414560611515, 0.10775623087661078, 0.10561084132345487, 0.10255823932316241, 0.09867165640678358, 0.0942660061908887, 0.0894808749733086, 0.08442070209358865, 0.07937658304093201, 0.07436425066607688, 0.06951444624021189, 0.06477476777796726, 0.060188159979385474, 0.0557877830920426, 0.05154768815327521, 0.04759022889628808, 0.04386900490202834, 0.040378464896811316, 0.03711814565046529, 0.03407606859978852, 0.03125385434696398, 0.02862188833786951, 0.026209088646168725, 0.02400027371606084, 0.021952341728671578, 0.02005289157636826, 0.018309309359123636, 0.016706057393150623, 0.015237529973143689, 0.013880727709822712, 0.01264953853243578, 0.011532332088999352, 0.010508541206675236, 0.009569738361063354, 0.008712337969664084, 0.007930343687304237, 0.007226438470631267, 0.0065742011378794145, 0.0059920214862541725, 0.0054566633180674505, 0.004967698519896197, 0.004522269807373662, 0.004113936319418301, 0.0037468245186529307, 0.0034180094049010043, 0.003113965807743574, 0.00283826909035549, 0.0025803302395044723, 0.0023469028259573152, 0.0021346100378524403, 0.00194545321871981, 0.0017725183030523775, 0.0016155933637615515, 0.001472747586230899, 0.0013443526356299881, 0.0012269693546163505, 0.0011224801553131583, 0.0010265945938751917, 0.0009418575628567978, 0.0008622967532821878, 0.0007904138572248268, 0.0007237264675598942, 0.0006615399906633511, 0.0006070065277429728, 0.00055805707045295, 0.0005101469429475532, 0.0004671549993508351, 0.0004284972244348981, 0.0003937769716631964, 0.000363085714496947, 0.0003369920605055824, 0.0003118536420961951, 0.0002892932104179383, 0.0002656089054782265, 0.0002465491886553667, 0.0002295587168364829, 0.0002122274569250757, 0.00019863721728682218, 0.0001853479823483329, 0.00017685476310238025, 0.0001644656013248421, 0.00015438456913002683, 0.0001453565097198807, 0.00013494837851188996, 0.00012748892606853485, 0.00011959753090064127, 0.00011041309635707709, 0.00010345901233230618, 9.768059380100575e-05, 9.44597073252767e-05, 9.102165355937438e-05, 8.5592778461235e-05, 7.899411326160659e-05, 7.501588172475767e-05, 7.259985251648402e-05, 7.132316064694264e-05, 6.779302088496514e-05, 6.41776143261393e-05, 6.109684352941328e-05, 5.8224866300318614e-05, 5.5886147803351176e-05, 5.3461166133775114e-05, 5.2127648926810566e-05, 5.0585073340806726e-05, 5.020013328840952e-05, 4.8310148099536666e-05, 4.6888279520133496e-05, 4.595158610140719e-05, 4.4802201564972215e-05, 4.3865472817541576e-05, 4.1655586723134785e-05, 3.948524636563592e-05, 3.7694438387027656e-05, 3.600443107687937e-05, 3.510575405177502e-05, 3.334826154848018e-05, 3.104399688331596e-05, 2.9206153588001042e-05, 2.9436963556042834e-05, 2.9159946417229986e-05, 2.765373841161044e-05, 2.7419527417116385e-05, 2.6131972633744637e-05, 2.59252082690124e-05, 2.5674380016866857e-05, 2.5391123067082134e-05, 2.4029065521675904e-05, 2.3469252646381296e-05, 2.3646943618963143e-05, 2.3247231926818543e-05, 2.279251823994862e-05, 2.2098271655329898e-05, 2.2467725770491733e-05, 2.115297485074668e-05, 2.0029554413268455e-05, 1.9333953479957806e-05, 1.9409273984381454e-05, 1.8385948690323044e-05, 1.7582493344938434e-05, 1.662437388470557e-05, 1.6799691008728237e-05, 1.623849625731503e-05, 1.5794187763610922e-05, 1.5928838673145922e-05, 1.53255280193018e-05, 1.462000776427276e-05, 1.417620948244257e-05, 1.420970937436603e-05, 1.3454475306671871e-05, 1.2658691539959436e-05, 1.2203173402117848e-05, 1.2319325795974786e-05, 1.1683711946500773e-05, 1.1168101758199825e-05, 1.1424520535655549e-05, 1.0435054488325399e-05, 1.0352451109189498e-05, 9.8663670906897e-06, 9.741146597601202e-06, 9.776502981131825e-06, 9.629339964612568e-06, 9.67383789754652e-06, 9.2144665878125e-06, 8.862163617780851e-06, 8.144653961074004e-06, 7.684865567191936e-06, 7.226816700273019e-06, 7.2101987000890485e-06, 6.885058295038903e-06, 6.638902636201379e-06, 6.253792635058897e-06, 6.100329502177763e-06, 6.024959563272741e-06, 6.254578900725254e-06, 5.983266009454028e-06, 5.862261305683531e-06, 5.595628424613597e-06, 5.2159156449163405e-06, 4.814306287444953e-06, 5.51709106852935e-06, 5.554638164236208e-06, 5.020882779340204e-06, 5.130343209087903e-06, 5.227990615090873e-06, 4.708907944363261e-06, 4.7541090989392135e-06, 5.154733581515595e-06, 4.960836256511428e-06, 4.6283621345323675e-06, 4.6695128920376156e-06, 4.550091416436698e-06, 4.361800074814392e-06, 4.204754737715183e-06, 4.286166425778386e-06, 4.132812419220228e-06, 4.197029907916866e-06, 4.129778409893079e-06, 3.844848631061755e-06, 3.6036448098390686e-06, 3.384631106886306e-06, 3.2251610400154657e-06, 2.9567164537430074e-06, 2.8044519189580907e-06, 2.699123232374109e-06, 2.6900662195354716e-06, 2.561654662827638e-06, 2.772451539014975e-06, 2.6753417358398458e-06, 3.091219087751577e-06, 3.480630529754909e-06, 3.606841224098537e-06, 3.2819611534119804e-06, 3.5149475456440864e-06, 3.4628261516943237e-06, 3.3191175724841512e-06, 3.042742518139142e-06, 2.758492724562856e-06, 2.6622732957029113e-06, 2.6994889756957106e-06, 2.610320421482634e-06, 2.7110761637205745e-06, 2.583599114850747e-06, 2.347889266026744e-06, 2.1376320152444277e-06, 2.103065470183096e-06, 2.0026994911570343e-06, 1.8957441549798638e-06, 2.174489237443317e-06, 1.9702029164356695e-06, 1.89590278377335e-06, 1.7100536977814247e-06, 1.5775526686045566e-06, 1.7632103620206818e-06, 1.9196161626177163e-06, 2.10086192814562e-06, 1.970063606144923e-06, 1.7991906230701494e-06, 1.7419103913461613e-06, 1.6031083445397522e-06, 1.7615726775786562e-06, 1.6429704454317232e-06, 1.7062903165669697e-06, 1.6313790830956182e-06, 1.7085463757902396e-06, 1.7853562163917734e-06, 1.70089568434522e-06, 1.8917076962191147e-06, 2.2687035849102956e-06], "duration": 187638.834375, "accuracy_train": [0.492, 0.558235294117647, 0.6152235294117647, 0.6515529411764706, 0.6719764705882353, 0.6892941176470588, 0.7082352941176471, 0.7188235294117648, 0.7335529411764706, 0.7424941176470589, 0.751835294117647, 0.7619764705882353, 0.7680470588235294, 0.7749647058823529, 0.7803529411764706, 0.7838117647058823, 0.7910588235294118, 0.7964705882352942, 0.8037411764705882, 0.8079294117647059, 0.8117882352941177, 0.815435294117647, 0.8167058823529412, 0.8222823529411765, 0.8258588235294118, 0.8290117647058823, 0.8321411764705883, 0.8347529411764706, 0.837764705882353, 0.8392235294117647, 0.8432470588235295, 0.8473176470588235, 0.848964705882353, 0.85, 0.8523294117647059, 0.8540941176470588, 0.8562117647058823, 0.856635294117647, 0.8596235294117647, 0.8625647058823529, 0.8640235294117647, 0.8651764705882353, 0.8666823529411765, 0.8682352941176471, 0.8713647058823529, 0.8710117647058824, 0.8747529411764706, 0.8753647058823529, 0.8764941176470589, 0.8777882352941176, 0.8783764705882353, 0.8806823529411765, 0.8832941176470588, 0.8834588235294117, 0.8849411764705882, 0.8839764705882353, 0.8852470588235294, 0.8861411764705882, 0.8883764705882353, 0.8890823529411764, 0.890164705882353, 0.8910588235294118, 0.8925647058823529, 0.8932941176470588, 0.8951294117647058, 0.8958117647058823, 0.8977882352941177, 0.8978352941176471, 0.8989882352941176, 0.8993411764705882, 0.8994352941176471, 0.9012235294117648, 0.9024235294117647, 0.9015058823529412, 0.9025176470588235, 0.9034823529411765, 0.9044705882352941, 0.9057411764705883, 0.9075529411764706, 0.9077176470588235, 0.9087294117647059, 0.9075529411764706, 0.9098117647058823, 0.9108235294117647, 0.9104235294117647, 0.9125176470588235, 0.9127764705882353, 0.9156470588235294, 0.9138117647058823, 0.9153176470588236, 0.9161882352941176, 0.915364705882353, 0.9174588235294118, 0.917435294117647, 0.9163764705882353, 0.9181176470588235, 0.9191764705882353, 0.9212941176470588, 0.9217882352941177, 0.9208235294117647, 0.9197411764705883, 0.9221411764705882, 0.9237176470588235, 0.9251058823529412, 0.9240941176470588, 0.9243294117647058, 0.9250823529411765, 0.9256, 0.9264235294117648, 0.9267529411764706, 0.9280941176470588, 0.9284705882352942, 0.9299294117647059, 0.9293411764705882, 0.9301882352941176, 0.9311058823529412, 0.9314823529411764, 0.9321882352941177, 0.9314588235294118, 0.9317411764705882, 0.9324, 0.9328, 0.9339529411764705, 0.933435294117647, 0.9328470588235294, 0.9335529411764706, 0.9360941176470589, 0.9362117647058823, 0.9352705882352941, 0.9370117647058823, 0.9362588235294118, 0.9378117647058823, 0.9382588235294118, 0.9387058823529412, 0.9377882352941177, 0.9390588235294117, 0.9402823529411765, 0.9401882352941177, 0.9405411764705882, 0.940635294117647, 0.9422117647058823, 0.9406117647058824, 0.9410352941176471, 0.9418588235294117, 0.9431529411764706, 0.9420941176470589, 0.9426117647058824, 0.9425411764705882, 0.9443764705882353, 0.9438352941176471, 0.9442823529411765, 0.9453882352941176, 0.9447764705882353, 0.9448235294117647, 0.9454588235294118, 0.9464470588235294, 0.9455529411764706, 0.9455764705882352, 0.9463529411764706, 0.9475058823529412, 0.9466117647058824, 0.9469882352941177, 0.9484705882352941, 0.9462588235294118, 0.9483529411764706, 0.9478823529411765, 0.9487529411764706, 0.9493176470588235, 0.9493411764705882, 0.9499529411764706, 0.9493176470588235, 0.9496941176470588, 0.9488, 0.9495529411764706, 0.9496235294117648, 0.9507529411764706, 0.9503294117647059, 0.9506588235294118, 0.9504235294117647, 0.9511294117647059, 0.9515058823529412, 0.9523529411764706, 0.9516, 0.9521176470588235, 0.9519294117647059, 0.9516470588235294, 0.9515294117647058, 0.9541176470588235, 0.9534117647058824, 0.9516, 0.953764705882353, 0.9540235294117647, 0.9518823529411765, 0.9540941176470589, 0.9550588235294117, 0.9541411764705883, 0.9537882352941176, 0.9549411764705882, 0.9547764705882353, 0.9547294117647059, 0.9549411764705882, 0.9557176470588236, 0.9553411764705882, 0.9560705882352941, 0.9559764705882353, 0.9553882352941176, 0.9555764705882352, 0.9556941176470588, 0.9559764705882353, 0.9554823529411764, 0.9560470588235294, 0.9563058823529412, 0.9567529411764706, 0.9564705882352941, 0.9576235294117647, 0.9569882352941177, 0.9584705882352941, 0.9587764705882353, 0.958564705882353, 0.9571294117647059, 0.9590588235294117, 0.958635294117647, 0.9584941176470588, 0.9579294117647059, 0.9576941176470588, 0.9586823529411764, 0.9592470588235295, 0.9590117647058823, 0.9597411764705882, 0.9592, 0.958564705882353, 0.958635294117647, 0.9595764705882353, 0.9594117647058824, 0.959435294117647, 0.9608, 0.9591294117647059, 0.9599529411764706, 0.9591058823529411, 0.9595764705882353, 0.9609411764705882, 0.9611058823529411, 0.9614117647058823, 0.9605176470588235, 0.9602117647058823, 0.9608941176470588, 0.9604705882352941, 0.9617882352941176, 0.9608941176470588, 0.961764705882353, 0.9613647058823529, 0.9620705882352941, 0.9622588235294117, 0.9617882352941176, 0.9628705882352941, 0.9635764705882353], "end": "2016-02-09 09:54:35.949000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0, 231.0, 232.0, 233.0, 234.0, 235.0, 236.0, 237.0, 238.0, 239.0, 240.0, 241.0, 242.0, 243.0, 244.0, 245.0, 246.0, 247.0, 248.0, 249.0, 250.0, 251.0, 252.0, 253.0, 254.0, 255.0], "moving_var_accuracy_valid": [0.020782105599999993, 0.041187098175999984, 0.05946937752255998, 0.07414990249087358, 0.0850086462972876, 0.09247016813993897, 0.09706893415668817, 0.09915193817031871, 0.09907007758533688, 0.09769470447768298, 0.09498966399778666, 0.09146369155434646, 0.08751813106120598, 0.08322655005543458, 0.07875614202222907, 0.07385075874363486, 0.06909633081175877, 0.06441839641779518, 0.059940533093353864, 0.05557582069242952, 0.05142663336457549, 0.04743018028497752, 0.04364507431274264, 0.040159994147238635, 0.03683935185775711, 0.03374944335148452, 0.030873368943765564, 0.02818052539940964, 0.025738664912809777, 0.023455709101618788, 0.02136583407401728, 0.019435214615957028, 0.017665835481182172, 0.016059949889414057, 0.014575153042884885, 0.013234020894368372, 0.012009607221391268, 0.010898410054139173, 0.009884728298875744, 0.008961772093957418, 0.008112446317413636, 0.007336251988081004, 0.006636363098986417, 0.006018796496885858, 0.005462462072457154, 0.0049407564776110325, 0.004466917002966987, 0.004033831198600675, 0.0036470924826036374, 0.0033034676763617978, 0.0029861718008385874, 0.0026961473847017226, 0.0024370186340249783, 0.002204038379193193, 0.001989720374066508, 0.0017960395017443372, 0.001621589486463837, 0.0014652059327858568, 0.001324850055595132, 0.0011963434639064546, 0.0010800768436752357, 0.0009737374490120451, 0.0008813923336803176, 0.0007948362041545496, 0.0007162219406149831, 0.0006461352486483756, 0.0005823593851189601, 0.0005262659069297017, 0.00047582185889228094, 0.0004296076577392564, 0.0003877549596016556, 0.0003517596301268284, 0.0003180736408722994, 0.0002871472098231892, 0.00025901728903405424, 0.0002337688283035135, 0.00021237279077674983, 0.00019188665035073933, 0.00017482962707120308, 0.0001585876890381505, 0.00014337310548720428, 0.00013020727737997174, 0.00011798599195844699, 0.00010624727273380811, 9.701549647931521e-05, 8.852881396576587e-05, 8.043629756351493e-05, 7.409514525221038e-05, 6.755856809454865e-05, 6.084306408178797e-05, 5.70111344507788e-05, 5.132227833973739e-05, 4.714974809273074e-05, 4.284652317478524e-05, 3.856517821875632e-05, 3.550389397157304e-05, 3.2475630517466414e-05, 2.9650989479590835e-05, 2.6690153588717374e-05, 2.4068995428157302e-05, 2.2067720418102968e-05, 1.986963423119371e-05, 1.8890442129240795e-05, 1.725490042523245e-05, 1.5547136986845978e-05, 1.4632146781224685e-05, 1.3168932127298628e-05, 1.2314248538574986e-05, 1.1221113958762602e-05, 1.0239392613072852e-05, 9.389524163194598e-06, 8.658048487832831e-06, 8.385061303211046e-06, 7.830592879266472e-06, 7.968474557809462e-06, 7.318845902994753e-06, 6.9295630633213e-06, 6.6048005315492595e-06, 6.161579714890307e-06, 5.549082032115376e-06, 5.1752663863251404e-06, 4.9525830362727265e-06, 4.696147796395321e-06, 4.232960928709422e-06, 4.528502870094293e-06, 4.392687851045957e-06, 4.572399293120306e-06, 4.561487134377886e-06, 4.813073467706385e-06, 4.642174344362774e-06, 4.187673226877514e-06, 4.167427612466932e-06, 4.029634860347398e-06, 3.816193564586061e-06, 3.48830884061833e-06, 4.239095673078592e-06, 4.78297757033388e-06, 4.540280871974879e-06, 4.721220764677275e-06, 4.516557190741657e-06, 4.065113246554972e-06, 4.051608994154524e-06, 4.011520822086701e-06, 3.660451257518154e-06, 3.295895156784599e-06, 2.979468407089861e-06, 2.7002204363865808e-06, 2.663554467357035e-06, 2.3972264521597694e-06, 2.511869512861028e-06, 2.310967208904844e-06, 2.0859828901998557e-06, 2.0001393010135554e-06, 1.8021210209326556e-06, 1.621908961040441e-06, 1.6899406085116456e-06, 1.6447334315475555e-06, 1.7471875612612617e-06, 1.5766918837027216e-06, 1.4390487104670478e-06, 1.557941585774196e-06, 1.4055844088115532e-06, 1.4475572525509827e-06, 1.4214908181251089e-06, 1.4018850066263632e-06, 1.3609565549178732e-06, 1.280769470944635e-06, 1.2470613281010853e-06, 1.7567285019230423e-06, 2.4965754690273153e-06, 2.2566901940062864e-06, 2.2921735286521323e-06, 2.2073011649058924e-06, 2.597993157012157e-06, 2.3413588753908444e-06, 2.1242879089832984e-06, 2.3245760720471918e-06, 2.208954281004226e-06, 2.467433282356657e-06, 2.334170112513195e-06, 2.1039814607538553e-06, 1.9157435702698185e-06, 2.0223104499991103e-06, 1.8922141921637345e-06, 1.7614219505506284e-06, 1.6055719993801642e-06, 1.5188869069627028e-06, 1.3825270680650996e-06, 1.2674249978034074e-06, 1.2325290744041807e-06, 1.1219900375772582e-06, 1.5018655338777974e-06, 1.4051935666203992e-06, 1.3618940867913195e-06, 2.146112898986973e-06, 2.4263310926966953e-06, 2.692205394034936e-06, 2.5844179292341774e-06, 2.3321213455560156e-06, 2.6310032360611805e-06, 2.388494307739389e-06, 2.388909977314722e-06, 2.1563818196465017e-06, 2.3399030897202087e-06, 2.5266097935318875e-06, 3.0907136784109653e-06, 2.931120892511561e-06, 2.880110058227624e-06, 2.5980536326399703e-06, 2.367335799055415e-06, 2.318520653863678e-06, 2.195472223751212e-06, 1.9853085201590155e-06, 2.0646982214193234e-06, 1.9185165538306477e-06, 1.817255554275672e-06, 1.671966904492904e-06, 1.5681714806342568e-06, 1.4120627692179447e-06, 1.340528642414925e-06, 1.2132722400481066e-06, 1.0974501501617842e-06, 9.87879984675188e-07, 9.110253707181298e-07, 8.654151654759538e-07, 8.221541310175948e-07, 8.605113179987246e-07, 8.487230251081071e-07, 7.694366585770183e-07, 7.781866514438628e-07, 7.009184195177747e-07, 8.434537885479637e-07, 1.0453373298797417e-06, 9.555666481567186e-07, 1.0443539541345687e-06, 1.1456554880410728e-06, 1.1049520811027082e-06, 9.964458825493077e-07, 8.968013134716543e-07, 1.0006115198765431e-06, 9.389224096514224e-07, 1.3871664546918708e-06, 1.3908283856754885e-06, 1.261665263347918e-06, 1.6401129151071252e-06, 1.5163717563496142e-06, 1.509596098636987e-06, 1.3681337517137534e-06, 1.2335964786076977e-06, 1.1664353836461596e-06, 1.1011593000955668e-06, 9.980959273210701e-07, 1.0957940894085706e-06, 1.1157978337859551e-06, 1.0055155671014769e-06, 1.2271122551540585e-06], "accuracy_test": 0.7931, "start": "2016-02-07 05:47:17.115000", "learning_rate_per_epoch": [0.00013198975648265332, 9.333084744866937e-05, 7.620432006660849e-05, 6.599487824132666e-05, 5.902761404286139e-05, 5.388459248933941e-05, 4.9887436034623533e-05, 4.666542372433469e-05, 4.399658428155817e-05, 4.17388255300466e-05, 3.979640678153373e-05, 3.8102160033304244e-05, 3.660737274913117e-05, 3.527574517647736e-05, 3.407960684853606e-05, 3.299743912066333e-05, 3.201221625204198e-05, 3.1110284908208996e-05, 3.02805256069405e-05, 2.9513807021430694e-05, 2.8802525775972754e-05, 2.814030995068606e-05, 2.75217662419891e-05, 2.6942296244669706e-05, 2.6397950932732783e-05, 2.5885319701046683e-05, 2.540143941587303e-05, 2.4943718017311767e-05, 2.4509881768608466e-05, 2.409792250546161e-05, 2.370605943724513e-05, 2.3332711862167343e-05, 2.2976466425461695e-05, 2.2636055291513912e-05, 2.2310339772957377e-05, 2.1998292140779085e-05, 2.1698981072404422e-05, 2.1411566194728948e-05, 2.113527625624556e-05, 2.08694127650233e-05, 2.0613337255781516e-05, 2.0366460375953466e-05, 2.0128249161643907e-05, 1.9898203390766867e-05, 1.967587013496086e-05, 1.9460827388684265e-05, 1.925268406921532e-05, 1.9051080016652122e-05, 1.8855678717955016e-05, 1.8666169125935994e-05, 1.8482262021279894e-05, 1.8303686374565586e-05, 1.8130187527276576e-05, 1.7961530829779804e-05, 1.779749436536804e-05, 1.763787258823868e-05, 1.7482469047536142e-05, 1.7331103663309477e-05, 1.7183601812575944e-05, 1.703980342426803e-05, 1.6899555703275837e-05, 1.6762714949436486e-05, 1.6629146557534114e-05, 1.6498719560331665e-05, 1.6371313904528506e-05, 1.6246814993792213e-05, 1.6125115507747978e-05, 1.600610812602099e-05, 1.5889698261162266e-05, 1.5775793144712225e-05, 1.566430182720069e-05, 1.5555142454104498e-05, 1.544823135191109e-05, 1.5343497580033727e-05, 1.5240863831422757e-05, 1.514026280347025e-05, 1.5041628103062976e-05, 1.4944896975066513e-05, 1.485000757384114e-05, 1.4756903510715347e-05, 1.4665528397017624e-05, 1.457582948205527e-05, 1.4487757653114386e-05, 1.4401262887986377e-05, 1.4316298802441452e-05, 1.4232820831239223e-05, 1.415078713762341e-05, 1.407015497534303e-05, 1.3990886145620607e-05, 1.3912941540183965e-05, 1.3836285688739736e-05, 1.376088312099455e-05, 1.368670018564444e-05, 1.3613703231385443e-05, 1.3541863154387102e-05, 1.3471148122334853e-05, 1.340152903139824e-05, 1.3332978596736211e-05, 1.3265469533507712e-05, 1.3198975466366392e-05, 1.31334709294606e-05, 1.3068933185422793e-05, 1.3005336768401321e-05, 1.2942659850523341e-05, 1.2880880603916012e-05, 1.2819978110201191e-05, 1.2759931451000739e-05, 1.2700719707936514e-05, 1.2642325600609183e-05, 1.2584729120135307e-05, 1.2527912986115552e-05, 1.2471859008655883e-05, 1.241655172634637e-05, 1.2361972949292976e-05, 1.2308108125580475e-05, 1.2254940884304233e-05, 1.2202456673549023e-05, 1.2150641850894317e-05, 1.2099480954930186e-05, 1.2048961252730805e-05, 1.1999068192380946e-05, 1.1949790859944187e-05, 1.1901115613000002e-05, 1.1853029718622565e-05, 1.1805522262875456e-05, 1.1758581422327552e-05, 1.1712196283042431e-05, 1.1666355931083672e-05, 1.1621049452514853e-05, 1.1576266842894256e-05, 1.153199809778016e-05, 1.1488233212730847e-05, 1.1444963092799298e-05, 1.1402177733543795e-05, 1.135986894951202e-05, 1.1318027645756956e-05, 1.1276645636826288e-05, 1.1235713827772997e-05, 1.119522494263947e-05, 1.1155169886478689e-05, 1.111554229282774e-05, 1.107633397623431e-05, 1.1037537660740782e-05, 1.0999146070389543e-05, 1.096115283871768e-05, 1.0923549780272879e-05, 1.0886331438086927e-05, 1.0849490536202211e-05, 1.081302161765052e-05, 1.0776918315968942e-05, 1.0741174264694564e-05, 1.0705783097364474e-05, 1.0670739357010461e-05, 1.0636037586664315e-05, 1.0601672329357825e-05, 1.056763812812278e-05, 1.053392952599097e-05, 1.0500541065994184e-05, 1.0467468200658914e-05, 1.043470638251165e-05, 1.040225015458418e-05, 1.0370094059908297e-05, 1.0338235369999893e-05, 1.0306668627890758e-05, 1.0275388376612682e-05, 1.0244391887681559e-05, 1.0213673704129178e-05, 1.0183230187976733e-05, 1.0153057701245416e-05, 1.0123151696461719e-05, 1.0093508535646833e-05, 1.0064124580821954e-05, 1.003499528451357e-05, 1.0006117008742876e-05, 9.977487934520468e-06, 9.949101695383433e-06, 9.92095738183707e-06, 9.89305044640787e-06, 9.865377251117025e-06, 9.83793506748043e-06, 9.81072116701398e-06, 9.783731911738869e-06, 9.756963663676288e-06, 9.730413694342133e-06, 9.704079275252298e-06, 9.677958587417379e-06, 9.652047083363868e-06, 9.62634203460766e-06, 9.60084162215935e-06, 9.575543117534835e-06, 9.550443792250007e-06, 9.525540008326061e-06, 9.500829946773592e-06, 9.476311788603198e-06, 9.451982805330772e-06, 9.427839358977508e-06, 9.403880540048704e-06, 9.380103620060254e-06, 9.356505870528053e-06, 9.333084562967997e-06, 9.309839697380085e-06, 9.286766726290807e-06, 9.263864740205463e-06, 9.241131010639947e-06, 9.218564628099557e-06, 9.196161954605486e-06, 9.173922080663033e-06, 9.151843187282793e-06, 9.129922545980662e-06, 9.108158337767236e-06, 9.086549653147813e-06, 9.065093763638288e-06, 9.043788850249257e-06, 9.022634003486019e-06, 9.001626494864468e-06, 8.980765414889902e-06, 8.960048035078216e-06, 8.939474355429411e-06, 8.919040737964679e-06, 8.89874718268402e-06, 8.87859187059803e-06, 8.858572073222604e-06, 8.838687790557742e-06, 8.81893629411934e-06, 8.799316674412694e-06, 8.779828021943104e-06, 8.760467608226463e-06, 8.741234523768071e-06, 8.722128768567927e-06, 8.703146704647224e-06, 8.684288332005963e-06, 8.665551831654739e-06, 8.64693629409885e-06, 8.628439900348894e-06, 8.610061740910169e-06, 8.591800906287972e-06, 8.5736555774929e-06, 8.555624845030252e-06, 8.537707799405325e-06, 8.519901712134015e-06, 8.502207492711022e-06, 8.484622412652243e-06, 8.467146471957676e-06, 8.449777851637919e-06, 8.432515642198268e-06, 8.415358934144024e-06, 8.398306817980483e-06, 8.381357474718243e-06, 8.364510904357303e-06, 8.34776528790826e-06, 8.331119715876412e-06, 8.314573278767057e-06, 8.298125067085493e-06, 8.281773261842318e-06, 8.265518772532232e-06, 8.249359780165832e-06], "accuracy_train_first": 0.492, "accuracy_train_last": 0.9635764705882353, "batch_size_eval": 1024, "accuracy_train_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_test_std": 0, "error_valid": [0.5194666666666667, 0.4521333333333334, 0.4030666666666667, 0.3733333333333333, 0.3536, 0.33799999999999997, 0.3248, 0.31586666666666663, 0.3110666666666667, 0.30066666666666664, 0.2976, 0.29213333333333336, 0.28359999999999996, 0.2773333333333333, 0.27080000000000004, 0.2753333333333333, 0.2678666666666667, 0.26426666666666665, 0.25826666666666664, 0.2566666666666667, 0.2526666666666667, 0.25239999999999996, 0.2508, 0.24480000000000002, 0.24586666666666668, 0.24373333333333336, 0.24239999999999995, 0.24319999999999997, 0.2381333333333333, 0.23946666666666672, 0.23733333333333329, 0.23746666666666671, 0.23653333333333337, 0.23386666666666667, 0.23519999999999996, 0.23226666666666662, 0.2314666666666667, 0.22973333333333334, 0.22906666666666664, 0.22826666666666662, 0.22973333333333334, 0.23053333333333337, 0.22893333333333332, 0.22373333333333334, 0.22160000000000002, 0.22533333333333339, 0.22519999999999996, 0.22640000000000005, 0.22386666666666666, 0.2208, 0.22253333333333336, 0.22360000000000002, 0.22160000000000002, 0.22040000000000004, 0.22199999999999998, 0.22173333333333334, 0.22106666666666663, 0.21986666666666665, 0.2188, 0.21960000000000002, 0.2194666666666667, 0.22066666666666668, 0.21706666666666663, 0.21960000000000002, 0.22026666666666672, 0.2189333333333333, 0.21960000000000002, 0.2174666666666667, 0.2169333333333333, 0.2174666666666667, 0.2174666666666667, 0.21506666666666663, 0.21599999999999997, 0.21653333333333336, 0.2168, 0.21640000000000004, 0.2141333333333333, 0.2154666666666667, 0.21319999999999995, 0.21386666666666665, 0.21453333333333335, 0.21333333333333337, 0.2136, 0.2154666666666667, 0.21226666666666671, 0.21213333333333328, 0.21253333333333335, 0.2108, 0.2116, 0.21373333333333333, 0.20933333333333337, 0.2134666666666667, 0.21053333333333335, 0.21133333333333337, 0.21306666666666663, 0.2102666666666667, 0.21053333333333335, 0.21053333333333335, 0.21226666666666671, 0.21173333333333333, 0.2102666666666667, 0.21186666666666665, 0.20879999999999999, 0.21013333333333328, 0.21120000000000005, 0.2089333333333333, 0.21133333333333337, 0.20906666666666662, 0.20986666666666665, 0.20973333333333333, 0.2094666666666667, 0.20920000000000005, 0.20799999999999996, 0.20853333333333335, 0.2069333333333333, 0.20853333333333335, 0.20773333333333333, 0.2074666666666667, 0.20773333333333333, 0.20933333333333337, 0.20773333333333333, 0.20720000000000005, 0.20720000000000005, 0.2089333333333333, 0.20586666666666664, 0.20653333333333335, 0.2056, 0.20573333333333332, 0.2049333333333333, 0.2056, 0.2076, 0.20520000000000005, 0.20533333333333337, 0.2054666666666667, 0.20599999999999996, 0.20320000000000005, 0.20306666666666662, 0.20440000000000003, 0.20320000000000005, 0.20386666666666664, 0.2054666666666667, 0.20333333333333337, 0.20320000000000005, 0.2042666666666667, 0.20506666666666662, 0.20533333333333337, 0.20453333333333334, 0.20333333333333337, 0.20479999999999998, 0.20279999999999998, 0.20533333333333337, 0.20440000000000003, 0.20346666666666668, 0.20466666666666666, 0.20453333333333334, 0.2029333333333333, 0.20320000000000005, 0.20253333333333334, 0.20386666666666664, 0.20453333333333334, 0.20240000000000002, 0.2041333333333334, 0.20253333333333334, 0.20266666666666666, 0.20253333333333334, 0.20253333333333334, 0.2042666666666667, 0.20253333333333334, 0.20079999999999998, 0.19999999999999996, 0.20320000000000005, 0.20120000000000005, 0.20146666666666668, 0.19999999999999996, 0.20253333333333334, 0.20279999999999998, 0.2002666666666667, 0.20333333333333337, 0.19999999999999996, 0.20320000000000005, 0.20199999999999996, 0.20266666666666666, 0.20040000000000002, 0.2029333333333333, 0.2029333333333333, 0.20173333333333332, 0.20306666666666662, 0.20266666666666666, 0.20279999999999998, 0.20133333333333336, 0.20186666666666664, 0.19986666666666664, 0.20120000000000005, 0.2029333333333333, 0.19879999999999998, 0.19933333333333336, 0.19906666666666661, 0.19986666666666664, 0.20133333333333336, 0.19866666666666666, 0.20133333333333336, 0.20253333333333334, 0.20079999999999998, 0.1989333333333333, 0.19866666666666666, 0.1976, 0.2016, 0.19879999999999998, 0.20053333333333334, 0.19973333333333332, 0.19879999999999998, 0.20120000000000005, 0.20053333333333334, 0.20199999999999996, 0.1996, 0.19933333333333336, 0.1996, 0.19933333333333336, 0.19999999999999996, 0.19920000000000004, 0.2002666666666667, 0.2002666666666667, 0.19999999999999996, 0.20053333333333334, 0.20079999999999998, 0.19946666666666668, 0.1989333333333333, 0.19906666666666661, 0.20013333333333339, 0.1989333333333333, 0.19973333333333332, 0.1982666666666667, 0.19786666666666664, 0.19906666666666661, 0.19799999999999995, 0.20079999999999998, 0.19853333333333334, 0.19920000000000004, 0.19933333333333336, 0.20079999999999998, 0.20013333333333339, 0.20199999999999996, 0.19853333333333334, 0.19933333333333336, 0.20199999999999996, 0.19920000000000004, 0.19853333333333334, 0.19999999999999996, 0.19986666666666664, 0.1989333333333333, 0.20040000000000002, 0.19999999999999996, 0.1982666666666667, 0.19840000000000002, 0.1996, 0.1976], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.07102731147871996, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.00013198975366722227, "optimization": "adam", "nb_data_augmentation": 2, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.4118843375971239e-05, "rotation_range": [0, 0], "momentum": 0.5773516148290642}, "accuracy_valid_max": 0.8024, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        y_pred = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            y_pred.extend((nnet.predict(X[mini_batch]) == y[mini_batch]).tolist())\n        return np.mean(y_pred)\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            acc = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = acc\n            status[\"accuracy_train_std\"] = 0\n            acc = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = acc\n            status[\"accuracy_valid_std\"] = 0\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n\n    # rescaling to [-1, 1]\n    X_min = X_train.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X_train.max(axis=(0, 2, 3))[None, :, None, None]\n    def preprocess(a):\n        return (a / 255.) * 2 - 1\n        # return 2 * ((a - X_min) / (X_max - X_min)) - 1\n    X_train = preprocess(X_train)\n    X_valid = preprocess(X_valid)\n\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = preprocess(X_test)\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    acc = evaluate(X_test, y_test, batch_size_eval)\n    light.set(\"accuracy_test\", acc)\n    light.set(\"accuracy_test_std\", 0)\n    print(\"Test accuracy : {}+-{}\".format(acc, 0))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8024, "accuracy_valid_std": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_valid": [0.4805333333333333, 0.5478666666666666, 0.5969333333333333, 0.6266666666666667, 0.6464, 0.662, 0.6752, 0.6841333333333334, 0.6889333333333333, 0.6993333333333334, 0.7024, 0.7078666666666666, 0.7164, 0.7226666666666667, 0.7292, 0.7246666666666667, 0.7321333333333333, 0.7357333333333334, 0.7417333333333334, 0.7433333333333333, 0.7473333333333333, 0.7476, 0.7492, 0.7552, 0.7541333333333333, 0.7562666666666666, 0.7576, 0.7568, 0.7618666666666667, 0.7605333333333333, 0.7626666666666667, 0.7625333333333333, 0.7634666666666666, 0.7661333333333333, 0.7648, 0.7677333333333334, 0.7685333333333333, 0.7702666666666667, 0.7709333333333334, 0.7717333333333334, 0.7702666666666667, 0.7694666666666666, 0.7710666666666667, 0.7762666666666667, 0.7784, 0.7746666666666666, 0.7748, 0.7736, 0.7761333333333333, 0.7792, 0.7774666666666666, 0.7764, 0.7784, 0.7796, 0.778, 0.7782666666666667, 0.7789333333333334, 0.7801333333333333, 0.7812, 0.7804, 0.7805333333333333, 0.7793333333333333, 0.7829333333333334, 0.7804, 0.7797333333333333, 0.7810666666666667, 0.7804, 0.7825333333333333, 0.7830666666666667, 0.7825333333333333, 0.7825333333333333, 0.7849333333333334, 0.784, 0.7834666666666666, 0.7832, 0.7836, 0.7858666666666667, 0.7845333333333333, 0.7868, 0.7861333333333334, 0.7854666666666666, 0.7866666666666666, 0.7864, 0.7845333333333333, 0.7877333333333333, 0.7878666666666667, 0.7874666666666666, 0.7892, 0.7884, 0.7862666666666667, 0.7906666666666666, 0.7865333333333333, 0.7894666666666666, 0.7886666666666666, 0.7869333333333334, 0.7897333333333333, 0.7894666666666666, 0.7894666666666666, 0.7877333333333333, 0.7882666666666667, 0.7897333333333333, 0.7881333333333334, 0.7912, 0.7898666666666667, 0.7888, 0.7910666666666667, 0.7886666666666666, 0.7909333333333334, 0.7901333333333334, 0.7902666666666667, 0.7905333333333333, 0.7908, 0.792, 0.7914666666666667, 0.7930666666666667, 0.7914666666666667, 0.7922666666666667, 0.7925333333333333, 0.7922666666666667, 0.7906666666666666, 0.7922666666666667, 0.7928, 0.7928, 0.7910666666666667, 0.7941333333333334, 0.7934666666666667, 0.7944, 0.7942666666666667, 0.7950666666666667, 0.7944, 0.7924, 0.7948, 0.7946666666666666, 0.7945333333333333, 0.794, 0.7968, 0.7969333333333334, 0.7956, 0.7968, 0.7961333333333334, 0.7945333333333333, 0.7966666666666666, 0.7968, 0.7957333333333333, 0.7949333333333334, 0.7946666666666666, 0.7954666666666667, 0.7966666666666666, 0.7952, 0.7972, 0.7946666666666666, 0.7956, 0.7965333333333333, 0.7953333333333333, 0.7954666666666667, 0.7970666666666667, 0.7968, 0.7974666666666667, 0.7961333333333334, 0.7954666666666667, 0.7976, 0.7958666666666666, 0.7974666666666667, 0.7973333333333333, 0.7974666666666667, 0.7974666666666667, 0.7957333333333333, 0.7974666666666667, 0.7992, 0.8, 0.7968, 0.7988, 0.7985333333333333, 0.8, 0.7974666666666667, 0.7972, 0.7997333333333333, 0.7966666666666666, 0.8, 0.7968, 0.798, 0.7973333333333333, 0.7996, 0.7970666666666667, 0.7970666666666667, 0.7982666666666667, 0.7969333333333334, 0.7973333333333333, 0.7972, 0.7986666666666666, 0.7981333333333334, 0.8001333333333334, 0.7988, 0.7970666666666667, 0.8012, 0.8006666666666666, 0.8009333333333334, 0.8001333333333334, 0.7986666666666666, 0.8013333333333333, 0.7986666666666666, 0.7974666666666667, 0.7992, 0.8010666666666667, 0.8013333333333333, 0.8024, 0.7984, 0.8012, 0.7994666666666667, 0.8002666666666667, 0.8012, 0.7988, 0.7994666666666667, 0.798, 0.8004, 0.8006666666666666, 0.8004, 0.8006666666666666, 0.8, 0.8008, 0.7997333333333333, 0.7997333333333333, 0.8, 0.7994666666666667, 0.7992, 0.8005333333333333, 0.8010666666666667, 0.8009333333333334, 0.7998666666666666, 0.8010666666666667, 0.8002666666666667, 0.8017333333333333, 0.8021333333333334, 0.8009333333333334, 0.802, 0.7992, 0.8014666666666667, 0.8008, 0.8006666666666666, 0.7992, 0.7998666666666666, 0.798, 0.8014666666666667, 0.8006666666666666, 0.798, 0.8008, 0.8014666666666667, 0.8, 0.8001333333333334, 0.8010666666666667, 0.7996, 0.8, 0.8017333333333333, 0.8016, 0.8004, 0.8024], "seed": 973632352, "model": "residualv5", "loss_std": [0.24073901772499084, 0.16753631830215454, 0.16911359131336212, 0.16755163669586182, 0.16920089721679688, 0.16804489493370056, 0.16538725793361664, 0.16715185344219208, 0.16690734028816223, 0.16660982370376587, 0.1641971468925476, 0.16175059974193573, 0.16406436264514923, 0.16464518010616302, 0.16169118881225586, 0.159983828663826, 0.16117478907108307, 0.15741397440433502, 0.15695129334926605, 0.16052362322807312, 0.15668867528438568, 0.15750427544116974, 0.15489724278450012, 0.15350279211997986, 0.1535005271434784, 0.15392552316188812, 0.15386836230754852, 0.15201307833194733, 0.15039758384227753, 0.1516704261302948, 0.1488637924194336, 0.14965341985225677, 0.14947326481342316, 0.14581726491451263, 0.1479027271270752, 0.14607253670692444, 0.14958803355693817, 0.14631325006484985, 0.14450886845588684, 0.1466854214668274, 0.14216990768909454, 0.14260321855545044, 0.14242792129516602, 0.14430658519268036, 0.14046649634838104, 0.14112140238285065, 0.14126773178577423, 0.14172562956809998, 0.1388530731201172, 0.1378612220287323, 0.13928090035915375, 0.13684149086475372, 0.1371813267469406, 0.13486775755882263, 0.13675744831562042, 0.13818912208080292, 0.13903062045574188, 0.13612942397594452, 0.13554103672504425, 0.13531683385372162, 0.13559579849243164, 0.13248412311077118, 0.13603708148002625, 0.13335025310516357, 0.1320735663175583, 0.13294059038162231, 0.1322498470544815, 0.12971852719783783, 0.12923231720924377, 0.13090041279792786, 0.13418760895729065, 0.13140830397605896, 0.12961900234222412, 0.12934809923171997, 0.1313997358083725, 0.12801261246204376, 0.12836520373821259, 0.1274569034576416, 0.12782920897006989, 0.1287689059972763, 0.13002142310142517, 0.1268126666545868, 0.12590448558330536, 0.12823572754859924, 0.12741412222385406, 0.1262393444776535, 0.12585017085075378, 0.12512242794036865, 0.12472472339868546, 0.12255772948265076, 0.12358962744474411, 0.12475888431072235, 0.12296994030475616, 0.12287665158510208, 0.12022042274475098, 0.12134582549333572, 0.1205843836069107, 0.12059269100427628, 0.12186741828918457, 0.11958359181880951, 0.12452297657728195, 0.12231884151697159, 0.11753139644861221, 0.11939705163240433, 0.12059158831834793, 0.11795216053724289, 0.12159033119678497, 0.11673629283905029, 0.11851485818624496, 0.11851771175861359, 0.1202353909611702, 0.1165602058172226, 0.11568251252174377, 0.1173863336443901, 0.11784348636865616, 0.1175176277756691, 0.1172211766242981, 0.11621800810098648, 0.11499004811048508, 0.11738769710063934, 0.11605857312679291, 0.11417515575885773, 0.11464264988899231, 0.11606687307357788, 0.11302486062049866, 0.11581341177225113, 0.11441831290721893, 0.11384814232587814, 0.1134740337729454, 0.1143997386097908, 0.11306067556142807, 0.11455615609884262, 0.11538378894329071, 0.11236315965652466, 0.1118653416633606, 0.11188516020774841, 0.11226299405097961, 0.11235857009887695, 0.11060202121734619, 0.11097566038370132, 0.11009477078914642, 0.10850095003843307, 0.10962813347578049, 0.11043489724397659, 0.11086183786392212, 0.10958617180585861, 0.11140941083431244, 0.10954763740301132, 0.10913652926683426, 0.11194401979446411, 0.10913661867380142, 0.10971079021692276, 0.10806499421596527, 0.10616504400968552, 0.10693889111280441, 0.11021784693002701, 0.10774599015712738, 0.1079844981431961, 0.10743498802185059, 0.10790171474218369, 0.10740923881530762, 0.10696063190698624, 0.1074308231472969, 0.10469639301300049, 0.10534293204545975, 0.10682393610477448, 0.10609985142946243, 0.10529980808496475, 0.1066092848777771, 0.10652324557304382, 0.10655487328767776, 0.1045423373579979, 0.10639644414186478, 0.1048840656876564, 0.10651235282421112, 0.10199324786663055, 0.10550826042890549, 0.10445485264062881, 0.102916419506073, 0.10413119196891785, 0.10362707078456879, 0.10322481393814087, 0.1044558733701706, 0.10133372992277145, 0.10271133482456207, 0.10428523272275925, 0.10181909799575806, 0.10298515111207962, 0.10416364669799805, 0.10107554495334625, 0.10214990377426147, 0.10173765569925308, 0.09775635600090027, 0.1002618744969368, 0.10210168361663818, 0.1013718992471695, 0.10180751234292984, 0.1007792204618454, 0.10114003717899323, 0.09919817000627518, 0.0984562411904335, 0.09899628162384033, 0.10004743188619614, 0.0996302142739296, 0.099210724234581, 0.09922479093074799, 0.09999152272939682, 0.09918434172868729, 0.09798303246498108, 0.09854952245950699, 0.09931210428476334, 0.09679737687110901, 0.0965404286980629, 0.09660709649324417, 0.09694453328847885, 0.09611179679632187, 0.09799554198980331, 0.09785398095846176, 0.09865748137235641, 0.09914644807577133, 0.09898160398006439, 0.099053755402565, 0.09636853635311127, 0.09624037146568298, 0.09846434742212296, 0.09591314196586609, 0.09592682868242264, 0.0957777127623558, 0.09594610333442688, 0.09648384898900986, 0.09586036950349808, 0.09523418545722961, 0.09491194784641266, 0.09642396122217178, 0.0930909663438797, 0.09564907103776932, 0.09482176601886749, 0.09463818371295929, 0.09508202224969864, 0.09435257315635681, 0.0964246317744255, 0.09408009797334671, 0.09296879172325134, 0.09527710825204849, 0.09378693997859955, 0.09519238770008087, 0.0941503718495369, 0.09187978506088257, 0.09424134343862534, 0.09287114441394806, 0.09469995647668839, 0.09442124515771866, 0.09227602928876877, 0.09384163469076157, 0.09275131672620773, 0.09375470131635666]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:47 2016", "state": "available"}], "summary": "8c5fee5f936fa1dbfe761b22c9d1681e"}