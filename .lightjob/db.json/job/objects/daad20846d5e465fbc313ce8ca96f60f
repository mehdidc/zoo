{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 64, "f3": 32, "nonlin": "rectify", "nbg1": 2, "nbg3": 2, "nbg2": 2, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.030098961933517358, 0.03494479320317241, 0.036683709752542135, 0.03507357914293937, 0.03446275854532267, 0.03204601922344855, 0.031674722735823196, 0.03163345346806707, 0.031749958014194155, 0.031243322432370012, 0.03193144724811996, 0.030354967007790827, 0.02912225319780193, 0.029635151312351355, 0.028590641135906744, 0.02922083435849826, 0.029451832691199244, 0.029640048773817554, 0.029097633664969865, 0.030049793651796216, 0.03194252533464999, 0.03226691505012467, 0.030900853125404547, 0.03248463334995746, 0.03428355589528287, 0.034329038850723186, 0.03475763664669178, 0.03661638287798587, 0.03667752677827049, 0.03835591056547289, 0.03775302168523645, 0.03840578316206193, 0.040316208802767824, 0.040680663354347626, 0.03970401621150934, 0.03997249351506769, 0.039174742130635445, 0.039433250963931736, 0.04012584544018123, 0.04059605814457361, 0.04192320193541534, 0.04314068468379929, 0.042896061233912215, 0.043279878681748825, 0.04265003474535628, 0.042983737319763564, 0.04313710970096572, 0.04351150463708758, 0.043414656436990426, 0.04272229273408639, 0.041976832653817825, 0.04139359326335964, 0.04171560152862198, 0.04208906229553157, 0.04196386376533283, 0.04182094055939048, 0.04216636961464544, 0.04156942431849044, 0.04182180823176053, 0.04168362145907307, 0.041966241694916355, 0.04199757454540163, 0.04183980837303549, 0.042274662786840556, 0.042082811215234314, 0.04213882227130116, 0.04229268468996476, 0.04212870270982548, 0.04193358742753335, 0.04152837628144043, 0.0412880428135947, 0.04160607127881819, 0.04059404690650981, 0.040659472777267315, 0.040620855226230095, 0.040659472777267315, 0.0406458603718584, 0.040472517287542704, 0.04004165385689999, 0.04004165385689999, 0.04011114728639034, 0.039962733413954045, 0.03967087190399734, 0.039803283538234493, 0.03975790234590561, 0.03975790234590561, 0.039678417524714016, 0.03932543788492333, 0.039164551592976195, 0.039056925880533847, 0.039056925880533847, 0.03933650923589857, 0.03928781813592056, 0.03936325214494196, 0.03935795111587128, 0.03940563470522963, 0.03940563470522963, 0.03940563470522963, 0.03940563470522963, 0.039518050045534665, 0.039518050045534665, 0.039518050045534665, 0.039518050045534665, 0.039518050045534665, 0.039518050045534665, 0.039518050045534665, 0.039518050045534665, 0.03994184332710372, 0.03994184332710372, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842, 0.04006769971598842], "moving_avg_accuracy_train": [0.014871987951807225, 0.030038121234939756, 0.04384765624999999, 0.056426840173192765, 0.06791047392695783, 0.07818220891378011, 0.08733029073324547, 0.09548826316594503, 0.1028139662168204, 0.10941415845056005, 0.11526961760550403, 0.12049482075459217, 0.12523986051648237, 0.12953392792868956, 0.13345741766594107, 0.13716033930296143, 0.1406388648606171, 0.14389896180829032, 0.147000123609389, 0.15003825130869106, 0.15301964831637618, 0.1559264560751, 0.1587943714615659, 0.1616625811527587, 0.16456635315796475, 0.16748095278192732, 0.17036764666036108, 0.17339394675336112, 0.17641176216838644, 0.17943841351178877, 0.182498901979887, 0.18553101479394649, 0.18848346677840724, 0.19127245067285567, 0.19387666268388337, 0.19627222307212153, 0.1985600045299696, 0.20069430904685215, 0.20272578175662476, 0.20461528942433577, 0.20636290957828773, 0.20805813217467584, 0.20962148311383477, 0.2111061533265477, 0.21250353874690497, 0.21378471725173256, 0.21498954748439061, 0.21607624785643348, 0.2170966351189829, 0.2180502810950364, 0.21896033205179782, 0.21980055637673854, 0.22059676203424539, 0.22136276354166423, 0.22207334336219658, 0.22275757529103715, 0.2234016219788009, 0.22399538297369187, 0.2245768311221058, 0.22510719394363016, 0.22561040527215873, 0.22608682709434044, 0.22653443203550883, 0.226951395458464, 0.22732901570177422, 0.22767122708340404, 0.2280074552786781, 0.22831947330502717, 0.22861440850464493, 0.22890102864815634, 0.22917781207852142, 0.2294433893044042, 0.2296753493197469, 0.22986293486969991, 0.23004823400321187, 0.23021735638602323, 0.23038603866910765, 0.23054961853713665, 0.23071566571956756, 0.230879227159659, 0.2310405514316449, 0.23119986225233582, 0.23134794831625885, 0.2314882852617414, 0.23162164800062748, 0.23173696814032377, 0.23185016891665283, 0.231963815428602, 0.23208021626525988, 0.23218733018090254, 0.23228843903028218, 0.23239120280797687, 0.2324978091838057, 0.23259846124735284, 0.23269140126719587, 0.23278210677300643, 0.23287315437883832, 0.23295980354938822, 0.23304249412818434, 0.23311926881175146, 0.23318836602696186, 0.23325055352065122, 0.23329946277701982, 0.23335054059570337, 0.23339886379516916, 0.23344000151203778, 0.23348173178252074, 0.2335122295380036, 0.2335443838432394, 0.2335662632299998, 0.23359066100338535, 0.23361261899943236, 0.23363238119587468, 0.23364781401002216, 0.2336640567054055, 0.23368102829390108, 0.2336986558861977, 0.23371452071926468, 0.23372879906902497, 0.23374400274645982, 0.23375533289350056, 0.23376553002583725, 0.23377706060759085, 0.2337874381311691, 0.2337967779023895, 0.23380753685913852, 0.23381721992021262, 0.23382593467517931, 0.23383377795464935, 0.23384083690617238, 0.2338471899625431, 0.23385526087592737, 0.2338601715353226, 0.23386223796612768, 0.23386409775385225, 0.23386812472545498, 0.23387174899989746, 0.2338750108468957, 0.23387559334654345, 0.23387611759622645, 0.23387658942094114, 0.23387701406318437, 0.2338797494038539], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.001990584230748294, 0.003861630196529127, 0.0051917964928657875, 0.006096739657140813, 0.006673930289142241, 0.006956114116983599, 0.007013689314065933, 0.006911293010573512, 0.006703157036222604, 0.006424904170301198, 0.006090991370508035, 0.005727616965000399, 0.005357493889577628, 0.004987695635085083, 0.004627470017041437, 0.004288127673186514, 0.003968216166365236, 0.0036670486387027673, 0.0033868986154818328, 0.003131280733189048, 0.002898151212927047, 0.0026843818737499357, 0.0024899681343502934, 0.002315010962409135, 0.0021593968928921873, 0.002019911222314993, 0.0018929171140135768, 0.0017860518328882474, 0.0016894115385119035, 0.0016029159498513847, 0.0015269236618365065, 0.00145697466870751, 0.0013897299563216763, 0.0013207628411609433, 0.0012497238388302768, 0.0011763998411105114, 0.001105865352989322, 0.0010362761196274575, 0.0009697904399996699, 0.0009049435490367516, 0.0008419367799555683, 0.0007836071188217546, 0.0007272430023703008, 0.0006743569128979237, 0.0006244953957253751, 0.0005768186214039275, 0.0005322013022692768, 0.0004896094313297321, 0.0004500191996869169, 0.00041320224554701284, 0.0003793357556874329, 0.00034775597236468806, 0.0003186858661696328, 0.00029209810433698086, 0.0002674326070354132, 0.000244902906323876, 0.00022414578091566357, 0.00020490417189558276, 0.00018745649224967054, 0.00017124240552680102, 0.0001563971597445561, 0.00014280024354395906, 0.00013032337083978816, 0.00011885576022055155, 0.00010825355763191548, 9.848217953617675e-05, 8.96514061762343e-05, 8.156246279751171e-05, 7.418909746552269e-05, 6.750954767896893e-05, 6.144807451699403e-05, 5.593804843146306e-05, 5.082849262677689e-05, 4.6062338411059795e-05, 4.176512648987629e-05, 3.784603526419874e-05, 3.431751515141803e-05, 3.112658899529568e-05, 2.8262075096905312e-05, 2.567663868937784e-05, 2.3343204507026107e-05, 2.1237303494626467e-05, 1.9310938486117805e-05, 1.7557094761912376e-05, 1.596145586682996e-05, 1.4484998891723106e-05, 1.3151828744404333e-05, 1.1952885637067828e-05, 1.0879539466332915e-05, 9.894846038018403e-06, 8.997368429022407e-06, 8.192675132174886e-06, 7.475691893263595e-06, 6.819300245003914e-06, 6.215110846099328e-06, 5.667647160548497e-06, 5.175489443243115e-06, 4.725513207731677e-06, 4.314501473353244e-06, 3.936100494349384e-06, 3.5854602712629307e-06, 3.2617198034789107e-06, 2.9570768613577955e-06, 2.6848496672752514e-06, 2.4373808850072372e-06, 2.2088736022489823e-06, 2.0036589812953195e-06, 1.8116641009712223e-06, 1.639802784980864e-06, 1.4801308745678774e-06, 1.3374750492266488e-06, 1.208066926617587e-06, 1.0907751336298466e-06, 9.83841166039456e-07, 8.878314758153497e-07, 8.016406415783894e-07, 7.242731655121389e-07, 6.541110853151119e-07, 5.90534818230492e-07, 5.335617026753301e-07, 4.813608824954854e-07, 4.3416062781696456e-07, 3.9194115387546154e-07, 3.5371627544847256e-07, 3.19129729841677e-07, 2.882585532104512e-07, 2.6027655293528895e-07, 2.3493242022892515e-07, 2.119928315016362e-07, 1.912420075209113e-07, 1.7248105869606533e-07, 1.5581920961216214e-07, 1.4045431983220827e-07, 1.2644731907543746e-07, 1.1383371646131826e-07, 1.0259629331778898e-07, 9.245488227311804e-08, 8.330515085836357e-08, 7.49776895250842e-08, 6.748239411214696e-08, 6.073615826798492e-08, 5.466416533049909e-08, 4.926508759465392e-08], "duration": 9769.520681, "accuracy_train": [0.14871987951807228, 0.16653332078313254, 0.16813347138554216, 0.16963949548192772, 0.17126317771084337, 0.1706278237951807, 0.16966302710843373, 0.16891001506024098, 0.1687452936746988, 0.16881588855421686, 0.16796875, 0.16752164909638553, 0.16794521837349397, 0.16818053463855423, 0.16876882530120482, 0.17048663403614459, 0.17194559487951808, 0.1732398343373494, 0.1749105798192771, 0.17738140060240964, 0.17985222138554216, 0.18208772590361447, 0.18460560993975902, 0.18747646837349397, 0.1907003012048193, 0.19371234939759036, 0.19634789156626506, 0.20063064759036145, 0.20357210090361447, 0.20667827560240964, 0.2100432981927711, 0.21282003012048192, 0.21505553463855423, 0.21637330572289157, 0.21731457078313254, 0.21783226656626506, 0.2191500376506024, 0.21990304969879518, 0.2210090361445783, 0.22162085843373494, 0.22209149096385541, 0.22331513554216867, 0.22369164156626506, 0.22446818524096385, 0.22508000753012047, 0.2253153237951807, 0.22583301957831325, 0.2258565512048193, 0.22628012048192772, 0.22663309487951808, 0.2271507906626506, 0.22736257530120482, 0.22776261295180722, 0.22825677710843373, 0.22846856174698796, 0.2289156626506024, 0.2291980421686747, 0.22933923192771086, 0.22980986445783133, 0.2298804593373494, 0.23013930722891565, 0.23037462349397592, 0.23056287650602408, 0.23070406626506024, 0.23072759789156627, 0.23075112951807228, 0.23103350903614459, 0.23112763554216867, 0.23126882530120482, 0.23148060993975902, 0.23166886295180722, 0.2318335843373494, 0.23176298945783133, 0.2315512048192771, 0.2317159262048193, 0.2317394578313253, 0.23190417921686746, 0.2320218373493976, 0.23221009036144577, 0.23235128012048192, 0.23249246987951808, 0.23263365963855423, 0.23268072289156627, 0.23275131777108435, 0.2328219126506024, 0.23277484939759036, 0.23286897590361447, 0.23298663403614459, 0.2331278237951807, 0.23315135542168675, 0.2331984186746988, 0.2333160768072289, 0.23345726656626506, 0.2335043298192771, 0.23352786144578314, 0.2335984563253012, 0.2336925828313253, 0.23373964608433734, 0.2337867093373494, 0.23381024096385541, 0.23381024096385541, 0.23381024096385541, 0.23373964608433734, 0.23381024096385541, 0.23383377259036145, 0.23381024096385541, 0.23385730421686746, 0.2337867093373494, 0.23383377259036145, 0.23376317771084337, 0.23381024096385541, 0.23381024096385541, 0.23381024096385541, 0.2337867093373494, 0.23381024096385541, 0.23383377259036145, 0.23385730421686746, 0.23385730421686746, 0.23385730421686746, 0.2338808358433735, 0.23385730421686746, 0.23385730421686746, 0.2338808358433735, 0.2338808358433735, 0.2338808358433735, 0.23390436746987953, 0.23390436746987953, 0.23390436746987953, 0.23390436746987953, 0.23390436746987953, 0.23390436746987953, 0.23392789909638553, 0.23390436746987953, 0.2338808358433735, 0.2338808358433735, 0.23390436746987953, 0.23390436746987953, 0.23390436746987953, 0.2338808358433735, 0.2338808358433735, 0.2338808358433735, 0.2338808358433735, 0.23390436746987953], "end": "2016-01-17 17:50:42.250000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0], "accuracy_valid": [0.15517241379310345, 0.16945043103448276, 0.1693157327586207, 0.17174030172413793, 0.17376077586206898, 0.1749730603448276, 0.17470366379310345, 0.17335668103448276, 0.17214439655172414, 0.17066271551724138, 0.17039331896551724, 0.17066271551724138, 0.17214439655172414, 0.17254849137931033, 0.1744342672413793, 0.17685883620689655, 0.1783405172413793, 0.18009159482758622, 0.18036099137931033, 0.1814385775862069, 0.1853448275862069, 0.1866918103448276, 0.1883081896551724, 0.1927532327586207, 0.19679418103448276, 0.1984105603448276, 0.20083512931034483, 0.2048760775862069, 0.20662715517241378, 0.20864762931034483, 0.21336206896551724, 0.21578663793103448, 0.21942349137931033, 0.22063577586206898, 0.22103987068965517, 0.22225215517241378, 0.22306034482758622, 0.22359913793103448, 0.22427262931034483, 0.2248114224137931, 0.22561961206896552, 0.22588900862068967, 0.22588900862068967, 0.2265625, 0.22723599137931033, 0.2271012931034483, 0.2279094827586207, 0.22925646551724138, 0.22925646551724138, 0.23019935344827586, 0.23073814655172414, 0.2310075431034483, 0.23141163793103448, 0.23154633620689655, 0.23154633620689655, 0.23208512931034483, 0.2322198275862069, 0.2326239224137931, 0.2326239224137931, 0.23235452586206898, 0.2322198275862069, 0.2326239224137931, 0.23275862068965517, 0.2330280172413793, 0.23329741379310345, 0.2335668103448276, 0.2338362068965517, 0.23370150862068967, 0.23424030172413793, 0.23397090517241378, 0.23410560344827586, 0.23424030172413793, 0.23410560344827586, 0.23424030172413793, 0.234375, 0.23450969827586207, 0.23464439655172414, 0.23504849137931033, 0.23558728448275862, 0.23558728448275862, 0.23545258620689655, 0.23558728448275862, 0.23599137931034483, 0.23599137931034483, 0.2361260775862069, 0.2361260775862069, 0.2361260775862069, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.23679956896551724, 0.2369342672413793, 0.23679956896551724, 0.23666487068965517, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.23666487068965517, 0.23666487068965517, 0.23666487068965517, 0.23666487068965517, 0.23666487068965517, 0.23666487068965517, 0.23666487068965517, 0.23666487068965517, 0.23666487068965517, 0.23666487068965517, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931, 0.2365301724137931], "accuracy_test": 0.2320713141025641, "start": "2016-01-17 15:07:52.730000", "learning_rate_per_epoch": [0.00039522358565591276, 0.0003734799684025347, 0.00035293257678858936, 0.00033351563615724444, 0.0003151669225189835, 0.0002978277043439448, 0.00028144242241978645, 0.00026595857343636453, 0.0002513265935704112, 0.0002374996111029759, 0.00022443333000410348, 0.00021208589896559715, 0.00020041778043378145, 0.00018939159053843468, 0.00017897201178129762, 0.00016912567662075162, 0.0001598210510564968, 0.00015102833276614547, 0.0001427193492418155, 0.00013486749958246946, 0.0001274476235266775, 0.00012043596507282928, 0.00011381005606381223, 0.00010754867980722338, 0.00010163177648792043, 9.604039951227605e-05, 9.075663547264412e-05, 8.576356776757166e-05, 8.104519656626508e-05, 7.658640970475972e-05, 7.237293175421655e-05, 6.839125853730366e-05, 6.462864257628098e-05, 6.107303488533944e-05, 5.771303767687641e-05, 5.4537897085538134e-05, 5.153743768460117e-05, 4.8702051572036e-05, 4.602265835274011e-05, 4.3490676034707576e-05, 4.1097991925198585e-05, 3.88369444408454e-05, 3.67002903658431e-05, 3.468118666205555e-05, 3.277316500316374e-05, 3.097011722275056e-05, 2.9266264391480945e-05, 2.7656151360133663e-05, 2.6134619474760257e-05, 2.469679748173803e-05, 2.3338077880907804e-05, 2.2054109649616294e-05, 2.0840780052822083e-05, 1.96942037291592e-05, 1.8610706320032477e-05, 1.7586819012649357e-05, 1.6619262169115245e-05, 1.57049362314865e-05, 1.4840913536318112e-05, 1.4024425581737887e-05, 1.3252857570478227e-05, 1.2523738405434415e-05, 1.1834732504212297e-05, 1.1183632523170672e-05, 1.0568353900453076e-05, 9.986924851546064e-06, 9.437483640795108e-06, 8.918270395952277e-06, 8.42762256070273e-06, 7.963968528201804e-06, 7.525822638854152e-06, 7.111781997082289e-06, 6.720520104863681e-06, 6.350783678499283e-06, 6.001389010634739e-06, 5.6712165132921655e-06, 5.359208898880752e-06, 5.0643666327232495e-06, 4.785745204571867e-06, 4.522452400124166e-06, 4.273645117791602e-06, 4.038526185468072e-06, 3.816342541540507e-06, 3.6063825064047705e-06, 3.4079737361025764e-06, 3.2204804938373854e-06, 3.0433025131060276e-06, 2.875872041840921e-06, 2.717652932915371e-06, 2.568138370406814e-06, 2.4268495053547667e-06, 2.2933338641450973e-06, 2.167163756894297e-06, 2.0479349132074276e-06, 1.93526557268342e-06, 1.8287950069861836e-06, 1.728181928228878e-06, 1.6331042616002378e-06, 1.5432573263751692e-06, 1.4583534948542365e-06, 1.3781207144347718e-06, 1.3023019391766866e-06, 1.2306544476814452e-06, 1.1629487062236876e-06, 1.0989679140038788e-06, 1.0385070936536067e-06, 9.813725228013936e-07, 9.273813361687644e-07, 8.763605023887067e-07, 8.281466534754145e-07, 7.825853458598431e-07, 7.395306056423578e-07, 6.988445875322213e-07, 6.603969495699857e-07, 6.240645120669797e-07, 5.897309733882139e-07, 5.572863415181928e-07, 5.266266498438199e-07, 4.976537297807226e-07, 4.7027481286932016e-07, 4.444021612926008e-07, 4.199529257675749e-07, 3.9684877606305236e-07, 3.7501573046938574e-07, 3.5438384315966687e-07, 3.3488706208117947e-07, 3.164629163165955e-07, 2.9905237397542805e-07, 2.8259970008548407e-07, 2.670522007974796e-07, 2.5236005285478313e-07, 2.3847621832828736e-07, 2.2535620303187898e-07, 2.1295799967901985e-07, 2.012419031416357e-07, 1.9017036834156897e-07, 1.7970795340715995e-07, 1.6982113493213546e-07, 1.6047825113219005e-07, 1.516493739472935e-07, 1.4330622377656255e-07, 1.3542208421313262e-07, 1.279717025681748e-07], "accuracy_train_last": 0.23390436746987953, "error_valid": [0.8448275862068966, 0.8305495689655172, 0.8306842672413793, 0.8282596982758621, 0.826239224137931, 0.8250269396551724, 0.8252963362068966, 0.8266433189655172, 0.8278556034482758, 0.8293372844827587, 0.8296066810344828, 0.8293372844827587, 0.8278556034482758, 0.8274515086206897, 0.8255657327586207, 0.8231411637931034, 0.8216594827586207, 0.8199084051724138, 0.8196390086206897, 0.8185614224137931, 0.8146551724137931, 0.8133081896551724, 0.8116918103448276, 0.8072467672413793, 0.8032058189655172, 0.8015894396551724, 0.7991648706896551, 0.7951239224137931, 0.7933728448275862, 0.7913523706896551, 0.7866379310344828, 0.7842133620689655, 0.7805765086206897, 0.779364224137931, 0.7789601293103449, 0.7777478448275862, 0.7769396551724138, 0.7764008620689655, 0.7757273706896551, 0.7751885775862069, 0.7743803879310345, 0.7741109913793103, 0.7741109913793103, 0.7734375, 0.7727640086206897, 0.7728987068965517, 0.7720905172413793, 0.7707435344827587, 0.7707435344827587, 0.7698006465517242, 0.7692618534482758, 0.7689924568965517, 0.7685883620689655, 0.7684536637931034, 0.7684536637931034, 0.7679148706896551, 0.7677801724137931, 0.7673760775862069, 0.7673760775862069, 0.767645474137931, 0.7677801724137931, 0.7673760775862069, 0.7672413793103449, 0.7669719827586207, 0.7667025862068966, 0.7664331896551724, 0.7661637931034483, 0.7662984913793103, 0.7657596982758621, 0.7660290948275862, 0.7658943965517242, 0.7657596982758621, 0.7658943965517242, 0.7657596982758621, 0.765625, 0.7654903017241379, 0.7653556034482758, 0.7649515086206897, 0.7644127155172413, 0.7644127155172413, 0.7645474137931034, 0.7644127155172413, 0.7640086206896551, 0.7640086206896551, 0.7638739224137931, 0.7638739224137931, 0.7638739224137931, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7632004310344828, 0.7630657327586207, 0.7632004310344828, 0.7633351293103449, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7633351293103449, 0.7633351293103449, 0.7633351293103449, 0.7633351293103449, 0.7633351293103449, 0.7633351293103449, 0.7633351293103449, 0.7633351293103449, 0.7633351293103449, 0.7633351293103449, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069, 0.7634698275862069], "accuracy_train_std": [0.030822227077038026, 0.03331103502656325, 0.03328083352527371, 0.032805037006567124, 0.0334601856316963, 0.03430813498236532, 0.03379210390655625, 0.033438797301145004, 0.03422546251580148, 0.03397037860480576, 0.033386029486760074, 0.03293952842012573, 0.03281394824987134, 0.032818875398369605, 0.03251713191723856, 0.0324633691809116, 0.03248164946027873, 0.032325329466000925, 0.03274069621454861, 0.03258364766184049, 0.03340764221570545, 0.03366476958437183, 0.03405775967436583, 0.03400256518563072, 0.034870853476231145, 0.03505956492848003, 0.03429642332567677, 0.03479661642193051, 0.03542949538230524, 0.035904751279459614, 0.03630529620396682, 0.036211679212231715, 0.03649999530600529, 0.036347360418912775, 0.036107630401559475, 0.03565322541502318, 0.035678252928365475, 0.035444683785299164, 0.0355494891482565, 0.03563968740219313, 0.03556247759594326, 0.036048639273460435, 0.035917156457471705, 0.0360499218809356, 0.03613117842874479, 0.03631260892126177, 0.036458038849296534, 0.03647614645964635, 0.03676799887010019, 0.03664631352121335, 0.036701817017665436, 0.03683032584589105, 0.03677699630420736, 0.03674003630254148, 0.036792170251982365, 0.03669370660813276, 0.03634215743109806, 0.03666408650035545, 0.03675067543305626, 0.03673185895626759, 0.036649917154842455, 0.036776461790131665, 0.03675646086260738, 0.03685073747997315, 0.03679566178268586, 0.03682545424091182, 0.03685214994349379, 0.036930322022634514, 0.03690263250832902, 0.03695219945145908, 0.036781994764066815, 0.03665106540809708, 0.036688718759203175, 0.03648959417833962, 0.036610091298079836, 0.03676462520597582, 0.036738551703016285, 0.03663855355138904, 0.03648425211153326, 0.036572871781646754, 0.036625614094925665, 0.036637616501277705, 0.03663982306492396, 0.036720701667629224, 0.03664106230951937, 0.03666411670632924, 0.03661291205191286, 0.036594962904226355, 0.03661511254274521, 0.03661339602000409, 0.03658982539225701, 0.03660095449600939, 0.03662485058465591, 0.03668117155634329, 0.03670927701184414, 0.03665821851795297, 0.036655076457872054, 0.03677609289560488, 0.03674186747719077, 0.03671470699451981, 0.036729725763826666, 0.036729725763826666, 0.03677609289560487, 0.03683967631911765, 0.036827551816499146, 0.03684466627561599, 0.03690020155923639, 0.03679686568197313, 0.03682255954075943, 0.036808970205207, 0.03679473625832842, 0.03677974403564384, 0.03677974403564384, 0.03678687212428992, 0.03680972237483486, 0.03679758800263938, 0.036845357601827514, 0.036845357601827514, 0.036845357601827514, 0.036858152309592496, 0.036790431887935564, 0.036790431887935564, 0.03681323480352833, 0.03681323480352833, 0.03681323480352833, 0.0368160399930122, 0.0368160399930122, 0.0368160399930122, 0.0368160399930122, 0.0368160399930122, 0.0368160399930122, 0.0368437870702827, 0.03683600857074095, 0.03686812654591269, 0.03686812654591269, 0.036875913286818454, 0.036875913286818454, 0.036875913286818454, 0.036878098084550746, 0.036878098084550746, 0.036878098084550746, 0.036878098084550746, 0.036875913286818454], "accuracy_test_std": 0.03794699238682595, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.7772467377048317, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0004182330995566433, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adadelta", "nb_data_augmentation": 1, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.669062219977263e-10, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.055016023145793375}, "accuracy_valid_max": 0.2369342672413793, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256, 512],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.2365301724137931, "loss_train": [2.311864137649536, 2.2953009605407715, 2.2826716899871826, 2.271941900253296, 2.2620179653167725, 2.25270676612854, 2.243715763092041, 2.2347424030303955, 2.226137399673462, 2.2176716327667236, 2.2095847129821777, 2.201704263687134, 2.1939103603363037, 2.18650484085083, 2.1797373294830322, 2.1732380390167236, 2.1669118404388428, 2.1609575748443604, 2.155392646789551, 2.150272846221924, 2.145203113555908, 2.1410090923309326, 2.1369128227233887, 2.133143901824951, 2.129486322402954, 2.1261401176452637, 2.1228578090667725, 2.1199469566345215, 2.117579221725464, 2.114894151687622, 2.1124894618988037, 2.110280752182007, 2.108642578125, 2.1067299842834473, 2.104840040206909, 2.103410005569458, 2.101940631866455, 2.1006505489349365, 2.099280595779419, 2.0980114936828613, 2.0970418453216553, 2.095841884613037, 2.094971179962158, 2.0943076610565186, 2.093194007873535, 2.092668294906616, 2.0914669036865234, 2.0910089015960693, 2.090123176574707, 2.0894031524658203, 2.088977098464966, 2.0886404514312744, 2.0879595279693604, 2.0873961448669434, 2.0872952938079834, 2.086620330810547, 2.086221694946289, 2.0857841968536377, 2.085695505142212, 2.0852179527282715, 2.0850841999053955, 2.0845484733581543, 2.0842559337615967, 2.083728075027466, 2.083804130554199, 2.083448886871338, 2.0834426879882812, 2.0831072330474854, 2.0829813480377197, 2.0830063819885254, 2.0825929641723633, 2.0823776721954346, 2.0823628902435303, 2.0822370052337646, 2.0821704864501953, 2.0819597244262695, 2.081702947616577, 2.0816562175750732, 2.0813519954681396, 2.0813405513763428, 2.0814361572265625, 2.0813794136047363, 2.0812466144561768, 2.081425905227661, 2.081048011779785, 2.0811522006988525, 2.080784797668457, 2.0809707641601562, 2.080904483795166, 2.0807290077209473, 2.0806453227996826, 2.080660581588745, 2.0804340839385986, 2.0802671909332275, 2.080343246459961, 2.080582618713379, 2.0804107189178467, 2.0804145336151123, 2.080247163772583, 2.0802712440490723, 2.0804436206817627, 2.0802581310272217, 2.0803146362304688, 2.080237865447998, 2.080045461654663, 2.0797955989837646, 2.080085039138794, 2.080010414123535, 2.0802807807922363, 2.0802533626556396, 2.0801639556884766, 2.0802009105682373, 2.080104112625122, 2.0803494453430176, 2.0800492763519287, 2.079925775527954, 2.0799427032470703, 2.0797536373138428, 2.0801734924316406, 2.0800278186798096, 2.080038070678711, 2.0800509452819824, 2.0799129009246826, 2.0800960063934326, 2.0799057483673096, 2.0799920558929443, 2.0802395343780518, 2.0798757076263428, 2.080197811126709, 2.0798285007476807, 2.0797007083892822, 2.0800364017486572, 2.0799736976623535, 2.08003830909729, 2.0799033641815186, 2.0799219608306885, 2.08009934425354, 2.0799124240875244, 2.079603433609009, 2.0800023078918457, 2.0798556804656982, 2.080005168914795, 2.0797479152679443], "accuracy_train_first": 0.14871987951807228, "model": "residual", "loss_std": [0.01414559967815876, 0.01190540287643671, 0.011718769557774067, 0.012343804351985455, 0.01351086888462305, 0.014779563993215561, 0.015960201621055603, 0.017406504601240158, 0.018809104338288307, 0.02031341940164566, 0.021506240591406822, 0.02255278266966343, 0.02382153831422329, 0.025202417746186256, 0.026106148958206177, 0.02735094539821148, 0.028391234576702118, 0.029054293408989906, 0.030126038938760757, 0.030759301036596298, 0.031446024775505066, 0.032357919961214066, 0.03274301812052727, 0.03374422714114189, 0.03436557203531265, 0.03500940278172493, 0.03534932807087898, 0.035819441080093384, 0.03654474765062332, 0.03683863952755928, 0.037322115153074265, 0.03783216327428818, 0.03812451288104057, 0.038496069610118866, 0.03879270330071449, 0.03934638574719429, 0.03934406489133835, 0.03961322829127312, 0.039880190044641495, 0.040206532925367355, 0.04049026593565941, 0.04080214723944664, 0.04078160971403122, 0.04115282744169235, 0.04130633547902107, 0.04170738533139229, 0.041640132665634155, 0.04165666550397873, 0.041978541761636734, 0.042007867246866226, 0.04194684699177742, 0.04234009236097336, 0.04235260188579559, 0.04232994094491005, 0.042378515005111694, 0.04263829067349434, 0.042697276920080185, 0.04283984750509262, 0.04314441978931427, 0.04311571270227432, 0.04316479712724686, 0.04304127022624016, 0.04302367940545082, 0.04323268309235573, 0.043159205466508865, 0.04346884787082672, 0.04341670870780945, 0.0433182418346405, 0.04338835924863815, 0.043783027678728104, 0.04338805377483368, 0.043444208800792694, 0.043423477560281754, 0.043509166687726974, 0.04364965856075287, 0.04364815726876259, 0.04361864551901817, 0.04393152892589569, 0.043748773634433746, 0.04362557828426361, 0.04388358071446419, 0.043908823281526566, 0.04365593194961548, 0.04406474158167839, 0.0438002273440361, 0.043915022164583206, 0.04396626353263855, 0.044002726674079895, 0.04398740828037262, 0.04391101002693176, 0.04397651553153992, 0.044016409665346146, 0.04387330636382103, 0.04390154778957367, 0.04409045726060867, 0.04403909668326378, 0.04416588693857193, 0.04402787610888481, 0.04408666491508484, 0.043879080563783646, 0.04422461614012718, 0.04402189701795578, 0.04417454078793526, 0.043929461389780045, 0.04411277547478676, 0.0438339002430439, 0.04416334256529808, 0.04408566281199455, 0.0442989319562912, 0.0441267304122448, 0.04407183453440666, 0.04425535351037979, 0.04427764564752579, 0.04405143857002258, 0.044071171432733536, 0.044105492532253265, 0.04393932968378067, 0.04407939687371254, 0.04393678903579712, 0.04398832470178604, 0.04414862394332886, 0.04416629299521446, 0.04383888095617294, 0.044000446796417236, 0.04422557353973389, 0.04403848946094513, 0.04428746923804283, 0.04418762028217316, 0.0442567802965641, 0.04423099383711815, 0.04384658858180046, 0.044231850653886795, 0.044260844588279724, 0.04418691620230675, 0.044342417269945145, 0.04400766268372536, 0.04431331157684326, 0.04403908550739288, 0.04404080659151077, 0.044027723371982574, 0.044060491025447845, 0.043927524238824844, 0.04396507143974304]}, "state": "available", "life": [{"dt": "Sun May 15 22:04:58 2016", "state": "available"}], "summary": "f6d234a83eae0429742bfee44fb19c5f"}