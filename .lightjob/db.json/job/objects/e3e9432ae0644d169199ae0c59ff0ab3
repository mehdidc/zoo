{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 32, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [2.2685282230377197, 1.8868365287780762, 1.773066520690918, 1.7094752788543701, 1.6668109893798828, 1.634283423423767, 1.6073530912399292, 1.5839564800262451, 1.5630806684494019, 1.544258952140808, 1.527207612991333, 1.5117782354354858, 1.4977720975875854, 1.4850810766220093, 1.4736289978027344, 1.4632500410079956, 1.4538418054580688, 1.4453119039535522, 1.4376180171966553, 1.4306302070617676, 1.4243165254592896, 1.4185750484466553, 1.4133552312850952, 1.4086037874221802, 1.404275894165039, 1.4003386497497559, 1.3967459201812744, 1.393467903137207, 1.3904801607131958, 1.3877454996109009, 1.3852390050888062, 1.382954478263855, 1.3808567523956299, 1.3789350986480713, 1.3771750926971436, 1.3755686283111572, 1.3740931749343872, 1.3727432489395142, 1.3715070486068726, 1.3703737258911133, 1.3693339824676514, 1.3683806657791138, 1.3675063848495483, 1.3667027950286865, 1.3659659624099731, 1.3652899265289307, 1.3646694421768188, 1.3640990257263184, 1.363576054573059, 1.3630951642990112, 1.3626534938812256, 1.362248420715332, 1.3618764877319336, 1.3615351915359497, 1.3612213134765625, 1.3609330654144287, 1.360668420791626, 1.3604257106781006, 1.3602029085159302, 1.3599987030029297, 1.3598109483718872, 1.3596386909484863, 1.359480857849121, 1.359336018562317, 1.3592033386230469, 1.3590818643569946, 1.3589707612991333, 1.3588690757751465, 1.3587760925292969, 1.3586913347244263, 1.3586139678955078, 1.3585435152053833, 1.3584794998168945, 1.3584212064743042, 1.3583682775497437, 1.3583203554153442, 1.3582768440246582, 1.358237624168396, 1.3582024574279785, 1.358170747756958, 1.3581422567367554, 1.358116865158081, 1.3580940961837769, 1.3580739498138428, 1.3580560684204102, 1.3580398559570312, 1.3580255508422852, 1.3580129146575928, 1.358001470565796, 1.3579915761947632, 1.3579825162887573, 1.357974886894226, 1.3579678535461426, 1.3579617738723755, 1.3579564094543457, 1.3579517602920532, 1.357947826385498, 1.357944369316101, 1.3579413890838623, 1.3579387664794922, 1.3579366207122803, 1.357934832572937, 1.3579331636428833, 1.3579318523406982, 1.3579305410385132, 1.3579295873641968, 1.35792875289917, 1.3579280376434326, 1.3579273223876953, 1.3579267263412476, 1.357926368713379, 1.3579260110855103, 1.357925534248352, 1.3579251766204834, 1.3579250574111938, 1.3579248189926147, 1.357924461364746, 1.357924461364746, 1.3579243421554565, 1.3579241037368774, 1.357923984527588, 1.357923984527588, 1.357923984527588, 1.3579237461090088, 1.3579237461090088, 1.3579237461090088, 1.3579237461090088, 1.3579237461090088, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192, 1.3579235076904297, 1.3579236268997192, 1.3579236268997192, 1.3579236268997192], "moving_avg_accuracy_train": [0.026939138058324097, 0.05718380499607788, 0.08853736279819582, 0.11916820411273762, 0.1484843650541309, 0.17623148315258455, 0.20230135967928806, 0.22677568828546407, 0.24971404236435582, 0.27115601497938757, 0.2911394208411756, 0.30967990829171105, 0.32684053315906075, 0.3426897435301702, 0.35744460321415944, 0.37110057893925513, 0.38384414481678586, 0.39567843851847756, 0.40656643197975273, 0.41659817702467156, 0.4258545400507943, 0.4343620501814661, 0.44220246000620433, 0.4494750316389357, 0.4562063219643371, 0.4623620674095608, 0.46800450880906247, 0.4732244319995387, 0.478024705467405, 0.4824286569456276, 0.4864805689307898, 0.49020634477695957, 0.49365491018852165, 0.49684232441607035, 0.49976680079229274, 0.5024732342927977, 0.5049415765265854, 0.5072281166060236, 0.5093045317703567, 0.5112012072039708, 0.5129268162846996, 0.5145170307894892, 0.5159644998854664, 0.5172997381063605, 0.5185200536956415, 0.5196415892140895, 0.52066492207355, 0.5215859216470643, 0.5224194715608462, 0.523174316780869, 0.5238676283717467, 0.5245055957452125, 0.5251030178694269, 0.5256639492693151, 0.5261943641661193, 0.5266926639125288, 0.5271643851723926, 0.527593584603889, 0.527993814985093, 0.5283493720305575, 0.5286716985202851, 0.5289687678074685, 0.5292454307611716, 0.5294990777171235, 0.5297273599774801, 0.5299351391606106, 0.5301221404254282, 0.5302950918613829, 0.530462337848971, 0.5306128592378003, 0.5307506536365563, 0.5308793188930556, 0.5309974427727147, 0.5311084045620268, 0.5312083062212265, 0.5312935674168872, 0.5313726276417913, 0.5314507572906335, 0.5315210739745916, 0.5315843589901539, 0.5316459658017789, 0.531703737081051, 0.5317580563812054, 0.531813919197773, 0.5318641957326837, 0.5319094446141034, 0.5319501686073811, 0.531986820201331, 0.532019806635886, 0.5320494944269855, 0.532080863736594, 0.5321090961152417, 0.5321391555536438, 0.5321662090482056, 0.5321882320445016, 0.5322057275923586, 0.5322191484366203, 0.5322312271964559, 0.5322444232291175, 0.5322539745097034, 0.5322648958110401, 0.5322770501310528, 0.5322879890190642, 0.5322978340182744, 0.5323090196663731, 0.5323214118984715, 0.5323302397585505, 0.5323405099814311, 0.5323497531820237, 0.5323557469137475, 0.5323588161234895, 0.5323615784122572, 0.5323640644721481, 0.53236630192605, 0.5323683156345616, 0.532370127972222, 0.5323717590761164, 0.5323709019208119, 0.5323724556298474, 0.5323738539679793, 0.5323751124722981, 0.5323762451261849, 0.532377264514683, 0.5323781819643314, 0.5323790076690149, 0.5323797508032301, 0.5323804196240237, 0.532381021562738, 0.5323815633075809, 0.5323820508779394, 0.5323824896912621, 0.5323828846232526, 0.532383240062044, 0.5323835599569562, 0.5323838478623772, 0.5323841069772561, 0.5323843401806472, 0.5323845500636991, 0.5323847389584458, 0.5323849089637179, 0.5323850619684628, 0.5323851996727331, 0.5323853236065764, 0.5323854351470354, 0.5323855355334485, 0.5323856258812203, 0.5323857071942149, 0.53238578037591, 0.5323858462394356, 0.5323859055166087, 0.5323859588660644, 0.5323860068805746, 0.5323860500936337, 0.532386088985387, 0.5323861239879649, 0.5323861554902851, 0.5323861838423732, 0.5323862093592525, 0.5323862323244439, 0.5323862529931161, 0.5323862715949211, 0.5323862883365457, 0.5323863034040077, 0.5323863169647236], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.026534114975527108, 0.05588062700018825, 0.08668716614504893, 0.11688401923122174, 0.1457275203107351, 0.17293693601309534, 0.19844374128226774, 0.22205698669470364, 0.24410854164910376, 0.26448293197779277, 0.2837507061745918, 0.30147115042912054, 0.3182944855462235, 0.33393803445018555, 0.3482603395800917, 0.36140367332727835, 0.3737728716007252, 0.38508722600691775, 0.3956292964220392, 0.4052758512018985, 0.41431484293600085, 0.4227083421702622, 0.4304944250748474, 0.4374520420553145, 0.44381155358773483, 0.44958394209191316, 0.45482791987067367, 0.45964618563021775, 0.4640060093676478, 0.4679786788563348, 0.47168835873990317, 0.4751867915500243, 0.47849510199404294, 0.4814705223763404, 0.4842084063679985, 0.48671015256290046, 0.48894951710706225, 0.49107583798671745, 0.4930637984745668, 0.4949139980698812, 0.4965791777056641, 0.4980534253153688, 0.4994545198602626, 0.5007032979194171, 0.5018149911414062, 0.5028399291036962, 0.5037379592072573, 0.5046316355192123, 0.5054725652937218, 0.5062161655508707, 0.5069352634159643, 0.5075824514945486, 0.5081405067027744, 0.5086915845151777, 0.5091997615775907, 0.5096571209337624, 0.5100687443543168, 0.5104514124640659, 0.51079581376284, 0.5111057749317367, 0.5113969470149937, 0.5116590018899251, 0.5118948512773633, 0.5121559438510578, 0.5123909271673828, 0.5125902051208252, 0.5127573482476734, 0.5129077770618368, 0.5130431629945837, 0.5131528033028061, 0.5132636866114562, 0.5133634815892413, 0.5134532970692478, 0.5135463380325037, 0.5136300748994341, 0.5137054380796715, 0.5137732649418851, 0.5138343091178773, 0.5138892488762703, 0.513938694658824, 0.5139954028943724, 0.5140464403063659, 0.51409237397716, 0.5141337142808747, 0.514183127585468, 0.5142398065908519, 0.5142908176956974, 0.5143367276900583, 0.5143780466849832, 0.5144152337804156, 0.5144487021663048, 0.514478823713605, 0.5145059331061752, 0.5145303315594884, 0.5145522901674702, 0.5145720529146539, 0.5145898393871192, 0.514605847212338, 0.514620254255035, 0.5146332205934622, 0.5146448902980466, 0.5146553930321727, 0.5146770525241362, 0.5146965460669033, 0.5147140902553937, 0.5147298800250351, 0.5147440908177123, 0.5147568805311218, 0.5147683912731904, 0.5147787509410521, 0.5147880746421276, 0.5147964659730956, 0.5148040181709668, 0.5148230221803008, 0.5148401257887015, 0.5148555190362621, 0.5148693729590667, 0.5148818414895907, 0.5148930631670624, 0.5149031626767869, 0.5149000452042889, 0.5148972394790408, 0.5148947143263174, 0.5148924416888664, 0.5148903963151605, 0.5148885554788252, 0.5148868987261234, 0.5148854076486917, 0.5148840656790032, 0.5148828579062836, 0.514881770910836, 0.514880792614933, 0.5148799121486205, 0.5148791197289392, 0.514878406551226, 0.5148777646912841, 0.5148771870173364, 0.5148766671107835, 0.5148761991948858, 0.5148757780705779, 0.5148753990587009, 0.5148750579480116, 0.5148747509483911, 0.5148744746487327, 0.5148742259790402, 0.514874002176317, 0.514873800753866, 0.5148736194736601, 0.5148734563214747, 0.5148733094845079, 0.5148731773312378, 0.5148730583932948, 0.514872951349146, 0.5148728550094122, 0.5148727683036516, 0.5148726902684672, 0.5148726200368012, 0.5148725568283019, 0.5148724999406524, 0.514872448741768, 0.5148724026627719, 0.5148723611916755, 0.5148723238676887, 0.5148722902761005], "moving_var_accuracy_train": [0.006531454433929013, 0.014110967894117022, 0.021547281386362076, 0.02783678920445564, 0.03278804591508513, 0.03643836438850242, 0.038911274108710286, 0.040411081544347034, 0.04110548618054982, 0.04113276126910878, 0.04061351373073042, 0.039645909431788535, 0.03833170190115081, 0.03675930893552488, 0.03504273100062029, 0.03321682895759788, 0.03135673230331278, 0.029481513639742597, 0.027600297890283274, 0.02574599127907122, 0.023942514459406445, 0.022199662571077146, 0.020532944549948095, 0.018955662778332164, 0.017467888925502526, 0.01606213885082981, 0.01474245927026812, 0.013513441726271547, 0.012369481181941108, 0.011307086161349848, 0.010324139461834376, 0.009416658166554062, 0.008582025780479056, 0.007815259687552971, 0.00711070677747342, 0.006465559140359977, 0.005873837646771883, 0.005333508271908589, 0.00483896094412978, 0.004387441249021077, 0.003975496664414413, 0.003600706037514159, 0.003259491934817026, 0.0029495884912941516, 0.0026680321734017143, 0.0024125495333338076, 0.002180719471271689, 0.001970281686074242, 0.0017795067665957109, 0.001606684211691861, 0.0014503419191810828, 0.0013089707485894361, 0.0011812858924810006, 0.0010659890995513258, 0.0009619222492609592, 0.0008679647480703091, 0.0007831709617863453, 0.0007065117749756824, 0.0006373022567004621, 0.0005747098183436309, 0.0005181738858030887, 0.0004671507486752686, 0.0004211245553173074, 0.00037959113078994954, 0.0003421010328244966, 0.0003082794792425285, 0.0002777662565756657, 0.0002502588407108885, 0.00022548469762307874, 0.00020314013805722696, 0.0001829970099184609, 0.00016484630166068538, 0.0001484872507531281, 0.00013374933834600148, 0.00012046422758499899, 0.00010848323006986848, 9.769116173533866e-05, 8.797698374005876e-05, 7.92237852904386e-05, 7.133745170014717e-05, 6.423786512327993e-05, 5.784411629733054e-05, 5.208625994492087e-05, 4.69057198389025e-05, 4.223789742467596e-05, 3.803253483363598e-05, 3.4244207342928755e-05, 3.0831876662687536e-05, 2.775848194020063e-05, 2.499056603064386e-05, 2.250036572984734e-05, 2.0257502761699585e-05, 1.8239884614063014e-05, 1.6422483176768757e-05, 1.47845999703846e-05, 1.3308894821099476e-05, 1.197962641053581e-05, 1.0782976837434716e-05, 9.706246371193283e-06, 8.736442776721421e-06, 7.86387197245528e-06, 7.078814322664479e-06, 6.372009823836358e-06, 5.735681157537767e-06, 5.163239110294488e-06, 4.64829730601246e-06, 4.184168955433386e-06, 3.7667013571922096e-06, 3.3908001522877404e-06, 3.052043460438758e-06, 2.7469238948308393e-06, 2.472300177500881e-06, 2.225125784194824e-06, 2.002658261575007e-06, 1.8024289306152356e-06, 1.6222155986638728e-06, 1.4600179832967158e-06, 1.314022797403989e-06, 1.1826422437694917e-06, 1.0643956175383227e-06, 9.579703102825724e-07, 8.621848253977615e-07, 7.75975695234177e-07, 6.983857011354745e-07, 6.285532671159463e-07, 5.657029106405073e-07, 5.091366454677425e-07, 4.5822624189290986e-07, 4.1240625909089157e-07, 3.711677727054933e-07, 3.340527284491336e-07, 3.006488593457139e-07, 2.705851104417523e-07, 2.43527520392371e-07, 2.19175514358917e-07, 1.9725856718770962e-07, 1.7753319992333295e-07, 1.5978027638905903e-07, 1.4380256988118122e-07, 1.2942257300919586e-07, 1.164805264023438e-07, 1.0483264442430411e-07, 9.434951821825141e-08, 8.491467836789223e-08, 7.642330122799045e-08, 6.878104456967022e-08, 6.190299961893104e-08, 5.57127478570825e-08, 5.014151211341034e-08, 4.512739252611855e-08, 4.061467888898658e-08, 3.655323174862664e-08, 3.289792538008034e-08, 2.960814645518856e-08, 2.6647342836293873e-08, 2.398261748423006e-08, 2.1584362970375168e-08, 1.9425932533337825e-08, 1.7483344026604182e-08, 1.5735013468689878e-08, 1.4161515236065244e-08, 1.2745366234996647e-08, 1.1470831654752704e-08, 1.0323750144314569e-08], "duration": 68557.419142, "accuracy_train": [0.269391380583241, 0.32938580743586193, 0.3707193830172573, 0.3948457759436139, 0.41232981352667036, 0.4259555460386674, 0.4369302484196198, 0.44704464574104835, 0.45615922907438167, 0.4641337685146733, 0.47099007359726835, 0.4765442953465301, 0.4812861569652086, 0.4853326368701551, 0.4902383403700628, 0.4940043604651163, 0.4985362377145626, 0.5021870818337025, 0.5045583731312293, 0.5068838824289406, 0.5091618072858989, 0.5109296413575121, 0.5127661484288483, 0.514928176333518, 0.5167879348929494, 0.5177637764165743, 0.5187864814045773, 0.5202037407138244, 0.5212271666782022, 0.5220642202496308, 0.5229477767972499, 0.5237383273924879, 0.5246919988925802, 0.5255290524640088, 0.5260870881782945, 0.5268311357973422, 0.5271566566306755, 0.5278069773209672, 0.527992268249354, 0.5282712861064969, 0.5284572980112587, 0.528828961332595, 0.5289917217492617, 0.5293168820944075, 0.5295028939991694, 0.5297354088801218, 0.5298749178086932, 0.5298749178086932, 0.5299214207848837, 0.5299679237610742, 0.5301074326896457, 0.5302473021064046, 0.5304798169873569, 0.5307123318683094, 0.5309680982373569, 0.5311773616302141, 0.5314098765111664, 0.5314563794873569, 0.5315958884159284, 0.531549385439738, 0.5315726369278332, 0.5316423913921189, 0.5317353973444998, 0.5317819003206903, 0.5317819003206903, 0.5318051518087855, 0.5318051518087855, 0.531851654784976, 0.5319675517372646, 0.5319675517372646, 0.5319908032253599, 0.5320373062015504, 0.5320605576896457, 0.5321070606658361, 0.5321074211540237, 0.5320609181778332, 0.5320841696659284, 0.5321539241302141, 0.5321539241302141, 0.5321539241302141, 0.5322004271064046, 0.5322236785944998, 0.532246930082595, 0.5323166845468807, 0.5323166845468807, 0.5323166845468807, 0.5323166845468807, 0.5323166845468807, 0.5323166845468807, 0.5323166845468807, 0.5323631875230712, 0.5323631875230712, 0.5324096904992617, 0.5324096904992617, 0.5323864390111664, 0.5323631875230712, 0.532339936034976, 0.532339936034976, 0.5323631875230712, 0.532339936034976, 0.5323631875230712, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5324096904992617, 0.5324329419873569, 0.5324096904992617, 0.5324329419873569, 0.5324329419873569, 0.5324096904992617, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323631875230712, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664, 0.5323864390111664], "end": "2016-01-31 17:36:24.261000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0], "moving_var_accuracy_valid": [0.006336533317810429, 0.013453839898151618, 0.020649841595090965, 0.026791506862353008, 0.03159988416683271, 0.03510306647592406, 0.03744813386368698, 0.038721588707489596, 0.03922586951990299, 0.03903932459930468, 0.03847661624186373, 0.037455081918878114, 0.03625679516715146, 0.034833601251232166, 0.033196396944216174, 0.031431482247703965, 0.029665307616284137, 0.027850908395316083, 0.02606603479352089, 0.024296935486256246, 0.02260257228175315, 0.02097637251813775, 0.019424343049297646, 0.017917584650789832, 0.016489816668089737, 0.015140719222669295, 0.013874141026899569, 0.012695668088575966, 0.011597173846911495, 0.01057949538801745, 0.009645401372762668, 0.00879101252462879, 0.008010415534111935, 0.007289052118763266, 0.006627610985652939, 0.006021178493301004, 0.0054641934260257425, 0.004958465247772488, 0.0044981866051064905, 0.004079177091478356, 0.003696214791305357, 0.0033461539663073004, 0.003029206162990152, 0.002740320566460367, 0.002477411266192677, 0.0022391246200122996, 0.0020224702806131874, 0.001827411168706814, 0.0016510345178070418, 0.0014909075381082247, 0.0013464706999536426, 0.001215593301639833, 0.001096836802014702, 0.0009898863026111405, 0.0008932218676908903, 0.0008057822791479011, 0.0007267289557962518, 0.0006553739741565966, 0.0005909040870323127, 0.000532678361665096, 0.00048017355613720066, 0.00043277425534075817, 0.0003899974542086772, 0.0003516112327761547, 0.0003169470639290988, 0.0002856097628607429, 0.00025730021799834195, 0.00023177385565168317, 0.00020876143424358693, 0.00018799347979391205, 0.0001693047877877556, 0.0001524639403473002, 0.00013729014769660954, 0.00012363904251454092, 0.00011133824502903662, 0.00010025553700655228, 9.02713876550367e-05, 8.127778641233614e-05, 7.317717316457305e-05, 6.588145981682687e-05, 5.9322256250955214e-05, 5.3413473982666624e-05, 4.809111570341358e-05, 4.3297385319473244e-05, 3.898962185956335e-05, 3.5119572260468816e-05, 3.163103422978e-05, 2.848690035504203e-05, 2.565357565361225e-05, 2.3100664008851313e-05, 2.0800678803652414e-05, 1.872877669179302e-05, 1.686251329510346e-05, 1.5181619526309803e-05, 1.3667797197859344e-05, 1.2304532573659633e-05, 1.1076926543718511e-05, 9.97154014356078e-06, 8.97625419511814e-06, 8.08014190899621e-06, 7.2733533561423976e-06, 6.547010787345263e-06, 5.89653191093977e-06, 5.310298703732308e-06, 4.782039020307155e-06, 4.306078969704382e-06, 3.8772885923905765e-06, 3.491031924073391e-06, 3.1431212063127683e-06, 2.8297749901453306e-06, 2.5475798737465076e-06, 2.2934556162905815e-06, 2.06462337589569e-06, 1.8614114096430368e-06, 1.6779030694616335e-06, 1.5122453311496181e-06, 1.3627481786283157e-06, 1.227872539046348e-06, 1.1062186195492128e-06, 9.965147584643651e-07, 8.969507503309089e-07, 8.073265241453319e-07, 7.26651259297285e-07, 6.540326172964104e-07, 5.886670075491412e-07, 5.298308048999484e-07, 4.768724278755878e-07, 4.292051948951926e-07, 3.8630088334947583e-07, 3.476839234490082e-07, 3.129261651360361e-07, 2.8164216218829466e-07, 2.5348492295781356e-07, 2.2814208202259443e-07, 2.0533245142239037e-07, 1.8480291413781617e-07, 1.663256260887431e-07, 1.4969549620528274e-07, 1.3472791709233972e-07, 1.212567214942498e-07, 1.091323421948515e-07, 9.822015518388799e-08, 8.839898790440171e-08, 7.955977618747256e-08, 7.160435509826926e-08, 6.444437037737293e-08, 5.8000298478669455e-08, 5.220056439341989e-08, 4.698074752179796e-08, 4.228286681947141e-08, 3.805473731790538e-08, 3.424939090222354e-08, 3.0824554938049235e-08, 2.774218297634323e-08, 2.4968032339709025e-08, 2.2471283911148214e-08, 2.0224199912415573e-08, 1.8201815879003588e-08, 1.6381663416945188e-08, 1.4743520667182659e-08, 1.3269187709929306e-08, 1.1942284417602958e-08, 1.0748068513562596e-08, 9.673271817759482e-09], "accuracy_test": 0.5223234215561224, "start": "2016-01-30 22:33:46.842000", "learning_rate_per_epoch": [0.00011739690671674907, 0.00010784991172840819, 9.907930507324636e-05, 9.10219459910877e-05, 8.361983054783195e-05, 7.681966962991282e-05, 7.057251059450209e-05, 6.48333880235441e-05, 5.956098539172672e-05, 5.471734766615555e-05, 5.026760481996462e-05, 4.6179724449757487e-05, 4.2424282582942396e-05, 3.897424176102504e-05, 3.5804765502689406e-05, 3.289303640485741e-05, 3.0218097890610807e-05, 2.7760690500144847e-05, 2.5503126380499452e-05, 2.342915286135394e-05, 2.152383967768401e-05, 1.9773469830397516e-05, 1.8165444998885505e-05, 1.668818913458381e-05, 1.5331066606449895e-05, 1.408430853189202e-05, 1.293894001719309e-05, 1.188671467389213e-05, 1.0920059139607474e-05, 1.0032013960881159e-05, 9.216187208949123e-06, 8.466705367027316e-06, 7.778173312544823e-06, 7.145633844629629e-06, 6.564534032804659e-06, 6.030690656189108e-06, 5.540260644920636e-06, 5.08971379531431e-06, 4.6758063945162576e-06, 4.29555893788347e-06, 3.9462338463636115e-06, 3.6253168218536302e-06, 3.3304975204373477e-06, 3.0596536362281768e-06, 2.8108354399591917e-06, 2.5822516818152508e-06, 2.3722568585071713e-06, 2.1793393898406066e-06, 2.0021104774059495e-06, 1.8392942138234503e-06, 1.689718487796199e-06, 1.5523065712841344e-06, 1.4260693887990783e-06, 1.3100981277602841e-06, 1.2035578720315243e-06, 1.1056816902055289e-06, 1.015765064948937e-06, 9.331606634077616e-07, 8.572738465773e-07, 7.875583492022997e-07, 7.235122438942199e-07, 6.646745305260993e-07, 6.10621668784006e-07, 5.609645086224191e-07, 5.153455617801228e-07, 4.734364438263583e-07, 4.3493548673723126e-07, 3.995655220023764e-07, 3.670719195270067e-07, 3.3722076864250994e-07, 3.0979717280388286e-07, 2.846037432391313e-07, 2.614590925986704e-07, 2.4019661282181914e-07, 2.2066326721414953e-07, 2.027184109465452e-07, 1.8623288156049966e-07, 1.7108798999743158e-07, 1.571747105799659e-07, 1.4439289941492461e-07, 1.3265052700717206e-07, 1.2186308140371693e-07, 1.119528931781133e-07, 1.0284862383969084e-07, 9.448473292650306e-08, 8.680101615254898e-08, 7.97421577658497e-08, 7.325733974994364e-08, 6.729987944709137e-08, 6.182689560318977e-08, 5.6798988623540936e-08, 5.217996346118525e-08, 4.7936566716089146e-08, 4.403825215604229e-08, 4.045695689569584e-08, 3.716690244459642e-08, 3.414440286064746e-08, 3.1367701325279995e-08, 2.881680671862341e-08, 2.6473356840028828e-08, 2.432048162859246e-08, 2.2342682370890543e-08, 2.052572334321212e-08, 1.885652345379185e-08, 1.732306742496803e-08, 1.5914315198983786e-08, 1.4620125554642982e-08, 1.3431182388501384e-08, 1.233892721330676e-08, 1.1335496985509508e-08, 1.0413668150022204e-08, 9.56680423769285e-09, 8.788809680027043e-09, 8.074082735731736e-09, 7.417479519489234e-09, 6.814272701660684e-09, 6.260119977952172e-09, 5.751032539080825e-09, 5.2833453167977495e-09, 4.853691226713863e-09, 4.45897763157177e-09, 4.096363248606849e-09, 3.763237277354392e-09, 3.4572020801704184e-09, 3.176054308440257e-09, 2.9177700255900163e-09, 2.680490052142659e-09, 2.4625064209971015e-09, 2.262249720885734e-09, 2.078278216188778e-09, 1.909267854927066e-09, 1.7540017216433057e-09, 1.6113621548186075e-09, 1.4803224201997978e-09, 1.3599391612828526e-09, 1.2493457379747497e-09, 1.1477460093445302e-09, 1.0544085604635711e-09, 9.686615953796718e-10, 8.898877745799894e-10, 8.175199961435453e-10, 7.510373434271855e-10, 6.899612547961453e-10, 6.338519709103707e-10, 5.823056481446542e-10, 5.34951194453015e-10, 4.914477158557418e-10, 4.5148204619316346e-10, 4.1476647116844845e-10, 3.810367021905847e-10, 3.5004990572851113e-10, 3.215830379765805e-10, 2.95431151764447e-10, 2.7140600877828547e-10, 2.493346362708593e-10, 2.2905816132734458e-10, 2.1043061737557878e-10, 1.9331790335197496e-10, 1.7759684001195097e-10, 1.6315425399593408e-10, 1.4988617291766815e-10, 1.3769707596367198e-10, 1.2649922775942457e-10, 1.1621201223555033e-10], "accuracy_train_first": 0.269391380583241, "accuracy_train_last": 0.5323864390111664, "batch_size_eval": 1024, "accuracy_train_std": [0.012126030364236165, 0.014786202165753222, 0.014140829523342668, 0.01652004484303882, 0.017154497224343618, 0.016801291670796376, 0.015880815695875673, 0.015891695063368388, 0.016136102357956712, 0.014474327412122387, 0.013118362395594791, 0.012621134671321092, 0.012675263186323426, 0.012741436574738509, 0.012709555319885102, 0.012187813083838752, 0.011801854066892527, 0.012533626703558938, 0.012028874034307092, 0.012273389340115707, 0.01222429570991421, 0.01203996514250167, 0.012031394320413856, 0.01216872370377035, 0.011903195450837071, 0.011471000825098887, 0.01109628954997543, 0.011512160423960516, 0.011350459240657706, 0.011574412081861864, 0.011854015955291324, 0.012283524130635095, 0.012459140992897983, 0.012566795559238722, 0.012598810165526825, 0.01230150946691638, 0.01203280342299079, 0.012071275049062252, 0.011977973983093502, 0.011804814922436848, 0.011823342358944569, 0.011918412737065235, 0.011734615468355785, 0.011794412817188115, 0.011798405373020421, 0.012054853954353446, 0.012101621713280082, 0.01212786176409247, 0.012198369418285239, 0.01214552653325952, 0.012178103498521387, 0.012175560466915902, 0.012149567623360261, 0.012199358570902501, 0.012247871194834474, 0.01236619818271355, 0.012394723971556061, 0.01241051440921212, 0.012547534715019236, 0.012611902781633292, 0.01263561257612611, 0.012743693817403909, 0.012776085229365854, 0.012616841795891249, 0.012524720783536922, 0.012549973885823884, 0.01253367970592385, 0.012509771191711243, 0.012404239435199926, 0.012491796627174874, 0.012416595548412694, 0.01237342701103755, 0.012326878081997178, 0.012325574513767243, 0.012397241381056684, 0.012369201855364354, 0.012331802344992274, 0.01241590281915094, 0.01241590281915094, 0.012384773746376558, 0.012368447617172122, 0.012352860404286412, 0.012339050031348935, 0.012293568582613091, 0.012269533759703076, 0.012269533759703076, 0.012269533759703076, 0.012230608503426445, 0.012230608503426445, 0.012230608503426445, 0.012296833340628328, 0.012296833340628328, 0.012322054358653356, 0.012322054358653356, 0.012295629694080408, 0.012300525855936422, 0.01227581658484663, 0.01227581658484663, 0.01229498666711365, 0.012305376135160265, 0.012309752298319838, 0.012297476271033739, 0.012297476271033739, 0.012297476271033739, 0.01228144683001624, 0.012298630155645177, 0.012310992861726959, 0.012335500190874513, 0.012335500190874513, 0.012347825982544958, 0.012328825763319883, 0.012328825763319883, 0.012328825763319883, 0.012328825763319883, 0.012328825763319883, 0.012328825763319883, 0.012328825763319883, 0.012352105168260541, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153, 0.012350906908180153], "accuracy_test_std": 0.00955543967736442, "error_valid": [0.7346588502447289, 0.6800007647778614, 0.6360539815512049, 0.6113443029932228, 0.5946809699736446, 0.5821783226656627, 0.5719950112951807, 0.5654238045933735, 0.5574274637612951, 0.552147555064006, 0.5428393260542168, 0.5390448512801205, 0.5302954983998494, 0.5252700254141567, 0.522838914250753, 0.5203063229480421, 0.514904343938253, 0.5130835843373494, 0.5094920698418675, 0.5079051557793675, 0.5043342314570783, 0.5017501647213856, 0.49943082878388556, 0.4999294051204819, 0.4989528426204819, 0.4984645613704819, 0.4979762801204819, 0.49698942253388556, 0.4967555769954819, 0.4962672957454819, 0.4949245223079819, 0.49332731315888556, 0.4917301040097892, 0.4917506941829819, 0.49115063770707834, 0.4907741316829819, 0.4908962019954819, 0.48978727409638556, 0.4890445571347892, 0.4884342055722892, 0.4884342055722892, 0.4886783461972892, 0.4879356292356928, 0.4880576995481928, 0.4881797698606928, 0.4879356292356928, 0.4881797698606928, 0.4873252776731928, 0.4869590667356928, 0.4870914321347892, 0.4865928557981928, 0.4865928557981928, 0.4868369964231928, 0.4863487151731928, 0.4862266448606928, 0.4862266448606928, 0.4862266448606928, 0.4861045745481928, 0.4861045745481928, 0.4861045745481928, 0.4859825042356928, 0.4859825042356928, 0.4859825042356928, 0.4854942229856928, 0.4854942229856928, 0.4856162932981928, 0.4857383636106928, 0.4857383636106928, 0.4857383636106928, 0.4858604339231928, 0.4857383636106928, 0.4857383636106928, 0.4857383636106928, 0.4856162932981928, 0.4856162932981928, 0.4856162932981928, 0.4856162932981928, 0.4856162932981928, 0.4856162932981928, 0.4856162932981928, 0.4854942229856928, 0.4854942229856928, 0.4854942229856928, 0.4854942229856928, 0.4853721526731928, 0.4852500823606928, 0.4852500823606928, 0.4852500823606928, 0.4852500823606928, 0.4852500823606928, 0.4852500823606928, 0.4852500823606928, 0.4852500823606928, 0.4852500823606928, 0.4852500823606928, 0.4852500823606928, 0.4852500823606928, 0.4852500823606928, 0.4852500823606928, 0.4852500823606928, 0.4852500823606928, 0.4852500823606928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4850059417356928, 0.4850059417356928, 0.4850059417356928, 0.4850059417356928, 0.4850059417356928, 0.4850059417356928, 0.4850059417356928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928, 0.4851280120481928], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.08132234219699286, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.00012778900838982083, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 3.097853541718575e-08, "rotation_range": [0, 0], "momentum": 0.6748925091666462}, "accuracy_valid_max": 0.5149940582643072, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.5148719879518072, "accuracy_valid_std": [0.013334524324381897, 0.016028033827593505, 0.011468994658788114, 0.015494075033493182, 0.013953359699160183, 0.013347052415822661, 0.010859665574577843, 0.01536360571599915, 0.012726466815109487, 0.01121013665529841, 0.0105947473504544, 0.01102652101213187, 0.006908497505640779, 0.006845898150035545, 0.009512640801002106, 0.013546385331404317, 0.01279016927469946, 0.013847794889042395, 0.010522181247808206, 0.011479010814232067, 0.011261676500934466, 0.010711711261214132, 0.011094264979314207, 0.010851421140212257, 0.010634166969502456, 0.01161684449842432, 0.012551171292082285, 0.012300324157751166, 0.012777339701826854, 0.013518202285443858, 0.013159763553391398, 0.013082427593425216, 0.012548502449036715, 0.013680482037875063, 0.014780293843298016, 0.014205620720998445, 0.014078207022798135, 0.013207445067339903, 0.012543236349340793, 0.012739810210663609, 0.012692937851743422, 0.012883445637194521, 0.01260152921035109, 0.012609013705960494, 0.012728202365679866, 0.013029420289406724, 0.013142905748778268, 0.013391884553415946, 0.013454058178772462, 0.014096092999340129, 0.013759604244164282, 0.013923241790580002, 0.014196323726214282, 0.01429743815792542, 0.014425753068465242, 0.014141998440916376, 0.014141998440916376, 0.013889675850557339, 0.013958167640000898, 0.013958167640000898, 0.014062048115296743, 0.014062048115296743, 0.014062048115296743, 0.014268936890179223, 0.014268936890179223, 0.014462215120909748, 0.014380930324505586, 0.014380930324505586, 0.014380930324505586, 0.01430647629746095, 0.014405777061151162, 0.014405777061151162, 0.014405777061151162, 0.014511587703558832, 0.014511587703558832, 0.014511587703558832, 0.014511587703558832, 0.014511587703558832, 0.014511587703558832, 0.014511587703558832, 0.01462376691818099, 0.01462376691818099, 0.01462376691818099, 0.01462376691818099, 0.014644812507641899, 0.014451914146979153, 0.014451914146979153, 0.014451914146979153, 0.014451914146979153, 0.014451914146979153, 0.014451914146979153, 0.014451914146979153, 0.014451914146979153, 0.014451914146979153, 0.014451914146979153, 0.014451914146979153, 0.014451914146979153, 0.014451914146979153, 0.014451914146979153, 0.014451914146979153, 0.014451914146979153, 0.014451914146979153, 0.01456966764627112, 0.01456966764627112, 0.01456966764627112, 0.01456966764627112, 0.01456966764627112, 0.01456966764627112, 0.01456966764627112, 0.01456966764627112, 0.01456966764627112, 0.01456966764627112, 0.01456966764627112, 0.01463667634187986, 0.01463667634187986, 0.01463667634187986, 0.01463667634187986, 0.01463667634187986, 0.01463667634187986, 0.01463667634187986, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728, 0.014520492548401728], "accuracy_valid": [0.2653411497552711, 0.3199992352221386, 0.36394601844879515, 0.3886556970067771, 0.4053190300263554, 0.41782167733433734, 0.4280049887048193, 0.4345761954066265, 0.44257253623870485, 0.44785244493599397, 0.45716067394578314, 0.4609551487198795, 0.4697045016001506, 0.4747299745858434, 0.477161085749247, 0.47969367705195787, 0.485095656061747, 0.4869164156626506, 0.49050793015813254, 0.49209484422063254, 0.49566576854292166, 0.49824983527861444, 0.5005691712161144, 0.5000705948795181, 0.5010471573795181, 0.5015354386295181, 0.5020237198795181, 0.5030105774661144, 0.5032444230045181, 0.5037327042545181, 0.5050754776920181, 0.5066726868411144, 0.5082698959902108, 0.5082493058170181, 0.5088493622929217, 0.5092258683170181, 0.5091037980045181, 0.5102127259036144, 0.5109554428652108, 0.5115657944277108, 0.5115657944277108, 0.5113216538027108, 0.5120643707643072, 0.5119423004518072, 0.5118202301393072, 0.5120643707643072, 0.5118202301393072, 0.5126747223268072, 0.5130409332643072, 0.5129085678652108, 0.5134071442018072, 0.5134071442018072, 0.5131630035768072, 0.5136512848268072, 0.5137733551393072, 0.5137733551393072, 0.5137733551393072, 0.5138954254518072, 0.5138954254518072, 0.5138954254518072, 0.5140174957643072, 0.5140174957643072, 0.5140174957643072, 0.5145057770143072, 0.5145057770143072, 0.5143837067018072, 0.5142616363893072, 0.5142616363893072, 0.5142616363893072, 0.5141395660768072, 0.5142616363893072, 0.5142616363893072, 0.5142616363893072, 0.5143837067018072, 0.5143837067018072, 0.5143837067018072, 0.5143837067018072, 0.5143837067018072, 0.5143837067018072, 0.5143837067018072, 0.5145057770143072, 0.5145057770143072, 0.5145057770143072, 0.5145057770143072, 0.5146278473268072, 0.5147499176393072, 0.5147499176393072, 0.5147499176393072, 0.5147499176393072, 0.5147499176393072, 0.5147499176393072, 0.5147499176393072, 0.5147499176393072, 0.5147499176393072, 0.5147499176393072, 0.5147499176393072, 0.5147499176393072, 0.5147499176393072, 0.5147499176393072, 0.5147499176393072, 0.5147499176393072, 0.5147499176393072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5149940582643072, 0.5149940582643072, 0.5149940582643072, 0.5149940582643072, 0.5149940582643072, 0.5149940582643072, 0.5149940582643072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072, 0.5148719879518072], "seed": 704099472, "model": "residualv3", "loss_std": [0.39078882336616516, 0.18376797437667847, 0.1846744418144226, 0.19282761216163635, 0.2004823386669159, 0.20658481121063232, 0.21124231815338135, 0.2147938460111618, 0.21762189269065857, 0.2199423462152481, 0.22193767130374908, 0.22375166416168213, 0.22534191608428955, 0.2267305552959442, 0.2279888391494751, 0.2291041910648346, 0.23010584712028503, 0.23099271953105927, 0.23179787397384644, 0.23248188197612762, 0.23308931291103363, 0.23361946642398834, 0.23409166932106018, 0.2344958335161209, 0.23487447202205658, 0.23520560562610626, 0.23547811806201935, 0.23573502898216248, 0.23594413697719574, 0.23613286018371582, 0.2362920492887497, 0.23643213510513306, 0.23656247556209564, 0.2366773933172226, 0.2367773950099945, 0.2368725687265396, 0.2369571179151535, 0.2370428591966629, 0.23711702227592468, 0.2371848076581955, 0.2372463047504425, 0.23729965090751648, 0.23734815418720245, 0.23739126324653625, 0.23743125796318054, 0.23746979236602783, 0.23750658333301544, 0.23754072189331055, 0.23757211863994598, 0.23759903013706207, 0.23762370645999908, 0.2376457303762436, 0.23766480386257172, 0.23768296837806702, 0.23769843578338623, 0.23771223425865173, 0.23772522807121277, 0.237736314535141, 0.23774537444114685, 0.2377552092075348, 0.23776358366012573, 0.23777033388614655, 0.23777610063552856, 0.23778070509433746, 0.2377837747335434, 0.23778647184371948, 0.23778848350048065, 0.23778972029685974, 0.2377902865409851, 0.23779034614562988, 0.23778948187828064, 0.23778775334358215, 0.23778538405895233, 0.2377825826406479, 0.23777975142002106, 0.23777636885643005, 0.23777244985103607, 0.2377678006887436, 0.23776274919509888, 0.23775725066661835, 0.23775172233581543, 0.23774616420269012, 0.23774051666259766, 0.23773498833179474, 0.23772947490215302, 0.23772414028644562, 0.23771880567073822, 0.23771372437477112, 0.23770876228809357, 0.23770399391651154, 0.2376992106437683, 0.23769451677799225, 0.23769007623195648, 0.23768581449985504, 0.23768176138401031, 0.2376779019832611, 0.237674281001091, 0.2376708835363388, 0.23766767978668213, 0.23766465485095978, 0.23766188323497772, 0.23765935003757477, 0.2376571148633957, 0.23765507340431213, 0.2376532256603241, 0.237651526927948, 0.2376500368118286, 0.23764865100383759, 0.2376474291086197, 0.23764635622501373, 0.23764541745185852, 0.23764459788799286, 0.23764386773109436, 0.23764324188232422, 0.23764272034168243, 0.23764225840568542, 0.23764187097549438, 0.23764154314994812, 0.23764124512672424, 0.23764102160930634, 0.23764079809188843, 0.2376406192779541, 0.23764044046401978, 0.23764029145240784, 0.2376401722431183, 0.23764008283615112, 0.23763997852802277, 0.23763993382453918, 0.23763985931873322, 0.23763981461524963, 0.23763978481292725, 0.23763975501060486, 0.23763974010944366, 0.23763971030712128, 0.23763969540596008, 0.2376396656036377, 0.2376396656036377, 0.2376396358013153, 0.2376396358013153, 0.2376396358013153, 0.2376396358013153, 0.2376396209001541, 0.23763960599899292, 0.23763960599899292, 0.2376396209001541, 0.23763960599899292, 0.23763959109783173, 0.23763959109783173, 0.23763960599899292, 0.23763959109783173, 0.23763959109783173, 0.23763959109783173, 0.23763959109783173, 0.23763959109783173, 0.23763959109783173, 0.23763959109783173, 0.23763959109783173, 0.23763959109783173, 0.23763959109783173, 0.23763959109783173, 0.23763959109783173, 0.23763959109783173, 0.23763959109783173, 0.23763959109783173]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:26 2016", "state": "available"}], "summary": "d973099ccfa9d7a666ef010bdf5b0a63"}