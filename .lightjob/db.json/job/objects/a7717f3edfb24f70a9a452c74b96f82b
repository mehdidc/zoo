{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 64, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.05660818096532071, 0.05596496091328545, 0.05837822307720187, 0.058214395600330406, 0.058489320961980874, 0.05808128137085854, 0.05908410049315448, 0.060709859044608495, 0.06094617862052634, 0.06138689843444592, 0.06061342552706929, 0.06081596458283666, 0.0610070157170281, 0.061695971791107286, 0.0615939883331754, 0.06270967095065968, 0.06215088961133118, 0.06195832827635675, 0.06214457617625398, 0.06258839811151064, 0.06363637572696954, 0.06272460031842894, 0.06331593108293612, 0.06403742684156782, 0.06371074160860327, 0.06284944134497931, 0.06232125368469163, 0.06281495379388155, 0.06276652572937864, 0.06295109078864046, 0.06319412835605974, 0.06279663789150679, 0.06233784964238616, 0.062106107757533925, 0.06265390321257888, 0.0631958216682405, 0.06348991085848633, 0.06335830967402907, 0.06324166496431691, 0.06330367683630489, 0.0631077091879046, 0.06317846306689139, 0.0640975558941028, 0.0645529550142236, 0.06455240244835515, 0.0646163305664376, 0.0647416559275806, 0.0647416559275806, 0.0647416559275806, 0.0646709576089137, 0.06472994711838119, 0.06472677848825874, 0.06485216541835125, 0.0647067987692313, 0.0647067987692313, 0.0647067987692313, 0.06470197515408303, 0.06451301994837344, 0.06451301994837344, 0.06451758125793666, 0.06451758125793666, 0.06451758125793666, 0.06451758125793666, 0.06451758125793666, 0.06451758125793666, 0.06451301994837344, 0.06451301994837344, 0.06451301994837344, 0.06451301994837344, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873, 0.06444345456834873], "moving_avg_accuracy_train": [0.010721009036144575, 0.020732304216867465, 0.030006024096385538, 0.038686521084337344, 0.04667074924698794, 0.05407775188253011, 0.06094407307981927, 0.06746967706701806, 0.07379452788441264, 0.07989163759597137, 0.08574142338456701, 0.09137332396779706, 0.09672206084812579, 0.1017853592813855, 0.10651881507011442, 0.11099776940647647, 0.11518649020679267, 0.11916812356563147, 0.12289748967292376, 0.12637628362731812, 0.12960367785494775, 0.13261893130439273, 0.13538207582455586, 0.137979504537281, 0.1403948447462035, 0.14265101162700486, 0.14471215293418388, 0.1466542471287173, 0.14845154831946003, 0.15010441683088752, 0.15165788704538913, 0.1530983671661514, 0.15447715996760855, 0.15575337092867902, 0.1569607898599075, 0.15807335168716977, 0.15912172058471785, 0.16009819686961957, 0.16100996980313956, 0.16184703758186175, 0.16261216439596474, 0.16331725066721164, 0.16395888779928566, 0.1645528333567065, 0.16509679700898766, 0.16559813010929372, 0.16606815520077398, 0.16650294359635923, 0.16689425315238596, 0.16725584440341243, 0.16758127652933624, 0.1678765186053183, 0.16814929596165393, 0.1683971487450066, 0.16863668838857826, 0.16886639304369636, 0.16907548039595324, 0.16926836533828563, 0.16944666811168596, 0.16960949377039689, 0.16975839002588733, 0.16990180930643112, 0.17003323982157115, 0.17015388044784777, 0.17026245701149673, 0.1703625290814314, 0.1704478876190714, 0.170527063465598, 0.17060067489012254, 0.17066692517219462, 0.1707265504260595, 0.1707802131545379, 0.17082850961016846, 0.17087197642023597, 0.1709087433866461, 0.17093712733111402, 0.17096267288113515, 0.17098566387615416, 0.17100635577167125, 0.17102497847763665, 0.1710417389130055, 0.17105682330483749, 0.17107039925748627, 0.17108497077752077, 0.17109808514555183, 0.1711098880767798, 0.17112051071488496, 0.1711300710891796, 0.1711386754260448, 0.17114641932922348, 0.17115338884208428, 0.17115966140365899, 0.17116530670907623, 0.17117038748395175, 0.1711749601813397, 0.17117907560898887, 0.17118277949387314, 0.17118611299026898, 0.17118911313702523, 0.17119181326910585, 0.1711942433879784, 0.1711964304949637, 0.17119839889125046, 0.17120017044790856, 0.17120176484890084, 0.1712031998097939, 0.17120449127459766], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234423, "moving_var_accuracy_train": [0.001034460312777843, 0.0018330485622601472, 0.002423760629668051, 0.0028595438183218096, 0.0031473205306689956, 0.0033263616699884535, 0.00341804280404869, 0.0034594900902235244, 0.0034735747219618437, 0.003460789971278714, 0.0034226909181029417, 0.0033658865639071296, 0.0032867787834513153, 0.0031888338243244363, 0.0030716008752266545, 0.002944990075228937, 0.0028083995051930584, 0.0026702401925117144, 0.002528389717320526, 0.002384468811982651, 0.0022397665922892196, 0.0020976157133398067, 0.001956568850759594, 0.0018216316889428353, 0.001691973334972092, 0.0015685886024211064, 0.001449964473572433, 0.0013389135949591533, 0.001234094859595445, 0.0011352731424805174, 0.001043465255598559, 0.0009577935768435052, 0.0008791238454633052, 0.0008058698906713823, 0.0007384036458836445, 0.0006757034256706104, 0.0006180247792116649, 0.0005648038547052774, 0.0005158054381754462, 0.00047053103655347563, 0.0004287467042730625, 0.0003903463536948639, 0.000355017002208683, 0.00032269024391443416, 0.00029308428761801826, 0.00026603787275337874, 0.00024142239775763017, 0.00021898152652228753, 0.00019846148238779928, 0.0001797920682443893, 0.00016276601603720003, 0.00014727392538435168, 0.00013321620022108168, 0.00012044745921891454, 0.00010891912646460489, 9.850209187539064e-05, 8.904534037571568e-05, 8.047564774695123e-05, 7.271420988327642e-05, 6.568139865116058e-05, 5.9312789640136174e-05, 5.356663248640786e-05, 4.8365435060556817e-05, 4.365987900087687e-05, 3.939999093235353e-05, 3.555012161174725e-05, 3.206068417010486e-05, 2.89110350851532e-05, 2.6068699353022662e-05, 2.3501331316592072e-05, 2.1183194723018922e-05, 1.909079244656674e-05, 1.7202706130548326e-05, 1.5499439789690483e-05, 1.3961662099092472e-05, 1.2572746723915246e-05, 1.132134522765666e-05, 1.0193967977558674e-05, 9.178424570663628e-06, 8.26370336019453e-06, 7.439861233918862e-06, 6.697922960419442e-06, 6.0297894227903955e-06, 5.4287214432764e-06, 4.887397178788445e-06, 4.399911243579746e-06, 3.960935683184589e-06, 3.565664721676012e-06, 3.209764561024415e-06, 2.8893278172499366e-06, 2.600832202510593e-06, 2.34110308751791e-06, 2.1072796040254036e-06, 1.8967839720828837e-06, 1.707293760927212e-06, 1.53671681553711e-06, 1.3831686028525208e-06, 1.2449517523512572e-06, 1.120537585041162e-06, 1.0085494429563203e-06, 9.077476479603008e-07, 8.170159340969568e-07, 7.353492119427367e-07, 6.618425364653984e-07, 5.956811618495761e-07, 5.361315776794997e-07, 4.825334308436035e-07], "duration": 54617.639886, "accuracy_train": [0.10721009036144578, 0.11083396084337349, 0.1134695030120482, 0.11681099397590361, 0.11852880271084337, 0.12074077560240964, 0.12274096385542169, 0.12620011295180722, 0.13071818524096385, 0.134765625, 0.13838949548192772, 0.14206042921686746, 0.14486069277108435, 0.1473550451807229, 0.1491199171686747, 0.15130835843373494, 0.15288497740963855, 0.1550028237951807, 0.15646178463855423, 0.15768542921686746, 0.15865022590361447, 0.1597562123493976, 0.16025037650602408, 0.16135636295180722, 0.16213290662650603, 0.16295651355421686, 0.16326242469879518, 0.16413309487951808, 0.16462725903614459, 0.16498023343373494, 0.1656391189759036, 0.16606268825301204, 0.1668862951807229, 0.16723926957831325, 0.16782756024096385, 0.16808640813253012, 0.1685570406626506, 0.16888648343373494, 0.1692159262048193, 0.16938064759036145, 0.16949830572289157, 0.16966302710843373, 0.1697336219879518, 0.16989834337349397, 0.16999246987951808, 0.1701101280120482, 0.1702983810240964, 0.1704160391566265, 0.1704160391566265, 0.1705101656626506, 0.1705101656626506, 0.17053369728915663, 0.1706042921686747, 0.1706278237951807, 0.1707925451807229, 0.17093373493975902, 0.17095726656626506, 0.1710043298192771, 0.17105139307228914, 0.17107492469879518, 0.1710984563253012, 0.1711925828313253, 0.17121611445783133, 0.17123964608433734, 0.17123964608433734, 0.17126317771084337, 0.17121611445783133, 0.17123964608433734, 0.17126317771084337, 0.17126317771084337, 0.17126317771084337, 0.17126317771084337, 0.17126317771084337, 0.17126317771084337, 0.17123964608433734, 0.1711925828313253, 0.1711925828313253, 0.1711925828313253, 0.1711925828313253, 0.1711925828313253, 0.1711925828313253, 0.1711925828313253, 0.1711925828313253, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133, 0.17121611445783133], "end": "2016-01-21 07:20:08.258000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0], "accuracy_valid": [0.10496794871794872, 0.10950854700854701, 0.11217948717948718, 0.11658653846153846, 0.11912393162393162, 0.12166132478632478, 0.1264690170940171, 0.12994123931623933, 0.13341346153846154, 0.13808760683760685, 0.13982371794871795, 0.14369658119658119, 0.14730235042735043, 0.14890491452991453, 0.1517094017094017, 0.15357905982905984, 0.15491452991452992, 0.15705128205128205, 0.1578525641025641, 0.15998931623931623, 0.1622596153846154, 0.16279380341880342, 0.16372863247863248, 0.1642628205128205, 0.1642628205128205, 0.16479700854700854, 0.16546474358974358, 0.1657318376068376, 0.16666666666666666, 0.1673344017094017, 0.16800213675213677, 0.16800213675213677, 0.16813568376068377, 0.16880341880341881, 0.16907051282051283, 0.16933760683760685, 0.1702724358974359, 0.1702724358974359, 0.17107371794871795, 0.17080662393162394, 0.17094017094017094, 0.17134081196581197, 0.17147435897435898, 0.171875, 0.17214209401709402, 0.17227564102564102, 0.17267628205128205, 0.17267628205128205, 0.17267628205128205, 0.17307692307692307, 0.1733440170940171, 0.1734775641025641, 0.1736111111111111, 0.17414529914529914, 0.17414529914529914, 0.17414529914529914, 0.17427884615384615, 0.17414529914529914, 0.17414529914529914, 0.17401175213675213, 0.17401175213675213, 0.17401175213675213, 0.17401175213675213, 0.17401175213675213, 0.17401175213675213, 0.17414529914529914, 0.17414529914529914, 0.17414529914529914, 0.17414529914529914, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615, 0.17427884615384615], "accuracy_test": 0.1674679487179487, "start": "2016-01-20 16:09:50.618000", "learning_rate_per_epoch": [0.0001534565381007269, 0.000140476506203413, 0.0001285943726543337, 0.00011771728895837441, 0.00010776023555081338, 9.864539606496692e-05, 9.030153159983456e-05, 8.266342774732038e-05, 7.567138527520001e-05, 6.927076174179092e-05, 6.34115349384956e-05, 5.804790635011159e-05, 5.313795554684475e-05, 4.864330912823789e-05, 4.452884240890853e-05, 4.076239565620199e-05, 3.7314530345611274e-05, 3.415829996811226e-05, 3.1269039027392864e-05, 2.8624164770008065e-05, 2.6203006200375967e-05, 2.398663855274208e-05, 2.1957741410005838e-05, 2.0100458641536534e-05, 1.8400272892904468e-05, 1.6843896446516737e-05, 1.5419163901242428e-05, 1.4114942132437136e-05, 1.2921037523483392e-05, 1.1828118658741005e-05, 1.0827643563970923e-05, 9.911793313222006e-06, 9.073409273696598e-06, 8.305939445563126e-06, 7.6033861660107505e-06, 6.960257906030165e-06, 6.371528343152022e-06, 5.832595888932701e-06, 5.339249128155643e-06, 4.8876318032853305e-06, 4.474214165384183e-06, 4.095765234524151e-06, 3.7493271065613953e-06, 3.4321922157687368e-06, 3.1418821890838444e-06, 2.876127837225795e-06, 2.632852101669414e-06, 2.4101536837406456e-06, 2.206292037953972e-06, 2.019673956965562e-06, 1.8488409523342852e-06, 1.6924577721511014e-06, 1.5493021692236653e-06, 1.4182552376951207e-06, 1.2982928865312715e-06, 1.1884775403814274e-06, 1.0879508636207902e-06, 9.959271665138658e-07, 9.116872092818085e-07, 8.345726314473723e-07, 7.639807790837949e-07, 6.993598731241946e-07, 6.402049166354118e-07, 5.860535452484328e-07, 5.3648250286642e-07, 4.911044015898369e-07, 4.495645953284111e-07, 4.1153842289531894e-07, 3.7672867847504676e-07, 3.448632810432173e-07, 3.156931995818013e-07, 2.8899046355945757e-07, 2.645463723638386e-07, 2.4216987526415323e-07, 2.2168606506056676e-07, 2.0293487068556715e-07, 1.8576973559447651e-07, 1.7005650931878336e-07, 1.5567236744118418e-07, 1.4250490210088174e-07, 1.3045119828802854e-07, 1.194170522467175e-07, 1.0931622540510944e-07, 1.0006976935983403e-07, 9.160542191466448e-08, 8.385702443547416e-08, 7.67640244703216e-08, 7.027097836953544e-08, 6.432714627635505e-08, 5.888606935400276e-08, 5.3905221619743315e-08, 4.934567598979811e-08, 4.517179874596877e-08, 4.1350965318542876e-08, 3.7853315149050104e-08, 3.4651513658445765e-08, 3.172053553157639e-08, 2.903747109428423e-08, 2.6581352230437005e-08, 2.433298362802816e-08, 2.2274791788845505e-08, 2.0390690025351432e-08, 1.8665954115704153e-08, 1.7087105064206298e-08, 1.5641800743537715e-08, 1.4318747076913496e-08, 1.3107603891171493e-08], "accuracy_train_last": 0.17121611445783133, "error_valid": [0.8950320512820513, 0.8904914529914529, 0.8878205128205128, 0.8834134615384616, 0.8808760683760684, 0.8783386752136753, 0.873530982905983, 0.8700587606837606, 0.8665865384615384, 0.8619123931623931, 0.860176282051282, 0.8563034188034189, 0.8526976495726496, 0.8510950854700855, 0.8482905982905983, 0.8464209401709402, 0.8450854700854701, 0.842948717948718, 0.8421474358974359, 0.8400106837606838, 0.8377403846153846, 0.8372061965811965, 0.8362713675213675, 0.8357371794871795, 0.8357371794871795, 0.8352029914529915, 0.8345352564102564, 0.8342681623931624, 0.8333333333333334, 0.8326655982905983, 0.8319978632478633, 0.8319978632478633, 0.8318643162393162, 0.8311965811965811, 0.8309294871794872, 0.8306623931623931, 0.8297275641025641, 0.8297275641025641, 0.828926282051282, 0.8291933760683761, 0.829059829059829, 0.828659188034188, 0.828525641025641, 0.828125, 0.827857905982906, 0.827724358974359, 0.827323717948718, 0.827323717948718, 0.827323717948718, 0.8269230769230769, 0.826655982905983, 0.8265224358974359, 0.8263888888888888, 0.8258547008547008, 0.8258547008547008, 0.8258547008547008, 0.8257211538461539, 0.8258547008547008, 0.8258547008547008, 0.8259882478632479, 0.8259882478632479, 0.8259882478632479, 0.8259882478632479, 0.8259882478632479, 0.8259882478632479, 0.8258547008547008, 0.8258547008547008, 0.8258547008547008, 0.8258547008547008, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539, 0.8257211538461539], "accuracy_train_std": [0.053646411000595806, 0.05375239566926495, 0.05521077247688142, 0.056014762028651605, 0.05657155221644483, 0.057039376357010015, 0.0574934992826472, 0.0589334493745532, 0.05967430722695397, 0.06101945863648823, 0.06193981390558186, 0.06289407743138689, 0.06337398619209218, 0.06397914741235942, 0.06422842252701995, 0.06488298870318994, 0.06537307348631914, 0.06536904144164171, 0.0653468438012268, 0.06579129261080155, 0.06602541656422237, 0.06589228135300601, 0.06645876263430082, 0.06652126940824665, 0.06660630935405737, 0.06698557209087405, 0.06714616749099883, 0.06725211645110991, 0.06719782173852645, 0.06692801239461023, 0.06687176118307943, 0.06651307787488686, 0.06664615278019295, 0.0668918634272112, 0.0669190473187386, 0.06683525038782084, 0.06679424308005069, 0.06686480512809034, 0.06690070385242126, 0.06690704787579647, 0.06680133911191066, 0.06684001001442309, 0.06676525426008903, 0.06691335405822857, 0.06687209654601788, 0.0668643082393288, 0.06683601261365592, 0.06687168251745527, 0.06686068498047794, 0.06706034647876548, 0.06709323549972794, 0.06702791634601576, 0.0669853943604985, 0.06691993684694024, 0.06699967741896186, 0.06704569642709067, 0.06703505372560842, 0.06703568151430253, 0.06699238335551828, 0.06689380461318417, 0.06691606420994137, 0.06687311918558235, 0.06687335517710716, 0.06687358288743812, 0.06687358288743812, 0.06685180613098231, 0.06691732615152843, 0.06690656371517623, 0.06690678303623315, 0.06690678303623315, 0.06690678303623315, 0.06690678303623315, 0.06690678303623315, 0.06690678303623315, 0.06690656371517623, 0.06689510836318568, 0.06689510836318568, 0.06689510836318568, 0.06689510836318568, 0.06689510836318568, 0.06689510836318568, 0.06689510836318568, 0.06689510836318568, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287, 0.06690633611709287], "accuracy_test_std": 0.06736371104433037, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.6547593121461106, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00016763594475142763, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "adadelta", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 9.378469546551052e-10, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.08458445719443579}, "accuracy_valid_max": 0.17427884615384615, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234423\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.17427884615384615, "loss_train": [2.4266436100006104, 2.3945021629333496, 2.371600389480591, 2.353944778442383, 2.3397603034973145, 2.3280625343322754, 2.318211317062378, 2.309793472290039, 2.302525043487549, 2.2961957454681396, 2.2906651496887207, 2.285799264907837, 2.281484365463257, 2.2776355743408203, 2.2741873264312744, 2.2710821628570557, 2.2682852745056152, 2.265763998031616, 2.263479232788086, 2.2614052295684814, 2.2595207691192627, 2.257810354232788, 2.256253957748413, 2.254835605621338, 2.2535431385040283, 2.2523655891418457, 2.2512917518615723, 2.2503132820129395, 2.2494208812713623, 2.248607635498047, 2.247865676879883, 2.247187852859497, 2.246568441390991, 2.246002197265625, 2.2454841136932373, 2.245011806488037, 2.244579315185547, 2.2441842555999756, 2.243823528289795, 2.2434935569763184, 2.243191957473755, 2.2429158687591553, 2.2426633834838867, 2.2424325942993164, 2.2422213554382324, 2.2420287132263184, 2.2418532371520996, 2.2416934967041016, 2.241548538208008, 2.241417169570923, 2.2412984371185303, 2.2411911487579346, 2.2410941123962402, 2.241007089614868, 2.240928888320923, 2.240858554840088, 2.2407963275909424, 2.240741014480591, 2.240692615509033, 2.240649938583374, 2.2406129837036133, 2.2405805587768555, 2.2405521869659424, 2.240527629852295, 2.240506410598755, 2.240488290786743, 2.2404725551605225, 2.2404592037200928, 2.240447759628296, 2.240438222885132, 2.2404298782348633, 2.2404229640960693, 2.2404167652130127, 2.2404117584228516, 2.2404074668884277, 2.240403652191162, 2.240400791168213, 2.2403981685638428, 2.240396022796631, 2.240394115447998, 2.2403926849365234, 2.240391254425049, 2.2403900623321533, 2.240389108657837, 2.2403883934020996, 2.2403876781463623, 2.240386962890625, 2.240386486053467, 2.2403860092163086, 2.2403855323791504, 2.2403852939605713, 2.240385055541992, 2.240384817123413, 2.240384578704834, 2.240384578704834, 2.240384340286255, 2.240384340286255, 2.240384101867676, 2.2403838634490967, 2.2403838634490967, 2.2403838634490967, 2.2403838634490967, 2.2403838634490967, 2.2403838634490967, 2.2403836250305176, 2.2403836250305176, 2.2403836250305176], "accuracy_train_first": 0.10721009036144578, "model": "residualv4", "loss_std": [0.11203932762145996, 0.10621389746665955, 0.10231440514326096, 0.0994282215833664, 0.09720341861248016, 0.09546403586864471, 0.09406914561986923, 0.09292251616716385, 0.09195134788751602, 0.0911339744925499, 0.09045644849538803, 0.0898837149143219, 0.08940199762582779, 0.08899090439081192, 0.08863629400730133, 0.08832462877035141, 0.08805448561906815, 0.0878181904554367, 0.08760891854763031, 0.0874260887503624, 0.08726433664560318, 0.08712024241685867, 0.0869942381978035, 0.08688134700059891, 0.08678185939788818, 0.08669309318065643, 0.0866134986281395, 0.08654192090034485, 0.0864780992269516, 0.08642088621854782, 0.08636897802352905, 0.08632173389196396, 0.08627882599830627, 0.08623982220888138, 0.08620451390743256, 0.08617287874221802, 0.08614395558834076, 0.08611761033535004, 0.08609358221292496, 0.08607152104377747, 0.08605144172906876, 0.08603318780660629, 0.0860167145729065, 0.08600180596113205, 0.08598829805850983, 0.08597612380981445, 0.08596506714820862, 0.08595505356788635, 0.08594585955142975, 0.08593742549419403, 0.08592967689037323, 0.08592256158590317, 0.08591602742671967, 0.08591008931398392, 0.08590471744537354, 0.08589977771043777, 0.08589519560337067, 0.08589092642068863, 0.08588697016239166, 0.08588331937789917, 0.08587998896837234, 0.0858769416809082, 0.08587422221899033, 0.08587182313203812, 0.08586972206830978, 0.08586787432432175, 0.08586627244949341, 0.08586485683917999, 0.0858636200428009, 0.08586254715919495, 0.08586161583662033, 0.08586080372333527, 0.08586012572050095, 0.08585955202579498, 0.08585904538631439, 0.08585862070322037, 0.08585824817419052, 0.08585792779922485, 0.08585763722658157, 0.08585739880800247, 0.08585718274116516, 0.08585701137781143, 0.0858568400144577, 0.08585670590400696, 0.085856594145298, 0.08585648238658905, 0.08585639297962189, 0.08585631102323532, 0.08585622906684875, 0.08585617691278458, 0.085856132209301, 0.08585608005523682, 0.08585604280233383, 0.08585601300001144, 0.08585597574710846, 0.08585595339536667, 0.08585593849420547, 0.08585592359304428, 0.08585590124130249, 0.0858558863401413, 0.0858558863401413, 0.0858558714389801, 0.0858558714389801, 0.0858558639883995, 0.08585585653781891, 0.08585584908723831, 0.08585585653781891]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:04 2016", "state": "available"}], "summary": "4062b0120095bbcfc975c467cd978c5e"}