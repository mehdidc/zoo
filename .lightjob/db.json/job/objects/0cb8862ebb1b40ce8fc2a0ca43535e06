{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 32, "f3": 16, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.09119217166063129, 0.0926392794718663, 0.09592366575731563, 0.09018922982576759, 0.08993139245153683, 0.09562226933836109, 0.09404791480173257, 0.08410503906272758, 0.0815351243698036, 0.08923258949374671, 0.08325550157380511, 0.0808231968222529, 0.087472475040238, 0.08387305020155897, 0.08965968618012568, 0.08367218340598087, 0.08228620063710347, 0.08577887678570445, 0.07926034646142352, 0.0797466878881222, 0.07838435837427939, 0.07760954306322879, 0.07943308261770957, 0.07722401237436399, 0.0811483537681454, 0.085743003769476, 0.08205613342028076, 0.08353522842633854, 0.07905531720782427, 0.07964195410119719, 0.07388674565617462, 0.07808207053406731, 0.07555409468218743, 0.07125105250846085, 0.08156169666729514, 0.0781683622082931, 0.07765778642190196, 0.07978078618433508, 0.07729003554659077, 0.07998038805128434, 0.08485648803598607, 0.07356597912622273, 0.07687912833006236, 0.08496822813685584, 0.0834420903357272, 0.08615179522894623, 0.08105588349739597, 0.08261758077545416, 0.08261552996817455, 0.08402770985810198, 0.0823187053854542, 0.08150580825109045, 0.08313105518067132, 0.08758291395386562, 0.08152626501267246, 0.08232271341535338, 0.08022752480655218, 0.08106424424250687, 0.08295978437702399, 0.08245086749938269, 0.08315518724792513, 0.0835996806785148, 0.08109382995486228, 0.078899048224912, 0.08055763223790596, 0.07996700754945757, 0.08279699128711826, 0.08420252736679248, 0.07972756410256411, 0.08356084454502961, 0.0824174410978755, 0.0844554703077779, 0.08248590199380813, 0.08431936454937529, 0.08262880534506017, 0.09020317003219719, 0.0826030081456922, 0.08786469650507905, 0.08242306720150223, 0.08435140297273352, 0.08017148492840863, 0.080703950069358, 0.08619453344042427, 0.08540004892926398, 0.08338297064307953, 0.08333290529695024, 0.08708995570897689, 0.08656071651157948, 0.0858578485245867, 0.087494492439349, 0.07841927647217525, 0.08089477073355758, 0.08304390691084107, 0.07664632689563486, 0.07621369939351272, 0.08353341365334711, 0.08294290662699308, 0.08664679762528571, 0.08381731988726135, 0.08402983232202244, 0.0823963397878866, 0.07752274439057748, 0.07934131067545473, 0.07504819080249528, 0.08324179051089008, 0.08402473831854283, 0.08199797199942675, 0.08800575608212693, 0.07665970538029794, 0.08758372848353833, 0.07880948322652193, 0.08060676624532266, 0.08048398578261157, 0.08108800165353103, 0.0852209911860871, 0.08107799359286891, 0.08644154446885735, 0.07629976664849458, 0.07981598716604665, 0.07998484772113841, 0.08417753026390927, 0.07866950380748253, 0.0790309487560125, 0.08039308072584543, 0.08939683036609081, 0.0849094358876641, 0.07835067667808539, 0.08377347536090916, 0.08437846225881575, 0.09287692789545966, 0.08067178954864766, 0.0768794763067535, 0.08166353164113263, 0.08329308829867882, 0.07693339366506839, 0.07981911539091557, 0.08133012820512821, 0.08169934036237747, 0.07822253116580628, 0.07932208915744669, 0.07968952656743065, 0.08370713285897102, 0.08203754799169591, 0.0814119914513274, 0.08331492578310232, 0.07453323035027004, 0.08023719439567747, 0.07988042619381375, 0.08037621873702157, 0.08485648803598607, 0.0795847175560287, 0.08084537055435206, 0.08284092204957531, 0.0775920761698796, 0.07459051747587635, 0.08339879701879074, 0.08111098252451913, 0.07890006542474105, 0.08081525251285934, 0.08213097595604721, 0.07705234197615617, 0.08673073698733667, 0.08032028260347222, 0.0829859004959929, 0.08190036381657033, 0.08463098473915096, 0.07840392352394146, 0.08075078645733412, 0.08142042515204571, 0.08105731368598902, 0.07811575389073039, 0.08268285619946412, 0.07775796792225301, 0.0851129356886222, 0.07822720504356973, 0.0791770469681771, 0.07758885815537324, 0.08127078887997517, 0.07908058026350016, 0.07651812404880289, 0.0789384833729129, 0.08451616103986132, 0.08279709898905051, 0.08716395440958473, 0.07904099037073264, 0.07713203992904899, 0.08387783447532705, 0.08722531645348888, 0.08066515691903033, 0.08189611734356153, 0.07806596589804492, 0.07715180707548926, 0.08308641921035718, 0.08362229121406511, 0.0782717638557738, 0.0821244611749952, 0.08273848843134048, 0.07625662822421558, 0.0793956902141992, 0.08344326589026864, 0.0815189361748701, 0.08508317540703245, 0.08047700525513878, 0.07779144771771349, 0.0790981694109075, 0.0876927053386879, 0.07774087849823826, 0.0797462406004923, 0.076638764110374, 0.08146936718240015, 0.07799568338983397, 0.07681368070611373, 0.0775995460462197], "moving_avg_accuracy_train": [0.03691406249999999, 0.07679146272590359, 0.11822988045933733, 0.15865180958207828, 0.19907898555158127, 0.23873669317112192, 0.27784542219738323, 0.3152881954896931, 0.35196579536843464, 0.3868205147773743, 0.41908631721529954, 0.4493491839877455, 0.47849653215523597, 0.5051056515300738, 0.530136313786705, 0.5539910935526128, 0.5760157417274719, 0.5959673490306284, 0.6154792361155174, 0.6342400474437246, 0.6522448830607979, 0.6680350784896578, 0.6815450119057523, 0.6962900777332494, 0.7086146655924546, 0.7216763918042934, 0.733848455184105, 0.7449868589126825, 0.7568586549491251, 0.7678915394542125, 0.7779317341533696, 0.786890255015141, 0.7965083643027835, 0.8052987929327461, 0.8137231681575438, 0.8197896691128737, 0.826981447683514, 0.8330775423729939, 0.8388958235272608, 0.8445746711444142, 0.848758487915515, 0.8537663928890237, 0.8575746180579527, 0.8618515124268562, 0.8652747949191104, 0.8679180609091272, 0.871788905420624, 0.8754750374689231, 0.8783454554087778, 0.8802228827594663, 0.8828303008088209, 0.8859135169628785, 0.8879212804774339, 0.8882340093574014, 0.8900026641445528, 0.8903943405011818, 0.8924999553968468, 0.8912982467547524, 0.8933181773503616, 0.8938771728683375, 0.8955050805815037, 0.8974290642402207, 0.9002478106776445, 0.9027917419592776, 0.9053918975826268, 0.907520253005087, 0.9093275274033735, 0.9108246504160482, 0.9116190679045639, 0.9135341565960353, 0.915302446508721, 0.9172515881530295, 0.9188881575003771, 0.9194150985274478, 0.9194798951506067, 0.9193193679849436, 0.916948801668377, 0.918283853730455, 0.9199254420019878, 0.9196968285246806, 0.921799528955345, 0.9239366882586056, 0.9248976881074439, 0.9269321098087477, 0.9273959018399212, 0.9280321587944832, 0.9293083856861193, 0.9293109996777482, 0.9309535066376842, 0.9304598126004218, 0.9312014819427892, 0.9331020415798356, 0.9344689835061893, 0.9337955226555704, 0.9354602098478446, 0.9356641888630601, 0.9364713580791637, 0.9377555099218498, 0.9389677224838817, 0.939369237133084, 0.9394976372149563, 0.9400461792163521, 0.9412458158127892, 0.9413089224845224, 0.940982152977034, 0.9400762381311378, 0.939759785251759, 0.9411504294675469, 0.94265144450272, 0.9438799935765443, 0.9452610077731067, 0.944124873110254, 0.9433376681787466, 0.9440316686801491, 0.9455269393121342, 0.945201937398993, 0.9456859793518648, 0.9468181532540277, 0.9476394441033238, 0.9476985418616661, 0.9470128367718851, 0.9461509732754195, 0.9458671071225764, 0.9464493534886319, 0.9469569030795277, 0.9470536638257918, 0.9469407296721283, 0.9480768524880481, 0.9465344157332192, 0.946993455334596, 0.947999587963787, 0.9497969559746372, 0.9508921850759687, 0.9518096495502996, 0.95209884649286, 0.9520649784098391, 0.9521568615929515, 0.9519689427529335, 0.95333407784511, 0.9533108168979485, 0.9538711132202018, 0.953474118615049, 0.9541522150366767, 0.9543883489546957, 0.9543208431254911, 0.9538859350177613, 0.9530991863955033, 0.953113533569206, 0.9532323383448156, 0.9540287392994907, 0.9534136100984573, 0.9535753552633104, 0.9542598001586662, 0.9554546785765345, 0.9565630134297245, 0.9566404282012099, 0.9568159938148239, 0.9580117475959921, 0.9589326172641037, 0.9595472621641994, 0.9590179877550085, 0.959657039883122, 0.9606392839369785, 0.9594666394288228, 0.9604843956666633, 0.9604238137807198, 0.9606210784869852, 0.9613704352467204, 0.9621836929268677, 0.9626897212245423, 0.9624862611502809, 0.9617783918123612, 0.9622778869684745, 0.962397989837892, 0.9630049529022956, 0.9632358958650781, 0.9622624568809799, 0.9626900139037252, 0.9617335125133527, 0.9621174843041861, 0.9626207208135266, 0.9584355500875956, 0.9585304363438963, 0.9590464627396271, 0.9600003443271102, 0.9597152007076523, 0.9606045616609834, 0.9611437854647645, 0.9618008677616616, 0.9626322644192303, 0.9619968617724881, 0.9630745664084923, 0.9639291956110166, 0.9639265245438909, 0.9650183412160078, 0.9652479641727203, 0.9660170307072555, 0.9657302749256865, 0.9653004138487805, 0.9653606397831795, 0.9661043197807652, 0.9656111694291947, 0.9661462497754318, 0.966298379315961, 0.9668776904807505, 0.9661518943242416, 0.9667599729641065, 0.9667824884689007, 0.9672380875135769, 0.9671210182200505, 0.9673474517896117, 0.9677041975745059, 0.968733570738742], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.012263832092285152, 0.025349312322048713, 0.038268663268098836, 0.049147188127324, 0.05894167832641405, 0.06720211445650541, 0.07424733718530066, 0.07944025491314441, 0.08360344641761551, 0.0861767649615372, 0.08692882652805246, 0.08647851382282853, 0.0854767735873179, 0.08330150333372545, 0.08061015947740277, 0.07767059818878218, 0.07426930451494092, 0.0704249737692611, 0.0668089000308561, 0.06329572240300381, 0.05988371711308392, 0.05613931784690984, 0.05216805077038461, 0.048907998389661145, 0.045384257743788445, 0.04238131019410695, 0.03947661131699562, 0.03664552652388315, 0.03424942974167289, 0.031920007632029, 0.029635256455198933, 0.027394026673956195, 0.025487196242981895, 0.023633921338169904, 0.021909260085706577, 0.020049555981705096, 0.01851009549461668, 0.016993547279322962, 0.015598864111701539, 0.01432922149286103, 0.013053838248542221, 0.011974166433701241, 0.010907273000766493, 0.009981172129674672, 0.009088524683903169, 0.008242553911358662, 0.007553149455312468, 0.006920122635078696, 0.0063022640639157825, 0.005703760258638223, 0.0051945718927313015, 0.004760670700131951, 0.0043208836590921795, 0.003889675487354253, 0.0035288611964238506, 0.0031773557700965453, 0.002899522719886506, 0.0026225673808422134, 0.0023970317192576916, 0.0021601408312339767, 0.001967977499813855, 0.0018044951679035634, 0.001695553634419607, 0.001584242548268691, 0.0014866655768325395, 0.0013787680903881274, 0.0012702874481056307, 0.001163431099130789, 0.0010527678815322459, 0.0009804991756448332, 0.0009105909010181017, 0.0008537241892624948, 0.0007924570033943466, 0.0007157103046690051, 0.0006441770616234597, 0.0005799912761993562, 0.0005725684105305839, 0.0005313528455536529, 0.0005024708694773945, 0.0004526941596277135, 0.00044721688557499133, 0.0004436022460051155, 0.00040755370778980783, 0.0004040481819394496, 0.0003655792911791243, 0.0003326647682712696, 0.0003140570871545571, 0.0002826514399356715, 0.00027866675796304883, 0.00025299368638859995, 0.00023264497847040998, 0.00024188962302909792, 0.0002345174327964025, 0.00021514763517260966, 0.0002185735226884472, 0.00019709063736743717, 0.0001832452729215215, 0.000179762159225035, 0.00017501107696246264, 0.00015896089538793202, 0.0001432131850783622, 0.00013159995151618514, 0.0001313921080361667, 0.00011828873930070512, 0.0001074208701698524, 0.00010406491852500291, 9.455970849630709e-05, 0.00010250875966081704, 0.00011253529891707457, 0.00011486576446652016, 0.00012054398991983103, 0.0001201068086770696, 0.0001136733522470677, 0.00010664074728588257, 0.00011609918092318818, 0.00010543989902277793, 9.70045786297595e-05, 9.884048046943275e-05, 9.502710035472584e-05, 8.555582322462304e-05, 8.123196413352579e-05, 7.979404589903277e-05, 7.253986124369929e-05, 6.833697259639415e-05, 6.382173462172165e-05, 5.7523824937707735e-05, 5.1886229551510224e-05, 5.831458207203896e-05, 7.389512414865908e-05, 6.840206793448337e-05, 7.067258694874021e-05, 9.268011415171498e-05, 9.420784379617333e-05, 9.236272897148853e-05, 8.387916991861626e-05, 7.550157635018225e-05, 6.802740138921397e-05, 6.154248266419613e-05, 7.21605787768029e-05, 6.494939054408831e-05, 6.127983920825507e-05, 5.657029773611277e-05, 5.5051600775719224e-05, 5.004827374329868e-05, 4.50844597017581e-05, 4.2278319291104614e-05, 4.362124791361878e-05, 3.92609756947962e-05, 3.546190929768522e-05, 3.76240086933829e-05, 3.7267063229720654e-05, 3.377581039192869e-05, 3.461441268574212e-05, 4.400258131855528e-05, 5.0657978507859374e-05, 4.564611807867065e-05, 4.135891583295636e-05, 5.009146819626356e-05, 5.271432988746946e-05, 5.084299207764484e-05, 4.827987547190101e-05, 4.7127376526729817e-05, 5.109786930608441e-05, 5.8363938658045154e-05, 6.184999462921192e-05, 5.56980266504309e-05, 5.0478444264429464e-05, 5.048441981823435e-05, 5.13884703252771e-05, 4.855420503517719e-05, 4.40713485480258e-05, 4.4173924689324486e-05, 4.2001990919217645e-05, 3.793161412047699e-05, 3.7454090162380794e-05, 3.41886930146715e-05, 3.92980748150631e-05, 3.7013512402846884e-05, 4.154621535062331e-05, 3.871850284096329e-05, 3.7125875415865226e-05, 0.00019105417392098443, 0.00017202978714359878, 0.00015722335759905771, 0.000149690032585605, 0.000135452791280503, 0.00012902617830024379, 0.0001187404212652976, 0.00011075219344282719, 0.00010589795771849328, 9.894179065802821e-05, 9.950063713441002e-05, 9.612409308523423e-05, 8.651174798810712e-05, 8.858914599890883e-05, 8.020477171926234e-05, 7.750746455821393e-05, 7.049677800676137e-05, 6.511012511503432e-05, 5.863175707209905e-05, 5.774612081417018e-05, 5.416028415604007e-05, 5.1321054532799567e-05, 4.639723965343396e-05, 4.477792851893784e-05, 4.504115621427035e-05, 4.386487728318394e-05, 3.948295208647077e-05, 3.740279128341243e-05, 3.378585913045203e-05, 3.086872267022492e-05, 2.8927258398560794e-05, 3.5571014559950795e-05], "duration": 27385.90903, "accuracy_train": [0.369140625, 0.43568806475903615, 0.49117564006024095, 0.522449171686747, 0.5629235692771084, 0.5956560617469879, 0.6298239834337349, 0.6522731551204819, 0.6820641942771084, 0.7005129894578314, 0.7094785391566265, 0.721714984939759, 0.7408226656626506, 0.7445877259036144, 0.7554122740963856, 0.7686841114457831, 0.7742375753012049, 0.7755318147590361, 0.7910862198795181, 0.8030873493975904, 0.8142884036144579, 0.8101468373493976, 0.8031344126506024, 0.8289956701807228, 0.8195359563253012, 0.8392319277108434, 0.8433970256024096, 0.8452324924698795, 0.8637048192771084, 0.8671875, 0.8682934864457831, 0.8675169427710844, 0.8830713478915663, 0.8844126506024096, 0.8895425451807228, 0.8743881777108434, 0.8917074548192772, 0.8879423945783133, 0.8912603539156626, 0.8956842996987951, 0.8864128388554217, 0.8988375376506024, 0.8918486445783133, 0.9003435617469879, 0.8960843373493976, 0.8917074548192772, 0.9066265060240963, 0.9086502259036144, 0.9041792168674698, 0.8971197289156626, 0.9062970632530121, 0.9136624623493976, 0.9059911521084337, 0.8910485692771084, 0.9059205572289156, 0.8939194277108434, 0.9114504894578314, 0.8804828689759037, 0.9114975527108434, 0.8989081325301205, 0.91015625, 0.9147449171686747, 0.9256165286144579, 0.9256871234939759, 0.9287932981927711, 0.9266754518072289, 0.9255929969879518, 0.9242987575301205, 0.9187688253012049, 0.9307699548192772, 0.9312170557228916, 0.9347938629518072, 0.933617281626506, 0.9241575677710844, 0.9200630647590361, 0.9178746234939759, 0.8956137048192772, 0.9302993222891566, 0.9346997364457831, 0.9176393072289156, 0.9407238328313253, 0.9431711219879518, 0.9335466867469879, 0.9452419051204819, 0.9315700301204819, 0.9337584713855421, 0.9407944277108434, 0.9293345256024096, 0.9457360692771084, 0.9260165662650602, 0.9378765060240963, 0.950207078313253, 0.9467714608433735, 0.927734375, 0.9504423945783133, 0.9375, 0.9437358810240963, 0.9493128765060241, 0.9498776355421686, 0.9429828689759037, 0.9406532379518072, 0.9449830572289156, 0.9520425451807228, 0.9418768825301205, 0.9380412274096386, 0.9319230045180723, 0.9369117093373494, 0.9536662274096386, 0.9561605798192772, 0.9549369352409639, 0.9576901355421686, 0.9338996611445783, 0.9362528237951807, 0.9502776731927711, 0.958984375, 0.9422769201807228, 0.9500423569277109, 0.957007718373494, 0.9550310617469879, 0.948230421686747, 0.9408414909638554, 0.9383942018072289, 0.9433123117469879, 0.9516895707831325, 0.9515248493975904, 0.9479245105421686, 0.9459243222891566, 0.9583019578313253, 0.932652484939759, 0.9511248117469879, 0.957054781626506, 0.9659732680722891, 0.9607492469879518, 0.9600668298192772, 0.9547016189759037, 0.9517601656626506, 0.9529838102409639, 0.9502776731927711, 0.9656202936746988, 0.953101468373494, 0.9589137801204819, 0.9499011671686747, 0.9602550828313253, 0.9565135542168675, 0.9537132906626506, 0.9499717620481928, 0.9460184487951807, 0.9532426581325302, 0.9543015813253012, 0.9611963478915663, 0.9478774472891566, 0.9550310617469879, 0.9604198042168675, 0.9662085843373494, 0.9665380271084337, 0.9573371611445783, 0.9583960843373494, 0.968773531626506, 0.9672204442771084, 0.9650790662650602, 0.9542545180722891, 0.9654085090361446, 0.9694794804216867, 0.9489128388554217, 0.9696442018072289, 0.9598785768072289, 0.9623964608433735, 0.9681146460843374, 0.9695030120481928, 0.9672439759036144, 0.9606551204819277, 0.9554075677710844, 0.966773343373494, 0.9634789156626506, 0.9684676204819277, 0.9653143825301205, 0.9535015060240963, 0.9665380271084337, 0.953125, 0.9655732304216867, 0.9671498493975904, 0.9207690135542169, 0.9593844126506024, 0.9636907003012049, 0.9685852786144579, 0.9571489081325302, 0.9686088102409639, 0.9659967996987951, 0.9677146084337349, 0.9701148343373494, 0.9562782379518072, 0.9727739081325302, 0.9716208584337349, 0.963902484939759, 0.9748446912650602, 0.9673145707831325, 0.9729386295180723, 0.9631494728915663, 0.9614316641566265, 0.9659026731927711, 0.9727974397590361, 0.9611728162650602, 0.9709619728915663, 0.9676675451807228, 0.9720914909638554, 0.9596197289156626, 0.9722326807228916, 0.9669851280120482, 0.9713384789156626, 0.9660673945783133, 0.9693853539156626, 0.9709149096385542, 0.9779979292168675], "end": "2016-01-17 21:35:35.564000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0], "accuracy_valid": [0.3643162393162393, 0.42254273504273504, 0.46968482905982906, 0.4954594017094017, 0.5283119658119658, 0.5563568376068376, 0.5759882478632479, 0.5933493589743589, 0.6141826923076923, 0.6279380341880342, 0.6256677350427351, 0.6359508547008547, 0.6384882478632479, 0.6327457264957265, 0.6340811965811965, 0.6360844017094017, 0.6319444444444444, 0.6331463675213675, 0.6351495726495726, 0.6466346153846154, 0.6395566239316239, 0.6383547008547008, 0.6286057692307693, 0.6454326923076923, 0.6318108974358975, 0.6485042735042735, 0.6474358974358975, 0.6392895299145299, 0.6530448717948718, 0.657051282051282, 0.6483707264957265, 0.6555822649572649, 0.6511752136752137, 0.6618589743589743, 0.65625, 0.6483707264957265, 0.6655982905982906, 0.6513087606837606, 0.6523771367521367, 0.6553151709401709, 0.6442307692307693, 0.6571848290598291, 0.6465010683760684, 0.6591880341880342, 0.6606570512820513, 0.6402243589743589, 0.6589209401709402, 0.6594551282051282, 0.6587873931623932, 0.655448717948718, 0.6573183760683761, 0.6651976495726496, 0.655715811965812, 0.6438301282051282, 0.6538461538461539, 0.6494391025641025, 0.6579861111111112, 0.6378205128205128, 0.6430288461538461, 0.6475694444444444, 0.6521100427350427, 0.6550480769230769, 0.6539797008547008, 0.6605235042735043, 0.6561164529914529, 0.6579861111111112, 0.6599893162393162, 0.6569177350427351, 0.6486378205128205, 0.6643963675213675, 0.6639957264957265, 0.6694711538461539, 0.6595886752136753, 0.6440972222222222, 0.655982905982906, 0.6459668803418803, 0.6462339743589743, 0.6598557692307693, 0.6674679487179487, 0.6583867521367521, 0.6641292735042735, 0.6669337606837606, 0.6659989316239316, 0.6764155982905983, 0.655982905982906, 0.656517094017094, 0.6697382478632479, 0.6578525641025641, 0.672676282051282, 0.6529113247863247, 0.6594551282051282, 0.6750801282051282, 0.6618589743589743, 0.6700053418803419, 0.6785523504273504, 0.65625, 0.6631944444444444, 0.6693376068376068, 0.6682692307692307, 0.6591880341880342, 0.6553151709401709, 0.6646634615384616, 0.6712072649572649, 0.6645299145299145, 0.6606570512820513, 0.6503739316239316, 0.6619925213675214, 0.6613247863247863, 0.6730769230769231, 0.6705395299145299, 0.6688034188034188, 0.6603899572649573, 0.6626602564102564, 0.6698717948717948, 0.6784188034188035, 0.6611912393162394, 0.6631944444444444, 0.6766826923076923, 0.6717414529914529, 0.6670673076923077, 0.6598557692307693, 0.6501068376068376, 0.6538461538461539, 0.6768162393162394, 0.6670673076923077, 0.6594551282051282, 0.6670673076923077, 0.6832264957264957, 0.6666666666666666, 0.6623931623931624, 0.6740117521367521, 0.6784188034188035, 0.6733440170940171, 0.6725427350427351, 0.6729433760683761, 0.6642628205128205, 0.6766826923076923, 0.6754807692307693, 0.6810897435897436, 0.6772168803418803, 0.6700053418803419, 0.6670673076923077, 0.6782852564102564, 0.6706730769230769, 0.6765491452991453, 0.6694711538461539, 0.6768162393162394, 0.6774839743589743, 0.6770833333333334, 0.6858974358974359, 0.6665331196581197, 0.6676014957264957, 0.6712072649572649, 0.6794871794871795, 0.6821581196581197, 0.6690705128205128, 0.6748130341880342, 0.6772168803418803, 0.6813568376068376, 0.6841613247863247, 0.6746794871794872, 0.6805555555555556, 0.686698717948718, 0.6626602564102564, 0.6765491452991453, 0.6708066239316239, 0.6766826923076923, 0.6826923076923077, 0.6860309829059829, 0.6814903846153846, 0.6810897435897436, 0.6761485042735043, 0.6745459401709402, 0.6752136752136753, 0.6841613247863247, 0.6730769230769231, 0.6717414529914529, 0.6768162393162394, 0.6647970085470085, 0.6744123931623932, 0.6798878205128205, 0.6549145299145299, 0.6654647435897436, 0.6720085470085471, 0.6804220085470085, 0.6673344017094017, 0.6820245726495726, 0.6781517094017094, 0.6756143162393162, 0.6793536324786325, 0.6709401709401709, 0.6848290598290598, 0.6834935897435898, 0.6764155982905983, 0.6770833333333334, 0.6801549145299145, 0.6769497863247863, 0.6754807692307693, 0.6826923076923077, 0.6712072649572649, 0.6769497863247863, 0.6690705128205128, 0.6806891025641025, 0.6750801282051282, 0.6821581196581197, 0.6768162393162394, 0.6810897435897436, 0.6824252136752137, 0.6858974358974359, 0.6677350427350427, 0.6737446581196581, 0.6820245726495726, 0.6833600427350427], "accuracy_test": 0.6809895833333334, "start": "2016-01-17 13:59:09.655000", "learning_rate_per_epoch": [0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312, 0.0002691367990337312], "accuracy_train_last": 0.9779979292168675, "error_valid": [0.6356837606837606, 0.5774572649572649, 0.530315170940171, 0.5045405982905983, 0.4716880341880342, 0.44364316239316237, 0.42401175213675213, 0.4066506410256411, 0.3858173076923077, 0.3720619658119658, 0.3743322649572649, 0.36404914529914534, 0.36151175213675213, 0.36725427350427353, 0.36591880341880345, 0.3639155982905983, 0.3680555555555556, 0.36685363247863245, 0.3648504273504274, 0.3533653846153846, 0.36044337606837606, 0.3616452991452992, 0.3713942307692307, 0.3545673076923077, 0.36818910256410253, 0.35149572649572647, 0.35256410256410253, 0.3607104700854701, 0.3469551282051282, 0.34294871794871795, 0.35162927350427353, 0.3444177350427351, 0.3488247863247863, 0.33814102564102566, 0.34375, 0.35162927350427353, 0.33440170940170943, 0.34869123931623935, 0.3476228632478633, 0.3446848290598291, 0.3557692307692307, 0.3428151709401709, 0.35349893162393164, 0.3408119658119658, 0.3393429487179487, 0.3597756410256411, 0.34107905982905984, 0.3405448717948718, 0.3412126068376068, 0.34455128205128205, 0.34268162393162394, 0.3348023504273504, 0.34428418803418803, 0.3561698717948718, 0.34615384615384615, 0.35056089743589747, 0.34201388888888884, 0.3621794871794872, 0.35697115384615385, 0.3524305555555556, 0.3478899572649573, 0.34495192307692313, 0.3460202991452992, 0.33947649572649574, 0.34388354700854706, 0.34201388888888884, 0.3400106837606838, 0.3430822649572649, 0.3513621794871795, 0.33560363247863245, 0.33600427350427353, 0.33052884615384615, 0.34041132478632474, 0.3559027777777778, 0.344017094017094, 0.3540331196581197, 0.35376602564102566, 0.3401442307692307, 0.3325320512820513, 0.34161324786324787, 0.33587072649572647, 0.33306623931623935, 0.33400106837606836, 0.3235844017094017, 0.344017094017094, 0.343482905982906, 0.33026175213675213, 0.3421474358974359, 0.32732371794871795, 0.34708867521367526, 0.3405448717948718, 0.3249198717948718, 0.33814102564102566, 0.3299946581196581, 0.3214476495726496, 0.34375, 0.3368055555555556, 0.3306623931623932, 0.3317307692307693, 0.3408119658119658, 0.3446848290598291, 0.33533653846153844, 0.3287927350427351, 0.3354700854700855, 0.3393429487179487, 0.34962606837606836, 0.3380074786324786, 0.3386752136752137, 0.32692307692307687, 0.3294604700854701, 0.33119658119658124, 0.3396100427350427, 0.3373397435897436, 0.3301282051282052, 0.32158119658119655, 0.33880876068376065, 0.3368055555555556, 0.3233173076923077, 0.32825854700854706, 0.3329326923076923, 0.3401442307692307, 0.34989316239316237, 0.34615384615384615, 0.32318376068376065, 0.3329326923076923, 0.3405448717948718, 0.3329326923076923, 0.31677350427350426, 0.33333333333333337, 0.33760683760683763, 0.32598824786324787, 0.32158119658119655, 0.32665598290598286, 0.3274572649572649, 0.32705662393162394, 0.3357371794871795, 0.3233173076923077, 0.3245192307692307, 0.3189102564102564, 0.3227831196581197, 0.3299946581196581, 0.3329326923076923, 0.3217147435897436, 0.32932692307692313, 0.32345085470085466, 0.33052884615384615, 0.32318376068376065, 0.32251602564102566, 0.32291666666666663, 0.3141025641025641, 0.3334668803418803, 0.33239850427350426, 0.3287927350427351, 0.3205128205128205, 0.3178418803418803, 0.3309294871794872, 0.3251869658119658, 0.3227831196581197, 0.31864316239316237, 0.31583867521367526, 0.3253205128205128, 0.3194444444444444, 0.31330128205128205, 0.3373397435897436, 0.32345085470085466, 0.32919337606837606, 0.3233173076923077, 0.3173076923076923, 0.31396901709401714, 0.3185096153846154, 0.3189102564102564, 0.32385149572649574, 0.32545405982905984, 0.32478632478632474, 0.31583867521367526, 0.32692307692307687, 0.32825854700854706, 0.32318376068376065, 0.3352029914529915, 0.3255876068376068, 0.3201121794871795, 0.3450854700854701, 0.3345352564102564, 0.32799145299145294, 0.3195779914529915, 0.3326655982905983, 0.3179754273504274, 0.32184829059829057, 0.3243856837606838, 0.32064636752136755, 0.3290598290598291, 0.31517094017094016, 0.31650641025641024, 0.3235844017094017, 0.32291666666666663, 0.3198450854700855, 0.3230502136752137, 0.3245192307692307, 0.3173076923076923, 0.3287927350427351, 0.3230502136752137, 0.3309294871794872, 0.31931089743589747, 0.3249198717948718, 0.3178418803418803, 0.32318376068376065, 0.3189102564102564, 0.3175747863247863, 0.3141025641025641, 0.3322649572649573, 0.3262553418803419, 0.3179754273504274, 0.3166399572649573], "accuracy_train_std": [0.0868453573671087, 0.0874958625025497, 0.0923476458980064, 0.08999063787521115, 0.08857017881608502, 0.08746023717308736, 0.08615267194349074, 0.08302363021638637, 0.08306491834004752, 0.08029150621457234, 0.08169149115841388, 0.08267103481625311, 0.08134130059232438, 0.08100020640374299, 0.08159718988290171, 0.0812228168603413, 0.08077731151111807, 0.08109261735263715, 0.07984012817899529, 0.07560661427640956, 0.07639101019962377, 0.07419754313937442, 0.07787384270902185, 0.07523342897138094, 0.07693789348888574, 0.0715012101605033, 0.07193488131925295, 0.0707140646448202, 0.06892618669611238, 0.0671454582670232, 0.06965491425007325, 0.06539697687913762, 0.06427730437640523, 0.06511336697705598, 0.06253767822833473, 0.06487561456164828, 0.06307626038689634, 0.06328464227147566, 0.06338471070617477, 0.06287065366872265, 0.06347989805858426, 0.05956173559698701, 0.062346643647163934, 0.05933460777284986, 0.060244199367177405, 0.05977201742593798, 0.05713905337791887, 0.05615478407086393, 0.05734691658149352, 0.059080486842348574, 0.06002731123234786, 0.05362348613478125, 0.05700441689540642, 0.06160978891231898, 0.05686843927331106, 0.060413453753294065, 0.05460152274001617, 0.0633421689203253, 0.054191445514745626, 0.056951436735269034, 0.05275920809202637, 0.05487453706333044, 0.05160158407149371, 0.05205048974350737, 0.05114316303540975, 0.051055018602431876, 0.04866984366250338, 0.051609052318486656, 0.051529684460006095, 0.04849732922715058, 0.04942039907030273, 0.04833507783805398, 0.04720718179171329, 0.04996494698578288, 0.05196068211217085, 0.05115269008734044, 0.05861512302527642, 0.04821664150076192, 0.04750097064169774, 0.05288695233734041, 0.045204830249151305, 0.04375322965728302, 0.04629684704438597, 0.04193586074023198, 0.04698610016359551, 0.04635412076823883, 0.04545121324249391, 0.049084553112899165, 0.04287457183285476, 0.048529676755678784, 0.04635288436057831, 0.03971056621194648, 0.04266265065391207, 0.04924147331760995, 0.040533429868276336, 0.04632267470708133, 0.04397833100195791, 0.040858900331555995, 0.04097950161412251, 0.04230794707106898, 0.046326475896982924, 0.043522263282089545, 0.03946895675829703, 0.04417710514731635, 0.0445801990722538, 0.0468020270525636, 0.04578400116981173, 0.039461457173532326, 0.03853626264044462, 0.039479470614737264, 0.03790316153302381, 0.04572649169169178, 0.044648463145196694, 0.04113456873619895, 0.037763729882514196, 0.04296206758096005, 0.04030477176713591, 0.03851746306721851, 0.03876068917088822, 0.04170664652583104, 0.044959745079307376, 0.04435124203102111, 0.04358310707477575, 0.03951356702396455, 0.04161041031054919, 0.041581276284056425, 0.04216318537952199, 0.03780500625145988, 0.04763784688062067, 0.041839887049416546, 0.03779960837330468, 0.03414571176281998, 0.03596482525060377, 0.035898489238875904, 0.039077742968298294, 0.03946020125716148, 0.04023990698793583, 0.04004758601608199, 0.035040282816618, 0.03888324123931288, 0.03706717463077378, 0.04042127291395092, 0.0368127233786551, 0.037520246853717895, 0.03960958541460286, 0.04104064081922963, 0.04300785707675665, 0.039222012179325715, 0.03782992756983774, 0.03486913049009102, 0.04205021329658013, 0.03826424090361551, 0.03701957907718604, 0.03331400214338038, 0.033381584168816, 0.03668271131180022, 0.0373220795268666, 0.03175189080007715, 0.03367189928611907, 0.03382286392780395, 0.037753530243569054, 0.03488396761295373, 0.031604218412928466, 0.041124713658481624, 0.03144836153184434, 0.03696331682452625, 0.034526822594009805, 0.033503673346906154, 0.030814321232764477, 0.03313359972260383, 0.03509492608244776, 0.0372698102336507, 0.032594012558062053, 0.03430365580642475, 0.03338758852523891, 0.03292245270005571, 0.03834820595334709, 0.032960375530989384, 0.03846493229639518, 0.03349077928804285, 0.031723146194780096, 0.0484512875434108, 0.03628814862567553, 0.03363227619407462, 0.03248413832926254, 0.037319757507105415, 0.03354230102081878, 0.03284130819593093, 0.03216083846185556, 0.030960002164223125, 0.03696832753887701, 0.02949442276021736, 0.03090443547363088, 0.033826891123811285, 0.02831791475354462, 0.03249811336687307, 0.029082158279606633, 0.0339061587529284, 0.03614040335323887, 0.03247295396888749, 0.03089171126322508, 0.03527292698035452, 0.030503923182788044, 0.03254569420049194, 0.031165362901223827, 0.03471303359159687, 0.030117687417429075, 0.03223189512092056, 0.02976627046634259, 0.0327687003216792, 0.031396154998182886, 0.032264528806699486, 0.027610681316204234], "accuracy_test_std": 0.07903037753030666, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1.0, 1.2], "translation_range": [-3, 3], "momentum": 0.85395699723872, "shear_range": [1, 1.1], "patience_check_each": 1, "learning_rate": 0.00026913678855626093, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 9.133107568674272e-08, "valid_ratio": 0.15, "rotation_range": [-90, 90], "learning_rate_decay": 0.9888077884896386}, "accuracy_valid_max": 0.686698717948718, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.98, interval=[0.8, 1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256, 512],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1.0, 1.2)),\n        rotation_range=make_constant_param((-90, 90)),\n        shear_range=make_constant_param((1, 1.1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.6833600427350427, "loss_train": [3.133817434310913, 1.647891879081726, 1.4946740865707397, 1.3707233667373657, 1.2560654878616333, 1.1521021127700806, 1.0586931705474854, 0.9747949838638306, 0.9012525677680969, 0.8360995054244995, 0.777805745601654, 0.7217784523963928, 0.6682680249214172, 0.6221763491630554, 0.5773752927780151, 0.5431422591209412, 0.5078405141830444, 0.480224072933197, 0.45404699444770813, 0.42772358655929565, 0.4077623784542084, 0.3905778229236603, 0.3747854232788086, 0.35895004868507385, 0.3393729627132416, 0.3322855532169342, 0.316852331161499, 0.30329081416130066, 0.2945893406867981, 0.28472137451171875, 0.2736976146697998, 0.2622669041156769, 0.2562887370586395, 0.2486235648393631, 0.24104227125644684, 0.23327696323394775, 0.23395347595214844, 0.23187430202960968, 0.22410428524017334, 0.21710683405399323, 0.21571654081344604, 0.2074189931154251, 0.21225395798683167, 0.19793099164962769, 0.19942183792591095, 0.19394277036190033, 0.1955612748861313, 0.18687841296195984, 0.18595239520072937, 0.18444518744945526, 0.18441349267959595, 0.1781470626592636, 0.17585542798042297, 0.17818650603294373, 0.17556138336658478, 0.17211991548538208, 0.17196083068847656, 0.16692285239696503, 0.16460788249969482, 0.17489106953144073, 0.15544922649860382, 0.1567148119211197, 0.15472568571567535, 0.1598074585199356, 0.15651977062225342, 0.1504317671060562, 0.15719537436962128, 0.1491193175315857, 0.14943525195121765, 0.1539549082517624, 0.14794671535491943, 0.15147198736667633, 0.14970442652702332, 0.14286434650421143, 0.14680735766887665, 0.14525674283504486, 0.14008350670337677, 0.1335098296403885, 0.14713706076145172, 0.1426335722208023, 0.141681507229805, 0.13240155577659607, 0.14423595368862152, 0.1331660896539688, 0.13435876369476318, 0.13295461237430573, 0.12790362536907196, 0.13369877636432648, 0.13463006913661957, 0.1353372484445572, 0.12372717261314392, 0.1311822235584259, 0.1297144740819931, 0.12551578879356384, 0.13145975768566132, 0.12090416997671127, 0.12865012884140015, 0.12700797617435455, 0.12350080162286758, 0.12612327933311462, 0.12097590416669846, 0.11816029995679855, 0.12656952440738678, 0.11530820280313492, 0.12497498095035553, 0.12136027961969376, 0.1115969642996788, 0.12288304418325424, 0.11751174926757812, 0.12097732722759247, 0.12039470672607422, 0.12163565307855606, 0.11456120759248734, 0.12357276678085327, 0.11154351383447647, 0.12400739639997482, 0.1023976132273674, 0.11914289742708206, 0.10514254122972488, 0.11934538185596466, 0.10968957096338272, 0.11112919449806213, 0.11751343309879303, 0.1047564223408699, 0.11349881440401077, 0.11469605565071106, 0.1043820008635521, 0.10209127515554428, 0.10813019424676895, 0.11543868482112885, 0.1006566733121872, 0.10978797823190689, 0.10903092473745346, 0.10736037790775299, 0.10398783534765244, 0.10246270149946213, 0.11042217165231705, 0.10471226274967194, 0.1034902036190033, 0.10126053541898727, 0.11048522591590881, 0.09757827967405319, 0.10841488093137741, 0.0994320660829544, 0.10443331301212311, 0.102321557700634, 0.09826290607452393, 0.09974345564842224, 0.10598234832286835, 0.10440658032894135, 0.09184625744819641, 0.10266194492578506, 0.10272097587585449, 0.10405062139034271, 0.09335633367300034, 0.0965045765042305, 0.10140134394168854, 0.10382638871669769, 0.0916379913687706, 0.10493622720241547, 0.09705930948257446, 0.10180680453777313, 0.09896371513605118, 0.09627404063940048, 0.0931880921125412, 0.09798525273799896, 0.10054915398359299, 0.09685422480106354, 0.08985790610313416, 0.10465198755264282, 0.09134325385093689, 0.09791114926338196, 0.09269943088293076, 0.09574858844280243, 0.09742613881826401, 0.09596507251262665, 0.08649589121341705, 0.1011168584227562, 0.08703872561454773, 0.09069570153951645, 0.090032197535038, 0.09021464735269547, 0.09689955413341522, 0.09622518718242645, 0.08637074381113052, 0.08886014670133591, 0.08935992419719696, 0.09315874427556992, 0.09472895413637161, 0.08563181012868881, 0.09415021538734436, 0.08301890641450882, 0.09205721318721771, 0.08610731363296509, 0.08556223660707474, 0.09503460675477982, 0.08849743008613586, 0.08683201670646667, 0.0920320451259613, 0.08475898206233978, 0.08943472802639008, 0.08328185230493546, 0.088823102414608, 0.08561287820339203, 0.08315777778625488, 0.08890756964683533, 0.07936941087245941, 0.09355230629444122, 0.08148344606161118, 0.08242694288492203, 0.08883417397737503, 0.08505960553884506, 0.08501549065113068], "accuracy_train_first": 0.369140625, "model": "residualv2", "loss_std": [15.647992134094238, 0.18913869559764862, 0.19395390152931213, 0.19910705089569092, 0.2014721930027008, 0.20344896614551544, 0.2044234573841095, 0.2032357007265091, 0.19787649810314178, 0.19434653222560883, 0.19140218198299408, 0.18582192063331604, 0.17854762077331543, 0.17437870800495148, 0.17363996803760529, 0.16910730302333832, 0.1679701805114746, 0.1627407819032669, 0.15955528616905212, 0.15569745004177094, 0.14938786625862122, 0.15014444291591644, 0.14930234849452972, 0.1453372985124588, 0.14036591351032257, 0.1448972374200821, 0.13977321982383728, 0.13752911984920502, 0.13133350014686584, 0.1318366378545761, 0.12631221115589142, 0.12017495185136795, 0.12132549285888672, 0.12137512862682343, 0.11733086407184601, 0.11907617002725601, 0.11740074306726456, 0.11607944965362549, 0.11645490676164627, 0.11554750800132751, 0.11382116377353668, 0.1122080534696579, 0.11381395161151886, 0.11004098504781723, 0.11529179662466049, 0.11553172767162323, 0.11193803697824478, 0.10813753306865692, 0.11141722649335861, 0.10749635100364685, 0.10814841091632843, 0.10390771180391312, 0.10505513101816177, 0.10675392299890518, 0.10675155371427536, 0.10473696887493134, 0.106674425303936, 0.10508329421281815, 0.10512483865022659, 0.11448892205953598, 0.10050595551729202, 0.10335720330476761, 0.09973175078630447, 0.10774494707584381, 0.09937059134244919, 0.09946002066135406, 0.10132505744695663, 0.09900619834661484, 0.09702009707689285, 0.10170388966798782, 0.10224713385105133, 0.10373275727033615, 0.10281176120042801, 0.09820305556058884, 0.10211686789989471, 0.1015988364815712, 0.09484969079494476, 0.09768340736627579, 0.1012488380074501, 0.10294748097658157, 0.10072587430477142, 0.09762678295373917, 0.10144960880279541, 0.09565027058124542, 0.09890874475240707, 0.09466112405061722, 0.09362517297267914, 0.09725445508956909, 0.09989450871944427, 0.10179643332958221, 0.09065250307321548, 0.0951089933514595, 0.09835776686668396, 0.09371384978294373, 0.0980120450258255, 0.09180577099323273, 0.09619598090648651, 0.09481681138277054, 0.09351429343223572, 0.09665221720933914, 0.09707944840192795, 0.09281276911497116, 0.0975387915968895, 0.08673681318759918, 0.09349539875984192, 0.0916643738746643, 0.08696245402097702, 0.09341080486774445, 0.08774727582931519, 0.09955984354019165, 0.09216979146003723, 0.0956013947725296, 0.09102822840213776, 0.09391571581363678, 0.088591568171978, 0.09746605902910233, 0.08169542998075485, 0.09490787982940674, 0.08530727028846741, 0.09286603331565857, 0.08784233778715134, 0.08980055153369904, 0.09307149052619934, 0.08740361779928207, 0.09401457011699677, 0.09424161165952682, 0.09471586346626282, 0.08310909569263458, 0.09576819092035294, 0.08851232379674911, 0.08400075137615204, 0.08717573434114456, 0.09311052411794662, 0.09128175675868988, 0.08643251657485962, 0.08373182266950607, 0.09346892684698105, 0.08783676475286484, 0.08858641982078552, 0.08934687823057175, 0.09056723117828369, 0.08522813022136688, 0.09539274871349335, 0.08663354068994522, 0.08810674399137497, 0.08817588537931442, 0.08323188126087189, 0.08659373223781586, 0.08882081508636475, 0.08783871680498123, 0.08067840337753296, 0.09097278118133545, 0.08767257630825043, 0.08831323683261871, 0.08360561728477478, 0.08696094900369644, 0.08997455984354019, 0.08909548074007034, 0.08098310232162476, 0.0919753685593605, 0.08267898857593536, 0.08791734278202057, 0.0877506360411644, 0.08391604572534561, 0.08537942916154861, 0.08333497494459152, 0.09205211699008942, 0.08469881117343903, 0.07841391116380692, 0.0949050709605217, 0.08297061175107956, 0.08708818256855011, 0.08372556418180466, 0.08404924720525742, 0.08545541763305664, 0.0841495469212532, 0.08043068647384644, 0.08810800313949585, 0.08291585743427277, 0.08224690705537796, 0.08068431913852692, 0.08507932722568512, 0.08822619169950485, 0.08482328057289124, 0.07878002524375916, 0.0807420089840889, 0.08231783658266068, 0.08467216044664383, 0.0830559954047203, 0.07971246540546417, 0.08591829240322113, 0.07770848274230957, 0.08137883245944977, 0.07811997085809708, 0.08073104918003082, 0.08706701546907425, 0.08247820287942886, 0.08544813096523285, 0.08633113652467728, 0.08053197711706161, 0.08222626894712448, 0.07730687409639359, 0.08355763554573059, 0.08137306571006775, 0.07764628529548645, 0.084803007543087, 0.07628284394741058, 0.0871272012591362, 0.07785579562187195, 0.07569773495197296, 0.08328769356012344, 0.07958187162876129, 0.07911553233861923]}, "state": "available", "life": [{"dt": "Sun May 15 22:04:58 2016", "state": "available"}], "summary": "98cdc8348e72cf52c9d6f83a50e0e5b9"}