{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 64, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.04493353589346622, 0.0449506936104287, 0.03944981143962387, 0.0358013726168425, 0.03686034993161659, 0.03562584786381276, 0.034083721258223315, 0.03681897972505256, 0.03262396690534852, 0.03274830622909116, 0.03330290589122281, 0.02969997713662833, 0.028538238599313856, 0.02767713622222206, 0.028508978389333903, 0.026520171561767412, 0.025524523603386712, 0.02599676724137931, 0.02670323251162547, 0.025729838864275257, 0.02603407910668467, 0.026392613947315964, 0.025520613732806723, 0.025209817824055316, 0.024869256256129894, 0.02634203784866664, 0.025511725434809732, 0.028013355582655525, 0.026291019406702244, 0.027220052082407565, 0.026381612440398907, 0.029475233067949243, 0.028169979577159814, 0.02719737981559264, 0.02682221167479533, 0.027533856408731107, 0.027561847849987147, 0.027189373325273988, 0.027914734977107558, 0.026000954415342045, 0.02865624698115381, 0.029689895625853564, 0.028785434404966944, 0.029724097771544594, 0.028992689194450312, 0.027331803700886118, 0.02846343813305267, 0.027724295329878936, 0.027907259368400678, 0.02749066084679441, 0.028684408198813967, 0.028847138237347157, 0.02907112100920034, 0.02886977183010063, 0.029411454128064535, 0.029525972678656396, 0.029195987233953495, 0.02887071451139775, 0.02901520924900338, 0.028523294198810858, 0.027878638532206134, 0.02898017049130642, 0.028458338186650917, 0.029277902720353997, 0.028066738129165148, 0.028872285578496096, 0.028544595545597976, 0.02841239747488555, 0.027727240110909507, 0.02909763366496987, 0.029328365086760406, 0.028764942173135702, 0.028748537870590733, 0.028460569525651368, 0.028308437822967436, 0.028821654199880326, 0.02811033935505513, 0.027907259368400678, 0.029177338004587782, 0.02831772973098125, 0.028391955519787453, 0.028611892343430503, 0.02866131171201364, 0.028872285578496096, 0.02917485053997365, 0.02899394076747893, 0.028148072453406583, 0.02809871897680807, 0.027938123950609274, 0.027910834909330638, 0.028390357872171815, 0.0289225146660775, 0.028528064538883822, 0.02811937410702677, 0.028056069745582068, 0.02828535506122131, 0.02821631498712247, 0.028539510101847375, 0.028357426305668256, 0.028704641885058935, 0.02787310611997818, 0.029058636095156996, 0.028018536525104602, 0.028110662074769453, 0.027218718942428197, 0.028575089202165442, 0.028567468839559924, 0.027756017063220867, 0.028629642444522235, 0.02794494204709561, 0.028056393089534618, 0.028084509762763252, 0.028993940767478927, 0.028519159258648466, 0.028368940713395237, 0.028380130797678527, 0.028468537165833453, 0.02784086611169027, 0.028468537165833453, 0.028679347546760242, 0.028372777810894305, 0.027771374355858913, 0.02818543317748676, 0.028189295253851485, 0.028011088619003296, 0.028009469246918172, 0.02834558717926228, 0.028110339355055133, 0.028347187350154502, 0.028421017014877013, 0.028504204855085934, 0.028575089202165446, 0.028704641885058935, 0.028407288365181207, 0.027980952958187037, 0.02852074969119423, 0.02834558717926228, 0.028322534694121267, 0.02865276445937049, 0.02886222927130867, 0.028587785292542703, 0.028611892343430503, 0.028047014601458024, 0.02956895619046425, 0.028149361579693194, 0.029053952869238942, 0.02833182197882425, 0.028221458669970185, 0.027581589411001212, 0.027907259368400678, 0.028933490655340493, 0.028334063281118548, 0.028848396125030894, 0.02833406328111855, 0.028296898795770634, 0.02814807245340658, 0.028564928266933652, 0.02842357045066806, 0.02921182971788495, 0.02768631233981879, 0.028167081083206317, 0.027931628978081522, 0.027980952958187037, 0.027771701014499963, 0.02874001655893301, 0.0280292191944547, 0.027858781833609944, 0.028295936997765683, 0.027695485417168052, 0.02827509000733174, 0.02793162897808152, 0.027997807002965912, 0.027960195571830004, 0.02831901113420065, 0.028564928266933656, 0.027847382253265166, 0.028507705525023002, 0.028943835632908824, 0.027597043769876952, 0.028178995207284577, 0.028569056582722997, 0.027910834909330638, 0.028464394271276484, 0.027964413162924497], "moving_avg_accuracy_train": [0.054489834337349385, 0.11544474774096383, 0.1800650884789156, 0.24119779273343367, 0.29870687264683726, 0.3528699767978162, 0.4037510890577936, 0.45083597638695405, 0.49435130570609, 0.5346657986294568, 0.5707088196701257, 0.60471945125733, 0.6362561657701512, 0.6648345213316902, 0.6915010127226175, 0.7158185319322835, 0.7381749317511034, 0.759036937822981, 0.7781139481069479, 0.7959068454649278, 0.8124710931473507, 0.8276283513024951, 0.8415852074373058, 0.8543628689224909, 0.8659333591386756, 0.8764950495802297, 0.8862005898029297, 0.8952579592864922, 0.9034495955867586, 0.9108644251847093, 0.9176671957686481, 0.9239379385411809, 0.9296216108015206, 0.9348875182454649, 0.9397703778667015, 0.9443437918872603, 0.9484575113431125, 0.9522869296365121, 0.9557804693535837, 0.9589270082615989, 0.961874198248692, 0.9646043236045456, 0.9670543769368621, 0.9693323729781157, 0.9714108073670511, 0.9733002236182977, 0.975052467822733, 0.9766059559802188, 0.978039392761715, 0.979350664328917, 0.9805166897634952, 0.9816131759076275, 0.9825929539493949, 0.9834629883737326, 0.9842813167953954, 0.9850131060495908, 0.9856952480048727, 0.986335060553783, 0.986859122269489, 0.9873543094401305, 0.9878117437069608, 0.988235200360361, 0.9886327834869755, 0.9889482513732176, 0.989265116747944, 0.9895667677237521, 0.9898406067646299, 0.9900894150640706, 0.9903180488588683, 0.9905238192741863, 0.9906713620455628, 0.990825329003657, 0.9909827245671468, 0.9911243805742875, 0.9912448114927623, 0.9913508461567392, 0.9914368647037158, 0.9915025155827418, 0.9915639545365158, 0.9916545470346715, 0.9917431397709633, 0.9918016947697705, 0.9918402752927935, 0.9918749977635142, 0.9919156606377652, 0.9919428445739886, 0.9919837822551439, 0.9920159198428826, 0.9920307246959438, 0.9920699338528554, 0.9920911031181723, 0.9921407465714153, 0.9921666003781292, 0.9921851624788705, 0.9921642177671279, 0.9921806649663187, 0.9922001737708916, 0.9922059658817542, 0.992197059805627, 0.9922055164756667, 0.9922178338040036, 0.9922336257248081, 0.9922548979414839, 0.9922410986593837, 0.9922686830705538, 0.9922793900647032, 0.9922843200341365, 0.9922958164945783, 0.9923179291222289, 0.9923284178365122, 0.9923402108420175, 0.9923414118963699, 0.9923519054958895, 0.9923707623860596, 0.9923736146113091, 0.992373828451383, 0.9923599019315459, 0.9923591338769455, 0.9923584426278051, 0.9923695863168318, 0.9923513776851487, 0.9923444025672362, 0.9923404781237657, 0.9923510651005457, 0.9923629465422984, 0.992366580351924, 0.9923792634311894, 0.9923765592266247, 0.9923858912557695, 0.992387230594048, 0.9923954954864505, 0.9923841085884079, 0.9923762135428201, 0.9923785206523935, 0.9923758907257083, 0.992371170629041, 0.9923669225420405, 0.9924007498661497, 0.9924006033433902, 0.9924098841235091, 0.9923899988738087, 0.9923885742876327, 0.9923802326721225, 0.9923844910314163, 0.9923718514162265, 0.9923863605517124, 0.9923735339844929, 0.992364343236646, 0.9923819563527405, 0.9923789828560207, 0.9923951320101776, 0.9924002535983164, 0.9923883908890871, 0.9924106587278892, 0.9923954023430521, 0.9924004968979036, 0.9923933161840167, 0.9924009725174222, 0.9923984505668848, 0.9923891213234494, 0.992394843980261, 0.992397641208741, 0.9923836865758187, 0.9923899527073934, 0.9924002985511119, 0.9923907845092538, 0.9923798687089308, 0.9923700444886402, 0.992382381154234, 0.9924123094544732, 0.992394534834327, 0.9924044224653522, 0.9924015555200218, 0.9923942689439232, 0.9923877110254344, 0.9924029873626501, 0.9924167360661442, 0.9924149909233853, 0.9924016544816492, 0.9924061238226408, 0.9924266183680875, 0.9924403571336884, 0.992400952444416, 0.9923819603626249], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234423, "moving_var_accuracy_train": [0.026722278415006028, 0.057489563785884656, 0.08932270334109713, 0.11402530077222076, 0.13238841914737487, 0.1455523538940655, 0.1542971067679708, 0.1588202756243713, 0.1599805030337098, 0.15860977778735263, 0.1544406943002602, 0.149407132418879, 0.14341749843735982, 0.13642625025303962, 0.1291835410956573, 0.12158726265070383, 0.11392681390136379, 0.10645114218731494, 0.09908141886095469, 0.09202256174238369, 0.08528967427970718, 0.0788283891247717, 0.07269869471080477, 0.06689824293699413, 0.06141330483788014, 0.05627591809894107, 0.05149610388817698, 0.04708481697701519, 0.042980261426796264, 0.03917705256581644, 0.03567584649779411, 0.03246216178228815, 0.029506682777325932, 0.026805582530467033, 0.024339605140146662, 0.022093889668362985, 0.020036804891379805, 0.018165104402434237, 0.01645843733998363, 0.014901699969882144, 0.013489703332274129, 0.012207815259174793, 0.011041058585238073, 0.009983656120389968, 0.009024169513932954, 0.008153881606473935, 0.007366126683594335, 0.006651233944333938, 0.006004603218959459, 0.005419617795170085, 0.004889892553479825, 0.004411723834910312, 0.003979191136519447, 0.0035880846619632946, 0.0032353031484182765, 0.0029165924731894516, 0.0026291210846949085, 0.0023698932171051057, 0.002135375661531414, 0.0019240449883839838, 0.001733523704521821, 0.0015617851739054198, 0.001407029307597995, 0.0012672220567234468, 0.0011414034840424067, 0.0010280820754390194, 0.0009259487582778974, 0.0008339110325789425, 0.0007509903900301608, 0.0006762724242015259, 0.0006088411016058426, 0.0005481703438629218, 0.0004935762697472858, 0.0004443992405917888, 0.00040008984898773223, 0.00036018205423864105, 0.00032423044152859277, 0.00029184618771698547, 0.0002626955416506545, 0.00023649985049208782, 0.00021292050349919222, 0.00019165931134024092, 0.00017250677631702735, 0.0001552669495350811, 0.0001397551358056542, 0.0001257862729225862, 0.00011322272867397298, 0.00010190975122748659, 9.172074875780539e-05, 8.256251010389635e-05, 7.431029233365323e-05, 6.690144335233691e-05, 6.021731479099761e-05, 5.419868427615322e-05, 4.8782763977087676e-05, 4.390692217262991e-05, 3.95196552964697e-05, 3.556799170375694e-05, 3.2011906397109104e-05, 2.8811359394811643e-05, 2.593158890452672e-05, 2.3340674476938293e-05, 2.101067959406519e-05, 1.8911325416337e-05, 1.7027040972359692e-05, 1.5325368632637168e-05, 1.379305051076097e-05, 1.2414934977109076e-05, 1.1177842194112697e-05, 1.0061048092847244e-05, 9.056194958372177e-06, 8.150588445318978e-06, 7.336520641464987e-06, 6.606068818080471e-06, 5.945535152972287e-06, 5.350982049223253e-06, 4.817629375893885e-06, 4.335871747475319e-06, 3.9022888731561536e-06, 3.513177622086656e-06, 3.1648438482879556e-06, 2.8487973338881938e-06, 2.5640562118083606e-06, 2.3086593473235984e-06, 2.079063930514335e-06, 1.8712763786144558e-06, 1.6855964852498894e-06, 1.5171026512258518e-06, 1.3661761670148944e-06, 1.2295746947566225e-06, 1.107232001298774e-06, 9.976757541921966e-07, 8.984691644764737e-07, 8.086701528200797e-07, 7.278653861673944e-07, 6.552793613635858e-07, 5.899138414157009e-07, 5.412210479816338e-07, 4.870991364037421e-07, 4.391644186798915e-07, 3.9880678521269245e-07, 3.589443717033804e-07, 3.2367617747693085e-07, 2.9147176234411125e-07, 2.637624249590217e-07, 2.3928081757602254e-07, 2.1683342325812995e-07, 1.9591030954618068e-07, 1.791112753185674e-07, 1.6127972293139663e-07, 1.474989072581293e-07, 1.3298509251789438e-07, 1.209530980984192e-07, 1.133204980928349e-07, 1.040832637882345e-07, 9.390852781161765e-08, 8.498173889776492e-08, 7.701113997894265e-08, 6.93672680916664e-08, 6.321385433020315e-08, 5.718720810604262e-08, 5.153890767995828e-08, 4.813760293192392e-08, 4.367722228294174e-08, 4.0272828394875256e-08, 3.706019848769967e-08, 3.4426570909146915e-08, 3.1852551557108604e-08, 3.003703626315978e-08, 3.5094661033739796e-08, 3.442862902241304e-08, 3.186565334577333e-08, 2.875306239094386e-08, 2.635560387301218e-08, 2.4107100139851973e-08, 2.3796688434387712e-08, 2.3118261220848842e-08, 2.0833844808007694e-08, 2.0351206430833784e-08, 1.8495860867852244e-08, 2.042651231869101e-08, 2.0082644208945473e-08, 3.2048945617975436e-08, 3.209034359297892e-08], "duration": 116982.37874, "accuracy_train": [0.544898343373494, 0.664038968373494, 0.7616481551204819, 0.7913921310240963, 0.8162885918674698, 0.8403379141566265, 0.8616810993975904, 0.8745999623493976, 0.8859892695783133, 0.897496234939759, 0.8950960090361446, 0.9108151355421686, 0.9200865963855421, 0.9220397213855421, 0.9314994352409639, 0.9346762048192772, 0.9393825301204819, 0.9467949924698795, 0.9498070406626506, 0.956042921686747, 0.9615493222891566, 0.9640436746987951, 0.9671969126506024, 0.9693618222891566, 0.9700677710843374, 0.9715502635542169, 0.9735504518072289, 0.9767742846385542, 0.9771743222891566, 0.9775978915662651, 0.9788921310240963, 0.9803746234939759, 0.9807746611445783, 0.9822806852409639, 0.9837161144578314, 0.9855045180722891, 0.9854809864457831, 0.9867516942771084, 0.9872223268072289, 0.9872458584337349, 0.9883989081325302, 0.9891754518072289, 0.9891048569277109, 0.9898343373493976, 0.9901167168674698, 0.9903049698795181, 0.9908226656626506, 0.9905873493975904, 0.9909403237951807, 0.9911521084337349, 0.9910109186746988, 0.9914815512048193, 0.9914109563253012, 0.9912932981927711, 0.9916462725903614, 0.9915992093373494, 0.9918345256024096, 0.9920933734939759, 0.9915756777108434, 0.9918109939759037, 0.9919286521084337, 0.9920463102409639, 0.992211031626506, 0.9917874623493976, 0.9921169051204819, 0.9922816265060241, 0.9923051581325302, 0.9923286897590361, 0.9923757530120482, 0.9923757530120482, 0.9919992469879518, 0.992211031626506, 0.9923992846385542, 0.9923992846385542, 0.9923286897590361, 0.9923051581325302, 0.992211031626506, 0.9920933734939759, 0.9921169051204819, 0.9924698795180723, 0.9925404743975904, 0.9923286897590361, 0.9921875, 0.9921875, 0.9922816265060241, 0.9921875, 0.9923522213855421, 0.9923051581325302, 0.992163968373494, 0.9924228162650602, 0.9922816265060241, 0.9925875376506024, 0.9923992846385542, 0.9923522213855421, 0.9919757153614458, 0.9923286897590361, 0.9923757530120482, 0.9922580948795181, 0.9921169051204819, 0.9922816265060241, 0.9923286897590361, 0.9923757530120482, 0.9924463478915663, 0.9921169051204819, 0.9925169427710844, 0.9923757530120482, 0.9923286897590361, 0.9923992846385542, 0.9925169427710844, 0.9924228162650602, 0.9924463478915663, 0.9923522213855421, 0.9924463478915663, 0.9925404743975904, 0.9923992846385542, 0.9923757530120482, 0.9922345632530121, 0.9923522213855421, 0.9923522213855421, 0.9924698795180723, 0.9921875, 0.9922816265060241, 0.9923051581325302, 0.9924463478915663, 0.9924698795180723, 0.9923992846385542, 0.9924934111445783, 0.9923522213855421, 0.9924698795180723, 0.9923992846385542, 0.9924698795180723, 0.9922816265060241, 0.9923051581325302, 0.9923992846385542, 0.9923522213855421, 0.9923286897590361, 0.9923286897590361, 0.9927051957831325, 0.9923992846385542, 0.9924934111445783, 0.992211031626506, 0.9923757530120482, 0.9923051581325302, 0.9924228162650602, 0.9922580948795181, 0.9925169427710844, 0.9922580948795181, 0.9922816265060241, 0.9925404743975904, 0.9923522213855421, 0.9925404743975904, 0.9924463478915663, 0.9922816265060241, 0.9926110692771084, 0.9922580948795181, 0.9924463478915663, 0.9923286897590361, 0.9924698795180723, 0.9923757530120482, 0.9923051581325302, 0.9924463478915663, 0.9924228162650602, 0.9922580948795181, 0.9924463478915663, 0.9924934111445783, 0.9923051581325302, 0.9922816265060241, 0.9922816265060241, 0.9924934111445783, 0.9926816641566265, 0.9922345632530121, 0.9924934111445783, 0.9923757530120482, 0.9923286897590361, 0.9923286897590361, 0.9925404743975904, 0.9925404743975904, 0.9923992846385542, 0.9922816265060241, 0.9924463478915663, 0.9926110692771084, 0.9925640060240963, 0.9920463102409639, 0.992211031626506], "end": "2016-01-22 01:16:39.059000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0], "accuracy_valid": [0.537176724137931, 0.6544989224137931, 0.7432650862068966, 0.765625, 0.783270474137931, 0.7986260775862069, 0.8200431034482759, 0.8236799568965517, 0.8331088362068966, 0.841729525862069, 0.8362068965517241, 0.8481950431034483, 0.8518318965517241, 0.8512931034482759, 0.8558728448275862, 0.8550646551724138, 0.8566810344827587, 0.8627424568965517, 0.8581627155172413, 0.8620689655172413, 0.8639547413793104, 0.8669181034482759, 0.8657058189655172, 0.8669181034482759, 0.8666487068965517, 0.8643588362068966, 0.8634159482758621, 0.8666487068965517, 0.8667834051724138, 0.8654364224137931, 0.8663793103448276, 0.8650323275862069, 0.8654364224137931, 0.8686691810344828, 0.8701508620689655, 0.8696120689655172, 0.8719019396551724, 0.87109375, 0.8712284482758621, 0.8708243534482759, 0.8706896551724138, 0.8720366379310345, 0.8721713362068966, 0.8728448275862069, 0.8724407327586207, 0.8719019396551724, 0.8719019396551724, 0.8725754310344828, 0.8732489224137931, 0.8740571120689655, 0.8719019396551724, 0.8724407327586207, 0.8723060344827587, 0.8727101293103449, 0.8727101293103449, 0.8713631465517241, 0.8727101293103449, 0.8717672413793104, 0.8727101293103449, 0.872979525862069, 0.8743265086206896, 0.8732489224137931, 0.8737877155172413, 0.8743265086206896, 0.8743265086206896, 0.8740571120689655, 0.8736530172413793, 0.872979525862069, 0.8743265086206896, 0.8743265086206896, 0.8741918103448276, 0.8736530172413793, 0.8741918103448276, 0.8744612068965517, 0.8733836206896551, 0.8741918103448276, 0.8739224137931034, 0.8732489224137931, 0.8748653017241379, 0.8727101293103449, 0.8727101293103449, 0.8747306034482759, 0.873114224137931, 0.8740571120689655, 0.8745959051724138, 0.8745959051724138, 0.8735183189655172, 0.8736530172413793, 0.8744612068965517, 0.8736530172413793, 0.8747306034482759, 0.8735183189655172, 0.8741918103448276, 0.8741918103448276, 0.875, 0.8736530172413793, 0.8743265086206896, 0.875, 0.8740571120689655, 0.8748653017241379, 0.8736530172413793, 0.8725754310344828, 0.875, 0.8735183189655172, 0.8727101293103449, 0.8747306034482759, 0.8733836206896551, 0.8732489224137931, 0.8739224137931034, 0.8732489224137931, 0.8727101293103449, 0.8733836206896551, 0.8745959051724138, 0.8739224137931034, 0.8724407327586207, 0.8741918103448276, 0.8740571120689655, 0.8743265086206896, 0.8740571120689655, 0.8737877155172413, 0.8748653017241379, 0.8739224137931034, 0.8735183189655172, 0.872979525862069, 0.8737877155172413, 0.8733836206896551, 0.8733836206896551, 0.8739224137931034, 0.8737877155172413, 0.8728448275862069, 0.8732489224137931, 0.8747306034482759, 0.8748653017241379, 0.8754040948275862, 0.875, 0.8748653017241379, 0.8733836206896551, 0.8736530172413793, 0.8740571120689655, 0.8737877155172413, 0.873114224137931, 0.8747306034482759, 0.8733836206896551, 0.8754040948275862, 0.8748653017241379, 0.8740571120689655, 0.8724407327586207, 0.8745959051724138, 0.8735183189655172, 0.8732489224137931, 0.8733836206896551, 0.8739224137931034, 0.8754040948275862, 0.8739224137931034, 0.8739224137931034, 0.8735183189655172, 0.8741918103448276, 0.8744612068965517, 0.8739224137931034, 0.8725754310344828, 0.8747306034482759, 0.8741918103448276, 0.875, 0.8735183189655172, 0.8735183189655172, 0.8743265086206896, 0.8733836206896551, 0.8745959051724138, 0.8739224137931034, 0.8744612068965517, 0.8741918103448276, 0.8739224137931034, 0.8739224137931034, 0.8732489224137931, 0.8741918103448276, 0.8735183189655172, 0.8736530172413793, 0.8732489224137931, 0.8744612068965517, 0.8743265086206896, 0.8737877155172413, 0.8736530172413793, 0.8747306034482759, 0.872979525862069], "accuracy_test": 0.8629807692307693, "start": "2016-01-20 16:46:56.681000", "learning_rate_per_epoch": [0.0014628118369728327, 0.0013261525891721249, 0.0012022603768855333, 0.0010899424087256193, 0.000988117535598576, 0.0008958053076639771, 0.0008121171267703176, 0.0007362472824752331, 0.0006674653850495815, 0.0006051092641428113, 0.0005485785659402609, 0.0004973291070200503, 0.0004508674901444465, 0.00040874641854315996, 0.000370560388546437, 0.0003359417896717787, 0.0003045573248527944, 0.0002761048963293433, 0.0002503105497453362, 0.00022692597121931612, 0.0002057260280707851, 0.00018650662968866527, 0.00016908274847082794, 0.00015328664449043572, 0.00013896625023335218, 0.00012598370085470378, 0.00011421401723055169, 0.00010354388359701261, 9.38705779844895e-05, 8.510097541147843e-05, 7.715064566582441e-05, 6.994305294938385e-05, 6.34088137303479e-05, 5.7485016441205516e-05, 5.211463576415554e-05, 4.7245968744391575e-05, 4.283214366296306e-05, 3.883066528942436e-05, 3.520301470416598e-05, 3.191426731063984e-05, 2.8932763598277234e-05, 2.6229798095300794e-05, 2.37793501582928e-05, 2.1557829313678667e-05, 1.9543847884051502e-05, 1.771801726135891e-05, 1.6062758732005022e-05, 1.4562138858309481e-05, 1.3201710316934623e-05, 1.1968376384174917e-05, 1.0850263606698718e-05, 9.836607205215842e-06, 8.917649211070966e-06, 8.084542059805244e-06, 7.3292658271384425e-06, 6.6445491029298864e-06, 6.0238003243284766e-06, 5.461043201648863e-06, 4.9508598749525845e-06, 4.488339072850067e-06, 4.069028364028782e-06, 3.688890501507558e-06, 3.344265905980137e-06, 3.031836968148127e-06, 2.7485957616590895e-06, 2.4918156213971088e-06, 2.2590245407627663e-06, 2.047981297437218e-06, 1.856654193943541e-06, 1.6832012761369697e-06, 1.5259527117450489e-06, 1.3833947605235153e-06, 1.2541548812805559e-06, 1.1369888852641452e-06, 1.0307687716704095e-06, 9.344719842374616e-07, 8.471714636471006e-07, 7.680267799514695e-07, 6.962759471207391e-07, 6.312282607723319e-07, 5.722574769606581e-07, 5.18795900461555e-07, 4.703288141172379e-07, 4.2638961872398795e-07, 3.8655531398035237e-07, 3.5044243418269616e-07, 3.177032965595572e-07, 2.8802273277506174e-07, 2.611149909625965e-07, 2.3672103566241276e-07, 2.146060182894871e-07, 1.945570318184764e-07, 1.7638106442063872e-07, 1.5990313784186583e-07, 1.449646163109719e-07, 1.314216859782391e-07, 1.1914396935708282e-07, 1.0801326766340935e-07, 9.792242394723871e-08, 8.877428570031043e-08, 8.04807882559544e-08, 7.296208792695325e-08, 6.61458017248151e-08, 5.996631102789252e-08, 5.43641220929203e-08, 4.9285301173540574e-08, 4.4680955824105695e-08, 4.0506758836045265e-08, 3.672252546493837e-08, 3.3291822632008916e-08, 3.018162431089877e-08, 2.7361988230723e-08, 2.4805768106261894e-08, 2.248835606621924e-08, 2.038744284504901e-08, 1.848280106742095e-08, 1.6756095178038777e-08, 1.519070202959938e-08, 1.3771551898855705e-08, 1.2484981937177508e-08, 1.131860649650207e-08, 1.0261196337069123e-08, 9.302572046010482e-09, 8.433504561367045e-09, 7.645627242425235e-09, 6.931355045480814e-09, 6.283812137297673e-09, 5.6967639494587274e-09, 5.164559446768635e-09, 4.682074283834936e-09, 4.244664175701018e-09, 3.848117824389874e-09, 3.4886178390536315e-09, 3.1627032104353248e-09, 2.867236226222758e-09, 2.5993724950268415e-09, 2.356533190805976e-09, 2.136380405914906e-09, 1.936794946644227e-09, 1.755855127960615e-09, 1.5918191209607357e-09, 1.4431077444143625e-09, 1.3082893657312411e-09, 1.1860660231732822e-09, 1.0752609913566857e-09, 9.748076790216942e-10, 8.837389153804054e-10, 8.011780128214241e-10, 7.263301071702699e-10, 6.584746636839611e-10, 5.969584271348083e-10, 5.411892045614763e-10, 4.906300921092566e-10, 4.4479431249300205e-10, 4.0324060757157554e-10, 3.6556893623362896e-10, 3.314166441281685e-10, 3.0045493870645146e-10, 2.7238575284194155e-10, 2.4693885825044504e-10, 2.2386927034379056e-10, 2.0295488900590186e-10, 1.8399437529126317e-10, 1.668051946568383e-10, 1.5122186836080687e-10, 1.370943775169664e-10, 1.2428670592701252e-10, 1.1267555638516669e-10, 1.0214915024953086e-10, 9.260614497463848e-11, 8.395466960520181e-11, 7.611142965879836e-11, 6.90009230308597e-11, 6.255469753302378e-11, 5.671069516766103e-11, 5.14126519135516e-11, 4.6609563431054823e-11, 4.2255192400642017e-11, 3.830761402534577e-11, 3.472882745270134e-11, 3.148438107447582e-11, 2.85430377250373e-11, 2.5876481513087413e-11, 2.345904200062865e-11], "accuracy_train_last": 0.992211031626506, "error_valid": [0.46282327586206895, 0.34550107758620685, 0.2567349137931034, 0.234375, 0.21672952586206895, 0.20137392241379315, 0.1799568965517241, 0.1763200431034483, 0.16689116379310343, 0.15827047413793105, 0.1637931034482759, 0.1518049568965517, 0.1481681034482759, 0.1487068965517241, 0.1441271551724138, 0.1449353448275862, 0.14331896551724133, 0.1372575431034483, 0.14183728448275867, 0.13793103448275867, 0.1360452586206896, 0.1330818965517241, 0.13429418103448276, 0.1330818965517241, 0.1333512931034483, 0.13564116379310343, 0.1365840517241379, 0.1333512931034483, 0.1332165948275862, 0.13456357758620685, 0.13362068965517238, 0.13496767241379315, 0.13456357758620685, 0.13133081896551724, 0.12984913793103448, 0.13038793103448276, 0.12809806034482762, 0.12890625, 0.1287715517241379, 0.1291756465517241, 0.1293103448275862, 0.12796336206896552, 0.12782866379310343, 0.12715517241379315, 0.12755926724137934, 0.12809806034482762, 0.12809806034482762, 0.12742456896551724, 0.12675107758620685, 0.12594288793103448, 0.12809806034482762, 0.12755926724137934, 0.12769396551724133, 0.12728987068965514, 0.12728987068965514, 0.1286368534482759, 0.12728987068965514, 0.1282327586206896, 0.12728987068965514, 0.12702047413793105, 0.1256734913793104, 0.12675107758620685, 0.12621228448275867, 0.1256734913793104, 0.1256734913793104, 0.12594288793103448, 0.12634698275862066, 0.12702047413793105, 0.1256734913793104, 0.1256734913793104, 0.12580818965517238, 0.12634698275862066, 0.12580818965517238, 0.1255387931034483, 0.12661637931034486, 0.12580818965517238, 0.12607758620689657, 0.12675107758620685, 0.1251346982758621, 0.12728987068965514, 0.12728987068965514, 0.1252693965517241, 0.12688577586206895, 0.12594288793103448, 0.1254040948275862, 0.1254040948275862, 0.12648168103448276, 0.12634698275862066, 0.1255387931034483, 0.12634698275862066, 0.1252693965517241, 0.12648168103448276, 0.12580818965517238, 0.12580818965517238, 0.125, 0.12634698275862066, 0.1256734913793104, 0.125, 0.12594288793103448, 0.1251346982758621, 0.12634698275862066, 0.12742456896551724, 0.125, 0.12648168103448276, 0.12728987068965514, 0.1252693965517241, 0.12661637931034486, 0.12675107758620685, 0.12607758620689657, 0.12675107758620685, 0.12728987068965514, 0.12661637931034486, 0.1254040948275862, 0.12607758620689657, 0.12755926724137934, 0.12580818965517238, 0.12594288793103448, 0.1256734913793104, 0.12594288793103448, 0.12621228448275867, 0.1251346982758621, 0.12607758620689657, 0.12648168103448276, 0.12702047413793105, 0.12621228448275867, 0.12661637931034486, 0.12661637931034486, 0.12607758620689657, 0.12621228448275867, 0.12715517241379315, 0.12675107758620685, 0.1252693965517241, 0.1251346982758621, 0.12459590517241381, 0.125, 0.1251346982758621, 0.12661637931034486, 0.12634698275862066, 0.12594288793103448, 0.12621228448275867, 0.12688577586206895, 0.1252693965517241, 0.12661637931034486, 0.12459590517241381, 0.1251346982758621, 0.12594288793103448, 0.12755926724137934, 0.1254040948275862, 0.12648168103448276, 0.12675107758620685, 0.12661637931034486, 0.12607758620689657, 0.12459590517241381, 0.12607758620689657, 0.12607758620689657, 0.12648168103448276, 0.12580818965517238, 0.1255387931034483, 0.12607758620689657, 0.12742456896551724, 0.1252693965517241, 0.12580818965517238, 0.125, 0.12648168103448276, 0.12648168103448276, 0.1256734913793104, 0.12661637931034486, 0.1254040948275862, 0.12607758620689657, 0.1255387931034483, 0.12580818965517238, 0.12607758620689657, 0.12607758620689657, 0.12675107758620685, 0.12580818965517238, 0.12648168103448276, 0.12634698275862066, 0.12675107758620685, 0.1255387931034483, 0.1256734913793104, 0.12621228448275867, 0.12634698275862066, 0.1252693965517241, 0.12702047413793105], "accuracy_train_std": [0.047395196098544035, 0.04684557032187119, 0.04140867610969244, 0.037992399462270794, 0.038823924824442205, 0.03878305468478653, 0.035182575404338834, 0.03517488607795662, 0.03243388077246877, 0.03046712302483063, 0.031059001510192807, 0.02922112620194921, 0.027382725589403353, 0.027262453135361197, 0.024379980224954034, 0.02675114382297638, 0.02450626561830292, 0.021867288590435328, 0.020701559731936697, 0.019888888403783257, 0.019280028983717768, 0.018168366366332653, 0.017219556110552866, 0.016661122758952974, 0.016408682636114273, 0.015318112858254435, 0.014531651059471656, 0.014264184111878436, 0.0144257123589547, 0.01388150572938863, 0.014008790528859987, 0.013138404831728998, 0.013425880777083047, 0.012405676205250201, 0.01164508847455754, 0.010990655186310422, 0.01078197967350088, 0.010185117544719667, 0.010262676232111918, 0.010175136252506274, 0.009898147304500499, 0.009802271248706577, 0.009657346261817724, 0.009234716919673099, 0.00963222801616418, 0.009536784994626402, 0.009114367320432767, 0.009035409185060494, 0.008917306681990294, 0.008767923192279458, 0.008771080369618893, 0.00850309389584726, 0.00861511454231218, 0.008614375346820009, 0.00848271889772572, 0.008479584960539946, 0.008405601074569048, 0.00844519881632453, 0.008553479193744289, 0.008524037348394901, 0.008343181235398907, 0.008466285487891693, 0.00823630459001193, 0.008619484141746382, 0.008390831606520287, 0.008112099486871437, 0.008123116106517498, 0.008156619806285733, 0.007973298483222058, 0.007903824625463378, 0.008487058794314646, 0.008325109005657408, 0.007984229205004523, 0.008233614898923197, 0.008223958393423606, 0.008032078723676547, 0.008258595217324081, 0.008531829212066761, 0.008258327013629337, 0.00817547091513873, 0.00800224090302067, 0.008111416851326249, 0.008180346134272438, 0.008291952194585372, 0.008020937066719407, 0.008336176253271184, 0.00814490068972091, 0.00825779058010699, 0.008456567266983562, 0.008354887338866822, 0.008134730465667345, 0.008022973385072922, 0.008166355938317908, 0.008167440780903942, 0.008300328866752204, 0.008156619806285733, 0.008267608501651933, 0.008258327013629337, 0.008191270968022616, 0.008089405195732454, 0.008400889534449051, 0.00817817972900821, 0.008005838379855232, 0.008280558358480222, 0.008128601786824905, 0.00817817972900821, 0.008179127601917538, 0.008075806006503232, 0.00819617056931119, 0.008354887338866822, 0.008051634088349786, 0.00814490068972091, 0.00818748444136635, 0.008116296433149801, 0.00814381284455528, 0.008245342241073483, 0.00851064468116672, 0.008167440780903942, 0.00812229804804401, 0.008107729626215308, 0.008202788827018689, 0.008291417936526468, 0.008190730142815955, 0.008209907620781897, 0.008287142632117685, 0.008300328866752204, 0.008072788474834702, 0.00812229804804401, 0.008039417559707888, 0.00814381284455528, 0.008085023069086, 0.008246953866115634, 0.008280023365248456, 0.008121207175228748, 0.008189918838042867, 0.008156619806285733, 0.008223958393423606, 0.007981766761775227, 0.008030148150959633, 0.008072788474834702, 0.008325109005657408, 0.008223015688193357, 0.008280023365248456, 0.007972048303572583, 0.008168796631537202, 0.008060466612814867, 0.008324842944855472, 0.00822463168814768, 0.008025181688543485, 0.00814490068972091, 0.008093613910151853, 0.008097170791146099, 0.007928726554223373, 0.007941148235767269, 0.008236035660430534, 0.008119843378116734, 0.008223958393423606, 0.00799355186203394, 0.00817817972900821, 0.008324310797740172, 0.008028768886231542, 0.00801803700931891, 0.008123661433067007, 0.008298994505263826, 0.008049983359937297, 0.008077725667021054, 0.008379638300951859, 0.008246953866115634, 0.008140820520800951, 0.00799476404790266, 0.008225035638526754, 0.00816337187784867, 0.008042172200035327, 0.008156619806285733, 0.008111416851326249, 0.008116296433149801, 0.008048057082196318, 0.008166355938317908, 0.00822463168814768, 0.008119843378116734, 0.008281126750918561, 0.008149148693340215, 0.008246282393959062, 0.008325109005657408], "accuracy_test_std": 0.03416154927821294, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.8764741859734458, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0016135537412986377, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adam", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 6.555877895467794e-09, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.09342228721415143}, "accuracy_valid_max": 0.8754040948275862, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234423\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.872979525862069, "loss_train": [1.482357144355774, 1.0280694961547852, 0.8283547759056091, 0.7020602226257324, 0.6182298064231873, 0.5476089715957642, 0.49669480323791504, 0.45468538999557495, 0.4178439676761627, 0.3885784447193146, 0.3580208718776703, 0.33551570773124695, 0.311596155166626, 0.29082271456718445, 0.2722809612751007, 0.2560167610645294, 0.24197238683700562, 0.22910237312316895, 0.21524615585803986, 0.2046383023262024, 0.19279155135154724, 0.18543416261672974, 0.17766687273979187, 0.169170081615448, 0.16196346282958984, 0.1585492491722107, 0.15173053741455078, 0.14775384962558746, 0.14172260463237762, 0.1372574418783188, 0.1343853771686554, 0.13045763969421387, 0.12744121253490448, 0.1235613003373146, 0.12369688600301743, 0.11964379996061325, 0.11692214012145996, 0.11530029028654099, 0.11356134712696075, 0.11108684539794922, 0.11025670170783997, 0.10870286077260971, 0.10852639377117157, 0.1072467640042305, 0.10488749295473099, 0.10402995347976685, 0.10286804288625717, 0.10197045654058456, 0.10147825628519058, 0.10220623016357422, 0.10107903927564621, 0.09960057586431503, 0.0981479361653328, 0.09896425902843475, 0.09736942499876022, 0.09794718772172928, 0.0968637764453888, 0.09610972553491592, 0.09713602066040039, 0.09619253128767014, 0.09619121253490448, 0.09678678214550018, 0.09638871997594833, 0.09619463980197906, 0.09755480289459229, 0.09560786187648773, 0.09598209708929062, 0.09628410637378693, 0.09559451043605804, 0.09610509872436523, 0.09452793747186661, 0.0959402322769165, 0.09389097988605499, 0.09548801183700562, 0.09541086852550507, 0.09518974274396896, 0.09467211365699768, 0.0959954559803009, 0.09544534981250763, 0.09470308572053909, 0.09414244443178177, 0.09486830979585648, 0.09421935677528381, 0.09383580088615417, 0.09362131357192993, 0.09520986676216125, 0.09513705223798752, 0.09617701917886734, 0.093503937125206, 0.09528207033872604, 0.09362800419330597, 0.09383226186037064, 0.09530172497034073, 0.09506374597549438, 0.09471575170755386, 0.09504017233848572, 0.09490002691745758, 0.09482544660568237, 0.09394265711307526, 0.0952044278383255, 0.09358766674995422, 0.09405171871185303, 0.09527988731861115, 0.09536432474851608, 0.09484493732452393, 0.09508286416530609, 0.09470579028129578, 0.09448764473199844, 0.09285204857587814, 0.09497138112783432, 0.09368998557329178, 0.09408813714981079, 0.09321417659521103, 0.0946134477853775, 0.09412449598312378, 0.093517005443573, 0.09393644332885742, 0.0943223163485527, 0.09439709782600403, 0.09533058851957321, 0.09458102285861969, 0.09425844252109528, 0.09511267393827438, 0.09316624701023102, 0.09344899654388428, 0.09406950324773788, 0.09503199905157089, 0.09459588676691055, 0.09480226039886475, 0.09465280175209045, 0.09465175867080688, 0.09407936036586761, 0.09636461734771729, 0.09409730136394501, 0.09433824568986893, 0.09463892877101898, 0.09491557627916336, 0.09541734308004379, 0.094326451420784, 0.09445030242204666, 0.09504123777151108, 0.09498665481805801, 0.09443595260381699, 0.09555669128894806, 0.09391274303197861, 0.09512437134981155, 0.09543558955192566, 0.09434042125940323, 0.09500524401664734, 0.09544449299573898, 0.09428916871547699, 0.09453943371772766, 0.09450159221887589, 0.09573142230510712, 0.09395231306552887, 0.09399248659610748, 0.09471479803323746, 0.0934474840760231, 0.09466531872749329, 0.09654579311609268, 0.09395768493413925, 0.0942254364490509, 0.09413767606019974, 0.09484174102544785, 0.09492753446102142, 0.09462335705757141, 0.09372860193252563, 0.09490133821964264, 0.09514991194009781, 0.09380178898572922, 0.09425405412912369, 0.09446333348751068, 0.09483286738395691, 0.09467166662216187, 0.0937543734908104, 0.09461133927106857, 0.09502362459897995, 0.09575843065977097, 0.09467300772666931, 0.09396205097436905, 0.09473851323127747, 0.09498710185289383, 0.09505922347307205, 0.09521506726741791], "accuracy_train_first": 0.544898343373494, "model": "residualv4", "loss_std": [0.274278461933136, 0.12314987182617188, 0.10617556422948837, 0.09498660266399384, 0.0887361615896225, 0.08442021906375885, 0.08088666945695877, 0.07903826981782913, 0.07343056052923203, 0.0688931792974472, 0.06854475289583206, 0.06662966310977936, 0.06275250017642975, 0.05944092199206352, 0.05593551695346832, 0.05486786738038063, 0.051605433225631714, 0.051026601344347, 0.05035862699151039, 0.0461510606110096, 0.04576268419623375, 0.04351530596613884, 0.043270520865917206, 0.039367493242025375, 0.03971244767308235, 0.039790861308574677, 0.038948774337768555, 0.03658054769039154, 0.03676415607333183, 0.033678196370601654, 0.034189242869615555, 0.03384583070874214, 0.03383639082312584, 0.032605867832899094, 0.03264942020177841, 0.032997217029333115, 0.030284786596894264, 0.02992035448551178, 0.030568059533834457, 0.02897060289978981, 0.03045024909079075, 0.028889527544379234, 0.029099397361278534, 0.029001623392105103, 0.0300449226051569, 0.028188088908791542, 0.028975114226341248, 0.027793096378445625, 0.026846792548894882, 0.028092650696635246, 0.027069183066487312, 0.027120187878608704, 0.026890480890870094, 0.02738785184919834, 0.027035849168896675, 0.02780771628022194, 0.02765195444226265, 0.02710782177746296, 0.026416592299938202, 0.026544850319623947, 0.028505152091383934, 0.02715715393424034, 0.027108468115329742, 0.026596246287226677, 0.026371031999588013, 0.026676887646317482, 0.0260652843862772, 0.0279674232006073, 0.027548616752028465, 0.027302633970975876, 0.02610509842634201, 0.0274654570966959, 0.02625241130590439, 0.027170540764927864, 0.027199851348996162, 0.02622103877365589, 0.02641979418694973, 0.025917038321495056, 0.02623780257999897, 0.02617942914366722, 0.027335872873663902, 0.026547996327280998, 0.026966549456119537, 0.026522573083639145, 0.024759555235505104, 0.026210587471723557, 0.026483165100216866, 0.026469092816114426, 0.024941837415099144, 0.025980718433856964, 0.026151705533266068, 0.026269089430570602, 0.02579415775835514, 0.026387706398963928, 0.02618972584605217, 0.02636203169822693, 0.02517460286617279, 0.025759542360901833, 0.02630414254963398, 0.026768822222948074, 0.02525704726576805, 0.02671685442328453, 0.026724770665168762, 0.027193423360586166, 0.027472807094454765, 0.026622086763381958, 0.027043743059039116, 0.02496011182665825, 0.025204956531524658, 0.026093849912285805, 0.02562074549496174, 0.02679428830742836, 0.02612241543829441, 0.027072614058852196, 0.02607814408838749, 0.025549834594130516, 0.026462243869900703, 0.027980731800198555, 0.025623148307204247, 0.026284577324986458, 0.027101485058665276, 0.02711264230310917, 0.026744551956653595, 0.025446755811572075, 0.02581288106739521, 0.025942865759134293, 0.026353638619184494, 0.02633415162563324, 0.027238020673394203, 0.026279238983988762, 0.026950189843773842, 0.026003021746873856, 0.026871930807828903, 0.02684788964688778, 0.026673942804336548, 0.026847602799534798, 0.025013815611600876, 0.026589425280690193, 0.02562585286796093, 0.026290608569979668, 0.026820216327905655, 0.02682766504585743, 0.025432245805859566, 0.027176624163985252, 0.027054009959101677, 0.026417206972837448, 0.02599051035940647, 0.026021895930171013, 0.025925898924469948, 0.026075666770339012, 0.027031438425183296, 0.027495870366692543, 0.026261936873197556, 0.026310384273529053, 0.025597451254725456, 0.025319555774331093, 0.02564852684736252, 0.027632182464003563, 0.02635505422949791, 0.02765239216387272, 0.026216616854071617, 0.02680206671357155, 0.02537592314183712, 0.025324026122689247, 0.027069948613643646, 0.026623066514730453, 0.025046193972229958, 0.02612108550965786, 0.0257185660302639, 0.025939760729670525, 0.02491522580385208, 0.025436818599700928, 0.026987219229340553, 0.02624865062534809, 0.025754308328032494, 0.025674553588032722, 0.026772651821374893, 0.026149405166506767, 0.026777639985084534, 0.026973111554980278, 0.025873376056551933, 0.02660743147134781, 0.02637101709842682, 0.027610916644334793]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:07 2016", "state": "available"}], "summary": "47d71358384d1e592ccbe5a52778e878"}