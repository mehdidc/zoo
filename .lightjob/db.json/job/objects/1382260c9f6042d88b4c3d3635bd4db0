{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 32, "f3": 32, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.008398835517044827, 0.00969096413208888, 0.007421572921332504, 0.0069062014894126144, 0.006074953376928811, 0.007920723585940946, 0.008708719723386061, 0.012013920546831392, 0.014962683236141642, 0.015645929440356175, 0.015701094257386673, 0.013524253119691233, 0.017216480714562152, 0.017094829823868292, 0.018277076508143107, 0.020921492545098466, 0.01987335496505204, 0.01851500767096103, 0.01890830435718746, 0.019131393645447695, 0.019513806956856217, 0.019429805818929887, 0.01836632231054802, 0.018545749773630462, 0.018708470915832623, 0.018547273572773693, 0.0182059980977616, 0.018098886290487524, 0.01823274113969342, 0.018599755257820663, 0.019028160710313196, 0.019078275940179953, 0.018922964201249653, 0.018457797198900196, 0.018557986724113427, 0.017961651202870885, 0.018253604175953668, 0.018364936141717997, 0.018245753813984262, 0.01850975097505784, 0.018650019719331064, 0.018794791995681525, 0.018850594442222285, 0.018721689944367095, 0.018494905056451915, 0.018389950059253396, 0.01881417778875966, 0.018957696497637948, 0.018957696497637948, 0.019158479826438844, 0.01926620928015914, 0.019253830342783405, 0.019501366070931055, 0.019501366070931055, 0.01960561380598546, 0.019514273393288, 0.019514273393288, 0.019514273393288, 0.019605691641829712, 0.019702659464525915, 0.019572748625490726, 0.019572748625490726, 0.019572748625490726, 0.019673666297595597, 0.019673666297595597, 0.019673666297595597, 0.019673666297595597, 0.019673666297595597, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834, 0.019603782285171834], "moving_avg_accuracy_train": [0.01414321330518641, 0.028369681357281278, 0.04342595897067644, 0.0594455203214401, 0.07620400965603658, 0.09284896981307965, 0.10880087752215559, 0.12401550227340072, 0.13832726227931091, 0.15170066968579288, 0.16419464647182783, 0.17583686207450663, 0.18671939596095594, 0.19677631012778024, 0.20618073920405722, 0.2149936418894101, 0.22315308284074226, 0.23071739673620845, 0.23776008322307118, 0.24430543930529525, 0.250326504161449, 0.2559452450855127, 0.26114390994573217, 0.26599476933183447, 0.2704791253686123, 0.27466156622553106, 0.2785303946931865, 0.28214716289621006, 0.28549754933130295, 0.28862682941455325, 0.2915292480442496, 0.2941554117526523, 0.29651892304139593, 0.29872048796316997, 0.300743749071338, 0.3026041755496324, 0.30430646116581167, 0.305882696047754, 0.3073594361617401, 0.30870710345480384, 0.3099618566971326, 0.31110276035927614, 0.3121644508873482, 0.3131339232554701, 0.3140319889748659, 0.3148634996104173, 0.3156327855216993, 0.3163483943299483, 0.31700871829903915, 0.31761227441764023, 0.31816245037080976, 0.31866923447270995, 0.31914390530607756, 0.3195897102465846, 0.31999093469304096, 0.32035436184366123, 0.32069539717207657, 0.32100930441407893, 0.32128949578307153, 0.32154864346159345, 0.3217865266698823, 0.3220006215573422, 0.3221933069560561, 0.3223667238148986, 0.32252047383904736, 0.3226588488607812, 0.3227833863803417, 0.3228954701479462, 0.3230033209852188, 0.3231003867387641, 0.3231877459169549, 0.32326636917732665, 0.3233394552604707, 0.32340755788410985, 0.3234688502453851, 0.32352633851934237, 0.32357807796590393, 0.32362464346780934, 0.3236665524195242, 0.32370427047606753, 0.3237382167269566, 0.3237687683527567, 0.3237962648159768, 0.32381868648406537, 0.32383886598534506, 0.32385470238768727, 0.3238712802986048, 0.3238862004184305, 0.3238996285262737, 0.32391171382333256, 0.3239225905906855, 0.3239323796813032, 0.3239411898628591, 0.3239491190262594, 0.32395625527331967, 0.32396267789567396, 0.3239684582557928, 0.32397366057989974, 0.323978342671596, 0.3239825565541226, 0.32398634904839657, 0.32398976229324317, 0.3239928342136051, 0.32399327379312126, 0.3239936694146858, 0.32399402547409395, 0.32399434592756127, 0.3239946343356818, 0.3239948939029903, 0.32399512751356796, 0.32399533776308787, 0.32399552698765577, 0.3239956972897669, 0.3239958505616669, 0.3239959885063769, 0.3239961126566159, 0.32399622439183096, 0.32399632495352454, 0.3239964154590488], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 24991023, "moving_var_accuracy_train": [0.0018002743433640175, 0.0034417784481631, 0.005137824063491409, 0.006933678769980195, 0.008767933575982233, 0.010384632506049972, 0.011636339491474862, 0.012556068799218185, 0.013143900189497303, 0.013439142401485296, 0.013500123264706591, 0.01336998159548925, 0.01309884933004778, 0.012699238100072844, 0.012225303866322073, 0.011701778763363363, 0.011130789176771511, 0.010532679861476647, 0.009925806771899132, 0.009318801270897188, 0.008713200141825557, 0.008126012373788731, 0.0075566461833697866, 0.0070127590960864395, 0.006492468228059064, 0.006000656708947769, 0.005535301541462263, 0.005099500497425656, 0.004690576251063182, 0.004309650170511705, 0.003954501458578613, 0.0036211219351307317, 0.0033092854121258256, 0.0030219788638563157, 0.002756623247077113, 0.0025121116024996518, 0.0022869804291211432, 0.002080643033836493, 0.001892205582731146, 0.0017193308886531745, 0.001561567451080068, 0.0014171256564686936, 0.0012855577718184049, 0.0011654608846895325, 0.0010561734945477637, 0.0009567788345263037, 0.0008664271583933462, 0.0007843933062520041, 0.0007098782253242062, 0.0006421689226864926, 0.0005806762726328573, 0.0005249201165030208, 0.0004744559164531679, 0.0004287990032126755, 0.00038736793239931575, 0.00034981985280365557, 0.00031588461338033627, 0.00028518299185153654, 0.00025737125749570455, 0.00023223854941968388, 0.00020952399026478739, 0.00018898412082583686, 0.00017041985770915104, 0.0001536485326006132, 0.00013849643096988356, 0.00012481911669265383, 0.00011247679136739296, 0.0001013421771692973, 9.131264568027111e-05, 8.226617695684587e-05, 7.410824389528877e-05, 6.675305405940317e-05, 6.012582283340689e-05, 5.415498225618504e-05, 4.877329481252279e-05, 4.392570944605376e-05, 3.955723123442281e-05, 3.5621023224689814e-05, 3.2074728144325344e-05, 2.8880059195997467e-05, 2.6002424407942488e-05, 2.34105825836995e-05, 2.1076328824736072e-05, 1.8973220523061325e-05, 1.707956338120227e-05, 1.537386416783434e-05, 1.3838951195224405e-05, 1.2457059565482497e-05, 1.121297643565648e-05, 1.009299328173584e-05, 9.084758690174714e-06, 8.177145257813334e-06, 7.360129305723434e-06, 6.624682219841152e-06, 5.962672332055987e-06, 5.3667763495515375e-06, 4.8303994276643145e-06, 4.347603062482907e-06, 3.913040054078486e-06, 3.5218958599241723e-06, 3.169835721047118e-06, 2.85295700110585e-06, 2.5677462312476547e-06, 2.3109733471942488e-06, 2.079877421122625e-06, 1.8718908200150814e-06, 1.6847026622273958e-06, 1.5162331446178524e-06, 1.3646104365327562e-06, 1.2281498840445987e-06, 1.1053352934838846e-06, 9.9480208638893e-07, 8.953221387753185e-07, 8.057901363282646e-07, 7.252112939541254e-07, 6.526903032782496e-07, 5.874213853132492e-07, 5.286793377958123e-07, 4.7581147773748033e-07], "duration": 22595.775672, "accuracy_train": [0.14143213305186414, 0.15640789382613512, 0.17893245749123293, 0.20362157247831303, 0.22703041366740495, 0.2426536112264673, 0.252368046903839, 0.2609471250346069, 0.26713310233250276, 0.27206133634413066, 0.2766404375461425, 0.28061680249861576, 0.28466220093899963, 0.287288537629199, 0.29082060089055, 0.2943097660575858, 0.2965880514027316, 0.2987962217954042, 0.3011442616048357, 0.3032136440453119, 0.30451608786683276, 0.3065139134020856, 0.3079318936877076, 0.30965250380675524, 0.3108383296996124, 0.3123035339377999, 0.3133498509020856, 0.3146980767234219, 0.31565102724713917, 0.31679035016380586, 0.31765101571151716, 0.31779088512827613, 0.31779052464008856, 0.3185345722591362, 0.3189530990448505, 0.319348013854282, 0.31962703171142487, 0.3200688099852344, 0.3206500971876154, 0.32083610909237725, 0.32125463587809155, 0.3213708933185678, 0.3217196656399963, 0.3218591745685678, 0.3221145804494278, 0.3223470953303802, 0.32255635872323735, 0.32278887360418973, 0.3229516340208564, 0.3230442794850498, 0.3231140339493355, 0.32323029138981174, 0.3234159428063861, 0.323601954711148, 0.323601954711148, 0.32362520619924323, 0.3237647151278147, 0.3238344695921004, 0.32381121810400515, 0.32388097256829085, 0.3239274755444813, 0.3239274755444813, 0.3239274755444813, 0.3239274755444813, 0.3239042240563861, 0.3239042240563861, 0.3239042240563861, 0.3239042240563861, 0.32397397852067183, 0.32397397852067183, 0.32397397852067183, 0.32397397852067183, 0.32399723000876707, 0.3240204814968623, 0.3240204814968623, 0.32404373298495753, 0.32404373298495753, 0.32404373298495753, 0.32404373298495753, 0.32404373298495753, 0.32404373298495753, 0.32404373298495753, 0.32404373298495753, 0.3240204814968623, 0.3240204814968623, 0.32399723000876707, 0.3240204814968623, 0.3240204814968623, 0.3240204814968623, 0.3240204814968623, 0.3240204814968623, 0.3240204814968623, 0.3240204814968623, 0.3240204814968623, 0.3240204814968623, 0.3240204814968623, 0.3240204814968623, 0.3240204814968623, 0.3240204814968623, 0.3240204814968623, 0.3240204814968623, 0.3240204814968623, 0.3240204814968623, 0.32399723000876707, 0.32399723000876707, 0.32399723000876707, 0.32399723000876707, 0.32399723000876707, 0.32399723000876707, 0.32399723000876707, 0.32399723000876707, 0.32399723000876707, 0.32399723000876707, 0.32399723000876707, 0.32399723000876707, 0.32399723000876707, 0.32399723000876707, 0.32399723000876707, 0.32399723000876707], "end": "2016-01-23 01:25:23.841000", "learning_rate_per_epoch": [0.002813939470797777, 0.0025474736467003822, 0.0023062408436089754, 0.002087851520627737, 0.0018901425646618009, 0.0017111556371673942, 0.0015491178492084146, 0.001402424299158156, 0.0012696218909695745, 0.0011493951315060258, 0.0010405532084405422, 0.0009420181158930063, 0.0008528137695975602, 0.0007720566354691982, 0.0006989468238316476, 0.000632760114967823, 0.0005728409742005169, 0.0005185958580113947, 0.00046948748058639467, 0.0004250294005032629, 0.000384781276807189, 0.00034834444522857666, 0.00031535798916593194, 0.00028549518901854753, 0.0002584602334536612, 0.000233985367231071, 0.0002118281408911571, 0.0001917690970003605, 0.00017360954370815307, 0.000157169604790397, 0.00014228644431568682, 0.00012881263683084399, 0.00011661473399726674, 0.00010557191126281396, 9.557478915667161e-05, 8.652434189571068e-05, 7.83309296821244e-05, 7.091338920872658e-05, 6.41982551314868e-05, 5.811900700791739e-05, 5.261543265078217e-05, 4.763302058563568e-05, 4.31224143540021e-05, 3.903893957613036e-05, 3.53421492036432e-05, 3.199542697984725e-05, 2.896562364185229e-05, 2.6222725864499807e-05, 2.373956704104785e-05, 2.1491550796781667e-05, 1.9456410882412456e-05, 1.7613987438380718e-05, 1.594603236299008e-05, 1.4436023775488138e-05, 1.3069005944998935e-05, 1.1831437404907774e-05, 1.0711060895118862e-05, 9.696777851786464e-06, 8.778541996434797e-06, 7.94725838204613e-06, 7.194693353085313e-06, 6.513392690976616e-06, 5.8966074902855325e-06, 5.338228675100254e-06, 4.832725608139299e-06, 4.375091066322057e-06, 3.960792128054891e-06, 3.585725153243402e-06, 3.2461753107781988e-06, 2.9387790618784493e-06, 2.6604916456562933e-06, 2.4085566110443324e-06, 2.180478531954577e-06, 1.9739982235478237e-06, 1.7870706869871356e-06, 1.617844191059703e-06, 1.4646425370301586e-06, 1.3259483466754318e-06, 1.2003878282484948e-06, 1.086717247744673e-06, 9.838106507231714e-07, 8.906487778403971e-07, 8.063088898779824e-07, 7.299555591089302e-07, 6.608324838452972e-07, 5.982549851069052e-07, 5.416032990979147e-07, 4.90316210743913e-07, 4.438857672539598e-07, 4.018520485260524e-07, 3.637987049387448e-07, 3.2934880778157094e-07, 2.981611544328189e-07, 2.699268009109801e-07, 2.4436610601696884e-07, 2.2122587495232438e-07, 2.002769150521999e-07, 1.8131170520518936e-07, 1.6414240633366717e-07, 1.4859895713925653e-07, 1.3452739722197293e-07, 1.2178833230791497e-07, 1.1025559842892108e-07, 9.981495452393574e-08, 9.036298820319644e-08, 8.180607125041206e-08, 7.405945012806114e-08, 6.704640043153631e-08, 6.069744529213494e-08, 5.494970523045595e-08, 4.9746248009796545e-08, 4.50355308601047e-08, 4.0770895992636724e-08, 3.6910098089038e-08, 3.34148992919836e-08, 3.0250678406673615e-08, 2.738609339303366e-08, 2.4792770503268002e-08, 2.244502184112207e-08], "accuracy_valid": [0.14477097844503012, 0.15946059629141568, 0.17677546121987953, 0.20148513977786145, 0.2255138718938253, 0.2399799392884036, 0.24790421451430722, 0.2594097091490964, 0.26688688347138556, 0.2736316359186747, 0.2790027296686747, 0.28292956984186746, 0.28646960890436746, 0.292003953313253, 0.29423210419804213, 0.29679558076054213, 0.29862663544804213, 0.30131218232304213, 0.3042521649096386, 0.3079451595444277, 0.3107630718185241, 0.3128485622176205, 0.3136927593185241, 0.3153914486069277, 0.31623564570783136, 0.31684599727033136, 0.3175681240587349, 0.31965361445783136, 0.32063017695783136, 0.32111845820783136, 0.32124052852033136, 0.3217391048569277, 0.3224818218185241, 0.3227156673569277, 0.3230818782944277, 0.3235701595444277, 0.32355986445783136, 0.32380400508283136, 0.32429228633283136, 0.32478056758283136, 0.32490263789533136, 0.32502470820783136, 0.32563505977033136, 0.32624541133283136, 0.32648955195783136, 0.32636748164533136, 0.32685576289533136, 0.32697783320783136, 0.32697783320783136, 0.32722197383283136, 0.32734404414533136, 0.32734404414533136, 0.32746611445783136, 0.32746611445783136, 0.32771025508283136, 0.32758818477033136, 0.32758818477033136, 0.32758818477033136, 0.32758818477033136, 0.32771025508283136, 0.32795439570783136, 0.32795439570783136, 0.32795439570783136, 0.32807646602033136, 0.32807646602033136, 0.32807646602033136, 0.32807646602033136, 0.32807646602033136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136, 0.32819853633283136], "accuracy_test": 0.32426819620253167, "start": "2016-01-22 19:08:48.066000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0], "accuracy_train_last": 0.32399723000876707, "batch_size_eval": 1024, "accuracy_train_std": [0.009927445973040374, 0.010685004567835336, 0.009950341135610977, 0.011046173494536118, 0.012568005068675328, 0.012123149346709727, 0.012914190222557756, 0.012340092998675316, 0.012904900486749332, 0.012547457510357244, 0.012225425712904347, 0.012354785552302216, 0.01286436679357816, 0.013417625148680994, 0.014167990346354664, 0.014411061699396809, 0.014720509373192235, 0.014803082201829264, 0.014632848472605592, 0.014876186354461574, 0.0155052302771126, 0.015087130643025328, 0.014391127644228355, 0.014088092953172331, 0.014186458733553533, 0.014174098352154605, 0.014184420936440154, 0.014581751776073333, 0.014275446935342924, 0.014428612430147977, 0.014492144635637557, 0.01457599266281255, 0.014415045819125526, 0.014290777875663026, 0.014271560980365286, 0.014643690122160125, 0.01444406537280347, 0.014388211325791623, 0.014825492515483166, 0.014683094739429355, 0.014592334623870858, 0.014588255071383067, 0.014683767188714223, 0.014709917450466153, 0.014601210347861162, 0.014429694514041237, 0.014347293661865747, 0.014419647206161271, 0.014408881761556991, 0.014369670533608391, 0.014333281847545091, 0.014318792764781492, 0.01427977946129121, 0.014255612582715491, 0.014254019680279322, 0.014211135071022939, 0.014244997076639322, 0.014271731514373468, 0.014198507500853377, 0.014170725536999844, 0.014131644931244332, 0.014112350325531584, 0.01420696316631412, 0.014237297868628035, 0.014231219874623216, 0.014253539981263426, 0.014224836268622806, 0.014224836268622806, 0.014180645545062084, 0.014180645545062084, 0.014180645545062084, 0.014180645545062084, 0.014169013800371595, 0.014134862289196552, 0.014134862289196552, 0.014089312938954174, 0.014118292184968044, 0.014118292184968044, 0.014118292184968044, 0.014118292184968044, 0.014118292184968044, 0.014118292184968044, 0.014118292184968044, 0.014163748340848865, 0.014163748340848865, 0.01415137478782802, 0.014182973024550639, 0.014182973024550639, 0.014182973024550639, 0.014182973024550639, 0.014182973024550639, 0.014182973024550639, 0.014182973024550639, 0.014182973024550639, 0.014182973024550639, 0.014182973024550639, 0.014182973024550639, 0.014182973024550639, 0.014182973024550639, 0.014182973024550639, 0.014182973024550639, 0.014182973024550639, 0.014182973024550639, 0.014210618976841121, 0.014210618976841121, 0.014210618976841121, 0.014210618976841121, 0.014210618976841121, 0.014210618976841121, 0.014210618976841121, 0.014210618976841121, 0.014210618976841121, 0.014210618976841121, 0.014210618976841121, 0.014210618976841121, 0.014210618976841121, 0.014210618976841121, 0.014210618976841121, 0.014210618976841121], "accuracy_test_std": 0.0412813904726703, "error_valid": [0.8552290215549698, 0.8405394037085843, 0.8232245387801205, 0.7985148602221386, 0.7744861281061747, 0.7600200607115963, 0.7520957854856928, 0.7405902908509037, 0.7331131165286144, 0.7263683640813253, 0.7209972703313253, 0.7170704301581325, 0.7135303910956325, 0.707996046686747, 0.7057678958019579, 0.7032044192394579, 0.7013733645519579, 0.6986878176769579, 0.6957478350903614, 0.6920548404555723, 0.6892369281814759, 0.6871514377823795, 0.6863072406814759, 0.6846085513930723, 0.6837643542921686, 0.6831540027296686, 0.6824318759412651, 0.6803463855421686, 0.6793698230421686, 0.6788815417921686, 0.6787594714796686, 0.6782608951430723, 0.6775181781814759, 0.6772843326430723, 0.6769181217055723, 0.6764298404555723, 0.6764401355421686, 0.6761959949171686, 0.6757077136671686, 0.6752194324171686, 0.6750973621046686, 0.6749752917921686, 0.6743649402296686, 0.6737545886671686, 0.6735104480421686, 0.6736325183546686, 0.6731442371046686, 0.6730221667921686, 0.6730221667921686, 0.6727780261671686, 0.6726559558546686, 0.6726559558546686, 0.6725338855421686, 0.6725338855421686, 0.6722897449171686, 0.6724118152296686, 0.6724118152296686, 0.6724118152296686, 0.6724118152296686, 0.6722897449171686, 0.6720456042921686, 0.6720456042921686, 0.6720456042921686, 0.6719235339796686, 0.6719235339796686, 0.6719235339796686, 0.6719235339796686, 0.6719235339796686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686, 0.6718014636671686], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.8775683198057544, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0031082774208631317, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "adadelta", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.1214007158297712e-05, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.0946949136403783}, "accuracy_valid_max": 0.32819853633283136, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import os\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.32819853633283136, "loss_train": [2.518277645111084, 2.4600870609283447, 2.4111063480377197, 2.3656461238861084, 2.3238468170166016, 2.287982940673828, 2.2562453746795654, 2.2277097702026367, 2.2020163536071777, 2.179044246673584, 2.158522844314575, 2.1402862071990967, 2.1242291927337646, 2.110133409500122, 2.0978140830993652, 2.0870394706726074, 2.077619791030884, 2.0693483352661133, 2.062044620513916, 2.05557918548584, 2.049852132797241, 2.0447633266448975, 2.0402321815490723, 2.036191463470459, 2.032588481903076, 2.0293638706207275, 2.0264809131622314, 2.0238983631134033, 2.021578073501587, 2.0194942951202393, 2.0176217555999756, 2.015936851501465, 2.014420986175537, 2.013054847717285, 2.011824369430542, 2.0107157230377197, 2.0097157955169678, 2.0088136196136475, 2.0079994201660156, 2.0072643756866455, 2.0066001415252686, 2.0060007572174072, 2.0054595470428467, 2.0049705505371094, 2.0045289993286133, 2.0041298866271973, 2.0037691593170166, 2.0034430027008057, 2.003148078918457, 2.0028817653656006, 2.002640724182129, 2.002423048019409, 2.0022263526916504, 2.0020487308502197, 2.001887798309326, 2.0017426013946533, 2.0016119480133057, 2.0014936923980713, 2.001386880874634, 2.001290798187256, 2.001204013824463, 2.0011255741119385, 2.0010550022125244, 2.0009913444519043, 2.0009334087371826, 2.0008811950683594, 2.0008342266082764, 2.0007917881011963, 2.00075364112854, 2.0007193088531494, 2.0006887912750244, 2.000661849975586, 2.0006377696990967, 2.0006167888641357, 2.0005979537963867, 2.000581979751587, 2.00056791305542, 2.0005552768707275, 2.000544786453247, 2.000535488128662, 2.000527858734131, 2.000521183013916, 2.0005156993865967, 2.0005109310150146, 2.00050687789917, 2.0005037784576416, 2.0005006790161133, 2.0004985332489014, 2.0004966259002686, 2.000494956970215, 2.0004935264587402, 2.000492572784424, 2.0004913806915283, 2.000490427017212, 2.000490188598633, 2.0004897117614746, 2.0004889965057373, 2.000488758087158, 2.000488519668579, 2.00048828125, 2.00048828125, 2.000487804412842, 2.000487804412842, 2.000487804412842, 2.0004875659942627, 2.0004875659942627, 2.0004875659942627, 2.0004873275756836, 2.0004873275756836, 2.0004873275756836, 2.0004873275756836, 2.0004870891571045, 2.0004873275756836, 2.0004873275756836, 2.0004870891571045, 2.0004870891571045, 2.0004870891571045, 2.0004870891571045, 2.0004870891571045], "accuracy_train_first": 0.14143213305186414, "model": "residualv4", "loss_std": [0.04096512123942375, 0.03514460474252701, 0.03406078740954399, 0.035982467234134674, 0.03820217400789261, 0.04028564691543579, 0.04191334918141365, 0.04334111139178276, 0.044605813920497894, 0.04567916691303253, 0.04664594307541847, 0.04754233360290527, 0.04835161939263344, 0.04904653877019882, 0.04965140298008919, 0.05015351250767708, 0.05056817829608917, 0.050919149070978165, 0.051233336329460144, 0.05150960013270378, 0.05175499618053436, 0.05197099223732948, 0.05215638875961304, 0.0523177869617939, 0.052460312843322754, 0.052588727325201035, 0.05270247533917427, 0.052801162004470825, 0.05288991704583168, 0.05297038331627846, 0.0530431866645813, 0.05311055853962898, 0.05317225307226181, 0.053227491676807404, 0.05327827110886574, 0.05332525819540024, 0.053367506712675095, 0.053405631333589554, 0.05344041436910629, 0.05347162112593651, 0.053500015288591385, 0.05352602154016495, 0.05354946479201317, 0.05357052758336067, 0.05358976498246193, 0.053607165813446045, 0.05362287536263466, 0.05363718047738075, 0.05365023389458656, 0.05366192385554314, 0.05367264524102211, 0.05368227884173393, 0.05369109660387039, 0.05369912087917328, 0.053706374019384384, 0.05371289327740669, 0.05371871963143349, 0.053723931312561035, 0.05372856557369232, 0.053732775151729584, 0.05373653024435043, 0.05373988673090935, 0.05374296382069588, 0.05374569445848465, 0.05374816805124283, 0.05375044047832489, 0.053752440959215164, 0.053754210472106934, 0.05375578626990318, 0.05375722795724869, 0.05375849828124046, 0.05375958979129791, 0.05376053601503372, 0.053761355578899384, 0.05376202613115311, 0.053762633353471756, 0.05376312881708145, 0.05376354604959488, 0.05376392602920532, 0.05376424267888069, 0.053764473646879196, 0.05376467481255531, 0.053764842450618744, 0.05376497283577919, 0.05376509949564934, 0.053765203803777695, 0.053765274584293365, 0.05376533791422844, 0.05376539006829262, 0.0537654273211956, 0.053765468299388885, 0.053765490651130676, 0.053765520453453064, 0.05376552790403366, 0.053765539079904556, 0.05376555770635605, 0.053765565156936646, 0.05376557633280754, 0.05376558005809784, 0.053765591233968735, 0.053765591233968735, 0.05376560240983963, 0.05376560613512993, 0.05376560613512993, 0.05376562103629112, 0.05376562103629112, 0.053765613585710526, 0.053765617311000824, 0.05376562848687172, 0.05376562848687172, 0.05376562476158142, 0.05376562848687172, 0.05376562103629112, 0.05376562103629112, 0.05376562848687172, 0.05376562848687172, 0.05376563221216202, 0.05376562103629112, 0.05376563221216202]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:08 2016", "state": "available"}], "summary": "888f08594f94cd41b36dc9b622b972a9"}