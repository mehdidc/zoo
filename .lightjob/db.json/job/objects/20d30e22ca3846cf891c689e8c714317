{"content": {"hp_model": {"f0": 64, "f1": 32, "f2": 32, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 4, "nbg3": 2, "nbg2": 2, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.010625380233462925, 0.019073259200505933, 0.017207175051280367, 0.025233689195528555, 0.03128011918499565, 0.023112473807494916, 0.018807535406084864, 0.017340997109677344, 0.019666558997127576, 0.020125621972377498, 0.020931619869783032, 0.016409073881232524, 0.018513597292068005, 0.01687633839144792, 0.017657773311672422, 0.01512312342748017, 0.01707546197836274, 0.015286367446906646, 0.018779094416302686, 0.013144364489750173, 0.01474120472431142, 0.01718241229011723, 0.013958990483492369, 0.01687503946871828, 0.011849194702222192, 0.014337819016325597, 0.015483391075399122, 0.011395953851393038, 0.011603631799867065, 0.0140335019764684, 0.017567678900092837, 0.010275191552758451, 0.01274583948517955, 0.013539487474485138, 0.013965108299237717, 0.01619865516962953, 0.015497625108879094, 0.017101892816183104, 0.013112711765528003, 0.013484294369832533, 0.013956769159673872, 0.013877629596136707, 0.01345634842325894, 0.017309290872929435, 0.012468044440613911, 0.013271448067245737, 0.011473724869046764, 0.014535506285191603, 0.013175596422098725, 0.013565317776076611, 0.014093633297884097, 0.01460556420944338, 0.015598334099319312, 0.015012035433754534, 0.013655114326816853, 0.010637555859593566, 0.011789629260608852, 0.010649520143299979, 0.01133725094990892, 0.01574498935845639, 0.01340181127303964, 0.012390590938796164, 0.010846975521716614, 0.012277152203899359, 0.006229252119967959, 0.013629221863782261, 0.01741403661165562, 0.014942510742384176, 0.014471041232050605, 0.016762514015098973, 0.016005209380017323, 0.012767195640568442, 0.012966618759113245, 0.014330360780891992, 0.012162083538995814, 0.01229900238292109, 0.011411989744528011, 0.014141416984635644, 0.014679371607106615, 0.008854608496865878, 0.01304602657025041, 0.015225489710564605, 0.014631384568997602, 0.012490289761298446, 0.016962328631643295, 0.0123180941818289, 0.013205398388107557, 0.012461364029398899, 0.013882968426558442, 0.013238413074323758, 0.013694206159593582, 0.017492445152295342, 0.011868302662682469, 0.013323919875550181, 0.011736478337894624, 0.012036652308999795, 0.015993994836331044, 0.01348867097063572, 0.019224540413832678, 0.011532065156168192, 0.019224965319624903, 0.01576139380215413, 0.02079915843753347, 0.012790797022132767, 0.013241504723669476, 0.01869397274918332, 0.015345111717697231, 0.012427252806059559, 0.012006617444146018, 0.015685040396792624, 0.014195990574071152, 0.012067625448757209, 0.011440281064859944, 0.018907758851358692, 0.010952490253565593, 0.01113309410981687, 0.01028224870802584, 0.012093283598079107, 0.010223054314778662, 0.013238617638213608, 0.010950581517767656, 0.01077103804570041, 0.016692008156044255, 0.014760494602718282, 0.011219549613779299, 0.015703977932619725, 0.015017995699878845, 0.012147233174813876, 0.013938383019859482, 0.019014749144727686, 0.013490573422083863, 0.01380425504994545, 0.014728348662432467, 0.01616926571604533, 0.011236806629411771, 0.015291955150749515, 0.017148796679760853, 0.010526421184387513, 0.0133660994697137, 0.016307346072786683, 0.01360002402883789, 0.011306513325385447, 0.016190087634936647, 0.01626196397302136, 0.01328674803182507, 0.011405434301314714, 0.015064513220972336, 0.012041907419643949, 0.017882932097924114, 0.015678921761887232, 0.018730196113426814, 0.01244529557291121, 0.011505748338450266, 0.011474086352171484, 0.012247371568619023, 0.015935739629092383, 0.012488104763100508, 0.014497233084926023, 0.013017798267678871, 0.013807265508742757, 0.011838301852276807, 0.013052750035032246, 0.008498265238399283, 0.011621882677166124, 0.011691176783384844, 0.014347233356142693, 0.011997695799311765, 0.009110329940545297, 0.01017689956235447, 0.011976073613217548, 0.017082845037422256, 0.014640290642675793, 0.014736813713724468, 0.013622266234467386, 0.015371525064638115, 0.013131498897370692, 0.014752696492196741, 0.013032374963589348, 0.010981240240862873, 0.011405160349070353, 0.010285236840301554, 0.013722398943404528, 0.014244509893099052, 0.01675842139505205, 0.013896534290010061, 0.016779480914692675, 0.019009989406901663, 0.011561183461262541, 0.01298145754206429, 0.016441244019335256, 0.008761994493767512, 0.012805208138479346, 0.01069376173581477, 0.015730210292130814, 0.011672256695248647, 0.013445144594375623, 0.011888000560759693, 0.0181000937412657, 0.015436327735482903, 0.01408966384506159, 0.013366117594669963, 0.013513846372907125, 0.015162980134999287, 0.013598775774230888, 0.012850065613583827, 0.011413995667349798, 0.00866187320589363, 0.015377031184104354, 0.013531447999056116, 0.016966410822167162, 0.014018956307582129, 0.011546088155506024, 0.01400182942630347, 0.021120267929540297, 0.010392427945767271], "moving_avg_accuracy_train": [0.04914212823574196, 0.10285212666401344, 0.15635194623967558, 0.20828215138268844, 0.2571000918541926, 0.30326333430106384, 0.3464467246793277, 0.3871615851052211, 0.4249023576289828, 0.4598039429658907, 0.49214968305353013, 0.5224230270001613, 0.5503992053759573, 0.5759842343700153, 0.6000571855754095, 0.6220366286030938, 0.6424715662411233, 0.6619227732890303, 0.6797967378275137, 0.6961503375370565, 0.7112729729244084, 0.7256065381504245, 0.7384298366990311, 0.7504452078475573, 0.761565961524088, 0.7721976715674618, 0.7821239590837961, 0.791136781054477, 0.799827174735205, 0.8073741254395177, 0.8148683957697815, 0.8214830307336857, 0.8279824679761625, 0.8334276739920844, 0.8385491403968627, 0.843202637988544, 0.8481555074376593, 0.852136542582367, 0.8570166909554057, 0.8607528441363047, 0.8647269682336561, 0.868454886691529, 0.8718890323143197, 0.8749935700724134, 0.8777714140618498, 0.880424933473771, 0.8826130300004249, 0.8847357045982045, 0.8866997343541002, 0.8886091591629685, 0.8901418458791005, 0.8926297110270247, 0.8943088253340675, 0.895350095809151, 0.8969524480891624, 0.8990895683423078, 0.9005550664499375, 0.9024274001634709, 0.9039683412794605, 0.905494625114785, 0.9071611772189396, 0.9083541544698216, 0.9085421685920994, 0.9085417535855103, 0.9099896233485447, 0.9109904728388563, 0.9127513621955667, 0.9126695356036402, 0.9132698243374807, 0.9136775507157943, 0.9142605630514684, 0.9137136689429348, 0.9147396422225985, 0.9152771155695524, 0.915849016992479, 0.916515007141007, 0.9171864418389587, 0.9174255765575637, 0.9178454108995464, 0.91923480968593, 0.9199317389817334, 0.9209357936503744, 0.9214021706806859, 0.9221383825877927, 0.922393783871972, 0.9226472209551975, 0.9229238540645504, 0.9229149846868421, 0.9234206438408968, 0.9240454729426414, 0.9244126868783052, 0.9250057049429664, 0.9261810901749525, 0.9264508556325864, 0.926477585949265, 0.9276058005282495, 0.9276727131791436, 0.9279537155553969, 0.9275138675440617, 0.9284662662040151, 0.9285540170349332, 0.9276635859732819, 0.9277339846260921, 0.9280044619017627, 0.9283267302164775, 0.9294537171246932, 0.9292777093957049, 0.9306466368169225, 0.9313090460865038, 0.9320563851505648, 0.9323127886713148, 0.9329294183959147, 0.9334219666183848, 0.9340114920518767, 0.934737449539657, 0.9357720996346024, 0.936393967830749, 0.9370351015132519, 0.9372007146834936, 0.9377774857712073, 0.9378617048251312, 0.9386953923391113, 0.938680881338635, 0.9390468206941587, 0.9402805769033973, 0.9402982096488733, 0.9406814165828878, 0.940851844565149, 0.9410587081718031, 0.941670351601116, 0.9422440100779553, 0.9429253882725869, 0.943773504726336, 0.9448578242656994, 0.9448453433629852, 0.9453270781469801, 0.9454632006490502, 0.9455576288711053, 0.9450196185852775, 0.9453606929601384, 0.9454050262284933, 0.9453493787759097, 0.9459574573769548, 0.9459164294202671, 0.9459236460378103, 0.9463605819139111, 0.9467466324630608, 0.9465826894144753, 0.9472836757909496, 0.9471658656131097, 0.9465227991756913, 0.9472296303807689, 0.9475008742975185, 0.9472171489940124, 0.9475942727458663, 0.9485334283201168, 0.9482393814690944, 0.9487190032151344, 0.9494039958603522, 0.9491531726862771, 0.9489600199617616, 0.9487141389454212, 0.9485672868414383, 0.9495812020180364, 0.9502218635103542, 0.9507728822165354, 0.9510572105104319, 0.9499227030356622, 0.9502060547905123, 0.9506307351353352, 0.9514849166051904, 0.9518003480078405, 0.9521562077368645, 0.9526996957787004, 0.9527586824865908, 0.9521469221594433, 0.9510545781923915, 0.951396875541111, 0.9510120488097205, 0.9521674624918713, 0.9512032728249101, 0.950595882742493, 0.9498424376195448, 0.9497315651719591, 0.9505453832071811, 0.9514756373317841, 0.95210821689987, 0.9523264410409092, 0.9524600277011686, 0.9518457970621425, 0.9517044326798485, 0.9520326455119006, 0.9519051483679641, 0.9513906195384583, 0.9522710830668014, 0.9529101485673379, 0.953278261127317, 0.9537722507503272, 0.9535637269372916, 0.9542919478435994, 0.95386175450696, 0.954785676042006, 0.9553731008473662, 0.9552044187734268, 0.9554619392438138, 0.9555492600504217, 0.9559533335608835, 0.9559985985286692, 0.9564276728996857, 0.9566208164335912, 0.9567108321105071, 0.955824656412607, 0.9563477107106689, 0.9568905031432011, 0.9564747136896692], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 304549780, "moving_var_accuracy_train": [0.02173453890784297, 0.045523960397543015, 0.06673164060944434, 0.08432919240425851, 0.09734499497074656, 0.10678990005254944, 0.11289415688834828, 0.11652403993501326, 0.1176909291377249, 0.1168849221552176, 0.11461265205604995, 0.1113996650338442, 0.10730369753908897, 0.10246467116282118, 0.09743376686417467, 0.09203825342002216, 0.08659270816445132, 0.08133858244859106, 0.07608003167863864, 0.0708789905219144, 0.06584933837925207, 0.061113464370123, 0.05648205080411094, 0.052133168018231375, 0.04803289167741488, 0.044246901835690765, 0.04070899230683409, 0.037369171715027384, 0.034311961024458985, 0.03139337310641301, 0.028759512585819363, 0.02627734188858875, 0.02402979185994992, 0.021893665090957422, 0.01994036334507913, 0.01814122236909328, 0.016547878374203774, 0.015035728304193976, 0.013746498107060426, 0.012497477861674664, 0.011389873036577541, 0.010375962117176729, 0.009444506110885844, 0.008586798892020124, 0.007797566757884943, 0.007081180569521439, 0.006416152410258889, 0.005815088896265529, 0.005268296722577369, 0.004774280178226128, 0.004317994317531785, 0.00394190014272691, 0.0035730849521592623, 0.003225534654763861, 0.002926088984750796, 0.002674585633063355, 0.0024264562320882154, 0.0022153613106928975, 0.0020151956753301325, 0.001834641988910876, 0.001676174353262547, 0.001521365670426389, 0.001369547247175333, 0.001232592524007874, 0.0011282002132634707, 0.0010243954892574375, 0.0009498625222708816, 0.0008549365303641103, 0.0007726859964034816, 0.0006969135639592879, 0.0006302813380152934, 0.0005699450427073038, 0.0005224241289718283, 0.0004727816144628178, 0.00042844709415444556, 0.0003895942706404285, 0.00035469226455890737, 0.0003197377068257975, 0.0002893502840155897, 0.00027778911650246584, 0.0002543815988423611, 0.00023801657095670286, 0.00021617248167065122, 0.00019943330525307794, 0.00018007704307141444, 0.0001626474119606575, 0.00014707140365930404, 0.00013236497128612203, 0.00012142969477822465, 0.00011280042795788456, 0.00010273399983300792, 9.562563367483741e-05, 9.849684429949313e-05, 8.930212048873587e-05, 8.037833902832993e-05, 8.379631835159619e-05, 7.545698224208371e-05, 6.862194503701496e-05, 6.350094699099307e-05, 6.531442115922275e-05, 5.8852280918241934e-05, 6.010286010639903e-05, 5.413717782861659e-05, 4.938188165564266e-05, 4.537840529010066e-05, 5.227146018269614e-05, 4.732312265039918e-05, 5.945647094641337e-05, 5.745989821561684e-05, 5.6740549484098564e-05, 5.16581794247661e-05, 4.99144514376296e-05, 4.710644005699346e-05, 4.5523658181898304e-05, 4.571442083028575e-05, 5.077748611799058e-05, 4.9180217986599434e-05, 4.796166777749578e-05, 4.3412350499164086e-05, 4.20650994378503e-05, 3.792242513545985e-05, 4.038549646061003e-05, 3.634884193676243e-05, 3.391916225037622e-05, 4.422663547985208e-05, 3.980677015528409e-05, 3.714772112824659e-05, 3.3694360289660695e-05, 3.071005722651639e-05, 3.1006020665459084e-05, 3.086717503135956e-05, 3.195894372529796e-05, 3.523676302484718e-05, 4.229482649337044e-05, 3.8066745800426466e-05, 3.634868683937975e-05, 3.288058217557035e-05, 2.9672774160097742e-05, 2.9310592352996293e-05, 2.7426518680377858e-05, 2.470155576048734e-05, 2.2259269935249952e-05, 2.3361179207165202e-05, 2.104021092551841e-05, 1.893665854908546e-05, 1.8761209332593007e-05, 1.8226403637822688e-05, 1.664565918265607e-05, 1.9403530364412534e-05, 1.7588090469995256e-05, 1.9551091409401586e-05, 2.2092475440704565e-05, 2.0545387257997185e-05, 1.921534896284359e-05, 1.8573814984470057e-05, 2.4654552219836605e-05, 2.2967268953218524e-05, 2.274087523136664e-05, 2.468972202425281e-05, 2.2786960203705387e-05, 2.0844035958224617e-05, 1.9303749630171554e-05, 1.75674645311522e-05, 2.5062933946061464e-05, 2.6250664881105377e-05, 2.6358192924050003e-05, 2.4449956840035785e-05, 3.3588926048806286e-05, 3.0952627396715693e-05, 2.9480545214554133e-05, 3.3099124544094435e-05, 3.0684684817684886e-05, 2.875594165658572e-05, 2.8538760755494665e-05, 2.5716199565314922e-05, 2.6512835889627507e-05, 3.4600490381854e-05, 3.21949486181322e-05, 3.0308278275053542e-05, 3.929227743966139e-05, 4.373000512056842e-05, 4.267730901848033e-05, 4.351869409628049e-05, 3.927745898335528e-05, 4.1310411235092504e-05, 4.4967724738651916e-05, 4.40723644544244e-05, 4.0093723990572684e-05, 3.624496015370877e-05, 3.601597763960298e-05, 3.2594234872874926e-05, 3.030432435370017e-05, 2.7420191613737747e-05, 2.7060831699898396e-05, 3.1331692752591255e-05, 3.1874165903116454e-05, 2.9906311024133628e-05, 2.9111911650496836e-05, 2.6592060110873655e-05, 2.870560529523945e-05, 2.7500641527715835e-05, 3.2433256401239656e-05, 3.229554187868713e-05, 2.932207046943453e-05, 2.698671455650666e-05, 2.435666741025591e-05, 2.3390479285941943e-05, 2.106987161312558e-05, 2.0619827794581356e-05, 1.889358483732898e-05, 1.7077151752411712e-05, 2.2437202885109028e-05, 2.265575478508769e-05, 2.304179192990718e-05, 2.229354056393236e-05], "duration": 149642.125978, "accuracy_train": [0.4914212823574197, 0.5862421125184569, 0.637850322420635, 0.6756539976698044, 0.6964615560977299, 0.7187325163229051, 0.7350972380837025, 0.7535953289382613, 0.7645693103428387, 0.773918210998062, 0.783261343842285, 0.7948831225198413, 0.8021848107581212, 0.8062494953165374, 0.8167137464239571, 0.8198516158522517, 0.8263860049833887, 0.8369836367201919, 0.8406624186738648, 0.843332734922942, 0.8473766914105758, 0.8546086251845699, 0.8538395236364894, 0.858583548184293, 0.8616527446128645, 0.8678830619578257, 0.8714605467308048, 0.8722521787906055, 0.8780407178617571, 0.8752966817783315, 0.8823168287421558, 0.8810147454088224, 0.8864774031584534, 0.882434528135382, 0.884642338039867, 0.8850841163136766, 0.8927313324796974, 0.887965858884736, 0.9009380263127538, 0.8943782227643964, 0.9004940851098191, 0.9020061528123846, 0.9027963429194352, 0.9029344098952565, 0.9027720099667773, 0.9043066081810631, 0.9023058987403102, 0.9038397759782208, 0.9043760021571613, 0.9057939824427832, 0.9039360263242894, 0.9150204973583426, 0.909420854097453, 0.9047215300849022, 0.9113736186092655, 0.9183236506206165, 0.9137445494186047, 0.9192784035852714, 0.9178368113233666, 0.9192311796327058, 0.9221601461563308, 0.9190909497277593, 0.9102342956925988, 0.9085380185262089, 0.9230204512158545, 0.9199981182516611, 0.9285993664059615, 0.9119330962763011, 0.9186724229420451, 0.9173470881206165, 0.919507674072536, 0.9087916219661315, 0.9239734017395718, 0.9201143756921374, 0.9209961297988187, 0.9225089184777593, 0.9232293541205242, 0.9195777890250092, 0.9216239199773901, 0.9317393987633813, 0.9262041026439645, 0.9299722856681433, 0.9255995639534883, 0.9287642897517534, 0.9246923954295865, 0.9249281547042267, 0.9254135520487264, 0.9228351602874677, 0.9279715762273901, 0.9296689348583426, 0.9277176122992802, 0.9303428675249169, 0.9367595572628276, 0.928878744751292, 0.9267181587993725, 0.9377597317391103, 0.9282749270371908, 0.9304827369416758, 0.9235552354420451, 0.9370378541435955, 0.9293437745131967, 0.9196497064184201, 0.9283675725013842, 0.9304387573827981, 0.931227145048911, 0.9395965992986341, 0.9276936398348099, 0.9429669836078812, 0.9372707295127353, 0.9387824367271133, 0.9346204203580657, 0.9384790859173128, 0.9378549006206165, 0.9393172209533037, 0.9412710669296788, 0.9450839504891103, 0.9419907815960686, 0.9428053046557769, 0.9386912332156699, 0.9429684255606312, 0.9386196763104466, 0.9461985799649317, 0.9385502823343485, 0.9423402748938722, 0.9513843827865448, 0.940456904358158, 0.944130278989018, 0.9423856964055003, 0.9429204806316908, 0.9471751424649317, 0.9474069363695091, 0.949057792024271, 0.9514065528100776, 0.9546167001199704, 0.9447330152385567, 0.9496626912029347, 0.9466883031676817, 0.9464074828696014, 0.9401775260128276, 0.9484303623338871, 0.9458040256436876, 0.9448485517026578, 0.9514301647863603, 0.9455471778100776, 0.9459885955956996, 0.9502930047988187, 0.950221087405408, 0.9451072019772055, 0.9535925531792175, 0.9461055740125508, 0.9407352012389257, 0.9535911112264673, 0.9499420695482651, 0.9446636212624585, 0.9509883865125508, 0.9569858284883721, 0.945592959809893, 0.9530355989294942, 0.9555689296673128, 0.9468957641196014, 0.9472216454411223, 0.9465012097983574, 0.9472456179055924, 0.9587064386074198, 0.9559878169412146, 0.9557320505721669, 0.9536161651555003, 0.9397121357627353, 0.9527562205841639, 0.9544528582387413, 0.9591725498338871, 0.9546392306316908, 0.9553589452980805, 0.9575910881552234, 0.9532895628576044, 0.9466410792151162, 0.9412234824889257, 0.9544775516795865, 0.9475486082272055, 0.9625661856312293, 0.9425255658222591, 0.9451293720007383, 0.9430614315130121, 0.9487337131436876, 0.9578697455241787, 0.9598479244532114, 0.9578014330126431, 0.9542904583102622, 0.9536623076435032, 0.9463177213109081, 0.9504321532392026, 0.9549865610003692, 0.950757674072536, 0.9467598600729051, 0.96019525482189, 0.9586617380721669, 0.9565912741671282, 0.9582181573574198, 0.9516870126199704, 0.9608459360003692, 0.9499900144772055, 0.9631009698574198, 0.9606599240956073, 0.9536862801079733, 0.9577796234772978, 0.956335147309893, 0.9595899951550388, 0.9564059832387413, 0.9602893422388336, 0.9583591082387413, 0.9575209732027501, 0.9478490751315062, 0.9610551993932264, 0.9617756350359912, 0.9527326086078812], "end": "2016-01-25 11:42:30.318000", "learning_rate_per_epoch": [0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651, 0.0002877839142456651], "accuracy_valid": [0.4773846362010542, 0.5708434323230422, 0.6124811746987951, 0.6460711008094879, 0.6623064523719879, 0.681971538497741, 0.6972215032003012, 0.7153599750564759, 0.7229283344314759, 0.7297951571912651, 0.7350338855421686, 0.7492764024849398, 0.7550137071724398, 0.7586155167545181, 0.7693782944277108, 0.7702430817018072, 0.7771804993411144, 0.7860519225338856, 0.7857460113893072, 0.7887374694088856, 0.7895919615963856, 0.7955734069088856, 0.7947600950677711, 0.7983810240963856, 0.8028476209525602, 0.8069877164909638, 0.8079539839043675, 0.8100600644766567, 0.812023484563253, 0.8115043180534638, 0.8163974256400602, 0.8151061276355422, 0.8151870176016567, 0.8122367399284638, 0.8109042615775602, 0.8119720091302711, 0.8185946912650602, 0.8128367964043675, 0.8236407544239458, 0.817516648625753, 0.8204360410391567, 0.821789109563253, 0.8227862622364458, 0.8225009412650602, 0.8243834713855422, 0.8233966137989458, 0.8214743740587349, 0.822155320500753, 0.8238848950489458, 0.8224097562123494, 0.8225318265248494, 0.8255835843373494, 0.823864304875753, 0.8202124905873494, 0.8242511059864458, 0.8285647472703314, 0.8282397166792168, 0.8290736186935241, 0.8274661144578314, 0.8280249905873494, 0.8318709407944277, 0.8304163921310241, 0.8212508236069277, 0.8195212490587349, 0.8297148555158133, 0.8238848950489458, 0.8302428463855422, 0.8202330807605422, 0.8258689053087349, 0.8197036191641567, 0.8190020825489458, 0.8154517483998494, 0.8262351162462349, 0.8248820477221386, 0.8219626553087349, 0.8260115657944277, 0.8252482586596386, 0.8236510495105422, 0.8262248211596386, 0.832787203501506, 0.8279749858810241, 0.8298663403614458, 0.8281985363328314, 0.8276087749435241, 0.8280249905873494, 0.8256350597703314, 0.8249129329819277, 0.8226847820971386, 0.8276793698230422, 0.8253909191453314, 0.8269675381212349, 0.8276793698230422, 0.8305693477033133, 0.8299075207078314, 0.8281073512801205, 0.8354521602033133, 0.8235083890248494, 0.8301413662462349, 0.8200492399284638, 0.8328372082078314, 0.8255424039909638, 0.8201007153614458, 0.8240775602409638, 0.8277102550828314, 0.8280764660203314, 0.831432664250753, 0.8207007718373494, 0.8360316265060241, 0.8328475032944277, 0.8284117916980422, 0.8307620128953314, 0.8279543957078314, 0.8312708843185241, 0.829235398625753, 0.8302840267319277, 0.8351771343185241, 0.8306502376694277, 0.8329592785203314, 0.8340990916792168, 0.8311385189194277, 0.8295721950301205, 0.8343329372176205, 0.8249835278614458, 0.8264792568712349, 0.840721773814006, 0.8275572995105422, 0.8312399990587349, 0.8307928981551205, 0.8270793133471386, 0.8356139401355422, 0.8350962443524097, 0.8332034191453314, 0.8371096691453314, 0.8384524425828314, 0.8309149684676205, 0.8359507365399097, 0.8322165615587349, 0.8342417521649097, 0.8299281108810241, 0.8341902767319277, 0.8318812358810241, 0.8267439876694277, 0.8329592785203314, 0.8378729762801205, 0.8299384059676205, 0.8339667262801205, 0.8343226421310241, 0.8316473903426205, 0.8340373211596386, 0.8279646907944277, 0.8264689617846386, 0.8387377635542168, 0.8314341349774097, 0.8282294215926205, 0.8330210490399097, 0.8366007977221386, 0.8266322124435241, 0.8367846385542168, 0.8418718820594879, 0.8286868175828314, 0.8359507365399097, 0.8299075207078314, 0.8327769084149097, 0.8424616434487951, 0.8442721079631024, 0.8365199077560241, 0.8353094997176205, 0.8234583843185241, 0.8403658579631024, 0.8375376506024097, 0.8368655285203314, 0.8315856198230422, 0.8362860622176205, 0.8365096126694277, 0.8372729198042168, 0.8315253200301205, 0.8225935970444277, 0.8320033061935241, 0.8318812358810241, 0.8365507930158133, 0.8344755977033133, 0.8262248211596386, 0.8294295345444277, 0.8331931240587349, 0.8369978939194277, 0.8412703548569277, 0.8363360669239458, 0.8378935664533133, 0.8389010142131024, 0.8318812358810241, 0.8336314006024097, 0.8360316265060241, 0.8328475032944277, 0.8324607021837349, 0.8376185405685241, 0.8377303157944277, 0.8364390177899097, 0.8344035320971386, 0.8315959149096386, 0.8366213878953314, 0.8337225856551205, 0.8356654155685241, 0.8405585231551205, 0.8369170039533133, 0.8395113657756024, 0.8352080195783133, 0.8402540827371988, 0.8349432887801205, 0.8369978939194277, 0.8365199077560241, 0.8362757671310241, 0.8291956890060241, 0.8405791133283133, 0.8380347562123494, 0.8354418651167168], "accuracy_test": 0.7413723692602041, "start": "2016-01-23 18:08:28.192000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0], "accuracy_train_last": 0.9527326086078812, "batch_size_eval": 1024, "accuracy_train_std": [0.015582313486211932, 0.014228834615352855, 0.013635332047306638, 0.012264095440592676, 0.013432871603757923, 0.010287673894123146, 0.010896466678325549, 0.013072584348402116, 0.012527016365672581, 0.013997835006921497, 0.015313321955031755, 0.014952241597715214, 0.013822599127668542, 0.014485588860637633, 0.013383864808843455, 0.01417773986897388, 0.013791472736691632, 0.01420471834524876, 0.012912813454908793, 0.012769365599530005, 0.01368774058760363, 0.013620886609127887, 0.01425330403645627, 0.013427871343541788, 0.014533189816129095, 0.01248744155062051, 0.013426604024447647, 0.01393124625558689, 0.013888406832365531, 0.013050110752533279, 0.013818325719282891, 0.014475189079449136, 0.013524488646062423, 0.013135697415669575, 0.013505517508609366, 0.014130932828741529, 0.01449054239143553, 0.013318651051583202, 0.013421139116490923, 0.016494089020283854, 0.014825088615661676, 0.014800134343190174, 0.014711394677830184, 0.01620461596781818, 0.01615838912734681, 0.014823336045625744, 0.016495066274265877, 0.01520202093546271, 0.015018856383223218, 0.015016345624238469, 0.014869290205629944, 0.015496306791508103, 0.015036303313841459, 0.01599118097519716, 0.015094030624032584, 0.014889722109431732, 0.013163864615349553, 0.013338909430314676, 0.014676845127129242, 0.015907994185866293, 0.014585318419501546, 0.015900831039792934, 0.01445939612339695, 0.014772757797948595, 0.014093479342211213, 0.014384477142198067, 0.015282232282790945, 0.014099448977400267, 0.014160251907918562, 0.014198514255391237, 0.015616322145587518, 0.013586351055513477, 0.015415755155875264, 0.014407839589716507, 0.01492925191383691, 0.01416586363014774, 0.013770416477744405, 0.014045761982037741, 0.01444711584815131, 0.01232364738075695, 0.014279128112465287, 0.012904933202724391, 0.013079350545082987, 0.012415520133750475, 0.014808182670674862, 0.01224127500569611, 0.014366259346551495, 0.012972406309974972, 0.014332619473809219, 0.013217195040108176, 0.012314237947736017, 0.013216689476545126, 0.011673634880385503, 0.013579672433860513, 0.012250958884074495, 0.011899858894607943, 0.012283549568662155, 0.01321991263477078, 0.013936688912219103, 0.013392083786427134, 0.012919537738117376, 0.013219925330208923, 0.012509607315752068, 0.013704293457272166, 0.013104343819816226, 0.013209632036511635, 0.012454089973954271, 0.01236977919260717, 0.012560942643473119, 0.013246941380649755, 0.013031135861271909, 0.01358076368434402, 0.011593429028548378, 0.013988931252658593, 0.011813912842568412, 0.01208253624507739, 0.011851397768219274, 0.011669029803126243, 0.011960753568833525, 0.011299459945591972, 0.012499250987540988, 0.012268308292882302, 0.012475902617357695, 0.013405686166144123, 0.011541040866430038, 0.013296609998765658, 0.013804622836728437, 0.0124411895056236, 0.012903715460764168, 0.01397277089791025, 0.01298178343887988, 0.01282731768208923, 0.011490456901766305, 0.010856472908729759, 0.012163098941634556, 0.011350403380709336, 0.012960178780215445, 0.01129889370004014, 0.012765709643353385, 0.011633257396104445, 0.012381091736112649, 0.01242337470528694, 0.011679824025163666, 0.012346919121102784, 0.012807503135940222, 0.010033638362193013, 0.01186223764329553, 0.012501353063278998, 0.010569332023191678, 0.012233906665696875, 0.012110302040864704, 0.012334565356039608, 0.0111449399086234, 0.013280678837265027, 0.011372958741556268, 0.011665777886649714, 0.012535565265429234, 0.010322164890858118, 0.011111405349394289, 0.013699912489303325, 0.011405123168936573, 0.01185811585707538, 0.011951898870412076, 0.010402796227037067, 0.010953610808570876, 0.010893967168524608, 0.011046826205991756, 0.013216143571838047, 0.010918718858867405, 0.010929462439157649, 0.01144429847165936, 0.010052038146035185, 0.011296605845250133, 0.01096341371167791, 0.011860985033845443, 0.013171004186071547, 0.013840462885993207, 0.009906822373416398, 0.01084675111291229, 0.01077842322264309, 0.0126492301377374, 0.012929653903326774, 0.012356117416865306, 0.012795211091519973, 0.011267816690523744, 0.01060055653365524, 0.01159196090412502, 0.010837030039033982, 0.011068744933432803, 0.012188551692134314, 0.011791775080000826, 0.010926977577004567, 0.011183517228655475, 0.011801495581767516, 0.010805828756332276, 0.010226611039461618, 0.010472197335806364, 0.010018688481033083, 0.011402167079546152, 0.01015723770055935, 0.011618596763752605, 0.01165271413009456, 0.010115769433084707, 0.01052277501162998, 0.010349029969144543, 0.011512643056965166, 0.011131668350829727, 0.010640568712343184, 0.010596905505657938, 0.010567298675247836, 0.012212988552981533, 0.012932612489471943, 0.010712938254316729, 0.010816155839579894, 0.013027376218014709], "accuracy_test_std": 0.011884859555558529, "error_valid": [0.5226153637989458, 0.42915656767695776, 0.38751882530120485, 0.35392889919051207, 0.33769354762801207, 0.31802846150225905, 0.3027784967996988, 0.28464002494352414, 0.27707166556852414, 0.2702048428087349, 0.26496611445783136, 0.25072359751506024, 0.24498629282756024, 0.2413844832454819, 0.23062170557228923, 0.22975691829819278, 0.22281950065888556, 0.21394807746611444, 0.21425398861069278, 0.21126253059111444, 0.21040803840361444, 0.20442659309111444, 0.20523990493222888, 0.20161897590361444, 0.19715237904743976, 0.1930122835090362, 0.19204601609563254, 0.18993993552334332, 0.18797651543674698, 0.1884956819465362, 0.18360257435993976, 0.18489387236445776, 0.18481298239834332, 0.1877632600715362, 0.18909573842243976, 0.18802799086972888, 0.18140530873493976, 0.18716320359563254, 0.1763592455760542, 0.18248335137424698, 0.17956395896084332, 0.17821089043674698, 0.1772137377635542, 0.17749905873493976, 0.17561652861445776, 0.1766033862010542, 0.1785256259412651, 0.17784467949924698, 0.1761151049510542, 0.17759024378765065, 0.17746817347515065, 0.17441641566265065, 0.17613569512424698, 0.17978750941265065, 0.1757488940135542, 0.17143525272966864, 0.1717602833207832, 0.17092638130647586, 0.17253388554216864, 0.17197500941265065, 0.1681290592055723, 0.16958360786897586, 0.1787491763930723, 0.1804787509412651, 0.17028514448418675, 0.1761151049510542, 0.16975715361445776, 0.17976691923945776, 0.1741310946912651, 0.18029638083584332, 0.1809979174510542, 0.18454825160015065, 0.1737648837537651, 0.17511795227786142, 0.1780373446912651, 0.1739884342055723, 0.17475174134036142, 0.17634895048945776, 0.17377517884036142, 0.16721279649849397, 0.17202501411897586, 0.1701336596385542, 0.17180146366716864, 0.17239122505647586, 0.17197500941265065, 0.17436494022966864, 0.1750870670180723, 0.17731521790286142, 0.17232063017695776, 0.17460908085466864, 0.1730324618787651, 0.17232063017695776, 0.16943065229668675, 0.17009247929216864, 0.17189264871987953, 0.16454783979668675, 0.17649161097515065, 0.1698586337537651, 0.1799507600715362, 0.16716279179216864, 0.1744575960090362, 0.1798992846385542, 0.1759224397590362, 0.17228974491716864, 0.17192353397966864, 0.16856733574924698, 0.17929922816265065, 0.16396837349397586, 0.1671524967055723, 0.17158820830195776, 0.16923798710466864, 0.17204560429216864, 0.16872911568147586, 0.17076460137424698, 0.1697159732680723, 0.16482286568147586, 0.1693497623305723, 0.16704072147966864, 0.1659009083207832, 0.1688614810805723, 0.17042780496987953, 0.16566706278237953, 0.1750164721385542, 0.1735207431287651, 0.15927822618599397, 0.17244270048945776, 0.1687600009412651, 0.16920710184487953, 0.17292068665286142, 0.16438605986445776, 0.1649037556475903, 0.16679658085466864, 0.16289033085466864, 0.16154755741716864, 0.16908503153237953, 0.1640492634600903, 0.1677834384412651, 0.1657582478350903, 0.17007188911897586, 0.1658097232680723, 0.16811876411897586, 0.1732560123305723, 0.16704072147966864, 0.16212702371987953, 0.17006159403237953, 0.16603327371987953, 0.16567735786897586, 0.16835260965737953, 0.16596267884036142, 0.1720353092055723, 0.17353103821536142, 0.1612622364457832, 0.1685658650225903, 0.17177057840737953, 0.1669789509600903, 0.16339920227786142, 0.17336778755647586, 0.1632153614457832, 0.15812811794051207, 0.17131318241716864, 0.1640492634600903, 0.17009247929216864, 0.1672230915850903, 0.15753835655120485, 0.15572789203689763, 0.16348009224397586, 0.16469050028237953, 0.17654161568147586, 0.15963414203689763, 0.1624623493975903, 0.16313447147966864, 0.16841438017695776, 0.16371393778237953, 0.1634903873305723, 0.1627270801957832, 0.16847467996987953, 0.1774064029555723, 0.16799669380647586, 0.16811876411897586, 0.16344920698418675, 0.16552440229668675, 0.17377517884036142, 0.1705704654555723, 0.1668068759412651, 0.1630021060805723, 0.1587296451430723, 0.1636639330760542, 0.16210643354668675, 0.16109898578689763, 0.16811876411897586, 0.1663685993975903, 0.16396837349397586, 0.1671524967055723, 0.1675392978162651, 0.16238145943147586, 0.1622696842055723, 0.1635609822100903, 0.16559646790286142, 0.16840408509036142, 0.16337861210466864, 0.16627741434487953, 0.16433458443147586, 0.15944147684487953, 0.16308299604668675, 0.16048863422439763, 0.16479198042168675, 0.15974591726280118, 0.16505671121987953, 0.1630021060805723, 0.16348009224397586, 0.16372423286897586, 0.17080431099397586, 0.15942088667168675, 0.16196524378765065, 0.1645581348832832], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5076989511566075, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.00028778390598734987, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "optimization": "adam", "nb_data_augmentation": 4, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 2.7098243073425202e-05, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.02679736260001755}, "accuracy_valid_max": 0.8442721079631024, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8354418651167168, "loss_train": [1.9945014715194702, 1.5985946655273438, 1.4108446836471558, 1.2891780138015747, 1.2040929794311523, 1.1377323865890503, 1.0842329263687134, 1.0394266843795776, 1.0006242990493774, 0.9660658240318298, 0.936145007610321, 0.909101128578186, 0.8848443031311035, 0.8621532320976257, 0.8409860730171204, 0.8231827020645142, 0.8048816323280334, 0.7905737161636353, 0.7763477563858032, 0.7586994767189026, 0.7465682029724121, 0.734251081943512, 0.7221839427947998, 0.7105916738510132, 0.6998254656791687, 0.6900122761726379, 0.6807886958122253, 0.6702949404716492, 0.660935640335083, 0.6523004174232483, 0.6451841592788696, 0.6386311054229736, 0.6277292370796204, 0.6214970350265503, 0.6149474382400513, 0.6078432202339172, 0.6030158400535583, 0.5950660705566406, 0.5874683856964111, 0.5812026262283325, 0.5777193307876587, 0.569839358329773, 0.5641999244689941, 0.559120774269104, 0.5544025301933289, 0.5490803122520447, 0.5421372056007385, 0.5429989099502563, 0.5363128185272217, 0.5299862623214722, 0.5257596373558044, 0.5222448110580444, 0.5173093676567078, 0.5159722566604614, 0.5111442804336548, 0.5064364075660706, 0.5019007921218872, 0.49865278601646423, 0.4963030517101288, 0.49136292934417725, 0.4892691969871521, 0.4843887686729431, 0.4839830994606018, 0.4796395003795624, 0.4749799966812134, 0.47193413972854614, 0.4711674451828003, 0.46592578291893005, 0.463609516620636, 0.45954644680023193, 0.45568159222602844, 0.45683541893959045, 0.4553605318069458, 0.4496321380138397, 0.45010095834732056, 0.44636771082878113, 0.44374457001686096, 0.4406442642211914, 0.44092851877212524, 0.4385819137096405, 0.43665987253189087, 0.43094465136528015, 0.4318556785583496, 0.43094006180763245, 0.4271978735923767, 0.42655351758003235, 0.4246790409088135, 0.42079636454582214, 0.4182087481021881, 0.41866493225097656, 0.4157630503177643, 0.4155416488647461, 0.4136730134487152, 0.4110649526119232, 0.40907302498817444, 0.40957650542259216, 0.40826866030693054, 0.403891384601593, 0.4076458215713501, 0.3998979330062866, 0.39923426508903503, 0.4005833566188812, 0.39889612793922424, 0.39774611592292786, 0.39503151178359985, 0.3942912518978119, 0.3920627236366272, 0.39297962188720703, 0.38840726017951965, 0.3873749077320099, 0.3898296058177948, 0.3829285800457001, 0.38807353377342224, 0.3846486508846283, 0.38306230306625366, 0.37998270988464355, 0.38149550557136536, 0.3786042332649231, 0.3794529139995575, 0.37794598937034607, 0.3785134255886078, 0.3750136196613312, 0.37610572576522827, 0.3731873631477356, 0.37352821230888367, 0.3735752999782562, 0.37026968598365784, 0.37107735872268677, 0.3694581091403961, 0.3660907745361328, 0.36869388818740845, 0.36903515458106995, 0.36959308385849, 0.364210307598114, 0.36464160680770874, 0.3662630319595337, 0.36470529437065125, 0.36311963200569153, 0.3618679940700531, 0.360437273979187, 0.3584102690219879, 0.36057671904563904, 0.3558944761753082, 0.3592849671840668, 0.3581426739692688, 0.3584229648113251, 0.3567250967025757, 0.356382817029953, 0.35433873534202576, 0.3544861674308777, 0.35345956683158875, 0.3539767563343048, 0.35133883357048035, 0.3502742350101471, 0.35339421033859253, 0.3483717143535614, 0.3498494625091553, 0.34967267513275146, 0.34713155031204224, 0.34797224402427673, 0.34875771403312683, 0.3470516800880432, 0.34441688656806946, 0.3471561372280121, 0.34540611505508423, 0.3429127037525177, 0.3417279124259949, 0.34556543827056885, 0.342925488948822, 0.3409162163734436, 0.340072363615036, 0.3453921675682068, 0.3393280506134033, 0.33850109577178955, 0.3413076400756836, 0.3370777666568756, 0.33645132184028625, 0.3355320394039154, 0.34001824259757996, 0.33640578389167786, 0.33550626039505005, 0.3387317955493927, 0.33627673983573914, 0.3363814055919647, 0.33545875549316406, 0.3355036973953247, 0.333804190158844, 0.336018830537796, 0.3300339877605438, 0.33273249864578247, 0.3321869373321533, 0.33111539483070374, 0.3322997987270355, 0.332992285490036, 0.3274567723274231, 0.3315492570400238, 0.32919585704803467, 0.3331756889820099, 0.3298209607601166, 0.33071625232696533, 0.327153742313385, 0.3286117911338806, 0.32647809386253357, 0.33020496368408203, 0.32690906524658203, 0.32544389367103577, 0.32523903250694275, 0.3254213035106659, 0.32859328389167786, 0.3246617913246155, 0.32596343755722046, 0.32680782675743103, 0.32379814982414246, 0.32308676838874817, 0.32597923278808594], "accuracy_train_first": 0.4914212823574197, "model": "residualv5", "loss_std": [0.21336553990840912, 0.11968199163675308, 0.11667566001415253, 0.11707644909620285, 0.11696185171604156, 0.11646367609500885, 0.11420146375894547, 0.11275754123926163, 0.11004513502120972, 0.11061974614858627, 0.10972107946872711, 0.10924436151981354, 0.10812290012836456, 0.10676500201225281, 0.10564208775758743, 0.10599099844694138, 0.10467542707920074, 0.10391823202371597, 0.1032111719250679, 0.10160308331251144, 0.1015479564666748, 0.09958922117948532, 0.10008441656827927, 0.10101612657308578, 0.09748996794223785, 0.0986112430691719, 0.09887150675058365, 0.09499765932559967, 0.09482258558273315, 0.09497813880443573, 0.09394381940364838, 0.0932135358452797, 0.09018861502408981, 0.09087283164262772, 0.08995898067951202, 0.08940541744232178, 0.09012676030397415, 0.08764979988336563, 0.08540111035108566, 0.08818915486335754, 0.08408136665821075, 0.08385854959487915, 0.0845460295677185, 0.0818186029791832, 0.0826774537563324, 0.08120981603860855, 0.07887503504753113, 0.08046939969062805, 0.07962685078382492, 0.07892138510942459, 0.07690189778804779, 0.0775039866566658, 0.07449173182249069, 0.0751204863190651, 0.07403001189231873, 0.07300885021686554, 0.071546271443367, 0.07193568348884583, 0.07212638109922409, 0.06931015849113464, 0.07018263638019562, 0.06944400072097778, 0.06703747063875198, 0.06727471947669983, 0.06841183453798294, 0.06734898686408997, 0.0669318437576294, 0.06402810662984848, 0.0649125874042511, 0.06386852264404297, 0.06413081288337708, 0.06377401202917099, 0.06290213018655777, 0.06291154772043228, 0.06008438020944595, 0.06144479289650917, 0.05859073996543884, 0.06119425222277641, 0.060637637972831726, 0.06218944117426872, 0.058865323662757874, 0.058960434049367905, 0.059342414140701294, 0.059991467744112015, 0.05591098591685295, 0.05822864547371864, 0.05675986036658287, 0.05684572458267212, 0.05603579804301262, 0.054650671780109406, 0.054473236203193665, 0.054883960634469986, 0.05542650446295738, 0.052909936755895615, 0.052301496267318726, 0.05551065504550934, 0.05514633283019066, 0.05550086870789528, 0.053683847188949585, 0.050743378698825836, 0.05179104581475258, 0.052453044801950455, 0.05097072944045067, 0.05172747001051903, 0.05022059381008148, 0.04978547617793083, 0.05063191428780556, 0.04917114973068237, 0.04897848889231682, 0.05040116235613823, 0.05112390220165253, 0.04797699674963951, 0.04818468540906906, 0.04894702509045601, 0.048283424228429794, 0.04667966440320015, 0.04867672920227051, 0.04714789614081383, 0.04743213579058647, 0.047595735639333725, 0.04636327177286148, 0.04738861694931984, 0.04675611853599548, 0.04717124253511429, 0.04670850560069084, 0.04593642055988312, 0.04632645100355148, 0.04671921953558922, 0.045448705554008484, 0.043726738542318344, 0.04563463106751442, 0.045859821140766144, 0.045049965381622314, 0.04574231803417206, 0.04602387920022011, 0.04553714767098427, 0.04355309531092644, 0.04338543862104416, 0.0450359471142292, 0.04406355321407318, 0.04157967120409012, 0.04364430531859398, 0.04234766960144043, 0.042735401540994644, 0.04238314554095268, 0.045067381113767624, 0.043850064277648926, 0.04323456808924675, 0.042984362691640854, 0.04273787885904312, 0.04363995045423508, 0.04118213802576065, 0.0406637117266655, 0.03918369486927986, 0.0421670638024807, 0.03992050141096115, 0.04012469947338104, 0.04052022099494934, 0.039381109178066254, 0.04062819480895996, 0.03996153920888901, 0.04169536754488945, 0.0404934398829937, 0.04085627570748329, 0.038564637303352356, 0.038933124393224716, 0.03873039782047272, 0.038692738860845566, 0.03840416669845581, 0.03887973725795746, 0.03954498469829559, 0.040323540568351746, 0.03719933331012726, 0.03819561377167702, 0.03937099128961563, 0.0394558310508728, 0.03724086284637451, 0.037210896611213684, 0.03857612609863281, 0.03730647638440132, 0.037488747388124466, 0.038945864886045456, 0.03872009739279747, 0.03774694725871086, 0.03845876827836037, 0.03704454377293587, 0.036468781530857086, 0.03783177584409714, 0.036122966557741165, 0.03661803901195526, 0.037569310516119, 0.036142975091934204, 0.03787815943360329, 0.03792405128479004, 0.035408828407526016, 0.03628503158688545, 0.037432026118040085, 0.03686325252056122, 0.036809369921684265, 0.037014439702034, 0.03773139417171478, 0.03569144383072853, 0.03654465079307556, 0.037653498351573944, 0.03669106960296631, 0.036232251673936844, 0.03748968988656998, 0.034171443432569504, 0.03764106705784798, 0.035790570080280304, 0.03663916513323784, 0.03737175092101097, 0.034368786960840225, 0.035515930503606796, 0.036396633833646774]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:13 2016", "state": "available"}], "summary": "18e8d0f2a7b8ce086f8ba4d8a85e045c"}