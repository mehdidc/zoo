{"content": {"hp_model": {"f1": 93, "f2": 37, "f3": 93, "nonlin": "rectify", "ds2": 617, "ds1": 2843, "do2": 0.4356487939667466, "do3": 0.26591888053003077, "do1": 0.4710759146335889, "do4": 0.4191615059767523, "do5": 0.34995421886116107}, "accuracy_valid_std": [0.10855819384711499, 0.12257753149813348, 0.125850324790056, 0.1349226028794309, 0.12923452419217873, 0.13006649078919716, 0.13049886773407024, 0.11965893941931498, 0.11727556608259324, 0.11918529953147097, 0.11593418799795668, 0.11402318355802148, 0.11218966172825232, 0.11535400680198692, 0.10692537794209794, 0.10916142608202688, 0.10567298929589616, 0.1071272615953626, 0.10401623204823729, 0.10311662573334064, 0.10350806411321492, 0.10451194692738021, 0.10180058172245143, 0.1030654175704085, 0.09940491539685124, 0.09645931342746092, 0.10270407491041983, 0.09869214287779013, 0.09832252825854762, 0.10283527228575229, 0.0961757305342756, 0.09338975823290682, 0.09755990595067791, 0.09859595749773319, 0.09372564638510682, 0.09602568545215007, 0.09804742701048338, 0.09649120246997564, 0.09131061249391645, 0.09587707972071212, 0.09495887252629062, 0.0958160465928392, 0.09458824546691781, 0.09220509468219858, 0.09103743926463123, 0.09396652604602308, 0.09463951771648813, 0.09470356899285226, 0.09443227864632867, 0.09575562645199977, 0.09467004167074625, 0.09330225218504776, 0.0961479105480407, 0.09356566849245385, 0.09120654520534888, 0.096857586143634, 0.09176419126801177, 0.09282429780496525, 0.09312469526136109, 0.08983763943343866, 0.09220509468219858, 0.09094570892457238, 0.09190518447695113, 0.09193312437390604, 0.09063307228754011, 0.09158822871720734, 0.09594987788895779, 0.09328227478397196, 0.09138090069882386, 0.089855206967602, 0.09109981403760237, 0.08897799327741797, 0.08776985351171698, 0.09305486175249418, 0.09353049383998395, 0.089673112048094, 0.0890693473000718, 0.08718431094073661, 0.08787839660804612, 0.08984240384996464, 0.09023143929726571, 0.0910159850020682, 0.0911980386871974, 0.08988259355934464, 0.0912206232164535, 0.09243488780141577, 0.08639603846138742, 0.08877602057001165, 0.08732871607635735, 0.08962148604758653, 0.08964337353171152, 0.08978203589258318, 0.09121329119013459, 0.08918730773926133, 0.08729470580729537, 0.09008357012483366, 0.08754523367584464, 0.08973723013689618, 0.08822384657852869, 0.09327242788794979, 0.08784541125901908, 0.08825224464506827, 0.091727548087561, 0.08842738302670358, 0.09215198412533598, 0.08906093703171718, 0.08740547136061284, 0.08642245756465995, 0.0876829426312936, 0.09040136245912243, 0.0872145812123445, 0.08861596256368319, 0.0873397435897436, 0.08788286136531226, 0.08701066743502696, 0.08860197392497943, 0.0879176584046254, 0.09155404755405323, 0.08859110352978798, 0.08760144263016305, 0.08895664379085168, 0.08803979553338413, 0.08969707471278848, 0.09025722974569694, 0.08845087663875893, 0.08755990036004535, 0.09108219284282301, 0.08977567900937099, 0.08716548898714757, 0.09121681063631851, 0.08805630397949517, 0.08833294228589338, 0.08642988648243652, 0.08857570151961179, 0.08587674939997667, 0.08819038374537196, 0.08858778175409056, 0.08879620838610731, 0.08794615534026266, 0.0878053046910431, 0.08839914209976038, 0.08813455039670005, 0.08644649605813444, 0.08833294228589339, 0.08756570524389161, 0.08838471557387224, 0.08723308589908223, 0.08974627256750012, 0.08703526067248207, 0.08827800722594163, 0.08673567206711538, 0.09022481757156506, 0.08945057986724136, 0.0898147071440191, 0.09006852233885634, 0.08687774365258007, 0.08789655863699154, 0.08683061768385997, 0.09001771749002382, 0.08860197392497943, 0.08841941596434288], "moving_avg_accuracy_train": [0.028906249999999994, 0.06591114457831325, 0.10596442018072288, 0.14767643072289155, 0.18734685240963853, 0.22839191114457827, 0.2684857019578313, 0.3067350233283132, 0.3450280119593373, 0.3823790322995481, 0.4181716260575451, 0.45222278046986286, 0.48401716281444285, 0.5123732590329986, 0.5401763134007831, 0.5664415322113072, 0.590576746460056, 0.6130396855188697, 0.6348447154609587, 0.6549492875895616, 0.6740882067221717, 0.6912567580379063, 0.7073955777160434, 0.7224499770227524, 0.7370531532662603, 0.7507301798071041, 0.7626512318565142, 0.7743638006889351, 0.785921678903174, 0.7952789650791217, 0.8055336363422938, 0.8157252840032452, 0.8245424393378604, 0.8323578678438335, 0.8413331126859561, 0.8488366613571195, 0.8562416812153835, 0.862687354961315, 0.8690602798567497, 0.875323020696376, 0.8812701049219192, 0.8872154777128598, 0.8928816370198871, 0.8982659130769346, 0.9038271229740604, 0.9086368993814736, 0.9133327915216396, 0.9181638572489937, 0.9223329760421667, 0.9263369713596368, 0.9301476454586128, 0.9336760849790167, 0.9371881828064162, 0.9410197222064974, 0.9445622341725947, 0.9475410634661785, 0.9506479322701631, 0.9536558988323034, 0.9564007193406393, 0.9590569576475393, 0.961466397424954, 0.9634442870499285, 0.9656926670798753, 0.9678126887755023, 0.969871310711205, 0.9716205312967111, 0.9733383727453533, 0.9750750362238301, 0.9765839106134953, 0.9782007454557602, 0.9793805767836782, 0.9807906930510935, 0.9819986154628515, 0.9832598796695784, 0.984333835226717, 0.9853545179691056, 0.9863978500577371, 0.98739332484112, 0.9883316090738755, 0.9891854775339578, 0.9898480668287547, 0.9906020590916623, 0.9911982914355082, 0.9919043282558129, 0.9924997576290268, 0.9930497630408229, 0.9935777121885478, 0.9940834575359581, 0.9944797992823622, 0.9948929827577404, 0.9952083719819663, 0.9955651703259384, 0.9959145267873204, 0.9961960033254558, 0.9965105144386933, 0.9967206263984384, 0.9970038536682331, 0.9972422860724941, 0.9974709942122326, 0.9976744783753466, 0.9978505546341975, 0.9980137295924645, 0.9981841186814109, 0.998346881512065, 0.9984816022464007, 0.9986004977446522, 0.9987145631810304, 0.9988125157484695, 0.9989053793844659, 0.9989936629821639, 0.9990754713827427, 0.9991514521059142, 0.9992198347567685, 0.9992860854678386, 0.9993433579451511, 0.9993949031747323, 0.999438940718705, 0.9994926934841839, 0.9995316583225125, 0.99957378616496, 0.9996093480605122, 0.9996460600918103, 0.9996767477573281, 0.9997067198189447, 0.9997313415117489, 0.9997511478726222, 0.9997713267600588, 0.9997871345961011, 0.9998060679738404, 0.9998136953632033, 0.9998276195015817, 0.9998377980634717, 0.9998493119318232, 0.9998573212506892, 0.9998715891256202, 0.9998797238877569, 0.99988704517368, 0.9998959874936614, 0.9999040355816446, 0.9999136320234802, 0.9999222688211321, 0.9999276887763683, 0.9999278604107796, 0.999932721207051, 0.9999347427610447, 0.9999342089969884, 0.9999360817719882, 0.9999424735947894, 0.9999458730726598, 0.9999512857653938, 0.9999538040262038, 0.9999560704609328, 0.9999604634148396, 0.9999644170733556, 0.99996797536602, 0.9999688246667674, 0.99996958903744, 0.9999702769710455, 0.9999708961112903, 0.9999714533375107, 0.999971954841109], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.007520141601562498, 0.019092387446174943, 0.0316215326799007, 0.04411840582314054, 0.053870246452065416, 0.06364549342585077, 0.07174855263925772, 0.07774079264305361, 0.08316389018340992, 0.08740338964916253, 0.09019303859557133, 0.09160906478731778, 0.09154610304664605, 0.08962810647678535, 0.08762238431870863, 0.08506890135932015, 0.08180458832488519, 0.07816538217283647, 0.07462797793253138, 0.07080292452354603, 0.06701931610127272, 0.06297021687967458, 0.05901734869713795, 0.0551553282737967, 0.05155907025400761, 0.04808671272359738, 0.044557044788920336, 0.04133599872791607, 0.0384046597944611, 0.03535222305622222, 0.032763425295041566, 0.030421909903942182, 0.02807939896730057, 0.025821187375158292, 0.02396406381742691, 0.022074386619628888, 0.020360456829577548, 0.018698331536970713, 0.017194025928779312, 0.015827620641320282, 0.01456316887425959, 0.013424979105442944, 0.012371429446532156, 0.01139520035980539, 0.010534023823503862, 0.009688826982957257, 0.008918406911590177, 0.0082366189849893, 0.0075693910500946576, 0.0069567397516060924, 0.006391756910242948, 0.005864630188260979, 0.005389180649777902, 0.004982388832369488, 0.004597094468402019, 0.0042172458372046395, 0.0038823949573707278, 0.003575586227184243, 0.003285833961072652, 0.00302075098245277, 0.002770924484576389, 0.0025290404624359855, 0.0023216333310239643, 0.002129920425830928, 0.001955069701715245, 0.0017871006854545423, 0.0016349494300931446, 0.0014985984874211038, 0.0013692289559930808, 0.0012558334545582296, 0.0011427781267634367, 0.0010463961650757569, 0.0009548882375436293, 0.0008737165003818002, 0.000796725275191999, 0.0007264288870182872, 0.0006635828749409741, 0.0006061433178460382, 0.0005534523817743721, 0.0005046689657210452, 0.0004581532903111569, 0.0004174545002727629, 0.00037890848731611743, 0.00034550403050913813, 0.00031414445270459737, 0.00028545256101118277, 0.0002594158776333159, 0.00023577629507782864, 0.00021361244658953034, 0.0001937876871895081, 0.00017530415173537797, 0.0001589194820861909, 0.00014412598331155644, 0.00013042644635408716, 0.0001182740568818278, 0.0001068439745142965, 9.688153624006518e-05, 8.770503271867394e-05, 7.940529616545054e-05, 7.183741879064967e-05, 6.4932702551963e-05, 5.867906689981562e-05, 5.307245218452166e-05, 4.80036326174523e-05, 4.336661644204654e-05, 3.915718005338212e-05, 3.535856036202916e-05, 3.19090566750375e-05, 2.879576390154804e-05, 2.5986333453995747e-05, 2.344793363824352e-05, 2.1155097907062178e-05, 1.9081673798796803e-05, 1.7213008829372813e-05, 1.5521229176353163e-05, 1.3993018454951134e-05, 1.2611170356968227e-05, 1.1376057559441067e-05, 1.0252116131130722e-05, 9.24287731400122e-06, 8.329971418338484e-06, 7.509104235682983e-06, 6.766669407449054e-06, 6.098087387002087e-06, 5.493734698110836e-06, 4.947891855679142e-06, 4.456767357594829e-06, 4.0133396109584115e-06, 3.615231904996136e-06, 3.254232308112968e-06, 2.9305540119679152e-06, 2.638431038870464e-06, 2.3757810574631847e-06, 2.138780294415122e-06, 1.9267344152690513e-06, 1.7346565429373432e-06, 1.5616732996917193e-06, 1.4062256555023947e-06, 1.2661860354338317e-06, 1.1403962571535743e-06, 1.027027979901349e-06, 9.245895651440744e-07, 8.321308737550075e-07, 7.491304324430384e-07, 6.742541693236794e-07, 6.068313165279215e-07, 5.461797504509337e-07, 4.919294739943353e-07, 4.428405346430276e-07, 3.98820156362421e-07, 3.589952154637442e-07, 3.231419244547978e-07, 2.910014144055568e-07, 2.620419557059545e-07, 2.359517131555315e-07, 2.1236303364581407e-07, 1.9113198864395963e-07, 1.7202304905337252e-07, 1.548241941598204e-07, 1.3934456925338433e-07, 1.2541237588077813e-07], "duration": 44373.547169, "accuracy_train": [0.2890625, 0.39895519578313254, 0.46644390060240964, 0.5230845256024096, 0.5443806475903614, 0.5977974397590361, 0.6293298192771084, 0.6509789156626506, 0.6896649096385542, 0.7185382153614458, 0.7403049698795181, 0.7586831701807228, 0.7701666039156626, 0.767578125, 0.7904038027108434, 0.8028285015060241, 0.8077936746987951, 0.8152061370481928, 0.831089984939759, 0.8358904367469879, 0.8463384789156626, 0.8457737198795181, 0.8526449548192772, 0.8579395707831325, 0.8684817394578314, 0.8738234186746988, 0.8699407003012049, 0.8797769201807228, 0.8899425828313253, 0.8794945406626506, 0.8978256777108434, 0.9074501129518072, 0.9038968373493976, 0.9026967243975904, 0.9221103162650602, 0.9163685993975904, 0.922886859939759, 0.9206984186746988, 0.9264166039156626, 0.9316876882530121, 0.9347938629518072, 0.9407238328313253, 0.9438770707831325, 0.9467243975903614, 0.9538780120481928, 0.9519248870481928, 0.9555958207831325, 0.9616434487951807, 0.9598550451807228, 0.9623729292168675, 0.9644437123493976, 0.9654320406626506, 0.9687970632530121, 0.9755035768072289, 0.9764448418674698, 0.9743505271084337, 0.9786097515060241, 0.9807275978915663, 0.9811041039156626, 0.9829631024096386, 0.9831513554216867, 0.9812452936746988, 0.9859280873493976, 0.9868928840361446, 0.9883989081325302, 0.9873635165662651, 0.9887989457831325, 0.9907050075301205, 0.9901637801204819, 0.9927522590361446, 0.9899990587349398, 0.9934817394578314, 0.9928699171686747, 0.9946112575301205, 0.9939994352409639, 0.9945406626506024, 0.9957878388554217, 0.9963525978915663, 0.9967761671686747, 0.9968702936746988, 0.9958113704819277, 0.9973879894578314, 0.9965643825301205, 0.9982586596385542, 0.9978586219879518, 0.9979998117469879, 0.9983292545180723, 0.9986351656626506, 0.998046875, 0.9986116340361446, 0.998046875, 0.9987763554216867, 0.999058734939759, 0.9987292921686747, 0.9993411144578314, 0.9986116340361446, 0.9995528990963856, 0.9993881777108434, 0.9995293674698795, 0.9995058358433735, 0.9994352409638554, 0.9994823042168675, 0.9997176204819277, 0.9998117469879518, 0.9996940888554217, 0.9996705572289156, 0.9997411521084337, 0.9996940888554217, 0.9997411521084337, 0.9997882153614458, 0.9998117469879518, 0.9998352786144579, 0.9998352786144579, 0.9998823418674698, 0.9998588102409639, 0.9998588102409639, 0.9998352786144579, 0.999976468373494, 0.9998823418674698, 0.9999529367469879, 0.9999294051204819, 0.999976468373494, 0.9999529367469879, 0.999976468373494, 0.9999529367469879, 0.9999294051204819, 0.9999529367469879, 0.9999294051204819, 0.999976468373494, 0.9998823418674698, 0.9999529367469879, 0.9999294051204819, 0.9999529367469879, 0.9999294051204819, 1.0, 0.9999529367469879, 0.9999529367469879, 0.999976468373494, 0.999976468373494, 1.0, 1.0, 0.999976468373494, 0.9999294051204819, 0.999976468373494, 0.9999529367469879, 0.9999294051204819, 0.9999529367469879, 1.0, 0.999976468373494, 1.0, 0.999976468373494, 0.999976468373494, 1.0, 1.0, 1.0, 0.999976468373494, 0.999976468373494, 0.999976468373494, 0.999976468373494, 0.999976468373494, 0.999976468373494], "end": "2016-01-18 13:43:44.058000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0], "accuracy_valid": [0.29246794871794873, 0.40357905982905984, 0.4652777777777778, 0.5241720085470085, 0.5412660256410257, 0.5925480769230769, 0.6212606837606838, 0.6422275641025641, 0.6792200854700855, 0.7048611111111112, 0.7211538461538461, 0.7398504273504274, 0.749465811965812, 0.7443910256410257, 0.7636217948717948, 0.7745726495726496, 0.7747061965811965, 0.7853899572649573, 0.7897970085470085, 0.7924679487179487, 0.7979433760683761, 0.7991452991452992, 0.8027510683760684, 0.8058226495726496, 0.8114316239316239, 0.8139690170940171, 0.8072916666666666, 0.8131677350427351, 0.8194444444444444, 0.8120993589743589, 0.8239850427350427, 0.8237179487179487, 0.8231837606837606, 0.8185096153846154, 0.8319978632478633, 0.828392094017094, 0.828125, 0.8255876068376068, 0.8314636752136753, 0.8330662393162394, 0.8334668803418803, 0.8341346153846154, 0.8386752136752137, 0.8418803418803419, 0.8396100427350427, 0.8372061965811965, 0.8397435897435898, 0.8392094017094017, 0.8390758547008547, 0.8364049145299145, 0.8405448717948718, 0.843215811965812, 0.8402777777777778, 0.8426816239316239, 0.8454861111111112, 0.8408119658119658, 0.8426816239316239, 0.8413461538461539, 0.84375, 0.8444177350427351, 0.8418803418803419, 0.8430822649572649, 0.8466880341880342, 0.8418803418803419, 0.8464209401709402, 0.8436164529914529, 0.8438835470085471, 0.8457532051282052, 0.8458867521367521, 0.8474893162393162, 0.8461538461538461, 0.8482905982905983, 0.8478899572649573, 0.8505608974358975, 0.8486912393162394, 0.8492254273504274, 0.8498931623931624, 0.8497596153846154, 0.8493589743589743, 0.8494925213675214, 0.8473557692307693, 0.8498931623931624, 0.8458867521367521, 0.8488247863247863, 0.8478899572649573, 0.8470886752136753, 0.8500267094017094, 0.8532318376068376, 0.8525641025641025, 0.8512286324786325, 0.8509615384615384, 0.8489583333333334, 0.8520299145299145, 0.8508279914529915, 0.8545673076923077, 0.8522970085470085, 0.8532318376068376, 0.8520299145299145, 0.8521634615384616, 0.8512286324786325, 0.8510950854700855, 0.8528311965811965, 0.8513621794871795, 0.8529647435897436, 0.8526976495726496, 0.8512286324786325, 0.8536324786324786, 0.8529647435897436, 0.8528311965811965, 0.8528311965811965, 0.8559027777777778, 0.8529647435897436, 0.8533653846153846, 0.8544337606837606, 0.8518963675213675, 0.8530982905982906, 0.8513621794871795, 0.8522970085470085, 0.8512286324786325, 0.8526976495726496, 0.8556356837606838, 0.8547008547008547, 0.8533653846153846, 0.8541666666666666, 0.8544337606837606, 0.8510950854700855, 0.8533653846153846, 0.8534989316239316, 0.8548344017094017, 0.8496260683760684, 0.8537660256410257, 0.8529647435897436, 0.8545673076923077, 0.8524305555555556, 0.8551014957264957, 0.8514957264957265, 0.8532318376068376, 0.8538995726495726, 0.8538995726495726, 0.8557692307692307, 0.8532318376068376, 0.8547008547008547, 0.8544337606837606, 0.8553685897435898, 0.8538995726495726, 0.8533653846153846, 0.8557692307692307, 0.8537660256410257, 0.8551014957264957, 0.8543002136752137, 0.8544337606837606, 0.8536324786324786, 0.8547008547008547, 0.8555021367521367, 0.8549679487179487, 0.8548344017094017, 0.8534989316239316, 0.8536324786324786, 0.8529647435897436, 0.8530982905982906, 0.8538995726495726], "accuracy_test": 0.8478, "start": "2016-01-18 01:24:10.511000", "learning_rate_per_epoch": [0.0029534397181123495, 0.0028975307941436768, 0.0028426803182810545, 0.0027888682670891285, 0.002736074849963188, 0.0026842807419598103, 0.002633467083796859, 0.0025836152490228415, 0.002534707309678197, 0.00248672510497272, 0.002439651172608137, 0.002393468515947461, 0.0023481599055230618, 0.0023037090431898832, 0.00226009963080287, 0.002217315835878253, 0.0021753418259322643, 0.0021341624669730663, 0.0020937626250088215, 0.0020541276317089796, 0.0020152428187429905, 0.001977094216272235, 0.0019396677380427718, 0.0019029496470466256, 0.001866926671937108, 0.0018315856577828526, 0.0017969135660678148, 0.0017628978239372373, 0.0017295260913670063, 0.0016967860283330083, 0.0016646657604724169, 0.0016331535298377275, 0.0016022378113120794, 0.0015719073126092553, 0.0015421509742736816, 0.0015129579696804285, 0.0014843175886198878, 0.0014562193537130952, 0.0014286530204117298, 0.0014016084605827928, 0.0013750758953392506, 0.0013490456622093916, 0.0013235080987215042, 0.001298454008065164, 0.001273874193429947, 0.001249759690836072, 0.0012261016527190804, 0.0012028914643451571, 0.0011801206273958087, 0.0011577808763831854, 0.0011358639458194375, 0.0011143619194626808, 0.001093266997486353, 0.0010725713800638914, 0.0010522675001993775, 0.0010323480237275362, 0.0010128056164830923, 0.0009936331771314144, 0.0009748236043378711, 0.0009563701460137963, 0.0009382659918628633, 0.0009205045644193888, 0.0009030793444253504, 0.0008859839872457087, 0.0008692122646607459, 0.0008527580066584051, 0.0008366152178496122, 0.0008207780192606151, 0.0008052406483329833, 0.0007899974007159472, 0.0007750426884740591, 0.0007603710982948542, 0.0007459772168658674, 0.0007318558054976165, 0.0007180017419159412, 0.0007044099038466811, 0.0006910754018463194, 0.0006779932882636786, 0.0006651588482782245, 0.0006525673670694232, 0.0006402142462320626, 0.0006280949455685914, 0.0006162050412967801, 0.0006045402260497212, 0.0005930962506681681, 0.0005818689242005348, 0.0005708541139028966, 0.0005600478034466505, 0.0005494460347108543, 0.0005390449659898877, 0.0005288408137857914, 0.000518829794600606, 0.0005090082995593548, 0.0004993727197870612, 0.0004899195628240705, 0.0004806453362107277, 0.0004715466930065304, 0.00046262028627097607, 0.0004538628563750535, 0.00044527120189741254, 0.0004368421796243638, 0.0004285727336537093, 0.0004204598080832511, 0.00041250046342611313, 0.00040469178929924965, 0.0003970309335272759, 0.0003895151021424681, 0.00038214155938476324, 0.0003749075985979289, 0.00036781057133339345, 0.0003608478873502463, 0.0003540170146152377, 0.0003473154501989484, 0.00034074074937961996, 0.0003342904965393245, 0.00032796236337162554, 0.0003217540215700865, 0.0003156632010359317, 0.00030968766077421606, 0.00030382524710148573, 0.00029807380633428693, 0.00029243124299682677, 0.0002868954907171428, 0.00028146454133093357, 0.0002761363866738975, 0.0002709091058932245, 0.00026578077813610435, 0.0002607495407573879, 0.00025581353111192584, 0.0002509709738660604, 0.00024622006458230317, 0.0002415591006865725, 0.0002369863650528714, 0.00023250019876286387, 0.0002280989574501291, 0.0002237810258520767, 0.00021954483236186206, 0.00021538883447647095, 0.00021131150424480438, 0.00020731135737150908, 0.0002033869386650622, 0.00019953680748585612, 0.00019575956685002893, 0.00019205381977371871, 0.00018841822748072445, 0.00018485145119484514, 0.00018135219579562545, 0.00017791918071452528, 0.000174551154486835, 0.00017124689475167543, 0.00016800517914816737, 0.00016482482897117734, 0.00016170468006748706, 0.00015864359738770872, 0.00015564046043436974, 0.000152694177813828, 0.00014980367268435657, 0.0001469678827561438, 0.0001441857748432085, 0.00014145633031148463, 0.00013877854507882148], "accuracy_train_last": 0.999976468373494, "error_valid": [0.7075320512820513, 0.5964209401709402, 0.5347222222222222, 0.4758279914529915, 0.45873397435897434, 0.40745192307692313, 0.3787393162393162, 0.3577724358974359, 0.3207799145299145, 0.29513888888888884, 0.27884615384615385, 0.2601495726495726, 0.25053418803418803, 0.25560897435897434, 0.23637820512820518, 0.2254273504273504, 0.22529380341880345, 0.2146100427350427, 0.21020299145299148, 0.20753205128205132, 0.20205662393162394, 0.2008547008547008, 0.19724893162393164, 0.1941773504273504, 0.18856837606837606, 0.18603098290598286, 0.19270833333333337, 0.1868322649572649, 0.18055555555555558, 0.18790064102564108, 0.1760149572649573, 0.17628205128205132, 0.17681623931623935, 0.18149038461538458, 0.1680021367521367, 0.17160790598290598, 0.171875, 0.1744123931623932, 0.16853632478632474, 0.16693376068376065, 0.16653311965811968, 0.16586538461538458, 0.1613247863247863, 0.1581196581196581, 0.1603899572649573, 0.16279380341880345, 0.16025641025641024, 0.16079059829059827, 0.16092414529914534, 0.1635950854700855, 0.1594551282051282, 0.15678418803418803, 0.1597222222222222, 0.15731837606837606, 0.15451388888888884, 0.15918803418803418, 0.15731837606837606, 0.15865384615384615, 0.15625, 0.1555822649572649, 0.1581196581196581, 0.1569177350427351, 0.15331196581196582, 0.1581196581196581, 0.15357905982905984, 0.15638354700854706, 0.15611645299145294, 0.15424679487179482, 0.15411324786324787, 0.15251068376068377, 0.15384615384615385, 0.15170940170940173, 0.1521100427350427, 0.14943910256410253, 0.15130876068376065, 0.1507745726495726, 0.15010683760683763, 0.15024038461538458, 0.15064102564102566, 0.1505074786324786, 0.15264423076923073, 0.15010683760683763, 0.15411324786324787, 0.1511752136752137, 0.1521100427350427, 0.15291132478632474, 0.14997329059829057, 0.14676816239316237, 0.14743589743589747, 0.14877136752136755, 0.14903846153846156, 0.15104166666666663, 0.1479700854700855, 0.14917200854700852, 0.1454326923076923, 0.14770299145299148, 0.14676816239316237, 0.1479700854700855, 0.14783653846153844, 0.14877136752136755, 0.1489049145299145, 0.14716880341880345, 0.14863782051282048, 0.1470352564102564, 0.1473023504273504, 0.14877136752136755, 0.1463675213675214, 0.1470352564102564, 0.14716880341880345, 0.14716880341880345, 0.1440972222222222, 0.1470352564102564, 0.14663461538461542, 0.14556623931623935, 0.14810363247863245, 0.14690170940170943, 0.14863782051282048, 0.14770299145299148, 0.14877136752136755, 0.1473023504273504, 0.14436431623931623, 0.14529914529914534, 0.14663461538461542, 0.14583333333333337, 0.14556623931623935, 0.1489049145299145, 0.14663461538461542, 0.14650106837606836, 0.14516559829059827, 0.15037393162393164, 0.14623397435897434, 0.1470352564102564, 0.1454326923076923, 0.14756944444444442, 0.14489850427350426, 0.14850427350427353, 0.14676816239316237, 0.1461004273504274, 0.1461004273504274, 0.14423076923076927, 0.14676816239316237, 0.14529914529914534, 0.14556623931623935, 0.14463141025641024, 0.1461004273504274, 0.14663461538461542, 0.14423076923076927, 0.14623397435897434, 0.14489850427350426, 0.1456997863247863, 0.14556623931623935, 0.1463675213675214, 0.14529914529914534, 0.1444978632478633, 0.14503205128205132, 0.14516559829059827, 0.14650106837606836, 0.1463675213675214, 0.1470352564102564, 0.14690170940170943, 0.1461004273504274], "accuracy_train_std": [0.1144124233084162, 0.12289871746535982, 0.12543683105366343, 0.12397609453364024, 0.12409528570505482, 0.12106026333154492, 0.11881867398907277, 0.11763599189271644, 0.11460709016704901, 0.11176899108720816, 0.10864706642644054, 0.10506609300692164, 0.10417159230320272, 0.10323864683319048, 0.10001225578435642, 0.09823454696302937, 0.09845542542609796, 0.09542939753236869, 0.09247869339761398, 0.09112475233636286, 0.08985250151447757, 0.09007949657068225, 0.08822349168497197, 0.08670080492686544, 0.082324187127973, 0.08274248588021732, 0.0825515182569506, 0.08034756969583312, 0.07705577968548119, 0.07935093001624256, 0.07516752948079061, 0.07097372228159739, 0.07286747396876059, 0.07304064865530217, 0.06531862412152364, 0.06821982082768324, 0.06593314416724395, 0.06571255471290856, 0.06475910027506938, 0.06102395955554302, 0.061120178037951244, 0.058813698105608, 0.05635434469736195, 0.05578772482007989, 0.0523813794598838, 0.0526321585469906, 0.05088214058267508, 0.047555012713594656, 0.049204827964622326, 0.04697449626714712, 0.045978359461584774, 0.045707645000572346, 0.042219882672522205, 0.03771172766689578, 0.037137044562207094, 0.038971527968403245, 0.03593259333940256, 0.0341391838383043, 0.03365696383116836, 0.03210072675417394, 0.03183243765435273, 0.033735842865035365, 0.029190411219870496, 0.028549274906841893, 0.026721226550366625, 0.02782500158826184, 0.025859790776591923, 0.02371062147134006, 0.02527077574581015, 0.021224013934388497, 0.025059745985203753, 0.020224375468447582, 0.02122921827877981, 0.018521752042391208, 0.019347505259629954, 0.01830125259507791, 0.015948107410222952, 0.014949362848504968, 0.013929748997981028, 0.013951198643430392, 0.016183122304406206, 0.01262416385850544, 0.014950344399407647, 0.010427989242860501, 0.011998245393756274, 0.011264720120458902, 0.010226028333050571, 0.009294128610535923, 0.01100895401696401, 0.009211151539920059, 0.010874539771152386, 0.008659126982008226, 0.007802854584360959, 0.008985877496299005, 0.006383276292615201, 0.009211151539920059, 0.005267248547191479, 0.006153419095005663, 0.005403058222351718, 0.005535436891110064, 0.005914261322441645, 0.005664625099855236, 0.0041915369123140655, 0.0034249633657116437, 0.004361864842904622, 0.004525664664262185, 0.004013849896536151, 0.004361864842904622, 0.004013849896536151, 0.003632036230065858, 0.0034249633657116437, 0.003204364751636525, 0.003204364751636525, 0.002709204652104018, 0.0029672252007055936, 0.0029672252007055936, 0.003204364751636525, 0.001212506873869376, 0.002709204652104018, 0.0017144207078395126, 0.0020993323540748256, 0.001212506873869376, 0.0017144207078395124, 0.001212506873869376, 0.0017144207078395126, 0.0020993323540748256, 0.0017144207078395124, 0.0020993323540748256, 0.001212506873869376, 0.002709204652104018, 0.0017144207078395124, 0.0020993323540748256, 0.0017144207078395124, 0.0020993323540748256, 0.0, 0.0017144207078395124, 0.0017144207078395124, 0.001212506873869376, 0.001212506873869376, 0.0, 0.0, 0.001212506873869376, 0.0020993323540748256, 0.001212506873869376, 0.0017144207078395124, 0.0020993323540748256, 0.0017144207078395124, 0.0, 0.001212506873869376, 0.0, 0.001212506873869376, 0.001212506873869376, 0.0, 0.0, 0.0, 0.001212506873869376, 0.001212506873869376, 0.001212506873869376, 0.001212506873869376, 0.001212506873869376, 0.001212506873869376], "accuracy_test_std": 0.09279094783436583, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5088110700536497, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.003010427368655228, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "nesterov_momentum", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 1.1071539302413285e-07, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.018930072275927235}, "accuracy_valid_max": 0.8559027777777778, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8538995726495726, "loss_train": [2.1950323581695557, 1.75370192527771, 1.5725308656692505, 1.4470359086990356, 1.3422030210494995, 1.2491101026535034, 1.161058783531189, 1.0834485292434692, 1.016626000404358, 0.9541309475898743, 0.9055513143539429, 0.8620615601539612, 0.8211650252342224, 0.7861732244491577, 0.7574158310890198, 0.7320958375930786, 0.7051875591278076, 0.6824491024017334, 0.6592254042625427, 0.6368248462677002, 0.6205736994743347, 0.5956278443336487, 0.5805644989013672, 0.5654541254043579, 0.5477868318557739, 0.5338940024375916, 0.5159971714019775, 0.5031935572624207, 0.4902076721191406, 0.47694143652915955, 0.4647308588027954, 0.4518336355686188, 0.44205784797668457, 0.42894303798675537, 0.42061102390289307, 0.40746164321899414, 0.39677518606185913, 0.3897494971752167, 0.37982165813446045, 0.36886754631996155, 0.35849717259407043, 0.3494703471660614, 0.3418341875076294, 0.3285806477069855, 0.32341521978378296, 0.3181280195713043, 0.30995678901672363, 0.29877516627311707, 0.29176685214042664, 0.28369981050491333, 0.2845538258552551, 0.2724810242652893, 0.2678820490837097, 0.2613164186477661, 0.2564057409763336, 0.24917815625667572, 0.24337084591388702, 0.23750734329223633, 0.23671825230121613, 0.2293727844953537, 0.22571900486946106, 0.21779300272464752, 0.21682500839233398, 0.20613357424736023, 0.20478516817092896, 0.2008817046880722, 0.2001844197511673, 0.19037595391273499, 0.1888284683227539, 0.1841641217470169, 0.18416184186935425, 0.1750348061323166, 0.17658883333206177, 0.17385238409042358, 0.16690124571323395, 0.16533233225345612, 0.1646420657634735, 0.16024038195610046, 0.1586218923330307, 0.1553070843219757, 0.15197868645191193, 0.1495063602924347, 0.1516474485397339, 0.14338858425617218, 0.14134995639324188, 0.14225903153419495, 0.13698577880859375, 0.13633401691913605, 0.1325027346611023, 0.13348881900310516, 0.13040544092655182, 0.1304393708705902, 0.12616191804409027, 0.1247924268245697, 0.12338600307703018, 0.12318611890077591, 0.1204003244638443, 0.12202970683574677, 0.11924045532941818, 0.11252809315919876, 0.1148761585354805, 0.11342249810695648, 0.1133565828204155, 0.1118999794125557, 0.1115708202123642, 0.11150756478309631, 0.10332480072975159, 0.10754013061523438, 0.10527889430522919, 0.10361652821302414, 0.10538937151432037, 0.10317318886518478, 0.10170014202594757, 0.10068531334400177, 0.09841694682836533, 0.09944749623537064, 0.10116640478372574, 0.09353310614824295, 0.09742164611816406, 0.09905563294887543, 0.09658410400152206, 0.09286346286535263, 0.09521553665399551, 0.09296462684869766, 0.09219935536384583, 0.0888909325003624, 0.09034787863492966, 0.089870385825634, 0.08911619335412979, 0.0877162516117096, 0.08766268193721771, 0.08715793490409851, 0.08709805458784103, 0.08931664377450943, 0.08533849567174911, 0.08403220027685165, 0.08608093112707138, 0.08436094969511032, 0.08243288099765778, 0.08506391942501068, 0.08146869391202927, 0.08184436708688736, 0.08160757273435593, 0.07958249002695084, 0.08222616463899612, 0.08208020031452179, 0.08309508860111237, 0.08002885431051254, 0.08196278661489487, 0.07999580353498459, 0.07839871942996979, 0.07929472625255585, 0.07982883602380753, 0.0774138942360878, 0.07633519917726517, 0.07710036635398865, 0.07730955630540848, 0.08003700524568558, 0.07709699869155884, 0.07666760683059692, 0.07746532559394836], "accuracy_train_first": 0.2890625, "model": "vgg", "loss_std": [0.1626686304807663, 0.2193506956100464, 0.23726095259189606, 0.24848560988903046, 0.254989355802536, 0.25899261236190796, 0.2560274302959442, 0.25699180364608765, 0.256320595741272, 0.2606603801250458, 0.26079320907592773, 0.2570021450519562, 0.25187790393829346, 0.25221505761146545, 0.25133243203163147, 0.2461267113685608, 0.24707163870334625, 0.2481229156255722, 0.24133847653865814, 0.23841027915477753, 0.23570725321769714, 0.2306670993566513, 0.2286873459815979, 0.22275123000144958, 0.2240612655878067, 0.2226046770811081, 0.2165444940328598, 0.2126910239458084, 0.21368321776390076, 0.2084960639476776, 0.20911110937595367, 0.20651847124099731, 0.20337741076946259, 0.20013070106506348, 0.19742350280284882, 0.19246867299079895, 0.19156067073345184, 0.19014886021614075, 0.18692541122436523, 0.19017280638217926, 0.18334953486919403, 0.18210263550281525, 0.1788671314716339, 0.1785285770893097, 0.17091679573059082, 0.17373952269554138, 0.17160671949386597, 0.17047977447509766, 0.16326910257339478, 0.16426919400691986, 0.16125167906284332, 0.16073431074619293, 0.15464916825294495, 0.153495192527771, 0.1539851278066635, 0.15021173655986786, 0.14691519737243652, 0.14907130599021912, 0.14848637580871582, 0.14397096633911133, 0.14911572635173798, 0.14186981320381165, 0.1396723836660385, 0.1336427479982376, 0.13410000503063202, 0.13471649587154388, 0.1376420557498932, 0.13241635262966156, 0.12889669835567474, 0.12783847749233246, 0.1307620406150818, 0.12365167587995529, 0.12551933526992798, 0.12899187207221985, 0.11909043788909912, 0.11901585757732391, 0.1206359714269638, 0.11668670177459717, 0.11800979822874069, 0.11881988495588303, 0.11434835940599442, 0.11346554756164551, 0.11478939652442932, 0.1132640689611435, 0.10731218010187149, 0.11280669271945953, 0.1097903773188591, 0.10811374336481094, 0.10501295328140259, 0.10665882378816605, 0.10663451999425888, 0.10491949319839478, 0.10569389164447784, 0.10380443185567856, 0.09927220642566681, 0.1021357923746109, 0.10282651335000992, 0.10527059435844421, 0.09899254143238068, 0.09696508944034576, 0.09745417535305023, 0.0976715236902237, 0.09678875654935837, 0.09957044571638107, 0.09599229693412781, 0.09905486553907394, 0.09193328768014908, 0.09756381064653397, 0.09133093059062958, 0.09469424188137054, 0.09249560534954071, 0.09155160188674927, 0.09072455763816833, 0.0928126722574234, 0.09521760046482086, 0.09002351015806198, 0.09422053396701813, 0.08564484864473343, 0.08870597183704376, 0.09456651657819748, 0.08823594450950623, 0.08493448048830032, 0.08907013386487961, 0.08639875054359436, 0.08441920578479767, 0.08549583703279495, 0.08489778637886047, 0.08425963670015335, 0.08418316394090652, 0.08170060813426971, 0.08363482356071472, 0.08044809848070145, 0.08249649405479431, 0.08565887063741684, 0.08179518580436707, 0.08171821385622025, 0.0823674350976944, 0.07800076901912689, 0.07666607201099396, 0.0836045891046524, 0.07643184810876846, 0.0781065821647644, 0.07915724813938141, 0.07826238125562668, 0.07871627807617188, 0.0805894285440445, 0.07950053364038467, 0.07976825535297394, 0.07784735411405563, 0.07822064310312271, 0.0779571384191513, 0.0796947181224823, 0.07746380567550659, 0.07626339793205261, 0.07511190325021744, 0.07579594850540161, 0.07657504081726074, 0.07674848288297653, 0.07672253996133804, 0.07660635560750961, 0.07542163133621216]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:00 2016", "state": "available"}], "summary": "b83108fe38d5566b012f8dc8bec46529"}