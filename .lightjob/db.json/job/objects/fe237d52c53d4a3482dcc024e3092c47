{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 32, "f3": 32, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.4419115781784058, 1.0043467283248901, 0.8214810490608215, 0.7048348784446716, 0.6208679676055908, 0.5520755052566528, 0.49286559224128723, 0.4411747455596924, 0.39104020595550537, 0.3477614223957062, 0.3113831877708435, 0.27826714515686035, 0.24806703627109528, 0.22302822768688202, 0.19851654767990112, 0.17856431007385254, 0.16344615817070007, 0.14429320394992828, 0.1304979920387268, 0.11773859709501266, 0.1059044897556305, 0.09574434161186218, 0.0906355232000351, 0.07890286296606064, 0.07073430716991425, 0.06216190755367279, 0.05666353553533554, 0.054725728929042816, 0.05139567330479622, 0.04620501399040222, 0.041336748749017715, 0.03873946890234947, 0.035492006689310074, 0.031151480972766876, 0.028406362980604172, 0.026581091806292534, 0.021773459389805794, 0.02090335823595524, 0.01815745420753956, 0.016285516321659088, 0.018117263913154602, 0.01586765982210636, 0.014029106125235558, 0.012090337462723255, 0.009935964830219746, 0.00810237042605877, 0.0069517274387180805, 0.00582343153655529, 0.005029056221246719, 0.004724570084363222, 0.004636949859559536, 0.004619330167770386, 0.004609980620443821, 0.004602984990924597, 0.004597196355462074, 0.004592177923768759, 0.0045877681113779545, 0.00458377692848444, 0.004580123815685511, 0.004576761741191149, 0.004573606885969639, 0.004570644348859787, 0.004567856900393963, 0.0045652189292013645, 0.004562722519040108, 0.0045603434555232525, 0.004558076150715351, 0.004555912688374519, 0.004553836304694414, 0.004551841411739588, 0.0045499419793486595, 0.004548114258795977, 0.004546356853097677, 0.004544663708657026, 0.00454303901642561, 0.00454147020354867, 0.004539958201348782, 0.004538498818874359, 0.004537095315754414, 0.004535738378763199, 0.004534425679594278, 0.004533158149570227, 0.00453193299472332, 0.004530748818069696, 0.004529605619609356, 0.004528497811406851, 0.004527428653091192, 0.004526391159743071, 0.004525387659668922, 0.0045244162902235985, 0.004523476120084524, 0.004522566217929125, 0.004521684255450964, 0.00452082883566618, 0.004520001355558634, 0.004519200883805752, 0.004518426954746246, 0.004517675843089819, 0.004516948945820332, 0.004516245797276497], "moving_avg_accuracy_train": [0.050166112956810616, 0.10397055748777222, 0.15746208421292446, 0.2133409313068129, 0.26968030869684956, 0.3233565315012602, 0.37141634110261074, 0.4160093112628369, 0.45869563731171015, 0.49769243700454247, 0.5345608514863049, 0.56731484948067, 0.5990810336159824, 0.628496099262782, 0.6553066607270077, 0.6795154373972472, 0.7023193182837684, 0.7239007537899707, 0.7442099995396189, 0.7615604421440272, 0.7778827578237275, 0.7928215968092581, 0.8074477274914738, 0.8208554217792865, 0.8332338444811567, 0.8442559504700105, 0.855294206388541, 0.8650450941545411, 0.8743624365236478, 0.8828433757570342, 0.8909482262754153, 0.8982494950907493, 0.9053183270162443, 0.9115151541348948, 0.9173341500666896, 0.922785160095781, 0.9281816755207729, 0.9328408296568186, 0.9374712324042689, 0.9415155061853444, 0.9449135010633032, 0.9482273546760575, 0.9515075140727928, 0.9548549688762924, 0.957912000222098, 0.9607049648189819, 0.9636325094442728, 0.9664928390415584, 0.9691926216171921, 0.9716875301019291, 0.973939923184621, 0.9759856781495199, 0.977840808510786, 0.9795174012823542, 0.9810309850743846, 0.9823932104872118, 0.9836285139539945, 0.9847426122229085, 0.9857546012601691, 0.9866677165425132, 0.9874918454454324, 0.9882405369044883, 0.9889166843664482, 0.9895252170822121, 0.990075221675209, 0.9905748761065254, 0.9910292153923291, 0.991445096195981, 0.9918193889192677, 0.9921585775190352, 0.992463847258826, 0.9927409151734473, 0.9929902762966065, 0.9932147013074498, 0.9934166838172087, 0.9936007932248012, 0.9937664916916344, 0.9939179454605939, 0.9940542538526573, 0.994179256554324, 0.9942940841346336, 0.9943974289569122, 0.9944927644457724, 0.9945785663857466, 0.9946557881317234, 0.9947252877031025, 0.9947901624661534, 0.994848549752899, 0.9949010983109702, 0.9949507171620438, 0.99499537412801, 0.9950355653973796, 0.9950717375398122, 0.9951089427656206, 0.9951447526176577, 0.9951769814844911, 0.9952083126134506, 0.9952365106295141, 0.9952642139927809, 0.9952891470197209], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05017692841679215, 0.10214283403143823, 0.1541533776708396, 0.2073483208713761, 0.2605778030726421, 0.3100327180778779, 0.3540050322075901, 0.39357805590701184, 0.43082040091872026, 0.46426526924175787, 0.4948367244806092, 0.5205942512042351, 0.5457159137155887, 0.5687089754878552, 0.589402731082895, 0.6078663606948614, 0.6242200731287639, 0.6398733552548935, 0.6543061945693891, 0.6660587217615466, 0.6770733903421691, 0.6871646970628468, 0.6960158219537007, 0.7046420435516288, 0.7121107622951105, 0.7188396686521958, 0.7252343927225937, 0.73094596380425, 0.7367333504339907, 0.741378415945938, 0.7453880764691906, 0.7496416845790487, 0.7528281067649691, 0.7560376836072975, 0.7591317632793239, 0.7617821576403975, 0.7642478142408156, 0.7666398626360111, 0.7690256692940969, 0.7712359894599432, 0.7727634694390242, 0.7744932348351067, 0.775717345321852, 0.7776308858743807, 0.7794374920817468, 0.7808548886284667, 0.7824591058556051, 0.7838927533460988, 0.7855492470250431, 0.787189664237072, 0.7886039750629883, 0.789852440743813, 0.7909028176690552, 0.7918359498705232, 0.7927490110393444, 0.7935707660912834, 0.794321523160619, 0.7950216185855209, 0.7956394974366827, 0.7961711743402283, 0.7966496835534194, 0.7970803418452913, 0.7974801413392261, 0.7978521679150172, 0.7981869918332294, 0.7984761263283703, 0.7987485544052471, 0.7990181537369363, 0.7992852071979566, 0.7995133482816248, 0.7997186752569262, 0.7999034695346974, 0.8000077197197819, 0.8000761013151982, 0.800137644751073, 0.8002174479058601, 0.8002892707451688, 0.8003539113005465, 0.8004120878003864, 0.8004522396189924, 0.8004761692244877, 0.8004977058694335, 0.8005292958811348, 0.8005821409541658, 0.8006419085511438, 0.800695699388424, 0.8007441111419762, 0.8007876817201732, 0.8008268952405505, 0.8008377733463901, 0.8008475636416457, 0.8008441678761257, 0.8008289046559076, 0.8007897241865518, 0.8007422547328816, 0.8006873251933284, 0.8006378886077306, 0.8005933956806924, 0.8005655590776082, 0.8005405061348323], "moving_var_accuracy_train": [0.022649750002759345, 0.04643903926405133, 0.06754722622013531, 0.08889451357100125, 0.10857219121617392, 0.12364520414549514, 0.13206839142120821, 0.1367583491684848, 0.1394816161355927, 0.13922020799857943, 0.13753170707631276, 0.13343395583021533, 0.12917237433786102, 0.12404235168712474, 0.11810737237265548, 0.11157121894621569, 0.10509424990297392, 0.09877665013925187, 0.09261117429160311, 0.08605939758956263, 0.07985121973293648, 0.07387461795176328, 0.06841246944518618, 0.0631891188957066, 0.05824923514341151, 0.05351769301293608, 0.04926251155514924, 0.04519197870966049, 0.04145409665810279, 0.03795602396481605, 0.03475161898566217, 0.03175623382391988, 0.029030325904645972, 0.026472899311227364, 0.024130355802992824, 0.021984741815728844, 0.020048369042745536, 0.018238901593841863, 0.016607977100890035, 0.015094384744547686, 0.01368886359280862, 0.012418811865428637, 0.011273765689897694, 0.010247238203861178, 0.009306623349318203, 0.008446166875541406, 0.007678684845784887, 0.006984449729852367, 0.006351604190468391, 0.005772464886546469, 0.005240877869282447, 0.00475445610274188, 0.004309984070383317, 0.0039042843332400554, 0.0035344743229755228, 0.0031977278133561438, 0.002891688803915938, 0.002613690858099518, 0.0023615388685933885, 0.002132888997403703, 0.0019257127937009743, 0.0017381863644386458, 0.0015684823065076135, 0.0014149668844522462, 0.0012761927414778815, 0.0011508203582866997, 0.0010375961401376515, 0.0009353931377095019, 0.0008431146793229002, 0.0007598386515465208, 0.0006846934929181564, 0.000616915043290154, 0.0005557831676888274, 0.0005006581501893725, 0.0004509595075786718, 0.00040616862328648134, 0.0003657988647950313, 0.0003294254225127165, 0.0002966501000611674, 0.0002671257211338665, 0.00024053181737927762, 0.00021657475701197606, 0.0001949990810097045, 0.00017556543066486418, 0.00015806255638084316, 0.00014229977245655583, 0.0001281076738248283, 0.00011532758811962717, 0.00010381968146626264, 9.345987159307309e-05, 8.413183263524951e-05, 7.573318741492638e-05, 6.817164448842722e-05, 6.136693809903157e-05, 5.5241785398654656e-05, 4.972695515750545e-05, 4.4763094398531816e-05, 4.0293941111667934e-05, 3.627145428752775e-05, 3.264990376126653e-05], "duration": 43777.814637, "accuracy_train": [0.5016611295681063, 0.5882105582664268, 0.6388858247392949, 0.7162505551518088, 0.7767347052071798, 0.806442536740956, 0.8039546275147655, 0.8173460427048725, 0.8428725717515688, 0.8486636342400333, 0.8663765818221669, 0.8621008314299556, 0.8849766908337948, 0.8932316900839794, 0.8966017139050388, 0.8973944274294019, 0.9075542462624585, 0.9181336733457919, 0.9269932112864526, 0.9177144255837025, 0.92478359894103, 0.927271147679033, 0.9390829036314139, 0.9415246703696014, 0.9446396487979882, 0.9434549043696937, 0.9546385096553157, 0.9528030840485419, 0.9582185178456073, 0.9591718288575121, 0.9638918809408453, 0.963960914428756, 0.9689378143456996, 0.9672865982027501, 0.9697051134528424, 0.9718442503576044, 0.9767503143456996, 0.9747732168812293, 0.9791448571313216, 0.9779139702150241, 0.9754954549649317, 0.9780520371908453, 0.981028948643411, 0.9849820621077889, 0.9854252823343485, 0.9858416461909376, 0.98998041107189, 0.9922358054171282, 0.9934906647978959, 0.9941417064645626, 0.9942114609288483, 0.9943974728336102, 0.9945369817621816, 0.9946067362264673, 0.9946532392026578, 0.9946532392026578, 0.9947462451550388, 0.9947694966431341, 0.994862502595515, 0.9948857540836102, 0.9949090055717055, 0.9949787600359912, 0.9950020115240864, 0.9950020115240864, 0.9950252630121816, 0.9950717659883721, 0.9951182689645626, 0.9951880234288483, 0.9951880234288483, 0.9952112749169435, 0.9952112749169435, 0.9952345264050388, 0.9952345264050388, 0.9952345264050388, 0.9952345264050388, 0.9952577778931341, 0.9952577778931341, 0.9952810293812293, 0.9952810293812293, 0.9953042808693245, 0.9953275323574198, 0.9953275323574198, 0.995350783845515, 0.995350783845515, 0.995350783845515, 0.995350783845515, 0.9953740353336102, 0.9953740353336102, 0.9953740353336102, 0.9953972868217055, 0.9953972868217055, 0.9953972868217055, 0.9953972868217055, 0.9954437897978959, 0.9954670412859912, 0.9954670412859912, 0.9954902927740864, 0.9954902927740864, 0.9955135442621816, 0.9955135442621816], "end": "2016-02-04 00:08:13.728000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0], "moving_var_accuracy_valid": [0.02265951730809496, 0.044697663694438214, 0.06457376717198904, 0.0835837082937649, 0.10072573744312245, 0.11266526126238607, 0.11880081482544626, 0.12101495118533656, 0.12139638642454303, 0.11932378073639664, 0.1158029275415468, 0.1101936864336568, 0.10485419913630026, 0.09912690722963957, 0.09306830019232078, 0.08682962073911969, 0.0805536538585447, 0.07470351564457212, 0.06910792573621735, 0.06344023022121723, 0.05818811351456407, 0.053285812405084815, 0.04866231287107777, 0.04446578687547905, 0.04052124402495426, 0.036876623249320645, 0.03355699338781732, 0.0304948924470229, 0.02774684779833952, 0.02516635272099811, 0.02279441384650387, 0.020677811099423734, 0.018701409566603697, 0.016923981061504625, 0.015317742916505778, 0.013849189937278098, 0.012518986105790957, 0.01131858455493648, 0.010237954760130724, 0.009258128921237575, 0.008353314784892258, 0.007544912101332393, 0.006803906909552993, 0.0061564709556132385, 0.00557019829394836, 0.005031259581289405, 0.00455129523936709, 0.004114663821573372, 0.003727893181191478, 0.003379322580738014, 0.0030593927986749484, 0.0027674815178132263, 0.0025006629911976347, 0.0022584333134266195, 0.0020400931083660394, 0.0018421613298179218, 0.001663017922430545, 0.0015011273326232084, 0.0013544505678333043, 0.0012215496340178484, 0.0011014554102200418, 0.00099297906827726, 0.0008951197181676883, 0.0008068533803087742, 0.0007271770057837592, 0.0006552116940119068, 0.0005903584781243534, 0.0005319767845087432, 0.00047942096401725484, 0.0004319473028020451, 0.0003891320050229183, 0.00035052614484649947, 0.0003155713432716609, 0.00028405629332782, 0.00025568475234553147, 0.00023017359400260437, 0.00020720266128456102, 0.00018652000076870075, 0.00016789846123803332, 0.00015112312463106626, 0.0001360159658021321, 0.0001224185436655986, 0.0001101856706585923, 9.919223700842604e-05, 8.930516279842016e-05, 8.04006876061559e-05, 7.238171212647828e-05, 6.516062647139022e-05, 5.8658403125874594e-05, 5.279362781196704e-05, 4.751512767970106e-05, 4.276371869274216e-05, 3.848944351649075e-05, 3.4654315147452165e-05, 3.120916377399272e-05, 2.8115402685433217e-05, 2.5325858200850112e-05, 2.2811088965772878e-05, 2.0536953957437053e-05, 1.8488907411168933e-05], "accuracy_test": 0.7835897640306122, "start": "2016-02-03 11:58:35.913000", "learning_rate_per_epoch": [0.0024267034605145454, 0.0023543545976281166, 0.002284162910655141, 0.0022160636726766825, 0.002149994717910886, 0.002085895510390401, 0.002023707376793027, 0.0019633732736110687, 0.0019048380199819803, 0.0018480479484423995, 0.0017929510213434696, 0.0017394967144355178, 0.0016876360168680549, 0.0016373215476050973, 0.0015885070897638798, 0.0015411479398608208, 0.0014952007913962007, 0.001450623502023518, 0.0014073752099648118, 0.0013654163340106606, 0.00132470834068954, 0.001285213977098465, 0.0012468971544876695, 0.0012097225990146399, 0.0011736564338207245, 0.0011386654805392027, 0.001104717724956572, 0.0010717820841819048, 0.0010398284066468477, 0.0010088273556903005, 0.0009787505259737372, 0.0009495704434812069, 0.0009212603326886892, 0.0008937942329794168, 0.0008671469986438751, 0.0008412942406721413, 0.0008162122103385627, 0.0007918779738247395, 0.0007682692375965416, 0.0007453643484041095, 0.0007231423514895141, 0.0007015828741714358, 0.000680666184052825, 0.0006603730726055801, 0.0006406849715858698, 0.0006215838366188109, 0.0006030522054061294, 0.0005850730231031775, 0.0005676298751495779, 0.0005507067544385791, 0.0005342881777323782, 0.0005183591274544597, 0.0005029049934819341, 0.00048791157314553857, 0.00047336515854112804, 0.00045925244921818376, 0.000445560464868322, 0.0004322766908444464, 0.00041938896174542606, 0.0004068854614160955, 0.00039475475205108523, 0.00038298568688333035, 0.00037156749749556184, 0.00036048973561264575, 0.00034974224399775267, 0.00033931515645235777, 0.0003291989560239017, 0.00031938435859046876, 0.00030986237106844783, 0.00030062426230870187, 0.000291661563096568, 0.00028296609525568783, 0.0002745298552326858, 0.00026634513051249087, 0.0002584044123068452, 0.00025070045376196504, 0.0002432261680951342, 0.00023597471590619534, 0.00022893946152180433, 0.00022211394389159977, 0.0002154919202439487, 0.00020906732243020087, 0.00020283427147660404, 0.00019678704848047346, 0.00019092012371402234, 0.0001852281129686162, 0.00017970579210668802, 0.0001743481116136536, 0.00016915016749408096, 0.0001641071867197752, 0.00015921455633360893, 0.0001544677943456918, 0.00014986254973337054, 0.00014539460244122893, 0.00014105986338108778, 0.00013685435988008976, 0.00013277423568069935, 0.00012881575094070286, 0.00012497528223320842, 0.00012124931527068838], "accuracy_train_first": 0.5016611295681063, "accuracy_train_last": 0.9955135442621816, "batch_size_eval": 1024, "accuracy_train_std": [0.020012677726896095, 0.021600523055327184, 0.023000610109201215, 0.02192525545916286, 0.02498007019566685, 0.027593489585825373, 0.02804445230396112, 0.02863267123176939, 0.027023235782181236, 0.02736477021806885, 0.028389110470883636, 0.028199327457065146, 0.025627307791560194, 0.02636806472853005, 0.025081871996677843, 0.023831928660839326, 0.023168918545385712, 0.020678282617985592, 0.02041137478159729, 0.022148191883200784, 0.018013908610239843, 0.01971896307449419, 0.016062139616247778, 0.017938270572147117, 0.01599058105730462, 0.0158251057865477, 0.01474024349635599, 0.011432498684889438, 0.013402992873440888, 0.010647360915047407, 0.010256202233444124, 0.011648455885578037, 0.00920530677187457, 0.009498851717541985, 0.00878162089116224, 0.008347507088998828, 0.007205642120267214, 0.006721513260451449, 0.0063455155204778565, 0.0057371176111106256, 0.006165356015742115, 0.005762222066225586, 0.00499698234817218, 0.003935830155666008, 0.004551646727916857, 0.004333926986184049, 0.002584468723241145, 0.0028255019525425264, 0.002545660304461264, 0.0021416739442494054, 0.002081340644741639, 0.0021632714491744216, 0.002142105387117159, 0.0021333450326535496, 0.0020794218264402024, 0.0020903129418730387, 0.0020762511423778334, 0.0020638334485935076, 0.002120633227517921, 0.002191705538493447, 0.002188855487937305, 0.0021891914873319726, 0.002174933661411774, 0.002174933661411774, 0.002139206887168225, 0.002086990127053795, 0.0020765781160088747, 0.001980189152532067, 0.001991622992057402, 0.001961936485354545, 0.0019503286158694723, 0.0019315139500510276, 0.0019315139500510276, 0.0019315139500510276, 0.0019315139500510276, 0.0019358345566134654, 0.0019358345566134654, 0.0019163134746556139, 0.0019163134746556139, 0.0019082429182374072, 0.0018998535400031435, 0.0018998535400031435, 0.0018911410969370977, 0.0018911410969370977, 0.0018911410969370977, 0.0018911410969370977, 0.0018941271401030997, 0.0018941271401030997, 0.0018941271401030997, 0.001896823485153155, 0.001896823485153155, 0.001896823485153155, 0.001896823485153155, 0.0018406720653374054, 0.0018302017676171697, 0.0018302017676171697, 0.0018564377966754785, 0.0018564377966754785, 0.001820696981607388, 0.0018454711117235829], "accuracy_test_std": 0.010075333804318003, "error_valid": [0.49823071583207834, 0.430164015436747, 0.37775172957454817, 0.31389719032379515, 0.2603568571159638, 0.244873046875, 0.250244140625, 0.2502647307981928, 0.23399849397590367, 0.23473091585090367, 0.23002017836972888, 0.24758800828313254, 0.22818912368222888, 0.22435346856174698, 0.22435346856174698, 0.22596097279743976, 0.22859651496611444, 0.21924710560993976, 0.21579825160015065, 0.2281685335090362, 0.22379459243222888, 0.2220135424510542, 0.22432405402861444, 0.2177219620670181, 0.2206707690135542, 0.2206001741340362, 0.21721309064382532, 0.21764989646084332, 0.21118016989834332, 0.2168159944465362, 0.2185249788215362, 0.21207584243222888, 0.21849409356174698, 0.21507612481174698, 0.21302151967243976, 0.21436429310993976, 0.21356127635542166, 0.21183170180722888, 0.20950207078313254, 0.20887112904743976, 0.21348921074924698, 0.20993887660015065, 0.21326566029743976, 0.20514724915286142, 0.20430305205195776, 0.2063885424510542, 0.20310293910015065, 0.20320441923945776, 0.19954230986445776, 0.19804658085466864, 0.1986672275037651, 0.1989113681287651, 0.1996437900037651, 0.1997658603162651, 0.1990334384412651, 0.1990334384412651, 0.19892166321536142, 0.19867752259036142, 0.19879959290286142, 0.19904373352786142, 0.19904373352786142, 0.19904373352786142, 0.19892166321536142, 0.19879959290286142, 0.19879959290286142, 0.19892166321536142, 0.19879959290286142, 0.19855545227786142, 0.19831131165286142, 0.19843338196536142, 0.19843338196536142, 0.19843338196536142, 0.19905402861445776, 0.1993084643260542, 0.1993084643260542, 0.1990643237010542, 0.1990643237010542, 0.1990643237010542, 0.1990643237010542, 0.1991863940135542, 0.1993084643260542, 0.1993084643260542, 0.1991863940135542, 0.1989422533885542, 0.1988201830760542, 0.1988201830760542, 0.1988201830760542, 0.1988201830760542, 0.1988201830760542, 0.1990643237010542, 0.1990643237010542, 0.1991863940135542, 0.1993084643260542, 0.19956290003765065, 0.19968497035015065, 0.19980704066265065, 0.19980704066265065, 0.19980704066265065, 0.19968497035015065, 0.19968497035015065], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.02981360411247701, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 32, "valid_ratio": 0.15, "learning_rate": 0.0025012754275019053, "optimization": "rmsprop", "nb_data_augmentation": 0, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.089586205547895e-07, "rotation_range": [0, 0], "momentum": 0.8200831322893984}, "accuracy_valid_max": 0.8019534191453314, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8003150296498494, "accuracy_valid_std": [0.017977815118288228, 0.0240052089595155, 0.011272105714756628, 0.016833711677174457, 0.009170813213146063, 0.013502985165063314, 0.010805808582192739, 0.008853016112250126, 0.008196911590133497, 0.009275748382677525, 0.005515253788516532, 0.010310493326053709, 0.012120099453502911, 0.011210063043235385, 0.012237066230210964, 0.004236021373825258, 0.00737540648388131, 0.010639844099587278, 0.01286748552206859, 0.007760405677733082, 0.017794443558147875, 0.012222389353301914, 0.014160093620246908, 0.010012592906240758, 0.011762123724393605, 0.006876123241167827, 0.012360104336360441, 0.011351717357842324, 0.010297027667532259, 0.0077123805064317, 0.010357509022919003, 0.011315040670309797, 0.012025020546235169, 0.011587436725694622, 0.009183321706429836, 0.011505039946983408, 0.009005064156290335, 0.010842468231590735, 0.012042627428019665, 0.011534471673294595, 0.00911131473387382, 0.00834912014950127, 0.01026786896316718, 0.008627204718693814, 0.00854965764413138, 0.007893451074885632, 0.011719875786441809, 0.008948797777482004, 0.008171583892511205, 0.009036937237447149, 0.008349646570530016, 0.008752257509734088, 0.00942940094809534, 0.008747363872781775, 0.008141925237113264, 0.007889095684678651, 0.0075126139962084565, 0.0074511372801328345, 0.007354383756650697, 0.007603177035501202, 0.007712148279622467, 0.00778905154575264, 0.007560067777032947, 0.007256476346373509, 0.007702737771748561, 0.007731573524415911, 0.007640582102426211, 0.007944873430642931, 0.007722570285530091, 0.007774366833815584, 0.007774366833815584, 0.007774366833815584, 0.007833818599128121, 0.00800708314864925, 0.00800708314864925, 0.008341535173288667, 0.008341535173288667, 0.008341535173288667, 0.008050642052978181, 0.007819216716593405, 0.007871951050245978, 0.007871951050245978, 0.007819216716593405, 0.008113761037694102, 0.007937814516629392, 0.007937814516629392, 0.007937814516629392, 0.007937814516629392, 0.007937814516629392, 0.007609119441251757, 0.007609119441251757, 0.007665243914582114, 0.007594476868991083, 0.008029117496214206, 0.008118960916625574, 0.008220519522517294, 0.008220519522517294, 0.008220519522517294, 0.008293283385662903, 0.008293283385662903], "accuracy_valid": [0.5017692841679217, 0.569835984563253, 0.6222482704254518, 0.6861028096762049, 0.7396431428840362, 0.755126953125, 0.749755859375, 0.7497352692018072, 0.7660015060240963, 0.7652690841490963, 0.7699798216302711, 0.7524119917168675, 0.7718108763177711, 0.775646531438253, 0.775646531438253, 0.7740390272025602, 0.7714034850338856, 0.7807528943900602, 0.7842017483998494, 0.7718314664909638, 0.7762054075677711, 0.7779864575489458, 0.7756759459713856, 0.7822780379329819, 0.7793292309864458, 0.7793998258659638, 0.7827869093561747, 0.7823501035391567, 0.7888198301016567, 0.7831840055534638, 0.7814750211784638, 0.7879241575677711, 0.781505906438253, 0.784923875188253, 0.7869784803275602, 0.7856357068900602, 0.7864387236445783, 0.7881682981927711, 0.7904979292168675, 0.7911288709525602, 0.786510789250753, 0.7900611233998494, 0.7867343397025602, 0.7948527508471386, 0.7956969479480422, 0.7936114575489458, 0.7968970608998494, 0.7967955807605422, 0.8004576901355422, 0.8019534191453314, 0.8013327724962349, 0.8010886318712349, 0.8003562099962349, 0.8002341396837349, 0.8009665615587349, 0.8009665615587349, 0.8010783367846386, 0.8013224774096386, 0.8012004070971386, 0.8009562664721386, 0.8009562664721386, 0.8009562664721386, 0.8010783367846386, 0.8012004070971386, 0.8012004070971386, 0.8010783367846386, 0.8012004070971386, 0.8014445477221386, 0.8016886883471386, 0.8015666180346386, 0.8015666180346386, 0.8015666180346386, 0.8009459713855422, 0.8006915356739458, 0.8006915356739458, 0.8009356762989458, 0.8009356762989458, 0.8009356762989458, 0.8009356762989458, 0.8008136059864458, 0.8006915356739458, 0.8006915356739458, 0.8008136059864458, 0.8010577466114458, 0.8011798169239458, 0.8011798169239458, 0.8011798169239458, 0.8011798169239458, 0.8011798169239458, 0.8009356762989458, 0.8009356762989458, 0.8008136059864458, 0.8006915356739458, 0.8004370999623494, 0.8003150296498494, 0.8001929593373494, 0.8001929593373494, 0.8001929593373494, 0.8003150296498494, 0.8003150296498494], "seed": 460465616, "model": "residualv4", "loss_std": [0.29910627007484436, 0.19323882460594177, 0.18933500349521637, 0.18245171010494232, 0.17833822965621948, 0.17546862363815308, 0.1690514236688614, 0.16133758425712585, 0.15567132830619812, 0.1472407430410385, 0.14222389459609985, 0.13474200665950775, 0.1284349411725998, 0.12094254046678543, 0.10981082171201706, 0.10492408275604248, 0.09803516417741776, 0.08829230070114136, 0.08510465919971466, 0.0804980993270874, 0.07483450323343277, 0.07000561058521271, 0.06892339885234833, 0.06840144097805023, 0.06473223119974136, 0.0563555546104908, 0.056472357362508774, 0.05382851883769035, 0.049485158175230026, 0.04770704358816147, 0.045326437801122665, 0.04672380909323692, 0.04178253933787346, 0.03964501991868019, 0.037299759685993195, 0.03570961952209473, 0.029790982604026794, 0.027588559314608574, 0.02638852968811989, 0.02135535143315792, 0.02727903239428997, 0.022487878799438477, 0.020348230376839638, 0.018041783943772316, 0.014026733115315437, 0.010439261794090271, 0.00828550010919571, 0.004562979098409414, 0.001594524597749114, 0.0005791699513792992, 7.060857024043798e-05, 4.864951552008279e-05, 4.088884816155769e-05, 3.596880560507998e-05, 3.2377272873418406e-05, 2.9598793844343163e-05, 2.7449688786873594e-05, 2.569086973380763e-05, 2.4239294361905195e-05, 2.3024025722406805e-05, 2.1952957467874512e-05, 2.1017362087150104e-05, 2.0198998754494824e-05, 1.946753400261514e-05, 1.8822589481715113e-05, 1.823577804316301e-05, 1.7707308870740235e-05, 1.722832712403033e-05, 1.6782612874521874e-05, 1.6372810932807624e-05, 1.600799623702187e-05, 1.566949867992662e-05, 1.5354982679127716e-05, 1.5062551938171964e-05, 1.4797073163208552e-05, 1.4545761587214656e-05, 1.4311278391687665e-05, 1.4090625882090535e-05, 1.3886544365959708e-05, 1.3694433619093616e-05, 1.351199625787558e-05, 1.3339930774236564e-05, 1.3177260370866861e-05, 1.302396776736714e-05, 1.2880495887657162e-05, 1.2743641491397284e-05, 1.2616810636245646e-05, 1.2494338989199605e-05, 1.2378028259263374e-05, 1.2267929378140252e-05, 1.2163499377493281e-05, 1.206463002745295e-05, 1.196966059069382e-05, 1.1878911209350917e-05, 1.1792993973358534e-05, 1.1711793376889545e-05, 1.1635246664809529e-05, 1.1560593520698603e-05, 1.148840419773478e-05, 1.1420630471548066e-05]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:35 2016", "state": "available"}], "summary": "cb04c29528958f81e216aa21ad7918d6"}