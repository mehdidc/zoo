{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 32, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.08938964802286783, 0.0885678484836286, 0.09029279069020178, 0.08214693498607302, 0.08045661421532187, 0.08434516542198764, 0.08083742842408509, 0.07927789572465721, 0.07949054052866733, 0.07891091474038071, 0.08055231865818696, 0.07750192125628104, 0.07434299401087739, 0.07415852277613047, 0.07253619015734526, 0.0741218381131556, 0.07344417704688383, 0.06813201386753508, 0.07058966933763246, 0.07441061475132939, 0.07211303513899903, 0.0702893822942954, 0.07076681059153969, 0.07257687098861701, 0.07086704442036772, 0.07693513230557782, 0.07276596480965057, 0.07172369590305622, 0.06942492329176006, 0.0651234425222791, 0.06909170357307633, 0.06911867313946099, 0.06833706256728271, 0.0685705045058159, 0.07094061844191313, 0.06362908851997222, 0.06766077178629593, 0.06721342796275365, 0.06197156803504362, 0.0681524287387524, 0.07022084070579053, 0.06992674909187709, 0.07401348229971466, 0.06789089121542353, 0.06315813473964958, 0.06887828570794134, 0.0654370802738775, 0.06505206248331664, 0.06701518761963161, 0.06750771560841522, 0.06774928063576575, 0.06358085980451399, 0.0686827733743992, 0.07032844646932776, 0.06798893813138199, 0.06245375508060856, 0.0628176510337168, 0.0661946059818536, 0.06483951385943561, 0.06510139293894103, 0.06343300154812428, 0.060069438506382404, 0.06334184030119126, 0.06329747838296311, 0.06538309332378756, 0.061815966035894294, 0.06706506857525661, 0.061313204565008744, 0.06951234045293485, 0.06617035285129069, 0.06197976951006736, 0.06267951700834762, 0.06535840262089557, 0.06665975888939388, 0.06694049631527613, 0.06322798598342508, 0.06591759131163366, 0.06350507806148781, 0.06600249314170221, 0.06146137448505606, 0.06370570259711845, 0.06734213014669839, 0.06920879643658351, 0.06729444229234535, 0.06335985785644231, 0.06622490990464626, 0.059990558939035425, 0.06622531386327278, 0.06673235911825838, 0.06622814150466795, 0.06547127622435922, 0.06617520418850266, 0.06625237348043354, 0.06293791539886678, 0.06696819906539074, 0.06164145694524649, 0.06275231686390205, 0.06532414758325311, 0.06574854378288335, 0.06637702806414132, 0.06456234791041775, 0.06693463464890097, 0.06376110982076205, 0.06741836055190138, 0.06709816904412029, 0.06788773876585276, 0.06059753456493469, 0.06483194924831344, 0.06772716426720606, 0.06474537475325565, 0.06217628036235308, 0.06289369392924424, 0.06411160571027924, 0.060373292598941, 0.0633780109915026, 0.06281495379388155, 0.06630888031973872, 0.06618288474514432, 0.06614460786033409, 0.06686972234338438, 0.06662925123394366, 0.06262557354528837, 0.06551539122051936, 0.06563736973333026, 0.06859806897079823, 0.06577349476356131, 0.06514521087315471, 0.06724088564874119, 0.06968492631515262, 0.0662313729470115, 0.06494450205672317, 0.06362740673829602, 0.06720865156792365, 0.06514808539459256, 0.06965996819456163, 0.06245489734277509, 0.06498911184598981, 0.06095963820731184, 0.06665534417191837, 0.06371018173811972, 0.06576807144255148, 0.06635243842986326, 0.06746596081553453, 0.06409866886313294, 0.06750771560841522, 0.07173550627357285, 0.06200781898439896, 0.06353919103076605, 0.0636711186247738, 0.06757069533408903, 0.06681368983086282, 0.06265390321257888, 0.0680970588286577, 0.06333789827775978, 0.06634208924747487, 0.06416763524205585, 0.06496097693088322, 0.061335889032240645, 0.06487196290503332, 0.06497305585026786, 0.06297870765711669, 0.0662803638411646, 0.06759391831627197, 0.06422542090800291, 0.06465509845056573, 0.06108691843764717, 0.06482410862894185, 0.06568463151396271, 0.06365641124477073, 0.06232125368469163, 0.06500392924514403, 0.0655525391432779, 0.0663717884141419, 0.06415540468152504, 0.06190937416590122, 0.06304493878333735, 0.06500379206249586, 0.06302626525611231, 0.0658944541894874, 0.06506686554699291, 0.06137004531688015, 0.06390192960697719, 0.060977628448438785, 0.06412537432480223, 0.06326182549024525, 0.06655533268242657, 0.06710335197595964, 0.06344930671496257, 0.06781834780129171, 0.06343033047476573, 0.06512015610138527, 0.062090598821506755, 0.0601507349480087, 0.06385795678218285, 0.06586846598077126, 0.06467867892164662, 0.06629960035940258, 0.06330917041259905, 0.058801340829601, 0.06270412485159033, 0.061318876468562204, 0.05883909031781825, 0.06705443041831408, 0.06557797264618628, 0.06161830609302709, 0.06139909952836682, 0.06008680482835316, 0.06489999900665915, 0.06400232547344827, 0.06237388771366342, 0.06461743460010674, 0.06427039106105938, 0.06742312209051045, 0.0643873880607734, 0.06637111663422698, 0.06538309332378756, 0.06468157417628907, 0.06289369392924425, 0.06727217639151542, 0.06873248204551076, 0.06532783325025339, 0.06608418235124816, 0.063292406469623, 0.06671471771226554, 0.06356066017418663, 0.06540872904776229, 0.06304621177696706, 0.06255376664051837, 0.06275189054823042], "moving_avg_accuracy_train": [0.044084149096385535, 0.0898028049698795, 0.13885622176204818, 0.18627754612198794, 0.2325406047628012, 0.27792359247929216, 0.32142735521931476, 0.3650164532817206, 0.40862117090535577, 0.44964911405578406, 0.4896945565658683, 0.5256930978972333, 0.5605084831376305, 0.5923223750347107, 0.6220326262360589, 0.650306114365465, 0.6755522348566293, 0.69961269284687, 0.721573016182665, 0.7414008425764467, 0.7597706416019345, 0.7772094283453556, 0.7939726722578079, 0.8090101753633524, 0.8226215825258123, 0.8355213218635925, 0.8475923071471128, 0.858639740589028, 0.8689306987590408, 0.8781549105096428, 0.8859813622297628, 0.8932628382055816, 0.9002562079994813, 0.9068067355429066, 0.9133093262958448, 0.9189216353831278, 0.9241068438327668, 0.929168862762743, 0.9327481172997217, 0.9368236444251712, 0.9411575638681963, 0.9446957043187261, 0.9479953356940825, 0.9507861635704574, 0.953450864231484, 0.955865566964962, 0.9583070599672611, 0.9606314744524627, 0.962617555169867, 0.964256778568543, 0.9660262249586766, 0.9676634368001584, 0.9693110614936365, 0.9711892550430681, 0.9722936917375564, 0.9738147931963309, 0.9752873236658545, 0.9764125822631244, 0.9771123443681373, 0.9781421679132513, 0.9791654887725286, 0.980105302847083, 0.98094407602623, 0.9818401616464986, 0.9824936831324511, 0.9832065700902903, 0.9838058114246347, 0.9845686790773519, 0.9849422893322672, 0.9855609180797634, 0.9862424015729919, 0.9866415989156927, 0.9869632259217137, 0.9873256382693014, 0.9877318169122509, 0.9882597459137968, 0.9883866139428991, 0.9884913825184887, 0.9886798007425435, 0.9891599946140722, 0.9894721578032675, 0.9892824721434227, 0.9893588371278756, 0.989820543776534, 0.9899866445193625, 0.9900396555192335, 0.9905015220456234, 0.9907760121603381, 0.9910065811250272, 0.9908893567474643, 0.9910756469763323, 0.9914715649594219, 0.9917314114755279, 0.9921299947255654, 0.9921122136265028, 0.9922538725349369, 0.9923931313657806, 0.9924643415725759, 0.9926366762406196, 0.9926952977731841, 0.9927598229657452, 0.9922790213920621, 0.9925828398853861, 0.992166799872751, 0.9923947734999337, 0.9925693586499402, 0.9927547232367535, 0.9929427298287409, 0.993055459857915, 0.993232218088991, 0.993553668719851, 0.9938876843779864, 0.9940776998257299, 0.994349899722675, 0.9942113141178773, 0.9942560147844028, 0.9943950782156011, 0.9944543467494628, 0.9946653503275286, 0.9948058371321251, 0.9948287360996355, 0.9948705236342502, 0.9949622551563673, 0.9948542073515738, 0.9950416970079827, 0.9951257238433291, 0.994850726760201, 0.9950197391745423, 0.9952612705281725, 0.9955421841380058, 0.9956632292784221, 0.995776876230098, 0.9956861991492569, 0.9957034226078252, 0.9956530351663198, 0.9958147647822179, 0.9958403101413454, 0.9960444944886567, 0.9962047287747308, 0.996144214481595, 0.99619564393705, 0.9961925140312967, 0.9963120615739501, 0.9963067025551093, 0.9964548350104419, 0.9965363846419277, 0.996562716057253, 0.9966899534876722, 0.9968680025666159, 0.9968423468882676, 0.9969133832837782, 0.9969467249252799, 0.9970567399327519, 0.9970145636804406, 0.9971789770413122, 0.9973034174395906, 0.9972883430149087, 0.9972253596170323, 0.997307511155329, 0.9974167449795551, 0.9973926909635273, 0.9974345777406686, 0.9974228594244331, 0.9974264319157248, 0.9975025952000559, 0.9975429042041467, 0.9976003607716839, 0.9975344135499372, 0.9975950723455459, 0.9974872970387021, 0.9976091433890488, 0.9975187862790595, 0.997545710361997, 0.9975770015245925, 0.9976616394745429, 0.997768404743956, 0.9976174114081146, 0.9977050678576644, 0.9978216092646691, 0.9979170838803709, 0.9979394756429363, 0.9980090446449077, 0.9980881288852362, 0.9979428137376765, 0.9980096957675232, 0.9980887148955901, 0.9980939435566336, 0.9980092291708498, 0.9980741759826804, 0.9980573269085088, 0.9981056981333205, 0.9981186411211933, 0.9982102973403993, 0.998219839895516, 0.9982543129842777, 0.9982759261135608, 0.9981965450985901, 0.9982921767333095, 0.9983405946021472, 0.998290044178077, 0.9982916120494258, 0.9983471458746037, 0.9984041858052156, 0.9983872800258989, 0.9984426597040319, 0.9983607243059178, 0.9984328785319525, 0.9982625010703235, 0.9982785890657008, 0.9983589568157571, 0.9983818713751453, 0.9984707361954621, 0.9984660006783256, 0.9983723185321798, 0.9983844842693232, 0.9984660283122704, 0.9984994141858626, 0.9985647589118547, 0.9985882717254886, 0.9985929611192048, 0.9986089473868024, 0.9986280413529415, 0.9986122816453582, 0.998605157396485, 0.9986246303616558, 0.9986139180785023, 0.998592511210411, 0.9986250146074422, 0.9985742601346499], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 1234, "moving_var_accuracy_train": [0.01749070981397115, 0.034553398286484724, 0.05275419774871219, 0.06771781601029651, 0.08020846976249679, 0.09072416295292364, 0.09868494301049288, 0.1059165339384899, 0.11243722313597354, 0.11634312989476915, 0.1191415540977484, 0.1188904534898475, 0.11791040758579813, 0.11522848028597032, 0.1116499234953982, 0.10767944232489143, 0.10264779749109179, 0.09759316849028384, 0.09217415385036938, 0.08649502276084996, 0.0808825661308963, 0.07553131106554932, 0.07050723707720974, 0.06549165186633212, 0.0606099203241793, 0.056046557766605436, 0.05175328016137956, 0.047676364216123535, 0.04386186217502377, 0.04024145069930088, 0.03676858574811712, 0.03356890620478326, 0.030652180573972992, 0.02797314721645026, 0.025556385673307015, 0.023284229225597107, 0.02119778378303327, 0.019308621725556897, 0.017493059120365553, 0.015893242500481474, 0.014472963970081002, 0.013138333513701975, 0.011922488267250899, 0.010800337922645769, 0.009784209796897071, 0.008858265920826964, 0.008026087321466742, 0.007272104713611204, 0.006580394891794491, 0.0059465388827719436, 0.005380063459242762, 0.004866181276843478, 0.00440399515333416, 0.00399534413708288, 0.003606787747083784, 0.003266932719206378, 0.002959754561138817, 0.0026751749672215054, 0.0024120644735318632, 0.0021804028549853175, 0.001971787239716074, 0.0017825577701970408, 0.0016106338571918461, 0.0014567971964223304, 0.001314961289773512, 0.0011880390311280761, 0.0010724669396063508, 0.0009704579491457768, 0.0008746684158344009, 0.0007906458879960188, 0.0007157610769603029, 0.0006456191959300471, 0.0005819882717160613, 0.0005249715289316112, 0.00047395920584834383, 0.0004290716665395697, 0.0003863093593568873, 0.00034777721151107825, 0.00031331900320437394, 0.0002840623782722208, 0.0002565331531551953, 0.00023120366368563266, 0.00020813578181472383, 0.00018924076089798828, 0.00017056498991910318, 0.00015353378242215877, 0.0001401002903737382, 0.00012676836474404958, 0.00011456998669694478, 0.0001032366620195055, 9.32253322619003e-05, 8.531355847971433e-05, 7.738988453913473e-05, 7.108071355011594e-05, 6.39754877024592e-05, 5.775854414926184e-05, 5.2157226932047144e-05, 4.698714228080907e-05, 4.2555721193015485e-05, 3.833107743043589e-05, 3.4535441191667734e-05, 3.316242845180498e-05, 3.067693669859493e-05, 2.9167046657755388e-05, 2.671808976419753e-05, 2.432060055920334e-05, 2.2197780773682877e-05, 2.029612100399069e-05, 1.8380881438889966e-05, 1.6823984545278916e-05, 1.6071560663473312e-05, 1.5468502736042532e-05, 1.4246605295868843e-05, 1.3488779821354179e-05, 1.2312755567932844e-05, 1.1099463357429992e-05, 1.0163564762756718e-05, 9.178823118435954e-06, 8.661643396201375e-06, 7.973107936972954e-06, 7.1805164076930305e-06, 6.478180549366345e-06, 5.906094543779072e-06, 5.4205540424871195e-06, 5.194869979581278e-06, 4.738927563147984e-06, 4.945645368393825e-06, 4.7081675973678505e-06, 4.762387390708592e-06, 4.9963607573442105e-06, 4.628592015775373e-06, 4.281973480824766e-06, 3.92777712965111e-06, 3.53766924441148e-06, 3.206752368323477e-06, 3.1214853494181825e-06, 2.8152099028329924e-06, 2.908910141731947e-06, 2.8490943654618216e-06, 2.5971427459790854e-06, 2.361233371376724e-06, 2.1251982010292656e-06, 2.041302915516681e-06, 1.8374310957114376e-06, 1.8511770050456363e-06, 1.7259123861005008e-06, 1.5595612383877516e-06, 1.549309387846297e-06, 1.6796917196756984e-06, 1.5176464721917478e-06, 1.4112973503568057e-06, 1.280172600843372e-06, 1.2610850575806322e-06, 1.1509860781538334e-06, 1.2791732494362944e-06, 1.290624639005877e-06, 1.163607319620715e-06, 1.0829487633311628e-06, 1.0353937641987833e-06, 1.0392426429745133e-06, 9.405257398606519e-07, 8.622636847681081e-07, 7.772731867098612e-07, 6.996607322851335e-07, 6.819022719775642e-07, 6.283353870769366e-07, 5.952131627475888e-07, 5.748331709778044e-07, 5.504652592422987e-07, 5.999583842054416e-07, 6.735813436200859e-07, 6.797028751885169e-07, 6.182567438478526e-07, 5.652433011722591e-07, 5.73191214201265e-07, 6.184614975569623e-07, 7.618062350178457e-07, 7.547784898454574e-07, 8.015377367805075e-07, 8.03422583293046e-07, 7.275928442407762e-07, 6.983921741344161e-07, 6.848418103360076e-07, 8.064060582952538e-07, 7.660243057136072e-07, 7.456180785464092e-07, 6.713023207585288e-07, 6.687608331112857e-07, 6.398475451028851e-07, 5.784178122965737e-07, 5.416340095750378e-07, 4.889782970332047e-07, 5.156882300020754e-07, 4.6493895022526437e-07, 4.2914059984165693e-07, 3.9043068607415226e-07, 4.080997273065028e-07, 4.4959844060760776e-07, 4.257372067519325e-07, 4.061615944397742e-07, 3.6556755898089803e-07, 3.567668547328164e-07, 3.50372152417442e-07, 3.1790718554445724e-07, 3.1371864574103957e-07, 3.4276746634387374e-07, 3.55346810721523e-07, 5.810684445296176e-07, 5.252910124339909e-07, 5.308926884327936e-07, 4.825291128770879e-07, 5.053488081988015e-07, 4.5501575348187896e-07, 4.885012786920606e-07, 4.4098319726504966e-07, 4.5672975600010816e-07, 4.2108832939975015e-07, 4.174088953945524e-07, 3.8064367749989904e-07, 3.427772234707394e-07, 3.1079954788898136e-07, 2.8300080898635123e-07, 2.5693604353571876e-07, 2.316992334802086e-07, 2.1194207748506718e-07, 1.9178064682981428e-07, 1.7672686816009345e-07, 1.685624187111824e-07, 1.7489032541599023e-07], "duration": 43817.497772, "accuracy_train": [0.44084149096385544, 0.5012707078313253, 0.5803369728915663, 0.6130694653614458, 0.6489081325301205, 0.6863704819277109, 0.7129612198795181, 0.7573183358433735, 0.8010636295180723, 0.8189006024096386, 0.8501035391566265, 0.8496799698795181, 0.8738469503012049, 0.8786474021084337, 0.8894248870481928, 0.9047675075301205, 0.9027673192771084, 0.9161568147590361, 0.9192159262048193, 0.9198512801204819, 0.9250988328313253, 0.9341585090361446, 0.9448418674698795, 0.944347703313253, 0.9451242469879518, 0.9516189759036144, 0.9562311746987951, 0.9580666415662651, 0.9615493222891566, 0.9611728162650602, 0.9564194277108434, 0.9587961219879518, 0.9631965361445783, 0.9657614834337349, 0.9718326430722891, 0.9694324171686747, 0.9707737198795181, 0.9747270331325302, 0.9649614081325302, 0.9735033885542169, 0.9801628388554217, 0.976538968373494, 0.9776920180722891, 0.9759036144578314, 0.9774331701807228, 0.9775978915662651, 0.9802804969879518, 0.9815512048192772, 0.980492281626506, 0.9790097891566265, 0.9819512424698795, 0.982398343373494, 0.9841396837349398, 0.9880929969879518, 0.9822336219879518, 0.9875047063253012, 0.9885400978915663, 0.9865399096385542, 0.983410203313253, 0.9874105798192772, 0.9883753765060241, 0.9885636295180723, 0.9884930346385542, 0.9899049322289156, 0.9883753765060241, 0.9896225527108434, 0.9891989834337349, 0.9914344879518072, 0.988304781626506, 0.9911285768072289, 0.9923757530120482, 0.990234375, 0.9898578689759037, 0.9905873493975904, 0.9913874246987951, 0.9930111069277109, 0.9895284262048193, 0.9894342996987951, 0.9903755647590361, 0.9934817394578314, 0.9922816265060241, 0.9875753012048193, 0.9900461219879518, 0.9939759036144579, 0.9914815512048193, 0.9905167545180723, 0.9946583207831325, 0.9932464231927711, 0.9930817018072289, 0.9898343373493976, 0.9927522590361446, 0.9950348268072289, 0.9940700301204819, 0.9957172439759037, 0.9919521837349398, 0.9935288027108434, 0.9936464608433735, 0.9931052334337349, 0.9941876882530121, 0.9932228915662651, 0.9933405496987951, 0.9879518072289156, 0.9953172063253012, 0.9884224397590361, 0.9944465361445783, 0.994140625, 0.9944230045180723, 0.9946347891566265, 0.9940700301204819, 0.9948230421686747, 0.9964467243975904, 0.9968938253012049, 0.9957878388554217, 0.9967996987951807, 0.9929640436746988, 0.9946583207831325, 0.9956466490963856, 0.9949877635542169, 0.9965643825301205, 0.996070218373494, 0.9950348268072289, 0.9952466114457831, 0.9957878388554217, 0.9938817771084337, 0.9967291039156626, 0.9958819653614458, 0.9923757530120482, 0.9965408509036144, 0.9974350527108434, 0.998070406626506, 0.9967526355421686, 0.9967996987951807, 0.9948701054216867, 0.9958584337349398, 0.9951995481927711, 0.9972703313253012, 0.996070218373494, 0.9978821536144579, 0.9976468373493976, 0.9955995858433735, 0.9966585090361446, 0.9961643448795181, 0.9973879894578314, 0.9962584713855421, 0.9977880271084337, 0.9972703313253012, 0.9967996987951807, 0.9978350903614458, 0.9984704442771084, 0.9966114457831325, 0.9975527108433735, 0.9972467996987951, 0.998046875, 0.9966349774096386, 0.9986586972891566, 0.9984233810240963, 0.9971526731927711, 0.9966585090361446, 0.998046875, 0.9983998493975904, 0.9971762048192772, 0.9978115587349398, 0.9973173945783133, 0.9974585843373494, 0.9981880647590361, 0.9979056852409639, 0.9981174698795181, 0.9969408885542169, 0.9981410015060241, 0.9965173192771084, 0.9987057605421686, 0.9967055722891566, 0.9977880271084337, 0.9978586219879518, 0.9984233810240963, 0.9987292921686747, 0.9962584713855421, 0.9984939759036144, 0.9988704819277109, 0.9987763554216867, 0.9981410015060241, 0.9986351656626506, 0.9987998870481928, 0.9966349774096386, 0.9986116340361446, 0.9987998870481928, 0.9981410015060241, 0.9972467996987951, 0.9986586972891566, 0.9979056852409639, 0.9985410391566265, 0.9982351280120482, 0.999035203313253, 0.9983057228915663, 0.9985645707831325, 0.9984704442771084, 0.9974821159638554, 0.9991528614457831, 0.9987763554216867, 0.9978350903614458, 0.9983057228915663, 0.9988469503012049, 0.9989175451807228, 0.9982351280120482, 0.9989410768072289, 0.9976233057228916, 0.9990822665662651, 0.9967291039156626, 0.9984233810240963, 0.9990822665662651, 0.9985881024096386, 0.9992705195783133, 0.9984233810240963, 0.9975291792168675, 0.9984939759036144, 0.9991999246987951, 0.9987998870481928, 0.9991528614457831, 0.9987998870481928, 0.9986351656626506, 0.9987528237951807, 0.9987998870481928, 0.9984704442771084, 0.9985410391566265, 0.9987998870481928, 0.9985175075301205, 0.9983998493975904, 0.9989175451807228, 0.9981174698795181], "end": "2016-01-18 03:15:18.964000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0], "accuracy_valid": [0.4282852564102564, 0.4878472222222222, 0.5552884615384616, 0.5876068376068376, 0.6192574786324786, 0.6446314102564102, 0.6651976495726496, 0.7016559829059829, 0.7357104700854701, 0.7426549145299145, 0.7682959401709402, 0.7633547008547008, 0.7779113247863247, 0.7789797008547008, 0.7877938034188035, 0.7964743589743589, 0.7864583333333334, 0.7978098290598291, 0.7983440170940171, 0.7877938034188035, 0.7955395299145299, 0.8028846153846154, 0.812767094017094, 0.8060897435897436, 0.8141025641025641, 0.8098290598290598, 0.8190438034188035, 0.8137019230769231, 0.8146367521367521, 0.8222489316239316, 0.811698717948718, 0.8155715811965812, 0.8195779914529915, 0.8241185897435898, 0.8193108974358975, 0.8203792735042735, 0.8255876068376068, 0.828125, 0.8215811965811965, 0.8255876068376068, 0.8365384615384616, 0.8299946581196581, 0.8309294871794872, 0.828659188034188, 0.8348023504273504, 0.8307959401709402, 0.8313301282051282, 0.8333333333333334, 0.8358707264957265, 0.8333333333333334, 0.8247863247863247, 0.8321314102564102, 0.8377403846153846, 0.8384081196581197, 0.8360042735042735, 0.8413461538461539, 0.8405448717948718, 0.8408119658119658, 0.8349358974358975, 0.8405448717948718, 0.8397435897435898, 0.8413461538461539, 0.8421474358974359, 0.8457532051282052, 0.8393429487179487, 0.8394764957264957, 0.8426816239316239, 0.8428151709401709, 0.8382745726495726, 0.8456196581196581, 0.8422809829059829, 0.8421474358974359, 0.8450854700854701, 0.8406784188034188, 0.8426816239316239, 0.843215811965812, 0.8384081196581197, 0.8377403846153846, 0.842948717948718, 0.8486912393162394, 0.8493589743589743, 0.8400106837606838, 0.8426816239316239, 0.8472222222222222, 0.8466880341880342, 0.8430822649572649, 0.8497596153846154, 0.8458867521367521, 0.8482905982905983, 0.8438835470085471, 0.8472222222222222, 0.8504273504273504, 0.8460202991452992, 0.8456196581196581, 0.8413461538461539, 0.8485576923076923, 0.8450854700854701, 0.8489583333333334, 0.8488247863247863, 0.8448183760683761, 0.8428151709401709, 0.8405448717948718, 0.8477564102564102, 0.8402777777777778, 0.8446848290598291, 0.8470886752136753, 0.8473557692307693, 0.8486912393162394, 0.8472222222222222, 0.8485576923076923, 0.8543002136752137, 0.8488247863247863, 0.8478899572649573, 0.8525641025641025, 0.8422809829059829, 0.8462873931623932, 0.8500267094017094, 0.8500267094017094, 0.8497596153846154, 0.8532318376068376, 0.8484241452991453, 0.8484241452991453, 0.8522970085470085, 0.8470886752136753, 0.8500267094017094, 0.8517628205128205, 0.8458867521367521, 0.8525641025641025, 0.8520299145299145, 0.8518963675213675, 0.8474893162393162, 0.8510950854700855, 0.8481570512820513, 0.8436164529914529, 0.8468215811965812, 0.8544337606837606, 0.8561698717948718, 0.8564369658119658, 0.8536324786324786, 0.8536324786324786, 0.8506944444444444, 0.8524305555555556, 0.8474893162393162, 0.8522970085470085, 0.8541666666666666, 0.8530982905982906, 0.8538995726495726, 0.8557692307692307, 0.8572382478632479, 0.8561698717948718, 0.8540331196581197, 0.8565705128205128, 0.8556356837606838, 0.8547008547008547, 0.8565705128205128, 0.8565705128205128, 0.8490918803418803, 0.8532318376068376, 0.8552350427350427, 0.8573717948717948, 0.8552350427350427, 0.8553685897435898, 0.8567040598290598, 0.8587072649572649, 0.8575053418803419, 0.8581730769230769, 0.8555021367521367, 0.8492254273504274, 0.8568376068376068, 0.8553685897435898, 0.859375, 0.8540331196581197, 0.8548344017094017, 0.8528311965811965, 0.8587072649572649, 0.8587072649572649, 0.8522970085470085, 0.8560363247863247, 0.8623130341880342, 0.8589743589743589, 0.8589743589743589, 0.8589743589743589, 0.8619123931623932, 0.8538995726495726, 0.8568376068376068, 0.8572382478632479, 0.8568376068376068, 0.8544337606837606, 0.8580395299145299, 0.8580395299145299, 0.8551014957264957, 0.8576388888888888, 0.8568376068376068, 0.8569711538461539, 0.8559027777777778, 0.8600427350427351, 0.8547008547008547, 0.8600427350427351, 0.8595085470085471, 0.8551014957264957, 0.8568376068376068, 0.8548344017094017, 0.8557692307692307, 0.8597756410256411, 0.8576388888888888, 0.8538995726495726, 0.8569711538461539, 0.8547008547008547, 0.8573717948717948, 0.8604433760683761, 0.8592414529914529, 0.8576388888888888, 0.8547008547008547, 0.8525641025641025, 0.8573717948717948, 0.860176282051282, 0.859642094017094, 0.8560363247863247, 0.8587072649572649, 0.8579059829059829, 0.8565705128205128, 0.8564369658119658, 0.8577724358974359, 0.8592414529914529, 0.8561698717948718, 0.8556356837606838, 0.8564369658119658, 0.8559027777777778, 0.8548344017094017], "accuracy_test": 0.8618790064102564, "start": "2016-01-17 15:05:01.466000", "learning_rate_per_epoch": [0.0010000000474974513, 0.0007071067811921239, 0.0005773502634838223, 0.0005000000237487257, 0.00044721359154209495, 0.0004082482773810625, 0.000377964461222291, 0.00035355339059606194, 0.00033333332976326346, 0.0003162277571391314, 0.0003015113470610231, 0.00028867513174191117, 0.00027735010371543467, 0.0002672612317837775, 0.00025819888105615973, 0.0002500000118743628, 0.00024253562150988728, 0.00023570226039737463, 0.00022941573115531355, 0.00022360679577104747, 0.00021821788686793298, 0.00021320072119124234, 0.00020851440785918385, 0.00020412413869053125, 0.00019999999494757503, 0.0001961161324288696, 0.00019245008297730237, 0.0001889822306111455, 0.00018569533131085336, 0.00018257419287692755, 0.00017960529658012092, 0.00017677669529803097, 0.00017407764971721917, 0.00017149858467746526, 0.00016903085634112358, 0.00016666666488163173, 0.0001643989817239344, 0.00016222141857724637, 0.00016012815467547625, 0.0001581138785695657, 0.00015617375902365893, 0.00015430334315169603, 0.00015249857096932828, 0.00015075567353051156, 0.00014907120203133672, 0.00014744195505045354, 0.00014586499310098588, 0.00014433756587095559, 0.0001428571413271129, 0.00014142136205919087, 0.00014002800162415951, 0.00013867505185771734, 0.00013736056280322373, 0.00013608275912702084, 0.0001348399673588574, 0.00013363061589188874, 0.0001324532349826768, 0.00013130642764735967, 0.00013018891331739724, 0.00012909944052807987, 0.00012803687423001975, 0.00012700012302957475, 0.00012598815374076366, 0.0001250000059371814, 0.00012403473374433815, 0.00012309149315115064, 0.00012216944014653563, 0.00012126781075494364, 0.0001203858555527404, 0.00011952286149607971, 0.0001186781664728187, 0.00011785113019868731, 0.00011704114876920357, 0.00011624764010775834, 0.00011547005124157295, 0.00011470786557765678, 0.00011396057379897684, 0.00011322770296828821, 0.00011250878742430359, 0.00011180339788552374, 0.00011111111234640703, 0.00011043152335332707, 0.00010976425983244553, 0.00010910894343396649, 0.00010846523218788207, 0.00010783276957226917, 0.00010721124999690801, 0.00010660036059562117, 0.00010599978850223124, 0.00010540925723034889, 0.00010482848301762715, 0.00010425720392959192, 0.00010369517258368433, 0.00010314212704543024, 0.00010259783448418602, 0.00010206206934526563, 0.00010153461334994063, 0.00010101525549544021, 0.00010050378477899358, 9.999999747378752e-05, 9.95037189568393e-05, 9.901475277729332e-05, 9.853292431216687e-05, 9.80580662144348e-05, 9.759000386111438e-05, 9.712858445709571e-05, 9.667364793131128e-05, 9.622504148865119e-05, 9.578262688592076e-05, 9.534625860396773e-05, 9.49157983995974e-05, 9.449111530557275e-05, 9.407208563061431e-05, 9.365857840748504e-05, 9.32504772208631e-05, 9.284766565542668e-05, 9.245003457181156e-05, 9.205746027873829e-05, 9.16698481887579e-05, 9.128709643846378e-05, 9.09090886125341e-05, 9.053574467543513e-05, 9.016696276376024e-05, 8.980264829006046e-05, 8.944272121880203e-05, 8.908707968657836e-05, 8.87356509338133e-05, 8.838834764901549e-05, 8.804508979665115e-05, 8.770580461714417e-05, 8.737040479900315e-05, 8.703882485860959e-05, 8.671099931234494e-05, 8.638684084871784e-05, 8.606629853602499e-05, 8.574929233873263e-05, 8.543576404917985e-05, 8.512565545970574e-05, 8.481889381073415e-05, 8.451542817056179e-05, 8.421519305557013e-05, 8.391813753405586e-05, 8.362420339835808e-05, 8.333333244081587e-05, 8.304548100568354e-05, 8.276059088530019e-05, 8.247861114796251e-05, 8.21994908619672e-05, 8.19231936475262e-05, 8.164966129697859e-05, 8.137884287862107e-05, 8.111070928862318e-05, 8.084520959528163e-05, 8.058229286689311e-05, 8.032192999962717e-05, 8.006407733773813e-05, 7.980869122548029e-05, 7.955572800710797e-05, 7.930515857879072e-05, 7.905693928478286e-05, 7.881104102125391e-05, 7.856742013245821e-05, 7.832604751456529e-05, 7.808687951182947e-05, 7.78498942963779e-05, 7.761505548842251e-05, 7.738232670817524e-05, 7.715167157584801e-05, 7.69230755395256e-05, 7.669650221941993e-05, 7.647190795978531e-05, 7.624928548466414e-05, 7.602859113831073e-05, 7.580980309285223e-05, 7.55928922444582e-05, 7.537783676525578e-05, 7.51646002754569e-05, 7.495316822314635e-05, 7.474351150449365e-05, 7.453560101566836e-05, 7.432941492879763e-05, 7.412493141600862e-05, 7.392212864942849e-05, 7.372097752522677e-05, 7.352146349148825e-05, 7.332355744438246e-05, 7.312724483199418e-05, 7.293249655049294e-05, 7.273929804796353e-05, 7.25476274965331e-05, 7.23574630683288e-05, 7.216878293547779e-05, 7.198157254606485e-05, 7.179581734817475e-05, 7.161148823797703e-05, 7.142857066355646e-05, 7.124705007299781e-05, 7.106690463842824e-05, 7.088811980793253e-05, 7.071068102959543e-05, 7.053455919958651e-05, 7.035975431790575e-05, 7.018623728072271e-05, 7.001400081207976e-05, 6.984303036006168e-05, 6.967330409679562e-05, 6.950480747036636e-05, 6.933752592885867e-05, 6.917144492035732e-05, 6.900655716890469e-05, 6.884284084662795e-05, 6.868028140161186e-05, 6.851887155789882e-05, 6.835858948761597e-05, 6.819943519076332e-05, 6.804137956351042e-05, 6.788442260585725e-05, 6.77285497658886e-05, 6.757373921573162e-05, 6.74199836794287e-05, 6.726727588102221e-05, 6.711560854455456e-05, 6.696495256619528e-05, 6.681530794594437e-05, 6.666666740784422e-05, 6.651900912402198e-05, 6.637233309447765e-05, 6.62266174913384e-05, 6.608186231460422e-05], "accuracy_train_last": 0.9981174698795181, "error_valid": [0.5717147435897436, 0.5121527777777778, 0.44471153846153844, 0.41239316239316237, 0.3807425213675214, 0.35536858974358976, 0.3348023504273504, 0.29834401709401714, 0.2642895299145299, 0.2573450854700855, 0.23170405982905984, 0.2366452991452992, 0.22208867521367526, 0.2210202991452992, 0.21220619658119655, 0.20352564102564108, 0.21354166666666663, 0.2021901709401709, 0.20165598290598286, 0.21220619658119655, 0.20446047008547008, 0.19711538461538458, 0.18723290598290598, 0.1939102564102564, 0.1858974358974359, 0.19017094017094016, 0.18095619658119655, 0.18629807692307687, 0.18536324786324787, 0.17775106837606836, 0.18830128205128205, 0.18442841880341876, 0.18042200854700852, 0.17588141025641024, 0.18068910256410253, 0.17962072649572647, 0.1744123931623932, 0.171875, 0.17841880341880345, 0.1744123931623932, 0.16346153846153844, 0.1700053418803419, 0.16907051282051277, 0.17134081196581197, 0.1651976495726496, 0.16920405982905984, 0.1686698717948718, 0.16666666666666663, 0.16412927350427353, 0.16666666666666663, 0.17521367521367526, 0.16786858974358976, 0.16225961538461542, 0.16159188034188032, 0.16399572649572647, 0.15865384615384615, 0.1594551282051282, 0.15918803418803418, 0.16506410256410253, 0.1594551282051282, 0.16025641025641024, 0.15865384615384615, 0.1578525641025641, 0.15424679487179482, 0.16065705128205132, 0.16052350427350426, 0.15731837606837606, 0.1571848290598291, 0.1617254273504274, 0.1543803418803419, 0.15771901709401714, 0.1578525641025641, 0.15491452991452992, 0.15932158119658124, 0.15731837606837606, 0.15678418803418803, 0.16159188034188032, 0.16225961538461542, 0.15705128205128205, 0.15130876068376065, 0.15064102564102566, 0.15998931623931623, 0.15731837606837606, 0.1527777777777778, 0.15331196581196582, 0.1569177350427351, 0.15024038461538458, 0.15411324786324787, 0.15170940170940173, 0.15611645299145294, 0.1527777777777778, 0.1495726495726496, 0.1539797008547008, 0.1543803418803419, 0.15865384615384615, 0.1514423076923077, 0.15491452991452992, 0.15104166666666663, 0.1511752136752137, 0.15518162393162394, 0.1571848290598291, 0.1594551282051282, 0.15224358974358976, 0.1597222222222222, 0.1553151709401709, 0.15291132478632474, 0.15264423076923073, 0.15130876068376065, 0.1527777777777778, 0.1514423076923077, 0.1456997863247863, 0.1511752136752137, 0.1521100427350427, 0.14743589743589747, 0.15771901709401714, 0.1537126068376068, 0.14997329059829057, 0.14997329059829057, 0.15024038461538458, 0.14676816239316237, 0.15157585470085466, 0.15157585470085466, 0.14770299145299148, 0.15291132478632474, 0.14997329059829057, 0.14823717948717952, 0.15411324786324787, 0.14743589743589747, 0.1479700854700855, 0.14810363247863245, 0.15251068376068377, 0.1489049145299145, 0.15184294871794868, 0.15638354700854706, 0.15317841880341876, 0.14556623931623935, 0.1438301282051282, 0.14356303418803418, 0.1463675213675214, 0.1463675213675214, 0.14930555555555558, 0.14756944444444442, 0.15251068376068377, 0.14770299145299148, 0.14583333333333337, 0.14690170940170943, 0.1461004273504274, 0.14423076923076927, 0.14276175213675213, 0.1438301282051282, 0.14596688034188032, 0.14342948717948723, 0.14436431623931623, 0.14529914529914534, 0.14342948717948723, 0.14342948717948723, 0.15090811965811968, 0.14676816239316237, 0.1447649572649573, 0.14262820512820518, 0.1447649572649573, 0.14463141025641024, 0.14329594017094016, 0.1412927350427351, 0.1424946581196581, 0.14182692307692313, 0.1444978632478633, 0.1507745726495726, 0.1431623931623932, 0.14463141025641024, 0.140625, 0.14596688034188032, 0.14516559829059827, 0.14716880341880345, 0.1412927350427351, 0.1412927350427351, 0.14770299145299148, 0.14396367521367526, 0.13768696581196582, 0.14102564102564108, 0.14102564102564108, 0.14102564102564108, 0.1380876068376068, 0.1461004273504274, 0.1431623931623932, 0.14276175213675213, 0.1431623931623932, 0.14556623931623935, 0.14196047008547008, 0.14196047008547008, 0.14489850427350426, 0.14236111111111116, 0.1431623931623932, 0.14302884615384615, 0.1440972222222222, 0.1399572649572649, 0.14529914529914534, 0.1399572649572649, 0.14049145299145294, 0.14489850427350426, 0.1431623931623932, 0.14516559829059827, 0.14423076923076927, 0.14022435897435892, 0.14236111111111116, 0.1461004273504274, 0.14302884615384615, 0.14529914529914534, 0.14262820512820518, 0.13955662393162394, 0.14075854700854706, 0.14236111111111116, 0.14529914529914534, 0.14743589743589747, 0.14262820512820518, 0.13982371794871795, 0.14035790598290598, 0.14396367521367526, 0.1412927350427351, 0.14209401709401714, 0.14342948717948723, 0.14356303418803418, 0.1422275641025641, 0.14075854700854706, 0.1438301282051282, 0.14436431623931623, 0.14356303418803418, 0.1440972222222222, 0.14516559829059827], "accuracy_train_std": [0.08851644565413189, 0.09055731812290728, 0.08794356624049113, 0.0863916966379422, 0.08500702284138606, 0.08184958666062238, 0.0797571910722084, 0.07767267363064302, 0.07285572459597743, 0.07343954850779427, 0.065437742809563, 0.06641417123031752, 0.0613552449321513, 0.06011858079187058, 0.058739724139676666, 0.05615377331817443, 0.057329842366743454, 0.053080793306922155, 0.053390553662395035, 0.051896216105822734, 0.050629818792605356, 0.048348577112165385, 0.04448192016440563, 0.043820153611912584, 0.0429191064472241, 0.041543178898072436, 0.038396808139176224, 0.0377196925855851, 0.0365583944445046, 0.03632060623460701, 0.037273026750808166, 0.03682667972072952, 0.035023282693405364, 0.03377042568412151, 0.029458203651386794, 0.032792847636797066, 0.030804795590024928, 0.028844476114886813, 0.033514976362252494, 0.029448173534392184, 0.026149461059512262, 0.028083386830624932, 0.02756492760245506, 0.02860114078590173, 0.027178111782376935, 0.027515224073146688, 0.0249009977190394, 0.02472250196110995, 0.02571884056640822, 0.025508630460445995, 0.024586113192922227, 0.02438603243309329, 0.022816263118687535, 0.01953975355602295, 0.024251130753637003, 0.020269009894727227, 0.01937358967442398, 0.02035854276208023, 0.02295778815744578, 0.019954361404122652, 0.018713843146453125, 0.018314742159001175, 0.018922924936579617, 0.018519838561033203, 0.019141209954779297, 0.01791718192594478, 0.018794552613122677, 0.016983249420602198, 0.019117400985339005, 0.017107201154278745, 0.015410610495649922, 0.017932751476362724, 0.01865247700832481, 0.01748018170994973, 0.016250866983185022, 0.014965799959518552, 0.018067002959590267, 0.018093586823737642, 0.017172848345966878, 0.014482468750729373, 0.015973797767768392, 0.019965472264401884, 0.017704777313058712, 0.013522149776387155, 0.01625522791262411, 0.017505110370698636, 0.013123286622397768, 0.014727997437421389, 0.01462622627321957, 0.01797693058605484, 0.014817377716612198, 0.012232259152586179, 0.014043157372008489, 0.011728385500164792, 0.015717083208046306, 0.014503559123755028, 0.014731249271356871, 0.015009396683442356, 0.013587492066781936, 0.01454125050286382, 0.014620678846200504, 0.019715852183109225, 0.012223383283575334, 0.019624642892445445, 0.013175357470581557, 0.013293494643444162, 0.013414410036243172, 0.0128015508679308, 0.014506784925841938, 0.012706381060745519, 0.01077324533036075, 0.009659324236426693, 0.011532873917472677, 0.010221289126881196, 0.015090635949573179, 0.01306713148005245, 0.011416089153365505, 0.012569127040553107, 0.010216845818262729, 0.011504992192910846, 0.012529237696650219, 0.012286213523074055, 0.011404636227564146, 0.014377342370866545, 0.010234905048443162, 0.011503066824254216, 0.015362818400020519, 0.01031638422061765, 0.00883107174469057, 0.00780952254011309, 0.009914050467310287, 0.010149089848783153, 0.012667535948516528, 0.011653454473299822, 0.012446428720652356, 0.009543982131183189, 0.0109145914009321, 0.008220725811140082, 0.008509473446754193, 0.011202809064153146, 0.009956853789498996, 0.010744629305293251, 0.008900275857536683, 0.010777767521088914, 0.008373424354165254, 0.009388617662206243, 0.010221289126881196, 0.008027285912333062, 0.00685053299295446, 0.010232551304188202, 0.008905780250387704, 0.009263424355886372, 0.007661017271942513, 0.010627936086150355, 0.006448824963384115, 0.0070516004506113086, 0.009470800004144225, 0.010319523761714638, 0.007943762685961926, 0.006993922435338827, 0.009360737869316129, 0.008066612735165892, 0.009244425753880377, 0.008961812270439391, 0.0075994767538662324, 0.008091561152203103, 0.007533649399007432, 0.009899741567132627, 0.007490564344077742, 0.010693616200905988, 0.006226550191158938, 0.010263242759066028, 0.008717633970374864, 0.008079233651731802, 0.007155123728607921, 0.006290123050700258, 0.010709320523824681, 0.006909167659451268, 0.005957546083752507, 0.006061483895734917, 0.007684400269768736, 0.006386571871807339, 0.0063620310135018955, 0.01013202540733645, 0.006552068602931156, 0.006126498616951362, 0.007391740494372236, 0.009498589935727744, 0.006561864831356453, 0.008091561152203103, 0.006592720198288761, 0.0073149301532047135, 0.005539836682675034, 0.007179575988525934, 0.00676299928313797, 0.007061958340347996, 0.00909061178944695, 0.005074981388067987, 0.0061816109287854935, 0.008297559827336008, 0.007578206780071666, 0.006014483031490259, 0.005966276726727181, 0.007414777798618785, 0.0057827984663598965, 0.008546063447147938, 0.005547827364119666, 0.009943358322224002, 0.006839854940114491, 0.005276071943105206, 0.006713404367858363, 0.004718487203764208, 0.006946534550294492, 0.008690245917142476, 0.006692917482918365, 0.004935811247915851, 0.006126498616951362, 0.005217869560065964, 0.0063620310135018955, 0.006500694342175088, 0.0063529726292490835, 0.00600526924016571, 0.0069570488674569565, 0.006703333966732607, 0.00600526924016571, 0.006860871597234456, 0.007201139100502698, 0.005966276726727181, 0.007914466494569995], "accuracy_test_std": 0.06209798579437104, "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "rotation_range": [0, 0], "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.001, "patience_threshold": 1, "do_flip": true, "nb_data_augmentation": 1, "optimization": "adam", "batch_size": 32, "learning_rate_decay_method": "sqrt", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0, "valid_ratio": 0.15, "momentum": 0.9, "learning_rate_decay": 0.05}, "accuracy_valid_max": 0.8623130341880342, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = 1234\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -6], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128, 256, 512],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_optimizer.learning_rate = learning_rate\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8548344017094017, "loss_train": [2.8996400833129883, 1.4504095315933228, 1.2428423166275024, 1.1029132604599, 0.9846057295799255, 0.8858208060264587, 0.8037640452384949, 0.7315218448638916, 0.6671575307846069, 0.6115683913230896, 0.5616187453269958, 0.5200579166412354, 0.4839988648891449, 0.44661781191825867, 0.41856035590171814, 0.38970333337783813, 0.3653470575809479, 0.35064902901649475, 0.33041590452194214, 0.3122239410877228, 0.2965301275253296, 0.2813720405101776, 0.2701431214809418, 0.2578381299972534, 0.24856656789779663, 0.23990531265735626, 0.23178566992282867, 0.22053152322769165, 0.2162940800189972, 0.20600926876068115, 0.2021169364452362, 0.19669491052627563, 0.19005997478961945, 0.1856231838464737, 0.1814357489347458, 0.17459921538829803, 0.16969726979732513, 0.16496045887470245, 0.16091811656951904, 0.1558382213115692, 0.1529080867767334, 0.15047934651374817, 0.1452157199382782, 0.142191544175148, 0.14055845141410828, 0.13635596632957458, 0.1334199607372284, 0.12922705709934235, 0.13078241050243378, 0.1280149668455124, 0.12254976481199265, 0.11754864454269409, 0.11839170753955841, 0.1147908940911293, 0.11141150444746017, 0.11112882941961288, 0.10962279140949249, 0.10814765840768814, 0.10436708480119705, 0.10441755503416061, 0.10110507160425186, 0.09783471375703812, 0.09660492092370987, 0.09516339749097824, 0.09094962477684021, 0.09255382418632507, 0.09030919522047043, 0.08994907140731812, 0.08787187933921814, 0.08540137112140656, 0.08507935702800751, 0.08294063061475754, 0.07996901869773865, 0.08033083379268646, 0.0799381360411644, 0.0780971348285675, 0.0765778049826622, 0.07632970809936523, 0.07369610667228699, 0.0737026259303093, 0.0737871304154396, 0.07004785537719727, 0.0697164535522461, 0.07059276849031448, 0.06772543489933014, 0.06852103024721146, 0.06847290694713593, 0.06532975286245346, 0.0650877058506012, 0.06463887542486191, 0.06318861991167068, 0.062088608741760254, 0.06072233244776726, 0.06043081358075142, 0.06024207919836044, 0.05829497054219246, 0.05827715992927551, 0.05833480879664421, 0.056302547454833984, 0.05737479031085968, 0.055613864213228226, 0.05540657415986061, 0.05427984148263931, 0.05286048725247383, 0.05337517708539963, 0.05303299427032471, 0.0537327341735363, 0.05242781713604927, 0.05006782338023186, 0.04984508082270622, 0.04971795529127121, 0.049815159291028976, 0.047042395919561386, 0.04967424273490906, 0.047117915004491806, 0.04921118915081024, 0.048061877489089966, 0.04745044931769371, 0.04761262610554695, 0.04404353350400925, 0.044451791793107986, 0.04399421811103821, 0.04461711272597313, 0.04510658606886864, 0.0425259992480278, 0.04255959391593933, 0.04232637956738472, 0.0405435785651207, 0.043225374072790146, 0.04049202799797058, 0.03926188126206398, 0.03938240557909012, 0.03858691081404686, 0.03953354060649872, 0.03947574272751808, 0.03819321095943451, 0.03829430043697357, 0.038266923278570175, 0.037788622081279755, 0.03623198717832565, 0.035856444388628006, 0.03656064346432686, 0.0373559333384037, 0.03633603826165199, 0.036003630608320236, 0.03587311878800392, 0.03437960892915726, 0.033276885747909546, 0.03627568110823631, 0.033312249928712845, 0.035226549953222275, 0.031205348670482635, 0.03487252816557884, 0.03435647487640381, 0.033340148627758026, 0.03248191624879837, 0.03165801614522934, 0.0338870994746685, 0.031106865033507347, 0.032248519361019135, 0.030724845826625824, 0.02888442762196064, 0.0317254364490509, 0.03059556894004345, 0.030488839372992516, 0.02896139957010746, 0.029458412900567055, 0.03232542425394058, 0.029916781932115555, 0.02888057380914688, 0.02886200323700905, 0.03045782260596752, 0.02738470397889614, 0.02762257307767868, 0.027824806049466133, 0.028347551822662354, 0.02824597992002964, 0.02721180021762848, 0.029483310878276825, 0.02743908390402794, 0.028249463066458702, 0.02768554538488388, 0.027102958410978317, 0.026383766904473305, 0.027837593108415604, 0.026359185576438904, 0.027902474626898766, 0.025838248431682587, 0.024785948917269707, 0.025998244062066078, 0.024907473474740982, 0.02786952443420887, 0.02767210081219673, 0.022947443649172783, 0.02495119348168373, 0.025995580479502678, 0.02440047450363636, 0.024188151583075523, 0.024361038580536842, 0.024410635232925415, 0.024708416312932968, 0.023351484909653664, 0.02342245914041996, 0.022964106872677803, 0.024389158934354782, 0.02381657063961029, 0.023347945883870125, 0.022489866241812706, 0.02293788455426693, 0.020533617585897446, 0.022409209981560707, 0.021084779873490334, 0.02234858088195324, 0.022583268582820892, 0.022627558559179306, 0.02116350643336773, 0.023182036355137825, 0.021725161001086235, 0.021555421873927116, 0.02282150462269783, 0.022356554865837097, 0.022809920832514763, 0.021555950865149498, 0.02137974463403225, 0.019691498950123787, 0.021639017388224602, 0.022453585639595985, 0.020456286147236824, 0.02145901322364807], "accuracy_train_first": 0.44084149096385544, "model": "residualv2", "loss_std": [27.89168930053711, 0.19631901383399963, 0.1954512894153595, 0.19461040198802948, 0.19186389446258545, 0.1915871948003769, 0.18520651757717133, 0.17741237580776215, 0.172138512134552, 0.16771765053272247, 0.16149605810642242, 0.15509115159511566, 0.1477060467004776, 0.1439070999622345, 0.14067092537879944, 0.1333923488855362, 0.12610013782978058, 0.12448767572641373, 0.1176692470908165, 0.11309081315994263, 0.11113851517438889, 0.10951410233974457, 0.1047721728682518, 0.10149544477462769, 0.10081737488508224, 0.09651562571525574, 0.0941433310508728, 0.09122319519519806, 0.09326265007257462, 0.09344802051782608, 0.08847369253635406, 0.08876205235719681, 0.08442874252796173, 0.08286221325397491, 0.0821409821510315, 0.08195382356643677, 0.08201777935028076, 0.07883529365062714, 0.07704341411590576, 0.07622846215963364, 0.07641134411096573, 0.0745134949684143, 0.07527779042720795, 0.07152658700942993, 0.07223953306674957, 0.0708397626876831, 0.06981787085533142, 0.06932377070188522, 0.0708162933588028, 0.07020456343889236, 0.066815584897995, 0.06471756845712662, 0.06413963437080383, 0.0645052120089531, 0.06434996426105499, 0.06384259462356567, 0.060709718614816666, 0.06415452063083649, 0.06252685934305191, 0.06077301874756813, 0.06113124638795853, 0.05980243533849716, 0.05876648798584938, 0.05869989097118378, 0.057735465466976166, 0.05815444886684418, 0.05538646876811981, 0.05756845697760582, 0.05615008622407913, 0.054590146988630295, 0.05552460998296738, 0.055678676813840866, 0.05241938307881355, 0.05342721566557884, 0.05535692349076271, 0.05287124589085579, 0.05251230299472809, 0.05326182022690773, 0.05270320922136307, 0.053000226616859436, 0.05170562490820885, 0.04938125982880592, 0.04901266098022461, 0.04987652972340584, 0.04916151985526085, 0.04824337735772133, 0.05151873081922531, 0.047880250960588455, 0.052005793899297714, 0.0489477775990963, 0.047996554523706436, 0.04699065536260605, 0.04694218188524246, 0.046195801347494125, 0.04627257212996483, 0.04660802334547043, 0.04701060429215431, 0.047276172786951065, 0.04364652931690216, 0.04665972292423248, 0.04303649067878723, 0.043738819658756256, 0.044043127447366714, 0.04216950759291649, 0.04396189749240875, 0.043995749205350876, 0.044182706624269485, 0.04331238940358162, 0.0420328713953495, 0.04216444119811058, 0.043898243457078934, 0.04294070973992348, 0.04123535379767418, 0.04449313506484032, 0.04025045409798622, 0.04250847548246384, 0.040356721729040146, 0.041448865085840225, 0.04117017239332199, 0.039206817746162415, 0.03864235058426857, 0.039344001561403275, 0.04017940163612366, 0.039516326040029526, 0.038799796253442764, 0.04083489626646042, 0.04073280468583107, 0.03800126165151596, 0.0408160574734211, 0.04015104100108147, 0.03771863505244255, 0.03909407928586006, 0.03758721798658371, 0.038521040230989456, 0.03782171383500099, 0.03862561658024788, 0.038261327892541885, 0.03809225559234619, 0.035893235355615616, 0.03744475916028023, 0.03542973846197128, 0.03686056658625603, 0.036601342260837555, 0.03637060895562172, 0.03699668124318123, 0.03620379790663719, 0.034785546362400055, 0.034510426223278046, 0.03621014952659607, 0.0356360487639904, 0.03628607839345932, 0.03299533948302269, 0.0367998369038105, 0.035474106669425964, 0.03340652585029602, 0.034229885786771774, 0.03342599794268608, 0.03595683351159096, 0.03434939682483673, 0.03531530126929283, 0.03346944972872734, 0.0337260439991951, 0.03458758443593979, 0.03282484412193298, 0.03489651903510094, 0.032943326979875565, 0.031611304730176926, 0.03684419021010399, 0.035640817135572433, 0.0336686335504055, 0.032399918884038925, 0.03475792333483696, 0.031730737537145615, 0.03237992152571678, 0.03229715675115585, 0.03199845552444458, 0.03228536248207092, 0.03128679096698761, 0.03437507897615433, 0.03319598734378815, 0.03255326300859451, 0.03237348049879074, 0.03220585733652115, 0.03195071220397949, 0.0319523885846138, 0.03194273263216019, 0.033109840005636215, 0.032058119773864746, 0.03068123571574688, 0.031271565705537796, 0.0312364399433136, 0.03193898871541023, 0.033241916447877884, 0.028542518615722656, 0.03126957640051842, 0.03198952600359917, 0.02956923469901085, 0.030075356364250183, 0.02954789437353611, 0.028580253943800926, 0.030797079205513, 0.029407117515802383, 0.02927948720753193, 0.03066459856927395, 0.03164800629019737, 0.029318515211343765, 0.02994975633919239, 0.028315672650933266, 0.02994508482515812, 0.02753693237900734, 0.029394075274467468, 0.027745187282562256, 0.027824079617857933, 0.029980303719639778, 0.029364610090851784, 0.02692910097539425, 0.03108755126595497, 0.027880152687430382, 0.028018038719892502, 0.02986585721373558, 0.029556741937994957, 0.02991567738354206, 0.029645804315805435, 0.028150269761681557, 0.027005759999155998, 0.03130876272916794, 0.02977818436920643, 0.026262566447257996, 0.029017077758908272]}, "state": "available", "life": [{"dt": "Sun May 15 22:04:59 2016", "state": "available"}], "summary": "6228d737495c61fe49723299b94f5307"}