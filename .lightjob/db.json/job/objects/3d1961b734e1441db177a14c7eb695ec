{"content": {"hp_model": {"f0": 16, "f1": 64, "f2": 16, "f3": 16, "nonlin": "leaky_rectify", "nbg1": 2, "nbg3": 5, "nbg2": 8, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.010534427179654103, 0.015642955141311452, 0.013876353938251032, 0.009452384707520294, 0.011599446505859276, 0.00922043478162454, 0.009635308408392355, 0.010078474926973342, 0.01108159724512465, 0.007499518747726815, 0.009393017527331765, 0.013388954529852327, 0.01159497167222726, 0.011093988119988499, 0.01090570294597262, 0.010170852701438192, 0.010668534688387769, 0.010890536846033704, 0.010916905780861777, 0.009692066795943338, 0.00997334686120044, 0.010815417443770651, 0.010940381108038726, 0.012244455705076337, 0.012121234806912134, 0.011613068900183555, 0.013772153849525768, 0.012365470817612184, 0.012421706494950268, 0.011924602302494489, 0.01158129689414557, 0.01240370509853042, 0.012114796040804484, 0.01260905822229598, 0.011893573513854805, 0.01295977325047116, 0.012616737468488827, 0.013038098485937183, 0.014048914290961683, 0.012960869011804037, 0.014383570756696742, 0.014979962885509367, 0.013897557203710992, 0.014074351865622193, 0.014288810526054876, 0.013320161362791032, 0.013790032254493389, 0.013593767607057236, 0.01541786534189817, 0.013064380617225198, 0.014134304810170653, 0.014058838429728807, 0.014605052896391336, 0.014685121635172979, 0.01442647644906295, 0.014544991352082101, 0.014897236997032293, 0.013646993027478483, 0.014102840702352633, 0.015469279438217161, 0.014740153997588967, 0.015322040684850574, 0.01448235038598254, 0.013812569209846356, 0.014973889758009989, 0.015324946285428022, 0.015178001633619561, 0.013321394562816707, 0.013766865059462379, 0.014881845476619826, 0.015072950492537679, 0.015555491718127213, 0.013316087150757005, 0.014762082449727074, 0.015277290077763583, 0.015198671152433021, 0.015188964205007089, 0.013937940423385293, 0.014357431886986549, 0.0140029990413144, 0.014097180990265521, 0.014428965159026431, 0.013665938595032668, 0.014183635339563721, 0.01462158304995674, 0.015110761789939331, 0.014184597291401422, 0.014778107345487195, 0.013898864530972098, 0.01431878316688823, 0.011586252798552072, 0.014121679866732354, 0.013182216799678815, 0.01319455008142085, 0.01527383040766421, 0.014186358159141496, 0.01395000755159456, 0.01443602326403624, 0.014694552153747074, 0.014315265763975755, 0.014418590625025863, 0.013925912309670588, 0.012365115101370968, 0.013668550905538491, 0.013704158617916858, 0.014620336722458866, 0.014665156146034584, 0.013646177366438077, 0.015091359422531848, 0.014238060250500412, 0.013793442803765788, 0.012980160467343299, 0.014381936833546933, 0.014197065119755235, 0.014215991141274642, 0.01477003120619422, 0.013773572959034173, 0.014699491720838696, 0.014220183311699953, 0.014784696446076872, 0.014870727701504221, 0.014806683396271267, 0.014804823688278627, 0.014623536838586394, 0.01373420455127912, 0.01465592270848924, 0.013693877377092373, 0.015107204564498983, 0.015077737904898175, 0.013241389069380634, 0.015194116808144963, 0.012609229081222676, 0.014115493528799724, 0.015581187780050293, 0.012791715926583645, 0.014105499377593098, 0.014812849317125561, 0.01572508974394256, 0.014710524085663734, 0.015826863413611118, 0.015185908453057213, 0.014788824524119218, 0.014343025763392622, 0.015381262349950044, 0.014399801627500044, 0.015008428146277591, 0.01433268757167402, 0.014965560612623826, 0.013678450870170292, 0.013667416449251965, 0.014071698372950971, 0.013954605996855505, 0.014516408859129503, 0.014333420327281987, 0.01385726109387502, 0.015364128247954053, 0.014583051255431863, 0.014412812003839247, 0.014806721378367407, 0.014849102813297798, 0.014132405753448455, 0.014992869373781614, 0.014086351811804346, 0.01502983257807204, 0.014233236373678645, 0.014194358678425311, 0.01469852071648898, 0.01476674572627755, 0.012798393418642797, 0.01443585664537208, 0.01462950834328961, 0.01543276692337336, 0.01414900126434688, 0.014192897744482376, 0.014721629067545669, 0.014212535048461905, 0.014267021262597818, 0.013933129927145608, 0.015363240151285403, 0.01409520050942314, 0.013213091363211745, 0.013577037326760024, 0.013449261814262389, 0.014410552637081936, 0.015166825558565349, 0.01268597594993002, 0.013468490423391346, 0.012799010622695256, 0.014008587869932455, 0.014675998623686954, 0.0147366945296798, 0.014748941961963745, 0.014043915091420943, 0.014910738863088992, 0.01588918518789612, 0.012606878109606444, 0.01418423191598385, 0.014689783648234714, 0.015131082582633992, 0.013858912861396431, 0.013752695369473215, 0.014799891799852104, 0.014799268666785393, 0.01426659803430425, 0.016472439787797315, 0.014697934418653814, 0.014180647810032688, 0.014386898035152756, 0.015430579731676905, 0.01577751330619186, 0.014315968059627156, 0.014667275495743546, 0.014501934916162328, 0.014230298778123386, 0.01400560315571128, 0.01571071381197924, 0.015633549582065004, 0.014751230221676554, 0.014203317050057668, 0.01474579502505792, 0.01498184878658632, 0.012555926904852075, 0.015046504589958817, 0.014189760339142269, 0.015380649551024177, 0.014700346640122675, 0.013128262632006655, 0.013990582835532764, 0.014475970924603895, 0.01402466894863291, 0.0134913782092142], "moving_avg_accuracy_train": [0.01812776133951642, 0.037957614519887405, 0.05774798234761442, 0.0771119360162306, 0.09571122512271016, 0.11339198219584888, 0.13032322293552018, 0.1464842073856928, 0.16177532196343025, 0.17633946537386094, 0.19002612044800124, 0.20319057232785043, 0.21576162820183908, 0.22790551241815388, 0.23958370612950386, 0.2504985482161197, 0.26085443726909247, 0.2704329731299001, 0.27947196589742873, 0.2878303457715563, 0.29569228728682406, 0.3029191693231841, 0.3095883766237468, 0.3158558383049952, 0.3215105047109759, 0.32677165734180713, 0.3317485822833831, 0.3361812036081548, 0.34055185115639247, 0.3444669048569677, 0.3479369387160476, 0.3512622210868293, 0.3541619692681519, 0.35688571097182764, 0.3593184412658409, 0.3616126023245189, 0.36384922804514014, 0.36577394368539357, 0.3677199933056416, 0.3694551979710169, 0.371026146716274, 0.37241903819890093, 0.3736889526237506, 0.37498533542754386, 0.37616377779264293, 0.3772195453795193, 0.37801166213747917, 0.3788361383136814, 0.3797433245353771, 0.3805527806396559, 0.3810674495406682, 0.38167237748250393, 0.3822749413503942, 0.3828636797100484, 0.3833634614944884, 0.3838084706075904, 0.3841787518748583, 0.38459098797728575, 0.3849180028861832, 0.3852122442065534, 0.38546082140203874, 0.3857449947470231, 0.3859868719622894, 0.38616954002861126, 0.3864269832895006, 0.38658431351121497, 0.38676772734051057, 0.387116486542829, 0.38714437652134415, 0.3872601222567605, 0.38739920669959677, 0.38750585360531076, 0.3875738619371016, 0.3875954698083139, 0.3877428361257475, 0.38778725435195094, 0.3877853059793251, 0.3878021175856192, 0.3878568476586834, 0.3879385126124994, 0.38787715243998144, 0.3879266681276001, 0.38809210393573346, 0.3881922040868722, 0.388154302891917, 0.38824117165219, 0.38820774639357863, 0.3882776811084567, 0.38831272056613264, 0.38820013290066924, 0.38832194618982896, 0.38831542885605275, 0.3884046780639322, 0.3884851465462987, 0.3884343713423425, 0.3884189366421245, 0.38833986914762414, 0.38833148742043094, 0.3883889759349862, 0.38836402173619045, 0.3884136425703695, 0.388397847452083, 0.388560198959874, 0.38860651416217007, 0.3885736488870568, 0.38863950938710157, 0.38861980087525555, 0.3886135808121855, 0.38861735544829806, 0.3885788638934092, 0.38856979813091397, 0.3884501038994486, 0.3883958214649301, 0.38827939376311227, 0.3882608195815223, 0.38834866241688243, 0.3884625621520306, 0.3884396580732248, 0.388444368697473, 0.38846034214980024, 0.3883654723116659, 0.38833582093113606, 0.38820918933866844, 0.3881859377578377, 0.3881488434398797, 0.3881247591489556, 0.3882402670668858, 0.38817441623229026, 0.3881616895061635, 0.38810369642764014, 0.38817706069268343, 0.3882151146478706, 0.3882679283491965, 0.38835734940778, 0.3883146675712379, 0.3882786151159783, 0.3883019354288544, 0.3882438326021004, 0.38828458205922145, 0.3883258708194307, 0.388353730108381, 0.38844616068627486, 0.38859456051950225, 0.38872575917177865, 0.3887600965528658, 0.3886724176065585, 0.38857944751556855, 0.388581949134905, 0.3886631835541941, 0.38878054445657273, 0.3888304017461037, 0.3888055188423959, 0.3887877745266779, 0.38866488384611236, 0.38862153130498583, 0.3886360645382285, 0.38856543909100405, 0.3886786235468446, 0.38861540399162486, 0.3885725654312405, 0.3885477453268392, 0.3884742900078873, 0.38855949513754334, 0.38857107558756715, 0.38852551417706627, 0.3885427097254911, 0.3885139718428737, 0.3884697228509543, 0.388459945448657, 0.3885325980925602, 0.3885118828684832, 0.3886072435561181, 0.38859312282499875, 0.38860352145981153, 0.38857567785019065, 0.3885994467265319, 0.3885534814974003, 0.388451694970953, 0.3883181623209415, 0.38823518531688356, 0.3882256822775355, 0.38837295056117915, 0.38841245001525876, 0.38836654721795955, 0.38836243708134266, 0.38847495935004484, 0.3886111426728384, 0.38870344468001006, 0.3886843180853018, 0.3887391116655221, 0.38869084173535784, 0.38873800750414395, 0.38859669784246154, 0.3885927880826709, 0.3885660899084016, 0.38871179741465445, 0.38858030050126213, 0.38863394219347625, 0.3887217472462309, 0.38866591316275767, 0.38864131122217405, 0.3886562276613262, 0.38853490197206697, 0.38860925141241104, 0.38853662093133046, 0.3885108531257574, 0.3885481159697892, 0.3885374386532182, 0.3884860845361891, 0.3885211739415587, 0.388534189264734, 0.3885225434210402, 0.3884168752558004, 0.38843094775467585, 0.3883809060834441, 0.38845212601981177, 0.38860922991492364, 0.3886180538895627, 0.38870043627746137, 0.38884662399084663, 0.3888246610138273, 0.38883050702023336, 0.3887359672712831, 0.38883442405790525, 0.38874875715160717, 0.3886436109549496, 0.38878614455652916, 0.38876332617415055, 0.3886590121752294, 0.3887023854535998, 0.38865070455174294], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 62367778, "moving_var_accuracy_train": [0.002957541580642197, 0.0062007951169736, 0.009105643534086834, 0.011569743495800979, 0.013526171143618565, 0.014987036565370815, 0.016068335125696078, 0.016812098378714956, 0.01723525220610892, 0.017420755445014355, 0.017364600644579332, 0.01718786571979318, 0.016891362159896367, 0.016529491258640068, 0.016103964008014393, 0.015565771607194806, 0.014974394389172651, 0.014302690093386385, 0.01360775159631066, 0.01287573906376158, 0.01214445627689084, 0.011400061064908927, 0.010660359892578946, 0.009947853586654308, 0.00924084549745522, 0.008565878490753813, 0.007932218678545166, 0.007315829996970035, 0.006756170037191276, 0.006218501842777638, 0.005705021873348324, 0.005234037211622377, 0.0047863103460958935, 0.004374448231301387, 0.003990266998321937, 0.0036386088731581345, 0.00331977043736962, 0.0030211341662951833, 0.002753104731785872, 0.002504892675683947, 0.002276614327757577, 0.0020664142151231913, 0.0018742869374288437, 0.0017019837190516973, 0.00154428388481727, 0.0013998873031130315, 0.0012655456134258957, 0.0011451089007694319, 0.0010380048922599997, 0.0009401013756967882, 0.0008484751948261319, 0.0007669211156768413, 0.000693496753043139, 0.0006272665934439797, 0.0005667879705881038, 0.0005118914715259872, 0.00046193629832539426, 0.00041727211593015583, 0.00037650735309291135, 0.00033963581937513916, 0.0003062283530366633, 0.0002763323081429935, 0.000249225618614079, 0.00022460336535475486, 0.0002027395221124754, 0.00018268834508921016, 0.00016472227627528115, 0.00014934474547856927, 0.00013441727158882653, 0.0001210961181073475, 0.00010916060663676456, 9.834690803557346e-05, 8.855384343075284e-05, 7.970266118856252e-05, 7.192784655333187e-05, 6.475281870737019e-05, 5.827757100203617e-05, 5.245235757278826e-05, 4.7234080243587886e-05, 4.2570694701365085e-05, 3.834751086817145e-05, 3.4534826011237476e-05, 3.132766446962819e-05, 2.8285078384987335e-05, 2.5469499051699904e-05, 2.2990464780132297e-05, 2.0701473533338216e-05, 1.867534395911002e-05, 1.681885943554703e-05, 1.5251057333722797e-05, 1.3859497897093721e-05, 1.24739303881403e-05, 1.1298226139290186e-05, 1.0226680115250432e-05, 9.227215195756522e-06, 8.306637745918247e-06, 7.53223898950553e-06, 6.779647370711648e-06, 6.131426997392433e-06, 5.523888705991043e-06, 4.993659880053545e-06, 4.496539263903348e-06, 4.284107446251006e-06, 3.875002583299463e-06, 3.497223461743959e-06, 3.186539564764899e-06, 2.8713814372410644e-06, 2.5845914961783155e-06, 2.3262605774605247e-06, 2.1069689178943377e-06, 1.8970117185514817e-06, 1.8362509281110235e-06, 1.6791450795752233e-06, 1.6332292593729772e-06, 1.4730113354312986e-06, 1.3951574754051414e-06, 1.3724000748661558e-06, 1.23988143881305e-06, 1.1160930047590159e-06, 1.0067800648963764e-06, 9.871046340954366e-07, 8.963070099918138e-07, 9.509963508905845e-07, 8.60762439901651e-07, 7.870700917344105e-07, 7.135835601848238e-07, 7.623039161074875e-07, 7.251005162491265e-07, 6.540481906453664e-07, 6.189121459903447e-07, 6.054617698593984e-07, 5.579485244219442e-07, 5.272572554094169e-07, 5.464966613321992e-07, 5.082426477344533e-07, 4.691163987332392e-07, 4.270992917936883e-07, 4.1477280890559513e-07, 3.882401923160014e-07, 3.6475902856096775e-07, 3.352683855321888e-07, 3.786322525471817e-07, 5.389716218098625e-07, 6.399922368612356e-07, 5.866045148344449e-07, 5.97132441980955e-07, 6.152101381509197e-07, 5.537454472295687e-07, 5.577621804017744e-07, 6.259481950258143e-07, 5.857251193976212e-07, 5.327250375302542e-07, 4.822862804399273e-07, 5.699767267246778e-07, 5.298940394513747e-07, 4.788055693226035e-07, 4.758165965512662e-07, 5.435314262913895e-07, 5.251486931218584e-07, 4.891501041118969e-07, 4.457794319431179e-07, 4.497626436897614e-07, 4.701256063981816e-07, 4.2432000716314967e-07, 4.005705855883235e-07, 3.63174709000153e-07, 3.342900311761214e-07, 3.1848278763140274e-07, 2.8749488722942166e-07, 3.062510585016333e-07, 2.79488037228509e-07, 3.3338218022143633e-07, 3.0183851762540495e-07, 2.726278503165917e-07, 2.5234246465540983e-07, 2.3219285353258246e-07, 2.2798878878140488e-07, 2.98434382599136e-07, 4.2906966191109056e-07, 4.4812934454192054e-07, 4.041291798993762e-07, 5.589077882155371e-07, 5.170588712472402e-07, 4.843165853215598e-07, 4.360369657964921e-07, 5.063846178018633e-07, 6.226592326856131e-07, 6.370702541683328e-07, 5.76655668377713e-07, 5.460111294401466e-07, 5.123798919186819e-07, 4.811633904334735e-07, 6.127628357532545e-07, 5.516241281725134e-07, 5.028768479390907e-07, 6.436652595510319e-07, 7.349216776813796e-07, 6.873263902056064e-07, 6.879812967882365e-07, 6.472401710050602e-07, 5.879634532288783e-07, 5.311696093187951e-07, 6.105319542549698e-07, 5.992293123447425e-07, 5.867830621481706e-07, 5.34080574169814e-07, 4.931691926609223e-07, 4.4487831919725077e-07, 4.241256953000671e-07, 3.9279452309281735e-07, 3.550396585197405e-07, 3.207563237458261e-07, 3.891725416776802e-07, 3.5203760453131524e-07, 3.393713638151398e-07, 3.510847414595494e-07, 5.381109720474079e-07, 4.850006375985421e-07, 4.975822943616486e-07, 6.401616928288272e-07, 5.804868747819087e-07, 5.227457694218215e-07, 5.509110696638185e-07, 5.830636121850706e-07, 5.908066204787412e-07, 6.312274624748565e-07, 7.509471644406535e-07, 6.805385551659927e-07, 7.104173929876935e-07, 6.563068251782682e-07, 6.147143832110701e-07], "duration": 101719.959367, "accuracy_train": [0.18127761339516427, 0.21642629314322628, 0.23586129279715762, 0.2513875190337763, 0.2631048270810262, 0.27251879585409744, 0.28270438959256183, 0.29193306743724623, 0.2993953531630676, 0.30741675606773716, 0.31320601611526394, 0.32167063924649314, 0.32890113106773716, 0.33720047036498707, 0.34468744953165376, 0.34873212699566264, 0.35405743874584716, 0.3566397958771687, 0.3608229008051864, 0.3630557646387043, 0.36644976092423404, 0.3679611076504245, 0.3696112423288114, 0.37226299343623104, 0.3724025023648025, 0.37412203101928754, 0.3765409067575674, 0.3760747955311, 0.37988767909053156, 0.3797023881621447, 0.3791672434477667, 0.3811897624238649, 0.3802597029000554, 0.38139938630490955, 0.3812130139119601, 0.3822600518526209, 0.3839788595307309, 0.3830963844476744, 0.38523443988787376, 0.38507203995939465, 0.38516468542358806, 0.3849550615425434, 0.3851181824473976, 0.3866527806616833, 0.3867697590785345, 0.3867214536614064, 0.3851407129591178, 0.38625642389950166, 0.3879080005306386, 0.3878378855781654, 0.3856994696497785, 0.3871167289590255, 0.3876980161614064, 0.3881623249469361, 0.38786149755444815, 0.38781355262550754, 0.38751128328026946, 0.38830111289913255, 0.38786113706626063, 0.38786041608988553, 0.3876980161614064, 0.3883025548518826, 0.38816376689968624, 0.38781355262550754, 0.3887439726375046, 0.3880002855066445, 0.3884184518041713, 0.39025531936369506, 0.38739538632798076, 0.38830183387550754, 0.38865096668512367, 0.3884656757567368, 0.3881859369232189, 0.3877899406492248, 0.3890691329826504, 0.38818701838778147, 0.3877677706256921, 0.3879534220422665, 0.38834941831626063, 0.3886734971968438, 0.38732491088732, 0.38837230931616834, 0.38958102620893315, 0.3890931054471207, 0.38781319213732, 0.38902299049464745, 0.38790691906607605, 0.3889070935423588, 0.38862807568521596, 0.3871868439114987, 0.3894182657922665, 0.3882567728520672, 0.38920792093484685, 0.3892093628875969, 0.3879773945067368, 0.38828002434016246, 0.3876282616971207, 0.3882560518756921, 0.3889063725659837, 0.38813943394702843, 0.38886023007798076, 0.3882556913875046, 0.39002136252999264, 0.38902335098283497, 0.3882778614110373, 0.3892322538875046, 0.38844242426864156, 0.38855760024455516, 0.3886513271733112, 0.38823243989940937, 0.388488206268457, 0.38737285581626063, 0.3879072795542636, 0.3872315444467516, 0.388093651947213, 0.38913924793512367, 0.3894876597683647, 0.38823352136397193, 0.38848676431570695, 0.3886041032207456, 0.387511643768457, 0.38806895850636763, 0.3870695050064599, 0.38797667353036175, 0.3878149945782577, 0.3879080005306386, 0.3892798383282577, 0.3875817587209302, 0.38804714897102255, 0.3875817587209302, 0.3888373390780731, 0.38855760024455516, 0.38874325166112955, 0.3891621389350314, 0.3879305310423588, 0.38795414301864156, 0.38851181824473974, 0.38772090716131413, 0.3886513271733112, 0.38869746966131413, 0.38860446370893315, 0.38927803588732, 0.38993015901854927, 0.3899065470422665, 0.3890691329826504, 0.38788330708979324, 0.38774271669665933, 0.38860446370893315, 0.3893942933277962, 0.38983679257798076, 0.3892791173518826, 0.3885815727090255, 0.38862807568521596, 0.38755886772102255, 0.38823135843484685, 0.3887668636374123, 0.3879298100659837, 0.38969728364940937, 0.38804642799464745, 0.38818701838778147, 0.3883243643872278, 0.38781319213732, 0.38932634130444815, 0.38867529963778147, 0.3881154614825581, 0.38869746966131413, 0.3882553308993171, 0.38807148192368035, 0.38837194882798076, 0.3891864718876892, 0.3883254458517903, 0.389465489744832, 0.3884660362449243, 0.3886971091731266, 0.3883250853636028, 0.3888133666136028, 0.38813979443521596, 0.38753561623292726, 0.3871163684708379, 0.38748839228036175, 0.3881401549234035, 0.38969836511397193, 0.3887679451019749, 0.3879534220422665, 0.3883254458517903, 0.3894876597683647, 0.38983679257798076, 0.38953416274455516, 0.38851217873292726, 0.3892322538875046, 0.38825641236387964, 0.3891624994232189, 0.38732491088732, 0.38855760024455516, 0.3883258063399778, 0.3900231649709302, 0.3873968282807309, 0.3891167174234035, 0.38951199272102255, 0.3881634064114987, 0.3884198937569214, 0.38879047561369506, 0.38744297076873385, 0.38927839637550754, 0.3878829466016057, 0.38827894287559983, 0.38888348156607605, 0.388441342804079, 0.38802389748292726, 0.38883697858988553, 0.3886513271733112, 0.3884177308277962, 0.38746586176864156, 0.38855760024455516, 0.3879305310423588, 0.3890931054471207, 0.3900231649709302, 0.38869746966131413, 0.38944187776854927, 0.39016231341131413, 0.38862699422065333, 0.38888312107788847, 0.3878851095307309, 0.3897205351375046, 0.3879777549949243, 0.3876972951850314, 0.3900689469707456, 0.3885579607327427, 0.38772018618493903, 0.38909274495893315, 0.3881855764350314], "end": "2016-01-27 05:47:01.763000", "learning_rate_per_epoch": [0.003646773286163807, 0.0033283333759754896, 0.0030376999638974667, 0.002772444859147072, 0.0025303522124886513, 0.00230939919129014, 0.0021077401470392942, 0.0019236900843679905, 0.0017557115061208606, 0.0016024010255932808, 0.0014624777249991894, 0.001334772678092122, 0.001218218938447535, 0.0011118428083136678, 0.0010147555731236935, 0.000926146050915122, 0.0008452740148641169, 0.0007714638486504555, 0.0007040988421067595, 0.0006426161853596568, 0.0005865022540092468, 0.0005352883017621934, 0.0004885463858954608, 0.00044588602031581104, 0.00040695079951547086, 0.0003714154299814254, 0.000338983052643016, 0.0003093827108386904, 0.0002823671093210578, 0.00025771051878109574, 0.00023520697141066194, 0.00021466845646500587, 0.00019592339231166989, 0.00017881515668705106, 0.0001632008352316916, 0.0001489499700255692, 0.0001359435118502006, 0.00012407278700266033, 0.0001132386241806671, 0.00010335051047150046, 9.432583465240896e-05, 8.608920325059444e-05, 7.857180753489956e-05, 7.171083416324109e-05, 6.544897041749209e-05, 5.9733898524427786e-05, 5.45178736501839e-05, 4.9757316446630284e-05, 4.541245652944781e-05, 4.1446994146099314e-05, 3.782780186156742e-05, 3.452464079600759e-05, 3.1509913242189214e-05, 2.8758435291820206e-05, 2.6247218556818552e-05, 2.395528463239316e-05, 2.18634850170929e-05, 1.9954342860728502e-05, 1.821190926420968e-05, 1.6621626855339855e-05, 1.5170209735515527e-05, 1.384553161187796e-05, 1.2636525752895977e-05, 1.1533091310411692e-05, 1.0526009646127932e-05, 9.606867934053298e-06, 8.76798640092602e-06, 8.002356480574235e-06, 7.303582606255077e-06, 6.665826276730513e-06, 6.083759672037559e-06, 5.552519724005833e-06, 5.067668098490685e-06, 4.625153906090418e-06, 4.221280960337026e-06, 3.852674581139581e-06, 3.516254992064205e-06, 3.2092120818560943e-06, 2.928980393335223e-06, 2.673218887139228e-06, 2.4397907054662937e-06, 2.2267456643021433e-06, 2.032303882515407e-06, 1.854841002568719e-06, 1.692874434411351e-06, 1.5450508499270654e-06, 1.4101353826845298e-06, 1.287000941374572e-06, 1.1746186601158115e-06, 1.0720497130023432e-06, 9.784372423382592e-07, 8.929990826800349e-07, 8.150215080604539e-07, 7.438529792125337e-07, 6.78898970818409e-07, 6.196167987582157e-07, 5.655112431668385e-07, 5.161302283340774e-07, 4.7106121314755e-07, 4.2992766680072236e-07, 3.9238594240487146e-07, 3.581224063964328e-07, 3.268507953180233e-07, 2.9830985681655875e-07, 2.7226113274991803e-07, 2.4848699808899255e-07, 2.2678885613913735e-07, 2.069854190267506e-07, 1.8891124398123793e-07, 1.724153264603956e-07, 1.5735984959519556e-07, 1.4361903311055357e-07, 1.3107806751122553e-07, 1.1963219037625095e-07, 1.091857839696786e-07, 9.965157232727506e-08, 9.094989650293428e-08, 8.300806086936063e-08, 7.575971494588885e-08, 6.914429917515008e-08, 6.310654754315692e-08, 5.7596018621097755e-08, 5.256667634512269e-08, 4.7976499217838864e-08, 4.378714280051099e-08, 3.996360575797553e-08, 3.64739420888327e-08, 3.328900177734795e-08, 3.038217144535338e-08, 2.772916829485439e-08, 2.5307828721565784e-08, 2.3097923573800472e-08, 2.1080989398569727e-08, 1.9240175674895e-08, 1.75601044816176e-08, 1.602673904699259e-08, 1.462726739731579e-08, 1.3349999328227113e-08, 1.218426337601386e-08, 1.1120320664304018e-08, 1.0149283191651648e-08, 9.26303744819279e-09, 8.454179578620824e-09, 7.715952321518671e-09, 7.0421872777615135e-09, 6.427256060703712e-09, 5.866021446365721e-09, 5.353794296780734e-09, 4.886295368322635e-09, 4.4596188963907935e-09, 4.070200620986952e-09, 3.7147864784259355e-09, 3.39040751029529e-09, 3.0943536621919066e-09, 2.8241515792615246e-09, 2.5775437340058716e-09, 2.352469996580453e-09, 2.1470498712261588e-09, 1.9595671751915233e-09, 1.788455716855708e-09, 1.6322859730522055e-09, 1.4897530986601737e-09, 1.3596663794857022e-09, 1.240938907187683e-09, 1.132578808515916e-09, 1.0336809186384244e-09, 9.43418787535677e-10, 8.610385182628022e-10, 7.858517725445324e-10, 7.172303861935347e-10, 6.54601095551044e-10, 5.974406525055542e-10, 5.452715501341743e-10, 4.97657859366285e-10, 4.5420187055889016e-10, 4.14540485271786e-10, 3.7834238519884877e-10, 3.453051455881706e-10, 3.1515273724025405e-10, 2.876332783063873e-10, 2.6251684159817046e-10, 2.395935949639494e-10, 2.1867203880976405e-10, 1.995773685203872e-10, 1.8215007280275586e-10, 1.662445459071904e-10, 1.51727908015431e-10, 1.384788672620374e-10, 1.2638674828924223e-10, 1.1535052774069854e-10, 1.0527800159421119e-10, 9.608502188340751e-11, 8.769478199166514e-11, 8.003718521276681e-11, 7.304825494491851e-11, 6.666960733481275e-11, 6.084795167726753e-11, 5.553464632601646e-11, 5.0685303176756236e-11, 4.6259409497428905e-11, 4.2219991391867495e-11, 3.85332980801234e-11, 3.516853128160413e-11, 3.2097578067658006e-11, 2.929478534752228e-11, 2.6736734354271263e-11, 2.44020550294044e-11, 2.227124214215781e-11, 2.0326493960221015e-11, 1.8551563063518017e-11, 1.6931621035776168e-11, 1.545313529915937e-11, 1.4103752020433458e-11, 1.2872198099089793e-11, 1.1748184890192359e-11, 1.0722321468203955e-11, 9.786037431791517e-12, 8.931510912801865e-12, 8.151602656769619e-12, 7.43979617495727e-12, 6.790145702650907e-12, 6.1972232900942e-12, 5.656075434401497e-12, 5.162181012513534e-12, 4.711413984642698e-12, 4.300008699803515e-12, 3.924527369747466e-12, 3.5818336144299856e-12, 3.2690642727539743e-12, 2.9836062006433073e-12, 2.7230745869993855e-12], "accuracy_valid": [0.1864396060805723, 0.2249858810240964, 0.23696936182228917, 0.24734533838478917, 0.2574065794427711, 0.2671722044427711, 0.2724212278802711, 0.28453707407756024, 0.292858445500753, 0.2999797039721386, 0.3074671733810241, 0.31987863563629515, 0.32807793674698793, 0.3334181452371988, 0.3423189829631024, 0.3435705713478916, 0.3475988916603916, 0.34992852268448793, 0.3541200936558735, 0.3568159356174699, 0.3607118905308735, 0.3611898766942771, 0.36306211172816266, 0.3674772331513554, 0.36797580948795183, 0.36957301863704817, 0.3712922980986446, 0.37215708537274095, 0.37494411238704817, 0.37581919474774095, 0.3759309699736446, 0.37691782756024095, 0.37789439006024095, 0.37970485457454817, 0.3778840949736446, 0.38095644295933734, 0.38042698136295183, 0.37948130412274095, 0.3825227668486446, 0.38143442912274095, 0.38180064006024095, 0.3816682746611446, 0.38132265389683734, 0.38275661238704817, 0.38201389542545183, 0.3827669074736446, 0.38226833113704817, 0.38324489363704817, 0.38210508047816266, 0.38190212019954817, 0.38348903426204817, 0.38274631730045183, 0.38361110457454817, 0.38286838761295183, 0.38348903426204817, 0.38287868269954817, 0.3838346550263554, 0.38311252823795183, 0.3828580925263554, 0.38408909073795183, 0.38467885212725905, 0.3835905144013554, 0.38445530167545183, 0.38263454207454817, 0.38396702042545183, 0.3826139519013554, 0.38543186417545183, 0.3845979621611446, 0.38348903426204817, 0.3834684440888554, 0.38360080948795183, 0.3834684440888554, 0.38595103068524095, 0.3833463737763554, 0.38348903426204817, 0.3861539909638554, 0.3843229362763554, 0.38296986775225905, 0.3829801628388554, 0.38396702042545183, 0.3844450065888554, 0.38506565323795183, 0.3845979621611446, 0.38385524519954817, 0.38300075301204817, 0.38445530167545183, 0.38616428605045183, 0.38555393448795183, 0.38483180769954817, 0.38312282332454817, 0.38619517131024095, 0.38396702042545183, 0.38544215926204817, 0.38568629988704817, 0.3840787956513554, 0.3850553581513554, 0.3839567253388554, 0.38309193806475905, 0.3835905144013554, 0.38422145613704817, 0.38494358292545183, 0.38409938582454817, 0.38448618693524095, 0.38495387801204817, 0.38445530167545183, 0.38372287980045183, 0.38384495011295183, 0.38384495011295183, 0.38483180769954817, 0.38323459855045183, 0.38555393448795183, 0.3835905144013554, 0.38372287980045183, 0.38446559676204817, 0.3843229362763554, 0.38350962443524095, 0.38445530167545183, 0.38335666886295183, 0.3859407355986446, 0.38421116105045183, 0.38394643025225905, 0.38372287980045183, 0.3845979621611446, 0.3846891472138554, 0.38347873917545183, 0.38360080948795183, 0.38422145613704817, 0.3851774284638554, 0.38422145613704817, 0.38409938582454817, 0.3845670769013554, 0.38507594832454817, 0.38300075301204817, 0.38333607868975905, 0.38641872176204817, 0.38434352644954817, 0.3837125847138554, 0.38397731551204817, 0.3843229362763554, 0.3842008659638554, 0.3851774284638554, 0.38287868269954817, 0.3831022331513554, 0.3845670769013554, 0.38580837019954817, 0.38702907332454817, 0.38495387801204817, 0.38446559676204817, 0.38470973738704817, 0.3840787956513554, 0.38422145613704817, 0.38165797957454817, 0.3849332878388554, 0.38445530167545183, 0.38409938582454817, 0.38469944230045183, 0.38519801863704817, 0.38323459855045183, 0.3837434699736446, 0.38433323136295183, 0.38445530167545183, 0.38421116105045183, 0.3851774284638554, 0.3859098503388554, 0.38300075301204817, 0.38324489363704817, 0.38347873917545183, 0.3829801628388554, 0.38631724162274095, 0.38324489363704817, 0.3835905144013554, 0.3854524543486446, 0.3835905144013554, 0.38299045792545183, 0.38460825724774095, 0.38506565323795183, 0.3842317512236446, 0.38543186417545183, 0.38532008894954817, 0.3845670769013554, 0.3888704230986446, 0.38397731551204817, 0.38361110457454817, 0.3850862434111446, 0.38433323136295183, 0.3850862434111446, 0.38544215926204817, 0.38485239787274095, 0.38469944230045183, 0.38446559676204817, 0.38443471150225905, 0.3823698112763554, 0.38543186417545183, 0.38434352644954817, 0.38360080948795183, 0.38473032756024095, 0.3831022331513554, 0.38309193806475905, 0.38335666886295183, 0.38482151261295183, 0.38373317488704817, 0.38445530167545183, 0.3842008659638554, 0.38408909073795183, 0.38309193806475905, 0.38469944230045183, 0.38396702042545183, 0.38311252823795183, 0.3835905144013554, 0.38309193806475905, 0.38226833113704817, 0.3837125847138554, 0.38309193806475905, 0.3833772590361446, 0.3842317512236446, 0.38333607868975905, 0.38382435993975905, 0.3835905144013554, 0.38458766707454817, 0.3832243034638554, 0.38396702042545183, 0.38705995858433734, 0.3818815300263554, 0.38434352644954817, 0.3839567253388554, 0.38446559676204817, 0.38299045792545183, 0.38494358292545183, 0.38348903426204817, 0.38519801863704817, 0.38629665144954817], "accuracy_test": 0.3805564413265306, "start": "2016-01-26 01:31:41.804000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 218.0, 219.0, 220.0, 221.0, 222.0, 223.0, 224.0, 225.0, 226.0, 227.0, 228.0, 229.0, 230.0], "accuracy_train_last": 0.3881855764350314, "batch_size_eval": 1024, "accuracy_train_std": [0.011815090454023337, 0.010827646165998144, 0.012439369386540593, 0.014374587555801264, 0.01447384985881215, 0.015198557151770703, 0.013378640069097851, 0.012600916623452892, 0.011778022480624182, 0.012104356949770826, 0.012936770697477302, 0.012687619223292272, 0.012943164658837882, 0.012833370621833479, 0.013736227373136576, 0.013116941632446822, 0.01353490510995102, 0.014150181758776468, 0.014517204923363723, 0.01518541353747388, 0.015243563409796832, 0.01516935997447954, 0.015281377955323444, 0.015002951677534165, 0.015127129011843574, 0.015001640717206999, 0.015002625715226212, 0.01446761628691184, 0.015148825245372562, 0.0147698663922843, 0.014919684069061233, 0.015162889758924339, 0.015052398041542745, 0.015651478046252824, 0.01565093452454202, 0.015048988321231964, 0.015697979188873258, 0.015581228383294647, 0.014377970258944467, 0.014950027639199403, 0.01506711937867115, 0.01525431844661073, 0.014766897183472774, 0.015193309582459465, 0.01571742876652088, 0.014655979991102611, 0.014639680720623953, 0.01487785460765089, 0.014717720522532566, 0.014674158331858747, 0.015213798091789715, 0.014728636366724574, 0.015368214646992255, 0.01490529168503624, 0.015758402008461, 0.015055343720358957, 0.014081300751858927, 0.014462540286482688, 0.014848506142341858, 0.015312758422512125, 0.015481564109057268, 0.014559238563940632, 0.014949876765406553, 0.015139567564780987, 0.014848077948074778, 0.014163287461670476, 0.015055676542617535, 0.015571244911298002, 0.014573693375058748, 0.014429566230698293, 0.014757737257633114, 0.015307126241397339, 0.014484111177051557, 0.014503985133530312, 0.015448008177953419, 0.01581347015515599, 0.015085456678100077, 0.014270639532434558, 0.01515378592043846, 0.014494788369598388, 0.01430564696807658, 0.014764481131344324, 0.015015223372449147, 0.014953312240245138, 0.014239169766229081, 0.0149036386220148, 0.015344775649773927, 0.015392818309939434, 0.014403949949912793, 0.014943643259652336, 0.014883738822352782, 0.015494180233373145, 0.014498918476579139, 0.015050251077080176, 0.014539998942374148, 0.01477233882208703, 0.014844227180610325, 0.015047314812273657, 0.01494409738594278, 0.014740443932753313, 0.014530938299598813, 0.015114531179440123, 0.015147085595924002, 0.015130963897923263, 0.014158060879262944, 0.015132396803424984, 0.014897077722759524, 0.014251001529013945, 0.015202393543598846, 0.01496060602059008, 0.01493935650926757, 0.015262973693410254, 0.015160718635068276, 0.014746047710329236, 0.014920375707904745, 0.015283354123336167, 0.014739174228803791, 0.015388396375360254, 0.013937532767746265, 0.014254484782376335, 0.014618400025988208, 0.014902608404680997, 0.014833027930796576, 0.015182240801339663, 0.014968117416552995, 0.01570299075820094, 0.015588643713110457, 0.015381825586730215, 0.014473517488812279, 0.01532562762168388, 0.015391513419099322, 0.014531846742175199, 0.014731574591428782, 0.015248556472914513, 0.014798850507742699, 0.014958402079922772, 0.015207538451487706, 0.014442171437190758, 0.015418919437062169, 0.014685236705277625, 0.01517033469938602, 0.015001188740057793, 0.015387439569165492, 0.015376287942920367, 0.01486065395913649, 0.015032748381140862, 0.01427196912243759, 0.014909144678576148, 0.014118652861903495, 0.015085858280153993, 0.015324957767211031, 0.015615485943361117, 0.014971318183959591, 0.01530290997385993, 0.014876218893273728, 0.014468565118137284, 0.014866534663977922, 0.015266492041733139, 0.014923585041829678, 0.015158040748307868, 0.014400970678099709, 0.015051591293911837, 0.015132920928708396, 0.015649903022893936, 0.015152207490648894, 0.014900134424429695, 0.015470267097070904, 0.015132564970154366, 0.015131547507126706, 0.015184410083470875, 0.014802852798504301, 0.015242562419844608, 0.015075793344637222, 0.014772474958876675, 0.014700619557149104, 0.014313556490232165, 0.014500274636828545, 0.015164809911423185, 0.014939300262819935, 0.01539654371271087, 0.015466078421554083, 0.014886685114017181, 0.0153252619114452, 0.014803276902112384, 0.01524567080653351, 0.01478224687597397, 0.014876697126654867, 0.014472214789875721, 0.015277986377352224, 0.015603723914923146, 0.015508486223377312, 0.015120559549622877, 0.015011991598720674, 0.014736658169769072, 0.0149514442737042, 0.015103945642897673, 0.015665661224995377, 0.015601827197616448, 0.015514720924351045, 0.014827511471600354, 0.015256503616001187, 0.014697144980323228, 0.014926970450041042, 0.014369509831317776, 0.014369384898314111, 0.014723247001851993, 0.015081938343560605, 0.014621598665384558, 0.015250936747823993, 0.015136619589560722, 0.0149857695307138, 0.013992759131016207, 0.015115170868101345, 0.014422054477433065, 0.014981840470143773, 0.015050182157242147, 0.014590078304145855, 0.014457726997625717, 0.015508422266464972, 0.014847617441993487, 0.014760166392894108, 0.015395728280383603, 0.01515236437404542, 0.014937878461829378, 0.015180594535965672, 0.01484545863732001, 0.015018715471103979, 0.015007531840485455, 0.014509750730439605, 0.014931515310451162, 0.01477217763883414], "accuracy_test_std": 0.01259344240524133, "error_valid": [0.8135603939194277, 0.7750141189759037, 0.7630306381777108, 0.7526546616152108, 0.7425934205572289, 0.7328277955572289, 0.7275787721197289, 0.7154629259224398, 0.707141554499247, 0.7000202960278614, 0.6925328266189759, 0.6801213643637049, 0.6719220632530121, 0.6665818547628012, 0.6576810170368976, 0.6564294286521084, 0.6524011083396084, 0.6500714773155121, 0.6458799063441265, 0.6431840643825302, 0.6392881094691265, 0.6388101233057228, 0.6369378882718373, 0.6325227668486446, 0.6320241905120482, 0.6304269813629518, 0.6287077019013554, 0.627842914627259, 0.6250558876129518, 0.624180805252259, 0.6240690300263554, 0.623082172439759, 0.622105609939759, 0.6202951454254518, 0.6221159050263554, 0.6190435570406627, 0.6195730186370482, 0.620518695877259, 0.6174772331513554, 0.618565570877259, 0.618199359939759, 0.6183317253388554, 0.6186773461031627, 0.6172433876129518, 0.6179861045745482, 0.6172330925263554, 0.6177316688629518, 0.6167551063629518, 0.6178949195218373, 0.6180978798004518, 0.6165109657379518, 0.6172536826995482, 0.6163888954254518, 0.6171316123870482, 0.6165109657379518, 0.6171213173004518, 0.6161653449736446, 0.6168874717620482, 0.6171419074736446, 0.6159109092620482, 0.615321147872741, 0.6164094855986446, 0.6155446983245482, 0.6173654579254518, 0.6160329795745482, 0.6173860480986446, 0.6145681358245482, 0.6154020378388554, 0.6165109657379518, 0.6165315559111446, 0.6163991905120482, 0.6165315559111446, 0.614048969314759, 0.6166536262236446, 0.6165109657379518, 0.6138460090361446, 0.6156770637236446, 0.617030132247741, 0.6170198371611446, 0.6160329795745482, 0.6155549934111446, 0.6149343467620482, 0.6154020378388554, 0.6161447548004518, 0.6169992469879518, 0.6155446983245482, 0.6138357139495482, 0.6144460655120482, 0.6151681923004518, 0.6168771766754518, 0.613804828689759, 0.6160329795745482, 0.6145578407379518, 0.6143137001129518, 0.6159212043486446, 0.6149446418486446, 0.6160432746611446, 0.616908061935241, 0.6164094855986446, 0.6157785438629518, 0.6150564170745482, 0.6159006141754518, 0.615513813064759, 0.6150461219879518, 0.6155446983245482, 0.6162771201995482, 0.6161550498870482, 0.6161550498870482, 0.6151681923004518, 0.6167654014495482, 0.6144460655120482, 0.6164094855986446, 0.6162771201995482, 0.6155344032379518, 0.6156770637236446, 0.616490375564759, 0.6155446983245482, 0.6166433311370482, 0.6140592644013554, 0.6157888389495482, 0.616053569747741, 0.6162771201995482, 0.6154020378388554, 0.6153108527861446, 0.6165212608245482, 0.6163991905120482, 0.6157785438629518, 0.6148225715361446, 0.6157785438629518, 0.6159006141754518, 0.6154329230986446, 0.6149240516754518, 0.6169992469879518, 0.616663921310241, 0.6135812782379518, 0.6156564735504518, 0.6162874152861446, 0.6160226844879518, 0.6156770637236446, 0.6157991340361446, 0.6148225715361446, 0.6171213173004518, 0.6168977668486446, 0.6154329230986446, 0.6141916298004518, 0.6129709266754518, 0.6150461219879518, 0.6155344032379518, 0.6152902626129518, 0.6159212043486446, 0.6157785438629518, 0.6183420204254518, 0.6150667121611446, 0.6155446983245482, 0.6159006141754518, 0.6153005576995482, 0.6148019813629518, 0.6167654014495482, 0.6162565300263554, 0.6156667686370482, 0.6155446983245482, 0.6157888389495482, 0.6148225715361446, 0.6140901496611446, 0.6169992469879518, 0.6167551063629518, 0.6165212608245482, 0.6170198371611446, 0.613682758377259, 0.6167551063629518, 0.6164094855986446, 0.6145475456513554, 0.6164094855986446, 0.6170095420745482, 0.615391742752259, 0.6149343467620482, 0.6157682487763554, 0.6145681358245482, 0.6146799110504518, 0.6154329230986446, 0.6111295769013554, 0.6160226844879518, 0.6163888954254518, 0.6149137565888554, 0.6156667686370482, 0.6149137565888554, 0.6145578407379518, 0.615147602127259, 0.6153005576995482, 0.6155344032379518, 0.615565288497741, 0.6176301887236446, 0.6145681358245482, 0.6156564735504518, 0.6163991905120482, 0.615269672439759, 0.6168977668486446, 0.616908061935241, 0.6166433311370482, 0.6151784873870482, 0.6162668251129518, 0.6155446983245482, 0.6157991340361446, 0.6159109092620482, 0.616908061935241, 0.6153005576995482, 0.6160329795745482, 0.6168874717620482, 0.6164094855986446, 0.616908061935241, 0.6177316688629518, 0.6162874152861446, 0.616908061935241, 0.6166227409638554, 0.6157682487763554, 0.616663921310241, 0.616175640060241, 0.6164094855986446, 0.6154123329254518, 0.6167756965361446, 0.6160329795745482, 0.6129400414156627, 0.6181184699736446, 0.6156564735504518, 0.6160432746611446, 0.6155344032379518, 0.6170095420745482, 0.6150564170745482, 0.6165109657379518, 0.6148019813629518, 0.6137033485504518], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "momentum": 0.6312137149209744, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.003995680195026005, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "l2_decay": 8.108201233965678e-07, "optimization": "adadelta", "nb_data_augmentation": 1, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.08732102354315714}, "accuracy_valid_max": 0.3888704230986446, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        #nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        nb_data_augmentation=make_constant_param(1),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.38629665144954817, "loss_train": [2.3019721508026123, 2.1792707443237305, 2.091383934020996, 2.033121109008789, 1.990112066268921, 1.9559520483016968, 1.927566647529602, 1.9032044410705566, 1.882979393005371, 1.8654282093048096, 1.8492268323898315, 1.8359456062316895, 1.82437002658844, 1.8135632276535034, 1.8031611442565918, 1.7952218055725098, 1.7886170148849487, 1.7808560132980347, 1.7743552923202515, 1.768117070198059, 1.7640291452407837, 1.7597424983978271, 1.753990650177002, 1.7511461973190308, 1.7481316328048706, 1.7451943159103394, 1.7420494556427002, 1.7397202253341675, 1.737575650215149, 1.7357748746871948, 1.7341190576553345, 1.7315285205841064, 1.730132818222046, 1.728495478630066, 1.726867914199829, 1.7257620096206665, 1.725485920906067, 1.724586009979248, 1.722378134727478, 1.7229373455047607, 1.7222751379013062, 1.7208622694015503, 1.720380187034607, 1.7195804119110107, 1.719140648841858, 1.7189879417419434, 1.7187734842300415, 1.718455195426941, 1.7180883884429932, 1.7172493934631348, 1.7166694402694702, 1.7160981893539429, 1.7170222997665405, 1.7154244184494019, 1.7147011756896973, 1.7163126468658447, 1.7161158323287964, 1.7153186798095703, 1.7158277034759521, 1.7149620056152344, 1.7158584594726562, 1.714959740638733, 1.7141252756118774, 1.71592378616333, 1.7149630784988403, 1.7156709432601929, 1.7152578830718994, 1.7151024341583252, 1.7152613401412964, 1.7141565084457397, 1.7142616510391235, 1.7134037017822266, 1.7148200273513794, 1.715329647064209, 1.7135705947875977, 1.7142908573150635, 1.714431643486023, 1.7153388261795044, 1.7136156558990479, 1.7140991687774658, 1.7141921520233154, 1.7143055200576782, 1.7134053707122803, 1.714715600013733, 1.714358925819397, 1.7138715982437134, 1.7145074605941772, 1.7146073579788208, 1.7140345573425293, 1.7154003381729126, 1.7145379781723022, 1.7138618230819702, 1.714561104774475, 1.7138876914978027, 1.7145822048187256, 1.714263916015625, 1.714900255203247, 1.7140485048294067, 1.7146741151809692, 1.714205265045166, 1.7146077156066895, 1.7141249179840088, 1.713329553604126, 1.7130837440490723, 1.7147127389907837, 1.7138950824737549, 1.7140045166015625, 1.7145694494247437, 1.7144078016281128, 1.7137471437454224, 1.7129582166671753, 1.714212417602539, 1.71345055103302, 1.7135605812072754, 1.713154911994934, 1.7133036851882935, 1.7146095037460327, 1.7141304016113281, 1.7141419649124146, 1.7162318229675293, 1.714134693145752, 1.714231252670288, 1.7139334678649902, 1.7135330438613892, 1.7146788835525513, 1.7145448923110962, 1.7148175239562988, 1.714239478111267, 1.7146389484405518, 1.714632272720337, 1.71516752243042, 1.714234709739685, 1.713923692703247, 1.7146859169006348, 1.7140439748764038, 1.7152756452560425, 1.7142348289489746, 1.7140213251113892, 1.7138495445251465, 1.7137454748153687, 1.7150206565856934, 1.713827133178711, 1.7135448455810547, 1.7141051292419434, 1.7136611938476562, 1.7138283252716064, 1.7137839794158936, 1.7146683931350708, 1.7135343551635742, 1.7147196531295776, 1.7139610052108765, 1.7141454219818115, 1.7135968208312988, 1.7142854928970337, 1.7153632640838623, 1.7143456935882568, 1.713922381401062, 1.7141904830932617, 1.7149531841278076, 1.7140613794326782, 1.7142819166183472, 1.713789701461792, 1.713966727256775, 1.7130168676376343, 1.7134405374526978, 1.714117169380188, 1.7142695188522339, 1.7141904830932617, 1.714877724647522, 1.7136907577514648, 1.7149337530136108, 1.7132184505462646, 1.7148594856262207, 1.7142442464828491, 1.7139960527420044, 1.713208556175232, 1.7143069505691528, 1.7151315212249756, 1.713998794555664, 1.7131966352462769, 1.7128539085388184, 1.7143962383270264, 1.7141928672790527, 1.714751958847046, 1.7152409553527832, 1.715090036392212, 1.715104103088379, 1.713713526725769, 1.7142640352249146, 1.7140297889709473, 1.7140341997146606, 1.714734673500061, 1.713940978050232, 1.713024616241455, 1.7136244773864746, 1.7142469882965088, 1.7138543128967285, 1.714071273803711, 1.7141872644424438, 1.71452796459198, 1.7153023481369019, 1.7139607667922974, 1.714097023010254, 1.7133747339248657, 1.7131102085113525, 1.7149487733840942, 1.7142126560211182, 1.7147666215896606, 1.7132681608200073, 1.7139925956726074, 1.7143205404281616, 1.7142962217330933, 1.7145559787750244, 1.7140860557556152, 1.7142739295959473, 1.7137033939361572, 1.7147343158721924, 1.7142783403396606, 1.7151283025741577, 1.7137829065322876, 1.7135411500930786, 1.7140206098556519, 1.714011311531067, 1.7146879434585571, 1.713850975036621, 1.7138657569885254, 1.7145825624465942, 1.7134965658187866, 1.714693546295166, 1.7139461040496826, 1.7136584520339966], "accuracy_train_first": 0.18127761339516427, "model": "residualv5", "loss_std": [0.0653827115893364, 0.05892987176775932, 0.06052771583199501, 0.06315469741821289, 0.06523215025663376, 0.06578559428453445, 0.06652963906526566, 0.06782618910074234, 0.06875121593475342, 0.0688423365354538, 0.06946791708469391, 0.0699397549033165, 0.07055506855249405, 0.07100868225097656, 0.07094071805477142, 0.07148825377225876, 0.0724320188164711, 0.07259668409824371, 0.0728047788143158, 0.07311604171991348, 0.07304713129997253, 0.07322507351636887, 0.07427088916301727, 0.07354868948459625, 0.07488121092319489, 0.0751042366027832, 0.07508682459592819, 0.07535476237535477, 0.07516385614871979, 0.07575526088476181, 0.07452882081270218, 0.07532878965139389, 0.07599562406539917, 0.07564452290534973, 0.07590416818857193, 0.07552817463874817, 0.07619182020425797, 0.07643058151006699, 0.07616070657968521, 0.07586898654699326, 0.07607384771108627, 0.07688360661268234, 0.0762074664235115, 0.07645878195762634, 0.07622949779033661, 0.07622770965099335, 0.07619798928499222, 0.0778297409415245, 0.07684633135795593, 0.07765047997236252, 0.07718376070261002, 0.0770595371723175, 0.07638727128505707, 0.07653667032718658, 0.07639236748218536, 0.0763496607542038, 0.07716406881809235, 0.07625327259302139, 0.07635010778903961, 0.07529949396848679, 0.07729510962963104, 0.07651454210281372, 0.07679546624422073, 0.0763690397143364, 0.07699628919363022, 0.07563112676143646, 0.07732394337654114, 0.07568900287151337, 0.07655477523803711, 0.07638543844223022, 0.07653127610683441, 0.0771980881690979, 0.07667798548936844, 0.07727153599262238, 0.07754495739936829, 0.07629457116127014, 0.07764068990945816, 0.07744121551513672, 0.07590875029563904, 0.07645942270755768, 0.07674026489257812, 0.07567265629768372, 0.0763515830039978, 0.07667182385921478, 0.07705719769001007, 0.07638028264045715, 0.0771656334400177, 0.07739277929067612, 0.07661540061235428, 0.07616078853607178, 0.07670393586158752, 0.07601292431354523, 0.07583508640527725, 0.07735226303339005, 0.0769062340259552, 0.07667621225118637, 0.07672152668237686, 0.07619757950305939, 0.0765937939286232, 0.07610808312892914, 0.07649136334657669, 0.07643460482358932, 0.07656095921993256, 0.07673650234937668, 0.07607243955135345, 0.0769507884979248, 0.07658455520868301, 0.07667937874794006, 0.07798309624195099, 0.07657448947429657, 0.07652697712182999, 0.07589265704154968, 0.0762702003121376, 0.07717254757881165, 0.07729728519916534, 0.07689634710550308, 0.07663631439208984, 0.07624904066324234, 0.07609274983406067, 0.07700929790735245, 0.07669251412153244, 0.0767923966050148, 0.07615182548761368, 0.07645229995250702, 0.07687488198280334, 0.07586419582366943, 0.07776963710784912, 0.07668934017419815, 0.07655399292707443, 0.07681475579738617, 0.07728683948516846, 0.07567929476499557, 0.07663276791572571, 0.07719775289297104, 0.07625853270292282, 0.0765826553106308, 0.07577316462993622, 0.07718949764966965, 0.07684933394193649, 0.07717283815145493, 0.07736674696207047, 0.07705732434988022, 0.07651091367006302, 0.07646127045154572, 0.07580184191465378, 0.0765627920627594, 0.07685839384794235, 0.07704582810401917, 0.07630926370620728, 0.07724550366401672, 0.07696940004825592, 0.07681978493928909, 0.07621631771326065, 0.07738999277353287, 0.0762048065662384, 0.07700324058532715, 0.07686738669872284, 0.07581043988466263, 0.07603398710489273, 0.07716389000415802, 0.07735441625118256, 0.07710067182779312, 0.07715244591236115, 0.0769648477435112, 0.07617682963609695, 0.07720962911844254, 0.07677305489778519, 0.0770772323012352, 0.0766548290848732, 0.0766875296831131, 0.07669119536876678, 0.0767505094408989, 0.07700858265161514, 0.0762421265244484, 0.0769711434841156, 0.07643967121839523, 0.07659024745225906, 0.07597173750400543, 0.07638531178236008, 0.07586176693439484, 0.07637296617031097, 0.07729015499353409, 0.07680418342351913, 0.07707168161869049, 0.07602304965257645, 0.0772097036242485, 0.07727893441915512, 0.07716672867536545, 0.07652776688337326, 0.07675044238567352, 0.07605394721031189, 0.07708035409450531, 0.07733042538166046, 0.07668264955282211, 0.07718371599912643, 0.07747452706098557, 0.07674220949411392, 0.0766783282160759, 0.07559513300657272, 0.07624581456184387, 0.07587513327598572, 0.0763169527053833, 0.07684993743896484, 0.07667052000761032, 0.07612405717372894, 0.07656586170196533, 0.07694846391677856, 0.0772123709321022, 0.07685641944408417, 0.07655614614486694, 0.07650285959243774, 0.07588344067335129, 0.07686144858598709, 0.07614847272634506, 0.07679532468318939, 0.07700788974761963, 0.0770614743232727, 0.07671293616294861, 0.07695508003234863, 0.0775604322552681, 0.07656709849834442, 0.07702381163835526, 0.07647741585969925, 0.07728270441293716, 0.07738472521305084, 0.07655888795852661, 0.07719527184963226, 0.07672541588544846, 0.07634555548429489, 0.07632540166378021, 0.0769515186548233]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:17 2016", "state": "available"}], "summary": "8d4f830a947405ec156d16cf5877afd9"}