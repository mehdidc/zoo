{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 16, "f3": 64, "nonlin": "very_leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.531936764717102, 1.0421420335769653, 0.8535309433937073, 0.7321398258209229, 0.6382825970649719, 0.5696120858192444, 0.512660801410675, 0.4629637897014618, 0.4188096523284912, 0.3783055245876312, 0.3446454405784607, 0.31132057309150696, 0.286216676235199, 0.2535167336463928, 0.23639586567878723, 0.2114429473876953, 0.19706836342811584, 0.17958515882492065, 0.16310523450374603, 0.15768344700336456, 0.1349485218524933, 0.13106729090213776, 0.12320207804441452, 0.11219317466020584, 0.11513024568557739, 0.10060011595487595, 0.10366680473089218, 0.09348701685667038, 0.09295200556516647, 0.08891484886407852, 0.08448509871959686, 0.07540835440158844, 0.08696121722459793, 0.07519567757844925, 0.07634428143501282, 0.07219981402158737, 0.07209650427103043, 0.0748286023736, 0.06503251940011978, 0.06138328090310097, 0.06971141695976257, 0.0664355531334877, 0.061940692365169525, 0.06070664897561073, 0.06084759533405304, 0.05997907742857933, 0.047968603670597076, 0.0688827708363533, 0.04993841424584389, 0.056041471660137177, 0.06270093470811844, 0.05557722970843315, 0.04859521612524986, 0.05746292322874069, 0.04717035964131355, 0.051596447825431824, 0.06066295504570007, 0.04550161212682724, 0.052706681191921234, 0.05462150275707245, 0.047244369983673096, 0.046170491725206375, 0.05276511237025261, 0.04863506183028221, 0.042144741863012314, 0.048675891011953354, 0.050658468157052994, 0.04482228681445122, 0.04364481568336487, 0.048511143773794174, 0.046791329979896545, 0.042481403797864914, 0.04729995131492615, 0.046237774193286896, 0.0449284091591835, 0.04309164732694626, 0.04674050584435463, 0.04262828826904297, 0.03788917139172554, 0.051958270370960236, 0.03992274031043053, 0.04411065950989723, 0.04248049110174179, 0.043265193700790405, 0.03836029767990112, 0.04376900568604469, 0.04643673822283745, 0.034931499511003494, 0.048528674989938736, 0.04393177479505539, 0.027510566636919975, 0.019275404512882233, 0.018549391999840736, 0.018295548856258392, 0.01810964196920395, 0.017951037734746933, 0.01780083030462265, 0.017646288499236107, 0.01747571863234043, 0.017277266830205917, 0.01703754998743534, 0.016741223633289337, 0.01637028343975544, 0.015904340893030167, 0.015321493148803711, 0.014600618742406368, 0.01413937471807003, 0.014079933986067772, 0.014072173275053501, 0.014071273617446423, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.0140712670981884, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.0140712670981884, 0.014071265235543251, 0.0140712670981884, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.0140712670981884, 0.0140712670981884, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251, 0.014071265235543251], "moving_avg_accuracy_train": [0.0511048241971207, 0.10983355539405683, 0.16993574874838496, 0.22843809747312194, 0.28272897260145574, 0.3338177480930174, 0.3799184456270619, 0.4237267420873347, 0.46520241266103934, 0.5035627380535271, 0.5394702421067016, 0.5719147347438074, 0.6012449504017172, 0.6275700648807407, 0.6528948142296914, 0.6759149532270804, 0.6955797498651973, 0.7138711960788438, 0.7332420064901085, 0.7497782644685894, 0.7656395680051192, 0.7808657991487288, 0.7947111331089021, 0.8059304123552286, 0.8159930306400546, 0.8252053523107298, 0.8343332790928536, 0.844450168630043, 0.8532975300373875, 0.8604673516575875, 0.8670037522776353, 0.8718615187476846, 0.8771701470039608, 0.883473210053657, 0.8890714178412038, 0.8934658107273953, 0.8983322947559385, 0.9023423956720944, 0.9067049789061954, 0.9118306480167755, 0.9149211742830659, 0.9188046603119852, 0.92380169967369, 0.927854967725424, 0.930710033227937, 0.9333493466444843, 0.9363176056169868, 0.9397563738482007, 0.9430000387312839, 0.9454310919248775, 0.948004834257399, 0.9506165683531154, 0.9528392458547362, 0.9542909565359662, 0.9559043797431116, 0.9555500362974549, 0.9582024510820228, 0.9596107727881431, 0.961331594243871, 0.9625198994397313, 0.9637427978886153, 0.9634043917212469, 0.9639692876301024, 0.9654775439849862, 0.966530380210334, 0.9678475954250425, 0.9685936359932803, 0.9696719374975422, 0.9702239181144824, 0.9709182662209005, 0.9719198896726384, 0.9726655297601458, 0.9727391146924922, 0.9726681934006608, 0.97206732696065, 0.9732354233931841, 0.9736636423991499, 0.9747789920354346, 0.975073636321186, 0.9760619735069431, 0.9766120412967527, 0.9768932246659239, 0.9773229289100642, 0.9777329142178858, 0.9786947418437162, 0.9785769208581725, 0.979396255148555, 0.9804986683241756, 0.9805096634334339, 0.9808893297901182, 0.9825260292515826, 0.9841060156121386, 0.9855512548247343, 0.9868705713065467, 0.9880788824794634, 0.9891826385767551, 0.9901876448083653, 0.9911014510120526, 0.9919355023394189, 0.992693123980477, 0.9933773086062389, 0.994000050215853, 0.9945605176645058, 0.9950672635171028, 0.9955279850820592, 0.9959403093417104, 0.9963114011753964, 0.9966453838257139, 0.9969459682109996, 0.9972164941577567, 0.9974599675098381, 0.9976790935267114, 0.9978763069418973, 0.9980537990155647, 0.9982135418818653, 0.9983573104615359, 0.9984867021832393, 0.9986031547327725, 0.9987079620273523, 0.9988022885924742, 0.9988871825010839, 0.9989635870188326, 0.9990323510848064, 0.9990942387441828, 0.9991499376376216, 0.9992000666417166, 0.999245182745402, 0.9992857872387189, 0.9993223312827041, 0.9993552209222908, 0.9993848215979187, 0.999411462205984, 0.9994354387532427, 0.9994570176457755, 0.9994764386490551, 0.9994939175520067, 0.9995096485646631, 0.9995238064760539, 0.9995365485963056, 0.9995480165045321, 0.9995583376219359, 0.9995676266275995, 0.9995759867326967, 0.999583510827284, 0.9995902825124127, 0.9995963770290285, 0.9996018620939827, 0.9996067986524415, 0.9996112415550544, 0.9996152401674061, 0.9996188389185225, 0.9996220777945274, 0.9996249927829317, 0.9996276162724956, 0.9996299774131031, 0.9996321024396498, 0.999634014963542], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.05090346738516566, 0.1084103356786521, 0.16701804434535011, 0.22366020169545364, 0.27579884078795647, 0.3238487218635283, 0.3672717198296604, 0.40715705255301965, 0.444835196130549, 0.4778492645257772, 0.5082506673747809, 0.5353922033763842, 0.5597991426772999, 0.5808305941851724, 0.6001637915908268, 0.6188586440770755, 0.633155979264624, 0.6465017141694869, 0.660607484488231, 0.6714857420635043, 0.6829517727611448, 0.6933748866183135, 0.7029693856729731, 0.7101669465879801, 0.7168177088663056, 0.7224076223735003, 0.7283470123857738, 0.7344900384627084, 0.7402049559266786, 0.7442954884307728, 0.7487807198023039, 0.7515518677486548, 0.7546562524628706, 0.7586098666744148, 0.7615749754041872, 0.7640544378129401, 0.7664547934009985, 0.7680494719581126, 0.7702059270119248, 0.7733533207094523, 0.7747840783881306, 0.7770739134427663, 0.7808437493669385, 0.783521387325576, 0.7854928378720094, 0.7867391524940706, 0.788164981926516, 0.790088141584015, 0.7918036897185352, 0.7935878527026304, 0.7947347326714487, 0.7959713556486563, 0.7970772568401913, 0.7979281425548921, 0.7990174995625805, 0.7982552568540635, 0.8000472657601481, 0.8005473219872207, 0.8008354455866914, 0.8017764386315613, 0.8022265303199866, 0.8021596566554579, 0.8018257681265837, 0.8025049194765759, 0.8033308817834364, 0.8040883668355144, 0.8046033229795232, 0.8050322214327004, 0.8047712573843099, 0.8053755863710295, 0.8058787433306887, 0.8067283866463397, 0.806210297840516, 0.8061367019325939, 0.8056616036049219, 0.8071993471412671, 0.807267015966297, 0.8082240316607066, 0.8085186748048769, 0.8093037555077477, 0.8094996918451507, 0.8095976448180151, 0.8101465811551142, 0.8106844515128708, 0.8118837492079692, 0.8111166198161633, 0.8116419060179054, 0.8128522330177715, 0.8131124787013106, 0.8127860588149899, 0.8144312869413975, 0.8160361215849837, 0.8177144653752806, 0.8193745476875267, 0.8209174498935481, 0.8223548900039673, 0.8237106507682543, 0.8249186284248626, 0.8259681577134005, 0.8269249411043346, 0.8278470813124252, 0.8287136285934568, 0.8294691070838851, 0.8301856588190207, 0.8308793835056427, 0.8314427005673525, 0.8320351351416414, 0.8325683262585014, 0.8330481982636754, 0.833480083068332, 0.8338687793925229, 0.8342186060842948, 0.8345334501068894, 0.8348168097272246, 0.8350718333855263, 0.8353013546779978, 0.8355079238412222, 0.8356938360881241, 0.8358611571103358, 0.8360117460303264, 0.8361472760583178, 0.8362692530835102, 0.8363790324061833, 0.8364778337965891, 0.8365667550479543, 0.836646784174183, 0.8367188103877888, 0.8367836339800341, 0.8368419752130548, 0.8368944823227734, 0.8369417387215202, 0.8369842694803923, 0.8370225471633772, 0.8370569970780636, 0.8370880020012813, 0.8371159064321773, 0.8371410204199836, 0.8371636230090094, 0.8371839653391325, 0.8372022734362434, 0.8372187507236433, 0.8372335802823031, 0.8372469268850969, 0.8372589388276114, 0.8372697495758744, 0.8372794792493111, 0.8372882359554041, 0.8372961169908879, 0.8373032099228233, 0.8373095935615651, 0.8373153388364327, 0.8373205095838135, 0.8373251632564563, 0.8373293515618349, 0.8373331210366755, 0.837336513564032, 0.837339566838653], "moving_var_accuracy_train": [0.023505327505967528, 0.05219636956738859, 0.07948719542465918, 0.10234119913898988, 0.1186345713248939, 0.13026168102344926, 0.13636298173923345, 0.13999918511407045, 0.1414813478485097, 0.1405768441416166, 0.13812329935341358, 0.1337847753403851, 0.12814865176120205, 0.12157089145608511, 0.1151858886767612, 0.10843664100421703, 0.10107331494516181, 0.09397717649192609, 0.08795651350663596, 0.0816218926073502, 0.07572393189551657, 0.07023808173951254, 0.06493951301777984, 0.059578411757265325, 0.05453187716225382, 0.04984249128110437, 0.04560811357905225, 0.04196846530631625, 0.03847610101053415, 0.035091147988070115, 0.03196655398685496, 0.028982279643867286, 0.026337685485150565, 0.02406147437091152, 0.021937386307713316, 0.019917443876485884, 0.018138843490037894, 0.01646968732525389, 0.014994007784998634, 0.013731059360979172, 0.012443915598304928, 0.01133525721210574, 0.010426465112337009, 0.009531679438196177, 0.008651874085589316, 0.007849380454827288, 0.007143737461295133, 0.00653578985769767, 0.005976903128791629, 0.005432402992583204, 0.004948780039672802, 0.004515292430586068, 0.0041082258450133675, 0.0037163704356300057, 0.003368161602075202, 0.003032475475365, 0.00279254566553305, 0.002531141429231113, 0.002304678324650445, 0.0020869191153319758, 0.0018916865293453246, 0.0017035485450178087, 0.0015360656570066016, 0.0014029326263943682, 0.0012726155408115732, 0.0011609694900271538, 0.0010498817297895466, 0.0009553581640174325, 0.0008625644910289877, 0.0007806471155620668, 0.0007116116498575029, 0.0006454542971326336, 0.0005809576000997862, 0.0005229071085565228, 0.00047386576200945073, 0.000438759229289796, 0.0003965336500144497, 0.0003680763283134487, 0.00033205003277823783, 0.00030763632303516445, 0.00027959586189212185, 0.0002523478524867957, 0.0002287748788750063, 0.00020741018256117152, 0.00019499517574135053, 0.00017562059422892575, 0.00016410031292060184, 0.00015862811491658047, 0.00014276639145677084, 0.0001297870711926751, 0.0001409174302178272, 0.0001492928992919323, 0.00015316205679735621, 0.00015351121493025431, 0.0001513002364525883, 0.00014713471050810796, 0.00014151157718747512, 0.00013487579546980386, 0.00012764899047295486, 0.00012005000638465563, 0.00011225798316534964, 0.00010452244885991874, 9.689731782292118e-05, 8.951870827274774e-05, 8.247721668921567e-05, 7.575959667616645e-05, 6.942301934980644e-05, 6.348461711124372e-05, 5.794931415421788e-05, 5.281304132961594e-05, 4.8065250655218445e-05, 4.369087149113355e-05, 3.967182252218414e-05, 3.598817119589854e-05, 3.261901412631428e-05, 2.9543137354187403e-05, 2.6739503577577362e-05, 2.4187603986454686e-05, 2.1867704708783626e-05, 1.976101174589454e-05, 1.7849773352776404e-05, 1.611733487049054e-05, 1.4548157854364835e-05, 1.3127812810376274e-05, 1.184295282991147e-05, 1.0681273800384314e-05, 9.631465585651719e-06, 8.683157550984279e-06, 7.826861000243017e-06, 7.053910455748021e-06, 6.356405210151961e-06, 5.72715218711955e-06, 5.1596108417736515e-06, 4.647840595022793e-06, 4.186451113835985e-06, 3.7705556108879197e-06, 3.395727232631912e-06, 3.0579585274632773e-06, 2.753623929373541e-06, 2.479445152708028e-06, 2.232459366617418e-06, 2.0099900005916334e-06, 1.809620022747595e-06, 1.6291675284670876e-06, 1.4666634770957243e-06, 1.3203314175811826e-06, 1.1885690492610399e-06, 1.0699314708196968e-06, 9.631159781903839e-07, 8.669482804779979e-07, 7.803700115165869e-07, 7.024274232249035e-07, 6.322611553189935e-07, 5.690969840645246e-07, 5.122374605227912e-07, 4.610543561109348e-07, 4.1498184022858394e-07], "duration": 54410.105511, "accuracy_train": [0.5110482419712071, 0.638392136166482, 0.7108554889373385, 0.7549592359957549, 0.77134684875646, 0.7936167275170728, 0.7948247234334626, 0.8180014102297897, 0.8384834478243817, 0.8488056665859173, 0.8626377785852714, 0.8639151684777593, 0.8652168913229051, 0.8644960951919527, 0.8808175583702473, 0.8830962042035806, 0.8725629196082503, 0.8784942120016611, 0.9075793001914912, 0.8986045862749169, 0.9083912998338871, 0.9179018794412146, 0.9193191387504615, 0.9069039255721669, 0.9065565952034883, 0.908116247346807, 0.9164846201319674, 0.9355021744647471, 0.9329237827034883, 0.9249957462393872, 0.9258313578580657, 0.9155814169781286, 0.9249478013104466, 0.9402007775009228, 0.9394552879291252, 0.9330153467031194, 0.9421306510128276, 0.9384333039174971, 0.9459682280131044, 0.9579616700119971, 0.9427359106796788, 0.9537560345722591, 0.968775053929033, 0.96433438019103, 0.9564056227505537, 0.957103167393411, 0.9630319363695091, 0.9707052879291252, 0.972193022679033, 0.9673105706672205, 0.9711685152500923, 0.9741221752145626, 0.9728433433693245, 0.9673563526670359, 0.9704251886074198, 0.9523609452865448, 0.9820741841431341, 0.9722856681432264, 0.9768189873454227, 0.9732146462024732, 0.9747488839285714, 0.9603587362149317, 0.9690533508098007, 0.9790518511789406, 0.9760059062384644, 0.9797025323574198, 0.9753080011074198, 0.9793766510358989, 0.9751917436669435, 0.9771673991786637, 0.9809345007382798, 0.9793762905477114, 0.9734013790836102, 0.9720299017741787, 0.9666595290005537, 0.9837482912859912, 0.9775176134528424, 0.9848171387619971, 0.9777254348929494, 0.984957008178756, 0.9815626514050388, 0.9794238749884644, 0.9811902671073275, 0.9814227819882798, 0.9873511904761905, 0.9775165319882798, 0.9867702637619971, 0.9904203869047619, 0.9806086194167589, 0.9843063270002769, 0.9972563244047619, 0.9983258928571429, 0.9985584077380952, 0.9987444196428571, 0.9989536830357143, 0.9991164434523809, 0.9992327008928571, 0.9993257068452381, 0.9994419642857143, 0.99951171875, 0.9995349702380952, 0.9996047247023809, 0.9996047247023809, 0.9996279761904762, 0.9996744791666666, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714, 0.9996512276785714], "end": "2016-02-02 00:54:54.856000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0], "moving_var_accuracy_valid": [0.02332046692649362, 0.050751779342163836, 0.07659037304439242, 0.09780634164341817, 0.11249164665684072, 0.122021601633546, 0.12678945224149182, 0.1284280649154203, 0.12836204095491852, 0.12533519526746995, 0.12111988339740959, 0.11563786184640555, 0.10943536383611228, 0.10247272502525295, 0.0955894052200617, 0.08917594228339891, 0.08209807219624503, 0.07549124273797841, 0.06973287327074704, 0.06382461433453826, 0.05862538164071755, 0.053740615198961304, 0.04919504338805395, 0.044741782997375676, 0.04066569844758329, 0.03688035279998623, 0.03350980470344865, 0.03049845515754088, 0.02774255217636671, 0.0251188890642335, 0.022788055861915655, 0.020578363624189175, 0.01860726210185497, 0.016887215479672993, 0.015277620759720046, 0.013805188288275812, 0.012476524821990341, 0.01125175933709598, 0.010168436088978392, 0.00924074726386567, 0.008335096145294974, 0.007548776631962424, 0.006921803934822793, 0.00629415124667834, 0.005699715677323798, 0.0051437238108258876, 0.004647648335877145, 0.004216170389903516, 0.0038210412995298656, 0.0034675863075612208, 0.0031326656797709884, 0.0028331622392837085, 0.002560853172364284, 0.002311283913623192, 0.002090835810472674, 0.0018869813549455938, 0.001727184882726414, 0.0015567169005258788, 0.0014017923473504386, 0.0012695823238098388, 0.00114444733418076, 0.0010300428495457513, 0.0009280418985385996, 0.0008393889276905062, 0.0007615899585126442, 0.0006905950150984748, 0.0006239221320608991, 0.0005631855038030493, 0.0005074798735337158, 0.00046001880789805024, 0.0004162954294427257, 0.0003811629303729272, 0.00034546238143211326, 0.00031096489050786786, 0.00028189986724569204, 0.0002749917771732647, 0.00024753381108486656, 0.000231023341330494, 0.00020870233843910368, 0.00019337926998537334, 0.00017438686242166992, 0.0001570345292435398, 0.00014404305623887576, 0.00013224249131076762, 0.0001319630768329071, 0.0001240631566835696, 0.00011414017135887834, 0.00011591017724243399, 0.00010492870986039728, 9.53947883540289e-05, 0.00011021628980993106, 0.00012237410892822648, 0.00013548823894125452, 0.00014674227459801823, 0.00015349297209432845, 0.00015673978152427553, 0.00015760858862166847, 0.0001549806199292854, 0.0001493961634838454, 0.00014269545724996608, 0.00013607899459536647, 0.00012922923284819875, 0.00012144303930887894, 0.00011391975288012338, 0.00010685906305957045, 9.902909176173394e-05, 9.228499110887577e-05, 8.561512690187345e-05, 7.912610848383314e-05, 7.28922179958889e-05, 6.696275968825564e-05, 6.13678921479141e-05, 5.6123243760194726e-05, 5.123355345410359e-05, 4.669553170533516e-05, 4.250009874808159e-05, 3.863412624603015e-05, 3.5081783893360066e-05, 3.1825572424289726e-05, 2.884710838727593e-05, 2.6127713044934626e-05, 2.364884729251405e-05, 2.139242606044168e-05, 1.9341038887112517e-05, 1.7478097898900417e-05, 1.5787930058414682e-05, 1.4255827031590697e-05, 1.2868063211435787e-05, 1.1611890185525573e-05, 1.0475514136112038e-05, 9.448061227503441e-06, 8.519534893805207e-06, 7.680768033556894e-06, 6.9233723997982904e-06, 6.239686907192101e-06, 5.62272613184554e-06, 5.066129930112832e-06, 4.5641148303775445e-06, 4.111427640893346e-06, 3.7033015545823907e-06, 3.3354149081246364e-06, 3.003852659602563e-06, 2.705070579897522e-06, 2.435862102774493e-06, 2.193327742999088e-06, 1.974846967605834e-06, 1.7780523899596402e-06, 1.6008061474463308e-06, 1.4411783198526472e-06, 1.2974272454596508e-06, 1.1679815945634226e-06, 1.051424064763367e-06, 9.464765683086222e-07, 8.519867885952489e-07, 7.6691599020089e-07, 6.903279743575853e-07, 6.213790792950218e-07], "accuracy_test": 0.8255102040816327, "start": "2016-02-01 09:48:04.750000", "learning_rate_per_epoch": [0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.007678820751607418, 0.0007678820984438062, 0.0007678820984438062, 0.0007678820984438062, 0.0007678820984438062, 0.0007678820984438062, 0.0007678820984438062, 0.0007678820984438062, 0.0007678820984438062, 0.0007678820984438062, 0.0007678820984438062, 0.0007678820984438062, 0.0007678820984438062, 0.0007678820984438062, 0.0007678820984438062, 0.0007678820984438062, 0.0007678820984438062, 7.67882083891891e-05, 7.67882102081785e-06, 7.678821134504688e-07, 7.678821134504688e-08, 7.678821134504688e-09, 7.678820912460083e-10, 7.678820773682205e-11, 7.6788211206269e-12, 7.678821229047117e-13, 7.678821229047117e-14, 7.678821229047117e-15, 7.678821123167999e-16, 7.678820858470203e-17, 7.678821023906325e-18, 7.678821023906325e-19, 7.678821023906325e-20, 7.678821023906325e-21, 7.678821023906325e-22, 7.678821276341815e-23, 7.678820960797453e-24, 7.6788208621898395e-25, 7.678820738930323e-26, 7.678820584855927e-27, 7.678820392262933e-28, 7.678820512633555e-29, 7.678820362170278e-30, 7.678820456209826e-31, 7.678820456209826e-32, 7.67882060314662e-33, 7.678820419475627e-34, 7.678820419475627e-35, 7.678820706461553e-36, 7.67882052709535e-37, 7.678820751303104e-38, 7.678821311822489e-39, 7.67882131182249e-40, 7.678835324807133e-41, 7.679115584499998e-42, 7.679115584499998e-43, 7.707141553786494e-44, 8.407790785948902e-45, 1.401298464324817e-45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "accuracy_train_first": 0.5110482419712071, "accuracy_train_last": 0.9996512276785714, "batch_size_eval": 1024, "accuracy_train_std": [0.019741795033543286, 0.02403974431121501, 0.023330259638044425, 0.022494353529981994, 0.024433295934370387, 0.025290673051596112, 0.025147183832500624, 0.027286111507377416, 0.025625176691515514, 0.02823834305749802, 0.028916838828530272, 0.028490279077017438, 0.02864066887870281, 0.02823143504232722, 0.027773294461673873, 0.024680644359100598, 0.026641404397638083, 0.02628771869101423, 0.023014402829003154, 0.024490010472877206, 0.022444607509906213, 0.02179688847349767, 0.021178431480984986, 0.020615498575257126, 0.021645309576359077, 0.0200630747477656, 0.020442477826802436, 0.017272291012563674, 0.01728217336138602, 0.017411807488906184, 0.01743758782447575, 0.015418332753210938, 0.01791387131282424, 0.014478827654068487, 0.016077173347684128, 0.017271358108282463, 0.013043179253271434, 0.012770651992327877, 0.01150284476279324, 0.012613829796537405, 0.013300253908573834, 0.01207927430971302, 0.008162707929991651, 0.009967053051555584, 0.010495991235854294, 0.0122388298267285, 0.009145423657611779, 0.008613382064519457, 0.007935558020467068, 0.008588464220230121, 0.009416819805399925, 0.008953398495722076, 0.008218317260086313, 0.009783562627503682, 0.010005463375449502, 0.009753621944862787, 0.005983605357038899, 0.008617439661018631, 0.007041034739130853, 0.008274169570804731, 0.007203309510840171, 0.009506302947751032, 0.010274776046541702, 0.006231365632781242, 0.007437804814816222, 0.007492712439835958, 0.00715781256599694, 0.006529224391677703, 0.0075312177480989706, 0.007514075697558102, 0.007316370804637726, 0.007466222960295017, 0.008637390035977304, 0.008278466710098725, 0.007764791822447945, 0.0048935777027280985, 0.005945931190991192, 0.005151695183404013, 0.007130405784088926, 0.0055259716150816, 0.0066909127210974155, 0.005918006219112142, 0.005045347434270613, 0.006346598824080506, 0.006109613326644102, 0.00678619246874664, 0.005333470140399973, 0.004486513056247149, 0.006310338453102446, 0.005912851024039763, 0.0017859806891435566, 0.0013857574800735992, 0.0011718380922948431, 0.001092572556219376, 0.0010947970021385374, 0.0010614501683885163, 0.0009657068293013014, 0.0008111404177135607, 0.0007425929791048787, 0.0006822637998103241, 0.0006818674804043518, 0.0005662107357647842, 0.0005662107357647842, 0.0005199190795897948, 0.0005072875797356638, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629, 0.0005141681637717629], "accuracy_test_std": 0.0087369266692429, "error_valid": [0.4909653261483433, 0.3740278496799698, 0.30551257765436746, 0.26656038215361444, 0.2549534073795181, 0.24370234845632532, 0.24192129847515065, 0.23387495293674698, 0.21606151167168675, 0.22502411991716864, 0.21813670698418675, 0.22033397260918675, 0.22053840361445776, 0.22988634224397586, 0.2258374317582832, 0.21288768354668675, 0.23816800404743976, 0.23338667168674698, 0.2124405826430723, 0.2306099397590362, 0.2138539509600903, 0.21281708866716864, 0.2106801228350903, 0.22505500517695776, 0.2233254306287651, 0.22728315606174698, 0.2181984775037651, 0.21022272684487953, 0.2083607868975903, 0.21888971903237953, 0.21085219785391573, 0.22350780073418675, 0.21740428510918675, 0.20580760542168675, 0.21173904602786142, 0.2136304005082832, 0.21194200630647586, 0.21759842102786142, 0.2103859775037651, 0.19832013601280118, 0.2123391025037651, 0.20231757106551207, 0.18522772731551207, 0.19237987104668675, 0.1967641072100903, 0.20204401590737953, 0.19900255318147586, 0.19260342149849397, 0.1927563770707832, 0.19035468044051207, 0.19494334760918675, 0.19289903755647586, 0.19296963243599397, 0.19441388601280118, 0.19117828736822284, 0.2086049275225903, 0.1838246540850903, 0.1949521719691265, 0.1965714420180723, 0.1897546239646084, 0.19372264448418675, 0.19844220632530118, 0.2011792286332832, 0.19138271837349397, 0.1892354574548193, 0.1890942676957832, 0.19076207172439763, 0.19110769248870485, 0.19757741905120485, 0.18918545274849397, 0.18959284403237953, 0.18562482351280118, 0.19845250141189763, 0.19452566123870485, 0.1986142813441265, 0.1789609610316265, 0.19212396460843373, 0.1831628270896084, 0.1888295368975903, 0.18363051816641573, 0.18873688111822284, 0.18952077842620485, 0.18491299181099397, 0.1844747152673193, 0.1773225715361446, 0.1957875447100903, 0.18363051816641573, 0.17625482398343373, 0.18454531014683728, 0.19015172016189763, 0.17076165992093373, 0.16952036662274095, 0.16718044051204817, 0.16568471150225905, 0.16519643025225905, 0.16470814900225905, 0.16408750235316272, 0.16420957266566272, 0.16458607868975905, 0.16446400837725905, 0.16385365681475905, 0.16348744587725905, 0.16373158650225905, 0.16336537556475905, 0.16287709431475905, 0.16348744587725905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905, 0.16263295368975905], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.06475288967204268, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 64, "valid_ratio": 0.15, "learning_rate": 0.007678820746132926, "optimization": "adam", "nb_data_augmentation": 0, "learning_rate_decay_method": "discrete", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 1.5456167999789853e-08, "rotation_range": [0, 0], "momentum": 0.9383317435146552}, "accuracy_valid_max": 0.837367046310241, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.837367046310241, "accuracy_valid_std": [0.017754139920157567, 0.009684997460675013, 0.015005580612160379, 0.01458999508739371, 0.00969212749963082, 0.010412235778580812, 0.01447689604493627, 0.014371313573522457, 0.017031821405191928, 0.01899744835367232, 0.018541413978983432, 0.01931020521120732, 0.010930149212467108, 0.01717197267201822, 0.020784759472375052, 0.019591385231155438, 0.014712250823227725, 0.014404056306919665, 0.01201689306918031, 0.013877033555532324, 0.019219574288580808, 0.01547325423483043, 0.01811740639768139, 0.014137106212476887, 0.016466331771574593, 0.01187405755451106, 0.015055968859280644, 0.014091118289654172, 0.018182626429862726, 0.017223416019517504, 0.026230038628887824, 0.02272248824321483, 0.018758623490544118, 0.01806273073565472, 0.015379728865272883, 0.017481003772213177, 0.01598008930713533, 0.012962065222722769, 0.014423975837670138, 0.01927608929079754, 0.010144806706926779, 0.02206109511200924, 0.018788240502550162, 0.012566596011661402, 0.014708899496957058, 0.014613546739514566, 0.00990927370709222, 0.012234954813857618, 0.00747235616905269, 0.017401296654410127, 0.012585561251313744, 0.010155017182096226, 0.013897778212909101, 0.013575814921065613, 0.0198089809328599, 0.016251017921129103, 0.008635239150375949, 0.020399702042768104, 0.008119602558896193, 0.01644401098809342, 0.009951027310372425, 0.01866247237974032, 0.015118882253785451, 0.013679473330535569, 0.016731383599322667, 0.0086856369842563, 0.015255733259866984, 0.013351195267934738, 0.017018503577537432, 0.009520707407067145, 0.015164738296356077, 0.01597783353806978, 0.014763271611247856, 0.016617448792420587, 0.024793294095990962, 0.01906206612597829, 0.02201792611006067, 0.017404912010389264, 0.012886383089364021, 0.015834941684049243, 0.019768808503605618, 0.014089400173005575, 0.010792364070329977, 0.01607546775864794, 0.01942819292078365, 0.013303638834650267, 0.015126495590846433, 0.017073408007102848, 0.022794207815714382, 0.01163514594514397, 0.014749041688447333, 0.016821651181353262, 0.01774502503384673, 0.020064882193252077, 0.019593828342762073, 0.01972507487266714, 0.02090261582519673, 0.02136186923659118, 0.020132425606752616, 0.0200491999112637, 0.019845971117680283, 0.019465681473326182, 0.019569073457568534, 0.019574682722850057, 0.01903838904619882, 0.019398199680663072, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395, 0.018965127846866395], "accuracy_valid": [0.5090346738516567, 0.6259721503200302, 0.6944874223456325, 0.7334396178463856, 0.7450465926204819, 0.7562976515436747, 0.7580787015248494, 0.766125047063253, 0.7839384883283133, 0.7749758800828314, 0.7818632930158133, 0.7796660273908133, 0.7794615963855422, 0.7701136577560241, 0.7741625682417168, 0.7871123164533133, 0.7618319959525602, 0.766613328313253, 0.7875594173569277, 0.7693900602409638, 0.7861460490399097, 0.7871829113328314, 0.7893198771649097, 0.7749449948230422, 0.7766745693712349, 0.772716843938253, 0.7818015224962349, 0.7897772731551205, 0.7916392131024097, 0.7811102809676205, 0.7891478021460843, 0.7764921992658133, 0.7825957148908133, 0.7941923945783133, 0.7882609539721386, 0.7863695994917168, 0.7880579936935241, 0.7824015789721386, 0.7896140224962349, 0.8016798639871988, 0.7876608974962349, 0.7976824289344879, 0.8147722726844879, 0.8076201289533133, 0.8032358927899097, 0.7979559840926205, 0.8009974468185241, 0.807396578501506, 0.8072436229292168, 0.8096453195594879, 0.8050566523908133, 0.8071009624435241, 0.807030367564006, 0.8055861139871988, 0.8088217126317772, 0.7913950724774097, 0.8161753459149097, 0.8050478280308735, 0.8034285579819277, 0.8102453760353916, 0.8062773555158133, 0.8015577936746988, 0.7988207713667168, 0.808617281626506, 0.8107645425451807, 0.8109057323042168, 0.8092379282756024, 0.8088923075112951, 0.8024225809487951, 0.810814547251506, 0.8104071559676205, 0.8143751764871988, 0.8015474985881024, 0.8054743387612951, 0.8013857186558735, 0.8210390389683735, 0.8078760353915663, 0.8168371729103916, 0.8111704631024097, 0.8163694818335843, 0.8112631188817772, 0.8104792215737951, 0.815087008189006, 0.8155252847326807, 0.8226774284638554, 0.8042124552899097, 0.8163694818335843, 0.8237451760165663, 0.8154546898531627, 0.8098482798381024, 0.8292383400790663, 0.830479633377259, 0.8328195594879518, 0.834315288497741, 0.834803569747741, 0.835291850997741, 0.8359124976468373, 0.8357904273343373, 0.835413921310241, 0.835535991622741, 0.836146343185241, 0.836512554122741, 0.836268413497741, 0.836634624435241, 0.837122905685241, 0.836512554122741, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241, 0.837367046310241], "seed": 130160282, "model": "residualv4", "loss_std": [0.2680499255657196, 0.15613868832588196, 0.14297893643379211, 0.1333766132593155, 0.12657809257507324, 0.12238381057977676, 0.11766468733549118, 0.11295340955257416, 0.10751352459192276, 0.10570406168699265, 0.10159040987491608, 0.09406954050064087, 0.0921257734298706, 0.08406469225883484, 0.08320660889148712, 0.07801596075296402, 0.07664764672517776, 0.07274280488491058, 0.06692994385957718, 0.06651094555854797, 0.05928497388958931, 0.05794902890920639, 0.06156989559531212, 0.05631304159760475, 0.058219894766807556, 0.052882686257362366, 0.05672309175133705, 0.052008140832185745, 0.05329723283648491, 0.048298899084329605, 0.04860170558094978, 0.046357445418834686, 0.05513757839798927, 0.04542673006653786, 0.04983241483569145, 0.04506316035985947, 0.04732786864042282, 0.04851847141981125, 0.04213578999042511, 0.040203340351581573, 0.044502049684524536, 0.04257084056735039, 0.04057222232222557, 0.04280707240104675, 0.041332244873046875, 0.040196746587753296, 0.033236682415008545, 0.048292841762304306, 0.034139763563871384, 0.040504053235054016, 0.043642472475767136, 0.0383947491645813, 0.03491812199354172, 0.043250590562820435, 0.031913306564092636, 0.0398709699511528, 0.043723586946725845, 0.032643307000398636, 0.037122443318367004, 0.0372476764023304, 0.03481018915772438, 0.03517553582787514, 0.039759498089551926, 0.03568849712610245, 0.033582571893930435, 0.03536874055862427, 0.03587821125984192, 0.033952806144952774, 0.03388791158795357, 0.03578193858265877, 0.03540362790226936, 0.030884163454174995, 0.03571362793445587, 0.036432892084121704, 0.031358152627944946, 0.03625637665390968, 0.03393285721540451, 0.030907344073057175, 0.026801658794283867, 0.036929722875356674, 0.02927538752555847, 0.03256189823150635, 0.03147679939866066, 0.03281722217798233, 0.027133481577038765, 0.034082118421792984, 0.03541472181677818, 0.023271456360816956, 0.03509879484772682, 0.03451757878065109, 0.017378060147166252, 0.0025017650332301855, 0.0007080969517119229, 0.0004606080474331975, 0.0003286946739535779, 0.00024303885584231466, 0.00018395365623291582, 0.000143270444823429, 0.00011663624900393188, 0.00010248458420392126, 0.00010015086445491761, 0.00010888618271565065, 0.0001275157992495224, 0.00015489703218918294, 0.00019003394118044525, 0.00023159064585343003, 3.26111949107144e-05, 9.578217941452749e-06, 7.7648237493122e-06, 7.58800115363556e-06, 7.586333140352508e-06, 7.586312676721718e-06, 7.586294032080332e-06, 7.586325864394894e-06, 7.586319952679332e-06, 7.586329502373701e-06, 7.586341780552175e-06, 7.58633905206807e-06, 7.586322226416087e-06, 7.586322226416087e-06, 7.586324954900192e-06, 7.586340871057473e-06, 7.5863313213631045e-06, 7.586325864394894e-06, 7.586311767227016e-06, 7.586306765006157e-06, 7.586317224195227e-06, 7.586302217532648e-06, 7.58628812036477e-06, 7.586301762785297e-06, 7.586310857732315e-06, 7.586276296933647e-06, 7.58629812480649e-06, 7.586320862174034e-06, 7.586336323583964e-06, 7.586317224195227e-06, 7.586318133689929e-06, 7.586321316921385e-06, 7.586352694488596e-06, 7.5862990343011916e-06, 7.586273568449542e-06, 7.586334959341912e-06, 7.586324500152841e-06, 7.5863072197535075e-06, 7.586337233078666e-06, 7.586309493490262e-06, 7.586304946016753e-06, 7.586315859953174e-06, 7.586305400764104e-06, 7.58629312258563e-06, 7.586327683384297e-06, 7.586355877720052e-06, 7.5863072197535075e-06, 7.586295851069735e-06, 7.586278115923051e-06, 7.586327683384297e-06, 7.586291758343577e-06]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:29 2016", "state": "available"}], "summary": "384a5996c9531153e8be28e25efee0f3"}