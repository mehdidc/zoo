{"content": {"hp_model": {"f0": 16, "f1": 16, "f2": 16, "f3": 64, "nonlin": "rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.011318339215155059, 0.009590216762122909, 0.012463942459170985, 0.008876305578527088, 0.008666737370939499, 0.014424007629412046, 0.013189865980171658, 0.010297691133939331, 0.0075355619280051585, 0.019501958803433885, 0.017035790192673955, 0.019062622307591495, 0.01974757771243412, 0.008651643657347888, 0.010898044333605491, 0.014124809117233924, 0.014811212293549864, 0.020327909489916363, 0.01074777215168024, 0.014457770272302217, 0.01123742962445757, 0.01400375692563025, 0.01346539113301196, 0.013375344214168305, 0.02284879204703648, 0.00822153828537154, 0.0120255697912289, 0.011711463410826904, 0.008683588545545979, 0.01052293720156664, 0.007567732303883359, 0.008858748016542293, 0.014967190870786276, 0.018912239575045325, 0.013359751957892201, 0.01594772052183328, 0.0096391805434893, 0.010231161017622465, 0.012975951497032838, 0.01037437020868945, 0.01090423046972888, 0.015012205743445216, 0.011840084378272874, 0.008251323394679983, 0.016187145974081208, 0.016484434492394104, 0.008001090271760469, 0.00990291523406617, 0.0071927572345867824, 0.01392832654380176, 0.015697012346845816, 0.01393542673026633, 0.012510151972534727, 0.010003067536748768, 0.01820540594394683, 0.012407098803050529, 0.008190406898201478, 0.009509739942531138, 0.011271319694089791, 0.016571936453932037, 0.013262477397989327, 0.012538871314427349, 0.0152062141006917, 0.010237310545827735, 0.010454664861049949, 0.012605619022521327, 0.0153840510325965, 0.016257690000762534, 0.019487208784855548, 0.021631880439751318, 0.014356623367436594, 0.016031943826494074, 0.016074674401934927, 0.019555351004909315, 0.01699578641482319, 0.022571755219228988, 0.015038026302164102, 0.015030471192791993, 0.01901443244610451, 0.020190648061619708, 0.012079737538776461, 0.01422436816399761, 0.02095422835048399, 0.017563539410950346, 0.022593360916230196, 0.015151680343643423, 0.015023117889556169, 0.014043494688880394, 0.026535226943939973, 0.02702159646329796, 0.022085211096145464, 0.015278451527119083, 0.012346807569633958, 0.01500365696686298, 0.014193903567109126, 0.021749937019351982, 0.014022390156619914, 0.01255696876377069, 0.024670679828912592, 0.012649799144009781, 0.014656285696441451, 0.011388748001002359, 0.011481555146138162, 0.02358275280508281, 0.011720789698952357, 0.020544772600138275, 0.012770896031352253, 0.013473009264963054, 0.012320607088962385, 0.017309846141216702, 0.01951598901417131, 0.01685737603919234, 0.010772661852006938, 0.02520601382455607, 0.009815881361751065, 0.015210427723715917, 0.014563061089605084, 0.017119343190852666, 0.013825102184671638, 0.014593927979715331, 0.01741568706599592, 0.013372995535922578, 0.014978279357785906, 0.01777242449427301, 0.00857135243284667, 0.009291489525607386, 0.008935210951580029, 0.012458121837538893, 0.015724064044825698, 0.012089759221781763, 0.018350228086826487, 0.011052063726343852, 0.01919743144243845, 0.016343165601228445, 0.009966508404036714, 0.012913766307225747, 0.017035420705519916, 0.015376279161305541, 0.010046845328302864, 0.012818330354088268, 0.012195752742675339, 0.00869958773871712, 0.013906375000253697, 0.021181646871300433, 0.01315864514967455, 0.01812262537617417, 0.019879401526260942, 0.018481244974619226, 0.0161360750835261, 0.015360816067500399, 0.017697271826169935, 0.011556727034092797, 0.010509336430403096, 0.01517874666446373, 0.016365039528486114, 0.018796904874894906, 0.01584112076275265, 0.01255746864620699, 0.015732023030384918, 0.01417760602371603, 0.01197456323045292, 0.016631107028317202, 0.013063583882098289, 0.01678448908039779, 0.016239350503246246, 0.015901358135099537, 0.01413795689034182, 0.009127960462327722, 0.018702606253857463, 0.01978302755509991, 0.009698222602219335, 0.014073237902290366, 0.013419002136651557, 0.022403717707376827, 0.018463723394706543, 0.0187256766237355, 0.012379558993440678, 0.01509332785308151, 0.01283532906163551, 0.01038672231228505, 0.012124759043128942, 0.007882309949842603, 0.015782580664238335, 0.012333302146126851, 0.021002287072409732, 0.01272070238177805, 0.00998887056153407, 0.012843609926803447, 0.016897836397408023, 0.014200843629376222, 0.019823299053147274, 0.016998814956506438, 0.013400257234831518, 0.012359987872308142, 0.0171232330796028, 0.015231523513470551, 0.018494330339878004, 0.016577456507729554, 0.012361472159963423, 0.013898132209403622, 0.014260911851851686, 0.012044137446722364, 0.009951944670577613, 0.01971804539434786, 0.008213818642112253, 0.019653247122718688], "moving_avg_accuracy_train": [0.009993976242386488, 0.025123865543673865, 0.040962995346818465, 0.04807421948900077, 0.06341743834390283, 0.06893887410184366, 0.08447910387050757, 0.101807757137019, 0.11719918432337248, 0.13408488676705035, 0.14804529214142503, 0.1622741030377938, 0.17289965609111113, 0.17614637808582062, 0.1802215935441267, 0.1839105021864379, 0.1846474598812308, 0.20187695646416715, 0.20752674697500403, 0.2085126094741168, 0.21341723818271285, 0.21980098530620973, 0.21856727035809154, 0.22962902684941416, 0.2217331785722339, 0.21571454403334756, 0.21203011695790133, 0.2053060051675191, 0.20396429901907653, 0.19659591221425268, 0.20253613728804704, 0.19825591736807402, 0.19504070206745155, 0.2010926308167326, 0.2031288420221579, 0.19672254667883837, 0.20036964200749385, 0.1972847790202919, 0.2005755987719172, 0.1963025909665232, 0.19751848055827065, 0.18961117485547976, 0.18736731855371733, 0.1934589222234471, 0.18680621562079602, 0.17914463648673415, 0.17804043997422722, 0.17528194726417992, 0.17599090401778225, 0.16852762393577145, 0.16421891319870963, 0.17242514226458985, 0.16598797306858493, 0.1644513996277748, 0.16152000652154033, 0.15920040625929033, 0.16163424318850156, 0.16520808786882085, 0.16840126054419421, 0.17509890427913433, 0.17949306919913524, 0.17827551508814565, 0.1870986389899992, 0.19538135552312366, 0.19778858726501578, 0.21420340946897748, 0.2087817866260849, 0.2044744929187182, 0.20616328941634435, 0.20288227865600375, 0.20834303709762006, 0.20985864052610095, 0.20698680321306265, 0.2055734118014094, 0.20047070390384505, 0.1948437558198928, 0.18651546225040813, 0.18667792070306205, 0.18161201485114825, 0.18734672816969714, 0.1957510480622143, 0.19405269667053698, 0.19399603566327472, 0.189871812628278, 0.1882360092954982, 0.19402371197775162, 0.20612094280101742, 0.21669770972431177, 0.22123952152573625, 0.23151060602760154, 0.22293058611098018, 0.24336139787409775, 0.26307660818704787, 0.2645867393349359, 0.2553916284090097, 0.266487708479469, 0.2755326034214815, 0.2881270567190528, 0.28414657984467434, 0.27888643863300133, 0.29014165695362515, 0.2969119459982497, 0.2923347197389102, 0.29720752184563004, 0.30007704671786845, 0.3059378986802178, 0.3141300062259096, 0.32224862690615175, 0.32435013435903975, 0.319186575017128, 0.31315801692363776, 0.31909616401888014, 0.3102385055004362, 0.3053124496278566, 0.30433508971836554, 0.3110829091887586, 0.3237973828105343, 0.32008561183752815, 0.3152280295954974, 0.3179622749009901, 0.3138863273227589, 0.3079188349155568, 0.2992325376931503, 0.2884860118158157, 0.2892960355304135, 0.287314329898653, 0.2849218221348859, 0.28833686396048, 0.29743105901861255, 0.29608463503927807, 0.2898257845152321, 0.2937267402610603, 0.30496516941532525, 0.3019799213345347, 0.3159751494276408, 0.3127852932727136, 0.32015827136284714, 0.3255580359174539, 0.3230004370945347, 0.3206920192444851, 0.3076528805929233, 0.3148647707474553, 0.3139329660322059, 0.3182286488725069, 0.3224996998098225, 0.3307728378426369, 0.33068128665688745, 0.3302569495194379, 0.3378028302181032, 0.33490198336392857, 0.3304130416893574, 0.3230321879287827, 0.333924448181447, 0.3377623401746257, 0.3402544802400812, 0.35491553943160464, 0.36044348573646007, 0.3529794697924744, 0.34624514681539564, 0.33599123778285017, 0.3329233546318723, 0.3386746476120682, 0.34637817595255943, 0.34426138563328174, 0.34682373235626773, 0.3458711753870215, 0.3292687414262079, 0.322313779485968, 0.3236116061273952, 0.33485743985066524, 0.33263245643799627, 0.32579692426088175, 0.32932767604275776, 0.32672851141697956, 0.32946852379156993, 0.32177039023739445, 0.32847211038447455, 0.3196143247855343, 0.3229234891600632, 0.3099122114419159, 0.3086708367297564, 0.3115524589041876, 0.3160237739276208, 0.31353640524425336, 0.30663224518425214, 0.30387752948480995, 0.3109716259560871, 0.3137086589815563, 0.31755316314615445, 0.3273621528773585, 0.3368475397721623, 0.3325545593103115, 0.3407540913822335, 0.3387877616436964, 0.33666950885049696, 0.33621590280556357, 0.3395320771233369, 0.33735141506322724, 0.3359791186161976, 0.3298733697413811, 0.32657077503260845, 0.3307769492193162, 0.3196724152308471, 0.3253125090655974, 0.32904905546085567, 0.31913103968036627], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 400571956, "moving_var_accuracy_train": [0.00089891605020047, 0.002869246397603315, 0.004840224054130757, 0.00481132722792089, 0.006448923788593601, 0.00607840768519585, 0.007644055588042079, 0.009582190045516671, 0.010756035318460392, 0.012246574309762217, 0.012775953142737825, 0.013320489364185641, 0.01300456182696683, 0.01179897647766852, 0.01076854525918622, 0.009814163156009463, 0.008837634800203747, 0.01062557129269611, 0.009850295358773582, 0.008874013146700636, 0.008203110276953236, 0.007749569295288706, 0.006988310838918728, 0.00739074186508672, 0.007212767458724302, 0.006817506368265955, 0.006257930757307892, 0.006039060795903119, 0.005451356294811725, 0.005394858782280073, 0.005172949369398084, 0.004820536975528281, 0.004431521762839665, 0.0043180021608330696, 0.003923517349407659, 0.003900531194699429, 0.0036301898142561914, 0.003352818249678849, 0.003115001876450148, 0.0029678290501497567, 0.0026843516326286593, 0.002978645820662297, 0.002726095258522699, 0.002787454450092017, 0.0029070355513514325, 0.003144630149663721, 0.002841140384141441, 0.002625509884009754, 0.0023674824727150834, 0.002632039171686425, 0.0025359201484588295, 0.0028884078929482257, 0.0029725014289753576, 0.00269650080752885, 0.002504188316665476, 0.0023021943933886017, 0.0021252870138316732, 0.0020277096046399236, 0.0019167058097886908, 0.0021287611132114657, 0.0020896631699878197, 0.0018940387951077262, 0.002405262554084092, 0.0027821668371885246, 0.00255610303540223, 0.00472551022375107, 0.0045175051496311345, 0.004232729646401708, 0.003835124984255087, 0.0035484977703148153, 0.003462026938102488, 0.0031364977280640464, 0.002897075001230673, 0.0026253465786504225, 0.00259715057177617, 0.0026223984172539976, 0.0029844028395442745, 0.0026862000903293956, 0.0026485507002005472, 0.002679678061793971, 0.0030474035913164144, 0.0027686228092312836, 0.0024917894225358513, 0.0023956934210638443, 0.0021802067518492617, 0.0022636635977078077, 0.0033543841802593784, 0.004025757749180681, 0.0038088344642186415, 0.004377407609396892, 0.004602217524383776, 0.007898758395644884, 0.010607088215235094, 0.00956690385846598, 0.009371164057080173, 0.009542154587742555, 0.009324230249576687, 0.00981938950940136, 0.008980048323788384, 0.008331065261510217, 0.00863807819036335, 0.008186801695056886, 0.007556680527613885, 0.0070147102781937835, 0.0063873468073059585, 0.006057758398096533, 0.0060559781926485, 0.006043588389130555, 0.005478976552388392, 0.005171040002846548, 0.00498102761674117, 0.004800279173389669, 0.005026374285914965, 0.004742131095461452, 0.004276515077449431, 0.004258661178149628, 0.005287715615644131, 0.0048829392478841795, 0.004607010270238597, 0.004213594119730217, 0.003941754845701592, 0.003868078051801561, 0.004160336081545295, 0.004783692839280764, 0.004311228801116585, 0.003915450335903486, 0.0035754221429103103, 0.003322842524654293, 0.0037348977259871194, 0.00337772367117755, 0.003392510193000945, 0.003190216275279042, 0.00400791525645003, 0.0036873290857398007, 0.005081393861568428, 0.004664831116013729, 0.004687595257670666, 0.004481252847110288, 0.004091999368050241, 0.0037307585679790672, 0.004887854942153045, 0.004867171684347056, 0.004388268856158599, 0.004115517990122847, 0.0038681430760928604, 0.004097332084673569, 0.0036876743107827213, 0.0033205274377604193, 0.0035009375336507815, 0.003226577992528079, 0.0030852755694945858, 0.0032670410326600375, 0.004008108930099982, 0.0037398627716517282, 0.003421773353439191, 0.005014115927651457, 0.004787728048030593, 0.004810359049336177, 0.004737483098039055, 0.005210018642264608, 0.004773723941290634, 0.00459404788565803, 0.004668742237126989, 0.004242195224716382, 0.003877066288803916, 0.0034975259429404597, 0.005628540669455014, 0.005501030062821174, 0.004966086242459841, 0.0056076966033968036, 0.005091481903736992, 0.005002854214662602, 0.004614764666503329, 0.004214089110620065, 0.003860249209874232, 0.004007575630848107, 0.004011035544131303, 0.0043160752811693135, 0.0039830228727712426, 0.005108360716222933, 0.004611393745184541, 0.004224988087871647, 0.003982423201433499, 0.0036398639079931244, 0.003704884352400853, 0.0034026920444235474, 0.0035153586826752836, 0.003231244962450336, 0.0030411423766498152, 0.003602974654906631, 0.004052430270313015, 0.0038130543744942067, 0.004036839872831079, 0.00366795395931387, 0.003341541517445559, 0.0030092391916970045, 0.002807288381480036, 0.0025693571265156465, 0.0023293701917108532, 0.002431954696440676, 0.0022869234130903273, 0.002217458183381636, 0.0031055084409530587, 0.003081253523040846, 0.0028987841814120197, 0.00349420909646915], "duration": 48764.355724, "accuracy_train": [0.09993976242386489, 0.16129286925526026, 0.18351516357511996, 0.11207523676864158, 0.20150640803802142, 0.11863179592331118, 0.22434117178848284, 0.257765636535622, 0.25572202900055374, 0.28605620876015136, 0.27368894051079734, 0.2903334011051126, 0.26852963357096715, 0.20536687603820597, 0.2168985326688815, 0.21711067996723882, 0.19128007913436693, 0.3569424257105943, 0.258374861572536, 0.21738537196613142, 0.2575588965600775, 0.27725470941768177, 0.20746383582502767, 0.32918483527131787, 0.15067054407761166, 0.16154683318337024, 0.17887027327888522, 0.144788999054079, 0.1918889436830934, 0.13028043097083794, 0.25599816295219635, 0.1597339380883167, 0.1661037643618494, 0.2555599895602621, 0.2214547428709856, 0.1390658885889627, 0.23319349996539313, 0.16952101213547435, 0.23019297653654486, 0.1578455207179771, 0.2084614868839978, 0.11844542353036176, 0.1671726118378553, 0.24828335525101514, 0.12693185619693614, 0.1101904242801772, 0.16810267136166482, 0.15045551287375417, 0.18237151480020303, 0.10135810319767441, 0.1254405165651532, 0.246281203857512, 0.10805345030454043, 0.15062223866048358, 0.13513746856543005, 0.13832400389904023, 0.18353877555140274, 0.19737268999169433, 0.19713981462255448, 0.23537769789359542, 0.21904055347914358, 0.16731752808923955, 0.2665067541066814, 0.26992580432124397, 0.21945367294204504, 0.3619368093046327, 0.1599871810400517, 0.16570884955241785, 0.2213624578949797, 0.17335318181293835, 0.25748986307216687, 0.22349907138242894, 0.18114026739571798, 0.19285288909653012, 0.15454633282576596, 0.14420122306432262, 0.11156082012504615, 0.1881400467769472, 0.13601886218392395, 0.23895914803663715, 0.271389927094869, 0.1787675341454411, 0.19348608659791436, 0.1527538053133075, 0.1735137793004799, 0.2461130361180325, 0.31499602021040973, 0.3118886120339609, 0.2621158277385567, 0.3239503665443891, 0.14571040686138798, 0.42723870374215583, 0.4405135010035991, 0.27817791966592836, 0.17263563007567367, 0.36635242911360283, 0.35693665789959395, 0.4014771363971945, 0.24832228797526762, 0.23154516772794387, 0.39143862183923955, 0.3578445473998708, 0.2511396834048542, 0.34106274080610927, 0.32590277056801403, 0.3586855663413621, 0.3878589741371355, 0.3953162130283315, 0.3432637014350314, 0.2727145409399225, 0.2589009940822259, 0.3725394878760613, 0.23051957883444074, 0.2609779467746401, 0.2955388505329457, 0.37181328442229605, 0.4382276454065154, 0.2866796730804725, 0.27150978941722037, 0.3425704826504245, 0.27720279911867846, 0.25421140325073827, 0.2210558626914913, 0.19176727891980438, 0.29658624896179403, 0.26947897921280917, 0.26338925226098187, 0.3190722403908269, 0.3792788145418051, 0.2839668192252676, 0.23349612979881876, 0.3288353419735142, 0.4061110318037099, 0.2751126886074197, 0.44193220226559615, 0.2840765878783684, 0.38651507417404946, 0.37415591690891475, 0.29998204768826137, 0.29991625859403837, 0.19030063272886674, 0.37977178213824286, 0.30554672359496127, 0.35688979443521596, 0.36093915824566264, 0.40523108013796605, 0.3298573259851421, 0.326437915282392, 0.4057157565060908, 0.3087943616763566, 0.29001256661821706, 0.2566045040836102, 0.43195479045542634, 0.37230336811323367, 0.3626837408291805, 0.4868650721553156, 0.4101950024801588, 0.28580332629660393, 0.28563624002168697, 0.24370605648994093, 0.30531240627307127, 0.39043628443383166, 0.4157099310169804, 0.3252102727597822, 0.3698848528631414, 0.33729816266380586, 0.17984683577888522, 0.25971912202380953, 0.3352920459002399, 0.43606994336009597, 0.3126076057239756, 0.26427713466685127, 0.36110444207964193, 0.303336029784976, 0.35412863516288295, 0.2524871882498154, 0.3887875917081949, 0.23989425439507198, 0.3527059685308232, 0.1928107119785899, 0.2974984643203212, 0.33748705847406796, 0.3562656091385198, 0.2911500870939461, 0.2444948046442414, 0.2790850881898302, 0.3748184941975821, 0.3383419562107789, 0.3521537006275378, 0.4156430604581949, 0.42221602182539686, 0.2939177351536545, 0.4145498800295312, 0.3210907939968623, 0.31760523371170174, 0.3321334484011628, 0.3693776459832964, 0.31772545652224066, 0.32362845059293094, 0.2749216298680325, 0.2968474226536545, 0.36863251689968624, 0.21973160933462535, 0.3760733535783499, 0.3626779730181801, 0.2298688976559616], "end": "2016-01-24 11:09:56.224000", "learning_rate_per_epoch": [0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044, 0.0025808741338551044], "accuracy_valid": [0.10116981598268072, 0.16210496282003012, 0.18316282708960843, 0.11573736351656627, 0.20689741387424698, 0.12348956372364459, 0.22044721856174698, 0.25009118505271083, 0.25787427051957834, 0.2967867564006024, 0.2806117046310241, 0.29741769813629515, 0.2737860622176205, 0.20592085137424698, 0.2198986375188253, 0.22469026496611444, 0.19685676298945784, 0.3683640813253012, 0.2669280638177711, 0.22507706607680722, 0.2562358810240964, 0.2870711361069277, 0.2150555346385542, 0.33513742469879515, 0.1692394578313253, 0.16448606927710843, 0.1906517672251506, 0.14311199877635541, 0.19502570830195784, 0.1258486092808735, 0.2559711502259036, 0.1717911685805723, 0.17478115587349397, 0.2655558758471386, 0.22780379329819278, 0.1512833560805723, 0.24075354150978917, 0.16873794004141568, 0.23732527767319278, 0.15685741010918675, 0.21584972703313254, 0.12848562217620482, 0.17415168486445784, 0.24349938817771083, 0.14001170698418675, 0.12189382530120482, 0.16669362998870482, 0.15038621282003012, 0.1800919498305723, 0.11035744540662651, 0.13363316547439757, 0.2561549910579819, 0.11904502776731928, 0.1508847891566265, 0.14787568241716867, 0.13687611775225905, 0.19432417168674698, 0.20923733998493976, 0.20214696677334337, 0.2448127470820783, 0.230712890625, 0.16911444606551204, 0.2685046827936747, 0.2779864575489458, 0.2257080078125, 0.3707319512424699, 0.16817023955195784, 0.1709984469126506, 0.21996776167168675, 0.1818229951054217, 0.26327624952936746, 0.22536238704819278, 0.18996052569653615, 0.19905549934111444, 0.16189317818147592, 0.1540203783885542, 0.11416221526731928, 0.1906517672251506, 0.14714326054216867, 0.250011765813253, 0.2813132412462349, 0.18505712302334337, 0.20176163638930722, 0.1557999576430723, 0.18930016942771083, 0.2487690017884036, 0.30602144907756024, 0.3109969173569277, 0.2739699030496988, 0.34406179405120485, 0.15819135918674698, 0.42890066123870485, 0.4386868764118976, 0.26974450536521083, 0.1775799487010542, 0.3704789862575301, 0.35956325301204817, 0.40784426769578314, 0.2560858669051205, 0.23403820359563254, 0.3887174675263554, 0.36539174275225905, 0.2539165450865964, 0.34834160862198793, 0.3173136883471386, 0.35772043251129515, 0.3930722891566265, 0.4032761907003012, 0.3422483880835843, 0.28270749011671686, 0.2563800122364458, 0.3784444418298193, 0.238525390625, 0.2545754306287651, 0.3013636577560241, 0.37417051016566266, 0.4469391236822289, 0.2931746517319277, 0.2870917262801205, 0.35023443382906627, 0.27592008659638556, 0.2553916839231928, 0.23136442253388553, 0.20124246987951808, 0.29609404414533136, 0.2728286191641566, 0.2635100950677711, 0.3244055322853916, 0.39380618175828314, 0.28868893542921686, 0.2314232516001506, 0.3417909920933735, 0.4118314076618976, 0.2751979598079819, 0.44163715408509036, 0.2900008236069277, 0.3894307699548193, 0.3750264730798193, 0.299938523625753, 0.3038859539721386, 0.1962670016001506, 0.3807828972138554, 0.3045874905873494, 0.3599206395896084, 0.36457696018448793, 0.4090840902673193, 0.33734645613704817, 0.33583013695406627, 0.41535085655120485, 0.3079245693712349, 0.29433505506400603, 0.2634997999811747, 0.4412003482680723, 0.3693891778049699, 0.36668451148343373, 0.49595256024096385, 0.41987922392695787, 0.28856539439006024, 0.2912935923381024, 0.2553004988704819, 0.30517872270331325, 0.4014451360128012, 0.42283832596009036, 0.3215861492846386, 0.3809873282191265, 0.3352580242846386, 0.1831834172628012, 0.2658191359186747, 0.3474076971950301, 0.44814953172063254, 0.3188814829631024, 0.2672222091490964, 0.3618002282567771, 0.3185255671121988, 0.3629106268825301, 0.24937788262424698, 0.39543280544051207, 0.24001964890813254, 0.3562658838478916, 0.19949230515813254, 0.29586902296686746, 0.33790386153990964, 0.36093691170933734, 0.29747652720256024, 0.2494793627635542, 0.276256883000753, 0.37965337914156627, 0.3431028802710843, 0.35512607068900603, 0.41522878623870485, 0.4279564547251506, 0.3018019342996988, 0.4217911685805723, 0.32659250282379515, 0.3146384365587349, 0.34254400414156627, 0.37742669898343373, 0.3217391048569277, 0.32453642695783136, 0.284923875188253, 0.30051798992846385, 0.3697245034826807, 0.22144437123493976, 0.3841714514307229, 0.36568735881024095, 0.2431861233998494], "accuracy_test": 0.16686463647959185, "start": "2016-01-23 21:37:11.868000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0], "accuracy_train_last": 0.2298688976559616, "batch_size_eval": 1024, "accuracy_train_std": [0.009034702354858419, 0.010594263546855403, 0.013433316858003305, 0.010525069788284838, 0.014457477392289335, 0.010636526936607832, 0.014419890958557276, 0.012582208481346363, 0.013951218759808056, 0.015056807103891671, 0.014423693125419119, 0.015314749057490931, 0.015010086266775037, 0.012054766138530576, 0.012291157960396582, 0.01258676651491132, 0.009692769027362887, 0.013996425432042547, 0.013494847922633352, 0.012573212015891093, 0.012270070940579768, 0.013431116542277187, 0.01235908651079915, 0.012875109818648211, 0.010988934716768973, 0.01019242620497992, 0.011472514825337872, 0.011363422950170357, 0.011428960130588065, 0.011126793504615956, 0.013472178279928266, 0.011659557002636512, 0.011935675117934313, 0.012632207285941914, 0.01160720533783206, 0.009092457857084518, 0.013594076970792261, 0.011214980855977266, 0.014302738373076083, 0.01190151251020718, 0.011631876761158073, 0.008723343779532797, 0.011639418232998156, 0.012947452256139521, 0.008224204585433742, 0.008769274586651202, 0.01093857527982628, 0.010507575416849909, 0.012041584568503272, 0.008228590252450225, 0.008829912202673713, 0.013393307798876335, 0.007985668413501727, 0.01019479542873125, 0.0094345505527923, 0.010494341275853969, 0.010938530194627177, 0.010227673269250583, 0.011027692194245306, 0.012063175079590609, 0.01182558440008095, 0.011428374115513767, 0.01350031151951821, 0.01269275219555965, 0.0129968696214735, 0.015494028949742888, 0.00921331261035076, 0.012105213422576826, 0.012514787879176065, 0.012113184665716646, 0.011076742858068462, 0.009689520678163904, 0.011987796098926756, 0.014140316711531148, 0.012405492232433129, 0.01227270541656702, 0.009558604816625075, 0.011698406000772455, 0.011144957661196145, 0.010444572780281288, 0.014462287128437753, 0.011707901134696176, 0.010676985913604647, 0.012877233138968058, 0.01247634382887271, 0.013827229465878478, 0.013227871142882238, 0.014573019043435087, 0.011657215468804943, 0.015761467130919467, 0.011792242622667433, 0.016087927289640116, 0.013337701321174235, 0.015127600852953612, 0.013374605335031767, 0.012375405480365951, 0.01008917185379007, 0.016278506233220375, 0.010226590091598148, 0.012270069604795866, 0.01517685226943616, 0.013088419427536316, 0.01287093019182425, 0.015939037493855994, 0.011291192090051767, 0.01494685795685574, 0.014888614576689357, 0.01385633923440074, 0.014251005646142064, 0.012286499821444256, 0.014075235283871548, 0.013561152849601247, 0.01158274608403627, 0.015841109332692963, 0.012890051442023182, 0.012938911947980773, 0.014712692914924119, 0.012947702126314433, 0.012320118497735918, 0.014650122591095655, 0.0117840688680548, 0.011861222754361123, 0.011308823968953594, 0.013765476878124759, 0.012561790719047918, 0.013413660033173624, 0.01572253048536448, 0.013147351319945452, 0.01333690447021485, 0.012030135321552782, 0.01411960242873864, 0.014904114052989652, 0.015189895344286615, 0.013792272029221148, 0.013808538283420964, 0.01428864929817791, 0.01429427082319078, 0.017076972468766, 0.015664902122255527, 0.01387511851271743, 0.013029113749888695, 0.013814970486425706, 0.015651962983811354, 0.013122086137490966, 0.011700342839156929, 0.01747363287907333, 0.01418455694405139, 0.015948545859544253, 0.013549089615020958, 0.014287702558751237, 0.012928728566241313, 0.011059260037325122, 0.012861464263202716, 0.014615545056523386, 0.014537997234934846, 0.015284643029708566, 0.012998764932111698, 0.011032464829132683, 0.013198525610464081, 0.013350629965536272, 0.013391425497078321, 0.01516382944630757, 0.013856377389619541, 0.013122071288747525, 0.015489030097810462, 0.01471547092907986, 0.013114781656250659, 0.011958410089617136, 0.013548446776227178, 0.015462984506023967, 0.013116281506241891, 0.013756935905886706, 0.014845028490571564, 0.014106334767383011, 0.013278724753059488, 0.013945103129489034, 0.013368670368431894, 0.01508206513415732, 0.012516814171020588, 0.012146653351470226, 0.01250687009077665, 0.014221626724106895, 0.014866898722825189, 0.012547184474478406, 0.014589700216550094, 0.013945260788844572, 0.01604011728185909, 0.013546633724718439, 0.014448155572556428, 0.015068461636471004, 0.011489142494543256, 0.014638214961225686, 0.01463234902437323, 0.017366174464549394, 0.013168770387189811, 0.013039561818075432, 0.0167758537785324, 0.013822469712140894, 0.013091523012339876, 0.01290086084063744, 0.015205142005122251, 0.015764483770999795, 0.014301245989908484, 0.012765277602360542, 0.013953406370968747, 0.01033691615061267], "accuracy_test_std": 0.012824902806683067, "error_valid": [0.8988301840173193, 0.8378950371799698, 0.8168371729103916, 0.8842626364834337, 0.793102586125753, 0.8765104362763554, 0.779552781438253, 0.7499088149472892, 0.7421257294804217, 0.7032132435993976, 0.7193882953689759, 0.7025823018637049, 0.7262139377823795, 0.794079148625753, 0.7801013624811747, 0.7753097350338856, 0.8031432370105421, 0.6316359186746988, 0.7330719361822289, 0.7749229339231928, 0.7437641189759037, 0.7129288638930723, 0.7849444653614458, 0.6648625753012049, 0.8307605421686747, 0.8355139307228916, 0.8093482327748494, 0.8568880012236446, 0.8049742916980421, 0.8741513907191265, 0.7440288497740963, 0.8282088314194277, 0.825218844126506, 0.7344441241528614, 0.7721962067018072, 0.8487166439194277, 0.7592464584902108, 0.8312620599585843, 0.7626747223268072, 0.8431425898908133, 0.7841502729668675, 0.8715143778237951, 0.8258483151355421, 0.7565006118222892, 0.8599882930158133, 0.8781061746987951, 0.8333063700112951, 0.8496137871799698, 0.8199080501694277, 0.8896425545933735, 0.8663668345256024, 0.7438450089420181, 0.8809549722326807, 0.8491152108433735, 0.8521243175828314, 0.863123882247741, 0.805675828313253, 0.7907626600150602, 0.7978530332266567, 0.7551872529179217, 0.769287109375, 0.8308855539344879, 0.7314953172063253, 0.7220135424510542, 0.7742919921875, 0.6292680487575302, 0.8318297604480421, 0.8290015530873494, 0.7800322383283133, 0.8181770048945783, 0.7367237504706325, 0.7746376129518072, 0.8100394743034638, 0.8009445006588856, 0.8381068218185241, 0.8459796216114458, 0.8858377847326807, 0.8093482327748494, 0.8528567394578314, 0.749988234186747, 0.7186867587537651, 0.8149428769766567, 0.7982383636106928, 0.8442000423569277, 0.8106998305722892, 0.7512309982115963, 0.6939785509224398, 0.6890030826430723, 0.7260300969503012, 0.6559382059487951, 0.841808640813253, 0.5710993387612951, 0.5613131235881024, 0.7302554946347892, 0.8224200512989458, 0.6295210137424698, 0.6404367469879518, 0.5921557323042168, 0.7439141330948795, 0.7659617964043675, 0.6112825324736446, 0.634608257247741, 0.7460834549134037, 0.6516583913780121, 0.6826863116528614, 0.6422795674887049, 0.6069277108433735, 0.5967238092996988, 0.6577516119164157, 0.7172925098832832, 0.7436199877635542, 0.6215555581701807, 0.761474609375, 0.7454245693712349, 0.6986363422439759, 0.6258294898343373, 0.5530608763177711, 0.7068253482680723, 0.7129082737198795, 0.6497655661709337, 0.7240799134036144, 0.7446083160768072, 0.7686355774661144, 0.7987575301204819, 0.7039059558546686, 0.7271713808358433, 0.7364899049322289, 0.6755944677146084, 0.6061938182417168, 0.7113110645707832, 0.7685767483998494, 0.6582090079066265, 0.5881685923381024, 0.7248020401920181, 0.5583628459149097, 0.7099991763930723, 0.6105692300451807, 0.6249735269201807, 0.700061476374247, 0.6961140460278614, 0.8037329983998494, 0.6192171027861446, 0.6954125094126506, 0.6400793604103916, 0.6354230398155121, 0.5909159097326807, 0.6626535438629518, 0.6641698630459337, 0.5846491434487951, 0.6920754306287651, 0.705664944935994, 0.7365002000188253, 0.5587996517319277, 0.6306108221950302, 0.6333154885165663, 0.5040474397590362, 0.5801207760730421, 0.7114346056099398, 0.7087064076618976, 0.7446995011295181, 0.6948212772966867, 0.5985548639871988, 0.5771616740399097, 0.6784138507153614, 0.6190126717808735, 0.6647419757153614, 0.8168165827371988, 0.7341808640813253, 0.6525923028049698, 0.5518504682793675, 0.6811185170368976, 0.7327777908509037, 0.6381997717432228, 0.6814744328878012, 0.6370893731174698, 0.750622117375753, 0.6045671945594879, 0.7599803510918675, 0.6437341161521084, 0.8005076948418675, 0.7041309770331325, 0.6620961384600903, 0.6390630882906627, 0.7025234727974398, 0.7505206372364458, 0.723743116999247, 0.6203466208584337, 0.6568971197289157, 0.644873929310994, 0.5847712137612951, 0.5720435452748494, 0.6981980657003012, 0.5782088314194277, 0.6734074971762049, 0.6853615634412651, 0.6574559958584337, 0.6225733010165663, 0.6782608951430723, 0.6754635730421686, 0.715076124811747, 0.6994820100715362, 0.6302754965173193, 0.7785556287650602, 0.6158285485692772, 0.634312641189759, 0.7568138766001506], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.9119465273386376, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.002580874066702465, "patience_threshold": 1, "do_flip": true, "batch_size": 128, "optimization": "rmsprop", "nb_data_augmentation": 1, "learning_rate_decay_method": "none", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0009934383818865746, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.04106018354787211}, "accuracy_valid_max": 0.49595256024096385, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.2431861233998494, "loss_train": [7.773796081542969, 3.61228609085083, 2.750589370727539, 2.403024911880493, 2.2392518520355225, 2.132812023162842, 2.061936855316162, 2.0161349773406982, 1.9806278944015503, 1.9508086442947388, 1.92545485496521, 1.9062076807022095, 1.8886082172393799, 1.8683642148971558, 1.8533378839492798, 1.8398332595825195, 1.8271279335021973, 1.8167823553085327, 1.8044863939285278, 1.796926736831665, 1.7897140979766846, 1.7799242734909058, 1.7722716331481934, 1.7678877115249634, 1.765482783317566, 1.7595607042312622, 1.7522988319396973, 1.7482606172561646, 1.7417796850204468, 1.7395941019058228, 1.7351019382476807, 1.7306450605392456, 1.7302720546722412, 1.724002718925476, 1.7243562936782837, 1.7198292016983032, 1.7182725667953491, 1.7148010730743408, 1.7093604803085327, 1.7087481021881104, 1.7046127319335938, 1.7031090259552002, 1.7021846771240234, 1.700108289718628, 1.6976710557937622, 1.6955246925354004, 1.6941319704055786, 1.6938971281051636, 1.691894769668579, 1.6894639730453491, 1.6884739398956299, 1.6882613897323608, 1.6823184490203857, 1.683514952659607, 1.6808222532272339, 1.680726408958435, 1.6818466186523438, 1.674710988998413, 1.67774498462677, 1.6718991994857788, 1.671384572982788, 1.6700719594955444, 1.6710706949234009, 1.6656545400619507, 1.6656595468521118, 1.6646937131881714, 1.6628230810165405, 1.6619147062301636, 1.6607509851455688, 1.6608290672302246, 1.6601125001907349, 1.6580322980880737, 1.6562458276748657, 1.65552818775177, 1.653691291809082, 1.6519503593444824, 1.6508655548095703, 1.6494371891021729, 1.649465799331665, 1.6483436822891235, 1.6445432901382446, 1.642615556716919, 1.6435000896453857, 1.6406700611114502, 1.63890540599823, 1.639017939567566, 1.6373225450515747, 1.6373775005340576, 1.635799765586853, 1.6329600811004639, 1.6320282220840454, 1.630508542060852, 1.6294041872024536, 1.627385139465332, 1.627648115158081, 1.626596212387085, 1.6229667663574219, 1.6219512224197388, 1.6226094961166382, 1.6265205144882202, 1.6203283071517944, 1.6190401315689087, 1.6199896335601807, 1.6227000951766968, 1.6223727464675903, 1.6231590509414673, 1.6203088760375977, 1.619195580482483, 1.6201157569885254, 1.6196057796478271, 1.6194281578063965, 1.6170954704284668, 1.6181387901306152, 1.6174572706222534, 1.6159147024154663, 1.61881422996521, 1.6172728538513184, 1.6166237592697144, 1.6161068677902222, 1.6141817569732666, 1.6141562461853027, 1.6150320768356323, 1.6143828630447388, 1.6153157949447632, 1.6136689186096191, 1.6160756349563599, 1.6147689819335938, 1.613031268119812, 1.6126060485839844, 1.616687536239624, 1.6141855716705322, 1.613691806793213, 1.61330246925354, 1.6127145290374756, 1.6140408515930176, 1.610469937324524, 1.6109493970870972, 1.6138803958892822, 1.6114780902862549, 1.612243890762329, 1.611990213394165, 1.6099411249160767, 1.6133112907409668, 1.6118170022964478, 1.611944317817688, 1.6112513542175293, 1.6112827062606812, 1.610662817955017, 1.609683871269226, 1.6092673540115356, 1.608828067779541, 1.609933614730835, 1.6103863716125488, 1.6114615201950073, 1.6097593307495117, 1.611753225326538, 1.609187364578247, 1.6116315126419067, 1.6140737533569336, 1.6119428873062134, 1.6112957000732422, 1.6090435981750488, 1.610837697982788, 1.6088577508926392, 1.6123944520950317, 1.6106103658676147, 1.60986328125, 1.610953688621521, 1.6101906299591064, 1.6096365451812744, 1.6078287363052368, 1.607589602470398, 1.609938383102417, 1.6096950769424438, 1.6084144115447998, 1.6073416471481323, 1.6076582670211792, 1.609397530555725, 1.6086316108703613, 1.6074737310409546, 1.608208179473877, 1.608516812324524, 1.6093460321426392, 1.609717607498169, 1.6099281311035156, 1.6055059432983398, 1.6099622249603271, 1.6072981357574463, 1.6081243753433228, 1.6092631816864014, 1.6073338985443115, 1.6054214239120483, 1.6094129085540771, 1.6100817918777466, 1.6072144508361816, 1.6070364713668823, 1.606484293937683, 1.6058990955352783, 1.6043477058410645, 1.6059870719909668, 1.60686194896698, 1.6052806377410889, 1.6077979803085327, 1.605945348739624, 1.6066073179244995, 1.606203556060791], "accuracy_train_first": 0.09993976242386489, "model": "residualv3", "loss_std": [4.704107761383057, 0.4474053680896759, 0.20966625213623047, 0.12098335474729538, 0.10588487237691879, 0.10804586857557297, 0.10743983834981918, 0.10646415501832962, 0.10440783202648163, 0.10341819375753403, 0.10057723522186279, 0.1021357998251915, 0.1001550480723381, 0.09645683318376541, 0.10162491351366043, 0.1001165583729744, 0.10099846124649048, 0.10340773314237595, 0.09663901478052139, 0.09641853719949722, 0.09795693308115005, 0.09836894273757935, 0.09860443323850632, 0.09772613644599915, 0.09709580987691879, 0.09482480585575104, 0.1006639301776886, 0.09833379089832306, 0.10285408049821854, 0.10443872213363647, 0.10397204011678696, 0.10203626751899719, 0.10429859906435013, 0.09702879190444946, 0.09717108309268951, 0.09710101783275604, 0.10305222123861313, 0.10096203535795212, 0.10138828307390213, 0.1017458438873291, 0.10110612213611603, 0.10092180967330933, 0.10481254756450653, 0.1036301702260971, 0.10029061138629913, 0.10309765487909317, 0.09878233820199966, 0.10274282097816467, 0.09916942566633224, 0.10263841599225998, 0.10115962475538254, 0.10440586507320404, 0.10169549286365509, 0.09969879686832428, 0.0976966917514801, 0.10527729988098145, 0.10295835882425308, 0.09966355562210083, 0.09975675493478775, 0.09800446778535843, 0.10105131566524506, 0.0964493602514267, 0.09771722555160522, 0.09772884845733643, 0.09706493467092514, 0.09767322242259979, 0.09990754723548889, 0.10205051302909851, 0.10086231678724289, 0.09948908537626266, 0.10496058315038681, 0.0985296219587326, 0.10028348863124847, 0.10548365116119385, 0.09743167459964752, 0.10138699412345886, 0.09845231473445892, 0.10172045230865479, 0.09867236018180847, 0.09994351863861084, 0.1000714898109436, 0.09600723534822464, 0.09767396003007889, 0.09789759665727615, 0.09932571649551392, 0.098957359790802, 0.10087599605321884, 0.0982256531715393, 0.09777109324932098, 0.09677346795797348, 0.09619840234518051, 0.09663619101047516, 0.09506147354841232, 0.09855124354362488, 0.09874048829078674, 0.10075540840625763, 0.09646036475896835, 0.09717892110347748, 0.09856241196393967, 0.10239390283823013, 0.0978817492723465, 0.09468322992324829, 0.09555605053901672, 0.10148123651742935, 0.10077016800642014, 0.10097958892583847, 0.10060224682092667, 0.09695911407470703, 0.10245038568973541, 0.10367035865783691, 0.09953439235687256, 0.10224592685699463, 0.10165289789438248, 0.1003323569893837, 0.10078230500221252, 0.10349759459495544, 0.0990830734372139, 0.0975676104426384, 0.10125043243169785, 0.10086527466773987, 0.09565258026123047, 0.10257430374622345, 0.09922200441360474, 0.09873805940151215, 0.10103496164083481, 0.09888804703950882, 0.10709067434072495, 0.10507942736148834, 0.09729863703250885, 0.1059844121336937, 0.09784852713346481, 0.09731888025999069, 0.10067543387413025, 0.09908203780651093, 0.09940431267023087, 0.10014288127422333, 0.09969756752252579, 0.09649381786584854, 0.10414746403694153, 0.10378684848546982, 0.10228342562913895, 0.10098659247159958, 0.10210729390382767, 0.10101340711116791, 0.09768261760473251, 0.10264874994754791, 0.09855374693870544, 0.09786491841077805, 0.09980706870555878, 0.098016656935215, 0.09757144004106522, 0.10267448425292969, 0.09908219426870346, 0.09977028518915176, 0.10600975900888443, 0.10113700479269028, 0.09828454256057739, 0.10179322212934494, 0.10432281345129013, 0.10401308536529541, 0.1035628393292427, 0.10159341990947723, 0.10210074484348297, 0.10215669125318527, 0.10079886764287949, 0.10152126848697662, 0.10192655771970749, 0.10278771072626114, 0.10284217447042465, 0.10306397080421448, 0.10129893571138382, 0.09996695816516876, 0.10032445192337036, 0.10262423008680344, 0.09808465838432312, 0.1027960404753685, 0.09985661506652832, 0.09860678017139435, 0.1011090874671936, 0.10277783125638962, 0.1033664420247078, 0.10101974010467529, 0.10507500916719437, 0.10194671154022217, 0.09912452101707458, 0.09859447926282883, 0.09770268201828003, 0.10425970703363419, 0.10004211217164993, 0.09784486144781113, 0.10017027705907822, 0.09650608152151108, 0.10544011741876602, 0.09781282395124435, 0.10200896114110947, 0.10195926576852798, 0.10332982242107391, 0.09730317443609238, 0.10296835005283356, 0.10103162378072739, 0.09977374970912933, 0.09953860938549042, 0.1023096814751625, 0.09924829006195068, 0.09923858195543289, 0.10153606534004211]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:11 2016", "state": "available"}], "summary": "4d49c3147485d464f560008736acaa5b"}