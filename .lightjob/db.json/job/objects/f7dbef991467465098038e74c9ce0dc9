{"content": {"hp_model": {"f0": 32, "f1": 32, "f2": 64, "f3": 16, "nonlin": "rectify", "nbg1": 3, "nbg3": 3, "nbg2": 8, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "accuracy_valid_std": [0.009065469397630217, 0.009027840636461303, 0.01061533854208975, 0.011345814579255422, 0.01647935134890598, 0.017180067616548438, 0.015745139856849454, 0.014101088809877815, 0.017162563161060143, 0.018061730304936605, 0.015620721169406354, 0.014224494682003574, 0.014960764484934578, 0.012921497316557801, 0.009058059693806549, 0.012455980514471691, 0.010799120215785406, 0.013842060428755028, 0.014357967836605258, 0.014160404245826578, 0.014120995480515833, 0.01302053637901277, 0.012521354498593087, 0.01433917844783759, 0.012257474572320098, 0.011692161111445302, 0.012284052710481745, 0.014429490956518241, 0.014547800056264801, 0.013961371962400063, 0.013193954905190595, 0.014245763895805987, 0.015274506827180357, 0.015123404261233653, 0.014869981784314801, 0.014185396936646649, 0.01450750747159054, 0.01624803104582104, 0.014943589145049944, 0.015370102067148486, 0.01570449912513785, 0.014902667543575773, 0.015396454278474558, 0.015158410287054003, 0.015866414503257993, 0.01412436876370379, 0.017409784493069444, 0.015477207326737762, 0.01539447571094615, 0.01648327285358537, 0.015000233828118947, 0.01604584512859048, 0.016408998941641568, 0.01659720440514913, 0.01543979211187585, 0.015605745691224653, 0.014074843269404775, 0.016178007127937617, 0.01673122684632554, 0.015592833991660823, 0.0146585197303101, 0.014920985105328499, 0.014000247436656836, 0.014233914679363573, 0.014539558129753652, 0.015721481859024826, 0.015008939840874631, 0.014811357165020671, 0.014711186412873419, 0.015470299588956618, 0.015129865358526657, 0.015634801331886616, 0.0155571799378207, 0.01546877486100149, 0.015506487918635825, 0.015341980902695958, 0.015422266093253022, 0.01658333511812595, 0.01595289577580764, 0.015791059928426443, 0.015111758622020565, 0.01576005260239956, 0.014489498703206283, 0.015321652740400494, 0.015215536096955811, 0.015256992260322125, 0.01572074246093554, 0.014626713406726263, 0.01662132626354713, 0.01527693780993189, 0.015384765277001212, 0.014380264293242701, 0.014756540434968783, 0.015189499437748192, 0.015369768533054204, 0.015081452747928449, 0.01546837605367424, 0.01630546238309961, 0.015680891682209992, 0.015060606055832575, 0.015604343498418834, 0.01582689778575265, 0.014538679844876725, 0.014791299648040764, 0.015689268597961017, 0.014472378883139953, 0.015452968023700355, 0.015550880803961581, 0.015724867387572925, 0.015103290040162327, 0.014965943334569062, 0.01625053049566734, 0.015729380819266683, 0.01657794710762288, 0.015908787089150844, 0.015346669803197075, 0.016457567122300006, 0.015294507037463401, 0.01533474312165447, 0.01544349934969464, 0.015555056406799134, 0.015483978993634804, 0.01601189499808412, 0.015140167391206959, 0.016859885352961963, 0.015575298810368692, 0.016102650279728452, 0.014904820530824541, 0.01689049188683715, 0.015459812359654743, 0.014560053586660947, 0.014942053591967059, 0.014799278020917739, 0.016933787503501628, 0.014802601579138406, 0.014279535201724222, 0.016760927456202775, 0.015205293735360996, 0.015423965599461577, 0.015952176866826335, 0.01453081025567402, 0.015672595926911094, 0.01589199301606018, 0.014713755084024479, 0.015447027161431506, 0.015907479324838134, 0.015612272975777778, 0.016405128047382096, 0.015050104978774051, 0.01427630229304678, 0.016353850787781586, 0.01609304235647876, 0.01602273502047622, 0.015150932054502235, 0.015363344337763108, 0.01596062105359427, 0.015220210728922308, 0.015524840950361756, 0.014911277990048306, 0.015654021837004692, 0.01539629039684798, 0.015733464986789927, 0.014729949960229307, 0.01672224047267407, 0.014813593365584864, 0.016081130456457145, 0.015468296137154459, 0.015435724619713067, 0.016105245019087715, 0.014761235868353902, 0.015275014847892166, 0.01529702611734061, 0.015138814018929098, 0.01555448633286139, 0.014911581961151563, 0.016245272239263316, 0.016689407891646354, 0.01691032370486358, 0.01563596679761973, 0.015146785778711884, 0.014813853930682274, 0.014714145531584942, 0.014768017925859255, 0.01675704149789608, 0.015159536897488954, 0.015391036339236287, 0.015888936880000863, 0.015318740655234078, 0.014949872151828796, 0.015400761639479699, 0.017286426806118662, 0.01635452743990391, 0.015013684849400888, 0.014772907158759396, 0.015550558658521146, 0.01609210664657308, 0.014194566151921047, 0.015881017035204927, 0.015588227809518051, 0.015844060933242303, 0.013718292114528581, 0.016450324789815597, 0.01585624724183568, 0.015228001173765493, 0.0165072252242922, 0.01493246837952139, 0.01520584880625098, 0.015091288330935747, 0.015951484706660145, 0.016684393030425928, 0.015469260911008972, 0.015809553876930862, 0.015187361242077707, 0.01550864264720853, 0.015109820507671099, 0.015318652756733627], "moving_avg_accuracy_train": [0.022060975855943146, 0.04547776580956994, 0.06947275898913342, 0.09373901971005212, 0.11852614997534258, 0.1436872364128397, 0.16913097239687871, 0.19471323206933555, 0.21953903980901515, 0.24342805814723506, 0.26644846148487406, 0.2882384657482084, 0.30887467996611545, 0.32847266338723186, 0.34683378950190474, 0.36426793618963416, 0.3805259684692957, 0.39562776943405864, 0.4095727047749366, 0.4225346618721938, 0.43443758843829666, 0.4456102773727062, 0.45598420675176077, 0.46553469293220484, 0.47434876658033726, 0.48252324833984694, 0.49009419561388184, 0.49719625846643795, 0.5037554896992054, 0.5098586164110401, 0.5154607845433765, 0.5206514372398325, 0.5255043141761483, 0.5298672170723946, 0.5340240194111592, 0.5377279751839137, 0.5411660589293651, 0.5444184804681377, 0.5473828622339855, 0.5501878814565541, 0.5528634252830286, 0.5552436210875976, 0.5574927902057666, 0.5596843089311293, 0.5615869934173077, 0.5634156668953443, 0.565056714581502, 0.5666359640466629, 0.5680457709677165, 0.5693935801585509, 0.5705740563469687, 0.5716829878927351, 0.5726810983815623, 0.5736909689155453, 0.5745556024711114, 0.5754360071211025, 0.5762237931061129, 0.5770164698009465, 0.5777020130894011, 0.5783259053978014, 0.5788898778194462, 0.5794787250608036, 0.5799854360899299, 0.5804811116923618, 0.5808712719678469, 0.5811922613788972, 0.581667091655967, 0.5819503878255955, 0.5822936379353856, 0.5825467594627681, 0.5827886278767258, 0.5830411145837929, 0.5833054468546495, 0.5835294671032007, 0.5837148092852302, 0.583944396266914, 0.584134712459944, 0.5843455606122515, 0.5844354146481563, 0.5844882733483001, 0.5846520315212681, 0.584785643228176, 0.5848895816739077, 0.5849993302190953, 0.5850562872800116, 0.5851726528015029, 0.5852820320684642, 0.5854200009384912, 0.5854977059941437, 0.5855768690418316, 0.5855667716252361, 0.5856065120753002, 0.5856725775125192, 0.585662209844093, 0.5857203443068044, 0.5857562811351218, 0.5857980330223018, 0.585805238590965, 0.5858559735277803, 0.5859388373518665, 0.585962189422097, 0.5859948320293521, 0.5861451902116144, 0.586138606400632, 0.5861699554493378, 0.5861958444443635, 0.5862610332672766, 0.5862639356852887, 0.5862758124079189, 0.5862701533189819, 0.5863117073104042, 0.5862699787455229, 0.5862625418251972, 0.5863071460659888, 0.5863890344148163, 0.5863768115692649, 0.5864215424820597, 0.5864478854595366, 0.5864924483809141, 0.5863697945934871, 0.5863036561098214, 0.5862999350459508, 0.5863244157765439, 0.5863348947876678, 0.5863442898488603, 0.5863457699575051, 0.5863704256410182, 0.5864414078323611, 0.5865284351462088, 0.5865393664620142, 0.586584081878382, 0.5865593297329026, 0.5865091149674382, 0.5864987268130255, 0.5865219295573875, 0.5864545645190075, 0.5864078868773226, 0.5864052963831119, 0.5864285415752271, 0.5864100068160063, 0.5864559964041085, 0.586402200127485, 0.5863862634642197, 0.5863928468065667, 0.5863383900432688, 0.5863800237110535, 0.5863639074429844, 0.5864145790660263, 0.5864323538386873, 0.5865645725257396, 0.5865160504833447, 0.5865723980928175, 0.5865394776818377, 0.5864982235679083, 0.5865215126856006, 0.5864588756808371, 0.5864279708669986, 0.5865001018845346, 0.5864721219943924, 0.586439928598017, 0.5864644329638982, 0.5864329724217534, 0.5864278733730997, 0.5863791064019303, 0.5863771409040869, 0.5864334646274472, 0.5864121845118324, 0.5863836597149036, 0.5863091232238489, 0.5862862903069181, 0.5863075212626143, 0.5863893360429604, 0.5863886366810047, 0.5864484250754735, 0.5865278473162189, 0.5864947677340987, 0.5864696103589908, 0.5864632087142416, 0.5864969387149105, 0.5865250426643405, 0.5865015080938275, 0.5865268299565563, 0.586498394261565, 0.5865123657146536, 0.5865249039736147, 0.5865013472233555, 0.5864313540719409, 0.5864056347142577, 0.5863683922042107, 0.5863884605142436, 0.5863414178266066, 0.5863478714839145, 0.5863467043290631, 0.5863945180635156, 0.5863770244578377, 0.5864124334865372, 0.5864512410099765, 0.5864676026394144, 0.5865242528821175, 0.5864751846041033, 0.5865147645598521, 0.5864736926581304, 0.5864855200227622, 0.5864543480211781, 0.5864727240983054, 0.5865171643534344, 0.5865198500556417, 0.5865060271947804, 0.5864843581224046, 0.5864462187179715, 0.5863910750611523, 0.5864343796247583, 0.5864524634415368, 0.586473317076619, 0.5865061083386875, 0.5864937317471592], "dataset": "Cifar10", "nb_examples_train": 42500, "seed": 561419144, "moving_var_accuracy_train": [0.004380179901448559, 0.008877276376894196, 0.013171386018390463, 0.017153910100931834, 0.020968135531934957, 0.024569044415178272, 0.027938593281089788, 0.03103480204252191, 0.03347820840761753, 0.03526655434133005, 0.03650934963564527, 0.03713165324424588, 0.03725116795504745, 0.03698277974711189, 0.036318680342191004, 0.03542235754453531, 0.034259034312540425, 0.03288571041270022, 0.03134729036638161, 0.02972467231586367, 0.02802732203181964, 0.026348050630863373, 0.024681811264631547, 0.02303453621471407, 0.021430273641714986, 0.01988864564587249, 0.018415654264921186, 0.01702804250928398, 0.015712449887639788, 0.014476438299822205, 0.01331125305988667, 0.012222613632635037, 0.011212306000402761, 0.010262389695501162, 0.009391661777103077, 0.008575969194691462, 0.00782475605378895, 0.007137484661202903, 0.006502824228365827, 0.005923355001080058, 0.005395446313878526, 0.004906889671103466, 0.004461729559492246, 0.004058781392455565, 0.003685485127495501, 0.0033470330349494237, 0.003036567069028674, 0.002755356621984708, 0.002497708959778091, 0.0022642873703343632, 0.002050400349583718, 0.001856427877184109, 0.0016797511103968613, 0.001520954545815838, 0.0013755874119029535, 0.001245004681842192, 0.001126089674481583, 0.0010191357341162144, 0.0009214518871076999, 0.0008328098729092588, 0.0007523914696497153, 0.0006802729923476311, 0.0006145564977162127, 0.0005553120966702074, 0.0005011509123682863, 0.0004519631289495154, 0.0004087959901827624, 0.0003686387016420216, 0.00033283521721865725, 0.0003001283300654113, 0.0002706420000259034, 0.0002441515458585237, 0.00022036523521741714, 0.00019878037734152428, 0.00017921150512732699, 0.00016176474625402196, 0.00014591425390858432, 0.0001317229410077095, 0.00011862331063685396, 0.00010678612595279656, 9.634886401044146e-05, 8.687464640340286e-05, 7.828441056757263e-05, 7.056437219935266e-05, 6.353713194051134e-05, 5.730528715778765e-05, 5.1682432858377584e-05, 4.668550825440849e-05, 4.2071300110033325e-05, 3.79205711921031e-05, 3.412943169328992e-05, 3.073070225430257e-05, 2.769691380682671e-05, 2.492818982308139e-05, 2.2465787382565953e-05, 2.0230831744974945e-05, 1.8223437551225244e-05, 1.6401561078080553e-05, 1.478457127459535e-05, 1.336791186721541e-05, 1.2036028553150309e-05, 1.0842015556110943e-05, 9.961282247258797e-06, 8.965544141636385e-06, 8.077834593165527e-06, 7.276083294419899e-06, 6.586721208673227e-06, 5.928124904078757e-06, 5.3365819225348e-06, 4.803211957869687e-06, 4.3384313699107845e-06, 3.920259691063195e-06, 3.5287314920122452e-06, 3.1937641874802664e-06, 2.9347390837954495e-06, 2.6426097569962634e-06, 2.396356472331751e-06, 2.1629663972597336e-06, 1.9645424431890005e-06, 1.903483763001686e-06, 1.7525040778960222e-06, 1.5773782869533855e-06, 1.4250342137914306e-06, 1.2835190794794725e-06, 1.1559615761048432e-06, 1.0403851349887624e-06, 9.418177460553536e-07, 8.929822148405032e-07, 8.718477735563478e-07, 7.857384391878644e-07, 7.251598114175841e-07, 6.581578486283221e-07, 6.150357678013832e-07, 5.545034147901639e-07, 5.038983794244617e-07, 4.943509770453889e-07, 4.645250994401384e-07, 4.181329854384238e-07, 3.811827375028186e-07, 3.4615629944687557e-07, 3.305760494264896e-07, 3.23564798890784e-07, 2.9349411412598785e-07, 2.6453476628150275e-07, 2.647711412732664e-07, 2.538942877847253e-07, 2.3084246587455264e-07, 2.3086673972240712e-07, 2.106235486384986e-07, 3.468972246271014e-07, 3.333969995479679e-07, 3.286327774329054e-07, 3.0552328082131696e-07, 2.902880699841021e-07, 2.661407100116996e-07, 2.7483718830216064e-07, 2.5594943713753796e-07, 2.771804466408126e-07, 2.565082702480941e-07, 2.401851761549182e-07, 2.2157083406459198e-07, 2.0832164206648433e-07, 1.8772348053439516e-07, 1.9035508977421794e-07, 1.713543494327497e-07, 1.8277017080790892e-07, 1.6856874361232208e-07, 1.5903484560960513e-07, 1.9313255753747983e-07, 1.78511380643869e-07, 1.6471702389742853e-07, 2.08488246055518e-07, 1.8764382341427183e-07, 2.0105131009119213e-07, 2.3771721000723266e-07, 2.2379381778574699e-07, 2.0711047770805333e-07, 1.8676825943670394e-07, 1.7833084999912731e-07, 1.6760625276126892e-07, 1.5583051156824297e-07, 1.460182309998937e-07, 1.3869370664657993e-07, 1.2658114949458534e-07, 1.1533790598508184e-07, 1.0879839973154764e-07, 1.4200993096283884e-07, 1.3376230610327907e-07, 1.3286911648438113e-07, 1.2320683844416253e-07, 1.3080328474074755e-07, 1.1809780350051036e-07, 1.063002834044836e-07, 1.162456338846784e-07, 1.0737530665272681e-07, 1.0792196980839296e-07, 1.1068398770698686e-07, 1.020249151970658e-07, 1.2070567366221115e-07, 1.3030436946155846e-07, 1.3137308858907374e-07, 1.3341788972945022e-07, 1.2133507974369987e-07, 1.1794681491414986e-07, 1.0919125531802826e-07, 1.1604655626947851e-07, 1.045068176096482e-07, 9.57757791902077e-08, 9.042413954983028e-08, 9.447325312949326e-08, 1.1239333380319047e-07, 1.1803156748488523e-07, 1.0917163059990694e-07, 1.0216833440518896e-07, 1.0162890277715566e-07, 9.284463266019063e-08], "duration": 267297.889134, "accuracy_train": [0.22060975855943152, 0.2562288753922112, 0.28542769760520487, 0.31213536619832044, 0.3416103223629568, 0.37013701435031376, 0.39812459625323, 0.42495356912144705, 0.44297130946613145, 0.4584292231912145, 0.47363209152362495, 0.48434850411821706, 0.49460060792727945, 0.5048545141772794, 0.5120839245339608, 0.5211752563791989, 0.5268482589862495, 0.5315439781169251, 0.5350771228428387, 0.5391922757475083, 0.5415639275332226, 0.546164477782392, 0.5493495711632521, 0.5514890685562015, 0.553675429413529, 0.5560935841754338, 0.5582327210801956, 0.5611148241394426, 0.5627885707941123, 0.5647867568175526, 0.5658802977344038, 0.5673673115079365, 0.5691802066029901, 0.5691333431386121, 0.5714352404600407, 0.5710635771387044, 0.5721088126384275, 0.5736902743170912, 0.574062298126615, 0.5754330544596714, 0.5769433197212994, 0.5766653833287191, 0.5777353122692875, 0.5794079774593947, 0.5787111537929125, 0.5798737281976744, 0.5798261437569214, 0.5808492092331119, 0.5807340332571982, 0.5815238628760613, 0.581198342042728, 0.5816633718046328, 0.5816640927810077, 0.5827798037213916, 0.5823373044712071, 0.5833596489710225, 0.5833138669712071, 0.5841505600544481, 0.5838719026854928, 0.5839409361734035, 0.5839656296142488, 0.5847783502330196, 0.5845458353520672, 0.5849421921142488, 0.584382714447213, 0.5840811660783499, 0.585940564149594, 0.5845000533522517, 0.5853828889234958, 0.5848248532092101, 0.584965443602344, 0.5853134949473976, 0.5856844372923588, 0.5855456493401624, 0.5853828889234958, 0.5860106791020672, 0.585847558197213, 0.5862431939830196, 0.5852441009712994, 0.584964001649594, 0.5861258550779808, 0.585988148590347, 0.5858250276854928, 0.5859870671257844, 0.5855689008282576, 0.5862199424949244, 0.5862664454711148, 0.5866617207687339, 0.5861970514950167, 0.5862893364710225, 0.5854758948758767, 0.5859641761258767, 0.5862671664474899, 0.5855689008282576, 0.5862435544712071, 0.5860797125899778, 0.5861738000069214, 0.5858700887089332, 0.5863125879591178, 0.5866846117686416, 0.5861723580541712, 0.5862886154946475, 0.5874984138519749, 0.5860793521017903, 0.5864520968876892, 0.586428845399594, 0.5868477326734958, 0.5862900574473976, 0.586382702911591, 0.5862192215185493, 0.5866856932332042, 0.585894421661591, 0.5861956095422665, 0.5867085842331119, 0.5871260295542635, 0.5862668059593024, 0.586824120697213, 0.5866849722568291, 0.5868935146733112, 0.5852659105066446, 0.5857084097568291, 0.5862664454711148, 0.5865447423518826, 0.5864292058877815, 0.586428845399594, 0.5863590909353082, 0.5865923267926357, 0.5870802475544481, 0.587311680970838, 0.5866377483042635, 0.5869865206256921, 0.5863365604235881, 0.5860571820782576, 0.5864052334233112, 0.5867307542566446, 0.5858482791735881, 0.5859877881021595, 0.586381981935216, 0.5866377483042635, 0.5862431939830196, 0.5868699026970284, 0.5859180336378738, 0.5862428334948321, 0.5864520968876892, 0.5858482791735881, 0.5867547267211148, 0.5862188610303617, 0.5868706236734035, 0.5865923267926357, 0.5877545407092101, 0.5860793521017903, 0.587079526578073, 0.5862431939830196, 0.5861269365425434, 0.5867311147448321, 0.585895142637966, 0.586149827542451, 0.5871492810423588, 0.5862203029831119, 0.5861501880306387, 0.5866849722568291, 0.586149827542451, 0.586381981935216, 0.5859402036614064, 0.5863594514234958, 0.5869403781376892, 0.5862206634712994, 0.5861269365425434, 0.5856382948043558, 0.5860807940545405, 0.5864985998638796, 0.587125669066076, 0.5863823424234035, 0.5869865206256921, 0.5872426474829273, 0.5861970514950167, 0.5862431939830196, 0.5864055939114987, 0.5868005087209303, 0.5867779782092101, 0.5862896969592101, 0.5867547267211148, 0.5862424730066446, 0.586638108792451, 0.5866377483042635, 0.5862893364710225, 0.5858014157092101, 0.586174160495109, 0.5860332096137874, 0.5865690753045405, 0.5859180336378738, 0.5864059543996862, 0.5863361999354005, 0.5868248416735881, 0.5862195820067369, 0.5867311147448321, 0.5868005087209303, 0.5866148573043558, 0.5870341050664452, 0.5860335701019749, 0.586870984161591, 0.5861040455426357, 0.5865919663044481, 0.5861738000069214, 0.586638108792451, 0.586917126649594, 0.5865440213755075, 0.5863816214470284, 0.5862893364710225, 0.586102964078073, 0.5858947821497785, 0.586824120697213, 0.5866152177925434, 0.5866609997923588, 0.5868012296973053, 0.5863823424234035], "end": "2016-01-27 12:23:33.360000", "learning_rate_per_epoch": [0.00037187946145422757, 0.0003471458039712161, 0.00032405718229711056, 0.0003025041660293937, 0.0002823846589308232, 0.00026360328774899244, 0.0002460710529703647, 0.00022970489226281643, 0.00021442724391818047, 0.00020016571215819567, 0.00018685271788854152, 0.0001744251640047878, 0.00016282417345792055, 0.00015199475456029177, 0.00014188559725880623, 0.00013244881120044738, 0.00012363966379780322, 0.00011541640560608357, 0.00010774007387226447, 0.00010057429608423263, 9.388511534780264e-05, 8.764083031564951e-05, 8.181184966815636e-05, 7.637055387021974e-05, 7.129115692805499e-05, 6.654958997387439e-05, 6.212338485056534e-05, 5.799156497232616e-05, 5.4134554375195876e-05, 5.053407221566886e-05, 4.717305637313984e-05, 4.4035583414370194e-05, 4.110678128199652e-05, 3.837277472484857e-05, 3.5820605262415484e-05, 3.343818025314249e-05, 3.1214211048791185e-05, 2.913815842475742e-05, 2.720018346735742e-05, 2.5391102099092677e-05, 2.3702343241893686e-05, 2.2125903342384845e-05, 2.065431181108579e-05, 1.928059646161273e-05, 1.7998247130890377e-05, 1.680118657532148e-05, 1.5683741366956383e-05, 1.4640617337136064e-05, 1.3666871382156387e-05, 1.2757889635395259e-05, 1.190936382045038e-05, 1.1117273970739916e-05, 1.0377865692134947e-05, 9.687634701549541e-06, 9.043311365530826e-06, 8.441841600870248e-06, 7.880375960667152e-06, 7.356253263424151e-06, 6.866989679110702e-06, 6.410266905731987e-06, 5.983920800645137e-06, 5.5859309213701636e-06, 5.21441143064294e-06, 4.8676015467208344e-06, 4.543857812677743e-06, 4.2416463656991255e-06, 3.959535206377041e-06, 3.6961869227525312e-06, 3.4503539154684404e-06, 3.2208713491854724e-06, 3.006651695613982e-06, 2.8066797312931158e-06, 2.620007762743626e-06, 2.445751306368038e-06, 2.283084768350818e-06, 2.131237124558538e-06, 1.9894887373084202e-06, 1.8571680584500427e-06, 1.7336479913865332e-06, 1.618343276277301e-06, 1.5107074204934179e-06, 1.410230424880865e-06, 1.3164361689632642e-06, 1.228880137205124e-06, 1.147147486335598e-06, 1.0708508852985688e-06, 9.99628696263244e-07, 9.331434966952656e-07, 8.710802603673073e-07, 8.131448225867643e-07, 7.590626864839578e-07, 7.085775450832443e-07, 6.614501444346388e-07, 6.174572035888559e-07, 5.763902208855143e-07, 5.380545644584345e-07, 5.022686195843562e-07, 4.6886279392310826e-07, 4.376787785531633e-07, 4.085688090071926e-07, 3.813949547293305e-07, 3.5602843695414776e-07, 3.323490318507538e-07, 3.102445305103174e-07, 2.896101989335875e-07, 2.7034826644012355e-07, 2.5236744249923504e-07, 2.3558251882604964e-07, 2.1991395726672636e-07, 2.0528750610537827e-07, 1.9163385900355934e-07, 1.7888831393975124e-07, 1.6699047478141438e-07, 1.5588395285703882e-07, 1.4551612537161418e-07, 1.3583786540039e-07, 1.268033003043456e-07, 1.1836962698907882e-07, 1.1049687742570313e-07, 1.0314774101516377e-07, 9.62874011634085e-08, 8.988333632942158e-08, 8.390520633838605e-08, 7.832468185142716e-08, 7.311531646791991e-08, 6.8252425933224e-08, 6.371296734641874e-08, 5.947542547346529e-08, 5.551972392936477e-08, 5.1827115044034144e-08, 4.8380101702605316e-08, 4.516234852758316e-08, 4.215860727185827e-08, 3.935464221171969e-08, 3.673716975072239e-08, 3.4293783812699985e-08, 3.2012909656486954e-08, 2.9883736374358705e-08, 2.7896172483110604e-08, 2.604080151513699e-08, 2.4308830504082835e-08, 2.2692052681350106e-08, 2.118280661989047e-08, 1.9773940707068505e-08, 1.8458777617524902e-08, 1.7231085891467046e-08, 1.6085047960245902e-08, 1.5015233501003422e-08, 1.401657190314154e-08, 1.308433184021851e-08, 1.2214094624596328e-08, 1.1401736443872323e-08, 1.0643408820953937e-08, 9.935517297776641e-09, 9.27470722444923e-09, 8.65784777204226e-09, 8.082015057198078e-09, 7.544481483989784e-09, 7.042698868531261e-09, 6.574290001282179e-09, 6.137034880282499e-09, 5.7288613852790604e-09, 5.347835507762966e-09, 4.992151581006965e-09, 4.660124286459677e-09, 4.350180216050603e-09, 4.060850322673559e-09, 3.790763702937738e-09, 3.5386404917403524e-09, 3.3032858670623e-09, 3.0835847208976475e-09, 2.8784958860939014e-09, 2.687047473415305e-09, 2.5083322086061344e-09, 2.3415034355878106e-09, 2.1857702314775906e-09, 2.0403949640979135e-09, 1.904688629039697e-09, 1.7780080741047755e-09, 1.6597530017037343e-09, 1.5493630822760451e-09, 1.4463151787325046e-09, 1.35012090396458e-09, 1.260324511420663e-09, 1.1765004526154144e-09, 1.098251600772926e-09, 1.0252070303806704e-09, 9.57020684921872e-10, 8.933693784740626e-10, 8.339515189526026e-10, 7.784854982872957e-10, 7.267085266882134e-10, 6.783752448669134e-10, 6.332566138134155e-10, 5.911388045731769e-10, 5.518222545575213e-10, 5.151206128317654e-10, 4.808600184702527e-10, 4.488780736000564e-10, 4.190232605338906e-10, 3.911540813472669e-10, 3.6513847501140617e-10, 3.408531512594237e-10, 3.181830632303928e-10, 2.970207413355297e-10, 2.7726593243571074e-10, 2.588250169743844e-10, 2.4161059264393714e-10, 2.2554109968542235e-10, 2.105403906771386e-10, 1.9653736971214641e-10, 1.8346568708693667e-10, 1.7126340623452307e-10, 1.5987269841311047e-10, 1.4923957902812646e-10, 1.3931367170982867e-10], "accuracy_valid": [0.2262565888554217, 0.2568565276731928, 0.28387524708207834, 0.3100909497364458, 0.3406805934676205, 0.3763368905308735, 0.40378359139683734, 0.42979633377259036, 0.45198371611445787, 0.46289944935993976, 0.4745578995670181, 0.4924419357115964, 0.5001735457454819, 0.5067344573606928, 0.5137336455195783, 0.5182296569088856, 0.5243331725338856, 0.5250450042356928, 0.5299689970820783, 0.5319015319088856, 0.5338546569088856, 0.5381682981927711, 0.5417289274284638, 0.544567429875753, 0.5497149731739458, 0.5500811841114458, 0.5520240140248494, 0.5558390789721386, 0.5543639401355422, 0.5589114269578314, 0.5573039227221386, 0.5609969173569277, 0.5634589137801205, 0.5612307628953314, 0.5650252376694277, 0.5667548122176205, 0.5658797298569277, 0.5687079372176205, 0.5684535015060241, 0.5694300640060241, 0.5694506541792168, 0.5697962749435241, 0.5711390483810241, 0.5720244258283133, 0.5715258494917168, 0.5724009318524097, 0.5721259059676205, 0.5734892695783133, 0.5734789744917168, 0.5740790309676205, 0.5743334666792168, 0.5761748164533133, 0.5762968867658133, 0.5756762401167168, 0.5762865916792168, 0.578026461314006, 0.5761851115399097, 0.5767954631024097, 0.5754320994917168, 0.5771616740399097, 0.5770396037274097, 0.5777617305158133, 0.577660250376506, 0.5784941523908133, 0.577049898814006, 0.5775278849774097, 0.5786162227033133, 0.5780161662274097, 0.579613375376506, 0.579125094126506, 0.5798678110881024, 0.579003023814006, 0.5798678110881024, 0.5786265177899097, 0.579369234751506, 0.5798472209149097, 0.5783720820783133, 0.5800913615399097, 0.579857516001506, 0.578514742564006, 0.578758883189006, 0.5787382930158133, 0.5793486445783133, 0.5786265177899097, 0.5789721385542168, 0.580345797251506, 0.5788603633283133, 0.580345797251506, 0.5797148555158133, 0.5802134318524097, 0.5797148555158133, 0.5788706584149097, 0.5807017131024097, 0.5802340220256024, 0.5795927852033133, 0.5792368693524097, 0.5786059276167168, 0.5808237834149097, 0.5788500682417168, 0.5804575724774097, 0.5793589396649097, 0.5795824901167168, 0.5798369258283133, 0.579735445689006, 0.5797251506024097, 0.580467867564006, 0.5793486445783133, 0.5789618434676205, 0.5808237834149097, 0.5794707148908133, 0.581078219126506, 0.580101656626506, 0.579613375376506, 0.5802340220256024, 0.580589937876506, 0.5786162227033133, 0.5810885142131024, 0.581322359751506, 0.5783617869917168, 0.5788706584149097, 0.5808443735881024, 0.5771307887801205, 0.5788500682417168, 0.581078219126506, 0.581688570689006, 0.580467867564006, 0.5798369258283133, 0.5820753717996988, 0.5788603633283133, 0.5800913615399097, 0.5806002329631024, 0.5797251506024097, 0.580101656626506, 0.5806914180158133, 0.581810641001506, 0.5798472209149097, 0.5815562052899097, 0.579613375376506, 0.5795927852033133, 0.5807017131024097, 0.5790942088667168, 0.5799589961408133, 0.5793589396649097, 0.580223726939006, 0.5809458537274097, 0.5804575724774097, 0.580345797251506, 0.5809458537274097, 0.5810885142131024, 0.5808443735881024, 0.5794707148908133, 0.5792265742658133, 0.5805693477033133, 0.5809458537274097, 0.5803560923381024, 0.5800913615399097, 0.580101656626506, 0.5799692912274097, 0.5788603633283133, 0.5800810664533133, 0.5802031367658133, 0.5792265742658133, 0.580223726939006, 0.5814547251506024, 0.5805796427899097, 0.582176851939006, 0.5798369258283133, 0.581566500376506, 0.5799692912274097, 0.5799692912274097, 0.579735445689006, 0.5800913615399097, 0.5792265742658133, 0.5811899943524097, 0.5786265177899097, 0.5799692912274097, 0.5792265742658133, 0.5798266307417168, 0.581078219126506, 0.5778735057417168, 0.5797251506024097, 0.579979586314006, 0.5797148555158133, 0.5797148555158133, 0.580956148814006, 0.5798369258283133, 0.5802134318524097, 0.5814341349774097, 0.5795016001506024, 0.5795927852033133, 0.5792059840926205, 0.5808443735881024, 0.5797251506024097, 0.5804575724774097, 0.5792265742658133, 0.581322359751506, 0.579979586314006, 0.5791045039533133, 0.5803355021649097, 0.5799692912274097, 0.580345797251506, 0.5789721385542168, 0.580589937876506, 0.5786265177899097, 0.580956148814006, 0.5798472209149097, 0.5816988657756024, 0.5798472209149097, 0.5798472209149097, 0.5794604198042168, 0.579491305064006, 0.5807017131024097, 0.5797251506024097, 0.5809458537274097, 0.5788603633283133, 0.5798472209149097], "accuracy_test": 0.5740393813775511, "start": "2016-01-24 10:08:35.471000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 168.0, 169.0, 170.0, 171.0, 172.0, 173.0, 174.0, 175.0, 176.0, 177.0, 178.0, 179.0, 180.0, 181.0, 182.0, 183.0, 184.0, 185.0, 186.0, 187.0, 188.0, 189.0, 190.0, 191.0, 192.0, 193.0, 194.0, 195.0, 196.0, 197.0, 198.0, 199.0, 200.0, 201.0, 202.0, 203.0, 204.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0], "accuracy_train_last": 0.5863823424234035, "batch_size_eval": 1024, "accuracy_train_std": [0.013564014437390887, 0.010498736299908083, 0.010430050175213819, 0.01134949789370239, 0.014166766892013327, 0.013905658424486243, 0.01576884713190763, 0.013633074114766318, 0.014104827243114277, 0.01578180889906084, 0.015798480117740245, 0.016755065035765564, 0.016574164143248995, 0.01524026673370429, 0.01538401171540119, 0.01566089118216848, 0.015081370462712466, 0.015660807365356536, 0.015785474072572435, 0.01565817994741821, 0.01608252376065822, 0.016473381074043697, 0.016608276299501155, 0.016478641498031393, 0.016490071260322547, 0.01707193473527537, 0.017066312641034677, 0.016319711513668785, 0.016237542960845796, 0.016637822598379173, 0.01610399081314083, 0.016711823639039645, 0.016873502482556084, 0.017075386854176946, 0.017131623301924498, 0.01682270663846198, 0.01708334981939428, 0.01696950790075904, 0.0160742846051684, 0.017059891378376643, 0.017799206374467018, 0.016890220699970836, 0.01619904161058231, 0.017270223519635173, 0.016711100887570406, 0.017197562163002743, 0.017240559457912766, 0.017410027452961205, 0.016728991178609983, 0.017100258823760678, 0.016843912267018094, 0.01673476521878523, 0.017316027724593147, 0.01675453958790058, 0.01642306387541586, 0.017241908496278817, 0.016988274435892896, 0.016993965082454928, 0.01671286940921728, 0.017438398798162297, 0.016625898019310366, 0.017112770347044545, 0.017006208614523965, 0.015517984496110774, 0.01681939388638104, 0.016294017546644, 0.016413167913596913, 0.016265176548022422, 0.016385879336957048, 0.016532819214264867, 0.016504331522487793, 0.016635764453374634, 0.017020284612018762, 0.01647969146237665, 0.01663207683562435, 0.015773231304882635, 0.0169653088469256, 0.015484197067265396, 0.016550137537487908, 0.016784069995534816, 0.01637564129354003, 0.015530239554481781, 0.015742743274278093, 0.01653713424661971, 0.01574675881434864, 0.016095088660098695, 0.01588116772197523, 0.016218351661910574, 0.015995711112698412, 0.016182974966307072, 0.016183813999982716, 0.015803580090568828, 0.015692651626761468, 0.016393888979356303, 0.01599062952019291, 0.016533677615372385, 0.01603647640763859, 0.01665499043295578, 0.01561183071078873, 0.015756325617266666, 0.01653373670146518, 0.01650899784947848, 0.01583702290554559, 0.01609640863217972, 0.015746897890709745, 0.015248771589573017, 0.016158466281632894, 0.015755058322874892, 0.01575847381473548, 0.01572807979704095, 0.015704319057496117, 0.01600981533652415, 0.016443187996165554, 0.015909441536813937, 0.015980351779504694, 0.015567810555059895, 0.0160192679464171, 0.016017951279937864, 0.01600304179698099, 0.0160952787118351, 0.016141015717909855, 0.01558530751582642, 0.015621793835607313, 0.01611050487937125, 0.015856130330832618, 0.016306415849478603, 0.015774927231769233, 0.016023687896144823, 0.01628902548410224, 0.016571604285773256, 0.016553872010804545, 0.015696190231447568, 0.016493554658523157, 0.016139817250053805, 0.01590879038302681, 0.016180332884135656, 0.0156613616549258, 0.01657564195986835, 0.01576693957891555, 0.015990633002501822, 0.016036059509087697, 0.015950897497369106, 0.015489974718487015, 0.016194749393755148, 0.01509718607812982, 0.015369908881996333, 0.016520645448208297, 0.015541750587484759, 0.01589252026293714, 0.016202028762327694, 0.016256428267006105, 0.016458282080569937, 0.015865181823866103, 0.01619068997095827, 0.016125847511061577, 0.016375728105603952, 0.01514915987296593, 0.01643465193144514, 0.01608238374145455, 0.01606580330059006, 0.015668247378410775, 0.016051577608509657, 0.015871645656663467, 0.015332954730113865, 0.016082155275235484, 0.016008864562612267, 0.015510182706711874, 0.01552047449012546, 0.016252636216471997, 0.015999303965717734, 0.01561788783938608, 0.01632035341483135, 0.015664451927484318, 0.01638567315987978, 0.015922305261447313, 0.015850256830945368, 0.015997731374608415, 0.01644105547117058, 0.016575573693610488, 0.015234358298311513, 0.01600670817770194, 0.016381121279078554, 0.016762673578528915, 0.015813121660639473, 0.01610745497175549, 0.015839782750723144, 0.016369748960474294, 0.015685646575115692, 0.016437598522643808, 0.015922722093159165, 0.01612785676422405, 0.015615219063562167, 0.01596868961799503, 0.015494784137036883, 0.01645772050503166, 0.016261857982117357, 0.016324351927824204, 0.016362451872198015, 0.01573523903428263, 0.016440532903107694, 0.01567131186499835, 0.016012555924999707, 0.015921081131218047, 0.01557970544132967, 0.01603556218014208, 0.01599824458300635, 0.016161299431897657, 0.016554833294810504, 0.015727571290590214, 0.016247385762147095, 0.01612147509224163, 0.015767823137845256, 0.015806312366919033, 0.01626146195248188, 0.015762705020156788, 0.015995795437205865], "accuracy_test_std": 0.01122477217904701, "error_valid": [0.7737434111445783, 0.7431434723268072, 0.7161247529179217, 0.6899090502635542, 0.6593194065323795, 0.6236631094691265, 0.5962164086031627, 0.5702036662274097, 0.5480162838855421, 0.5371005506400602, 0.5254421004329819, 0.5075580642884037, 0.4998264542545181, 0.4932655426393072, 0.48626635448042166, 0.48177034309111444, 0.47566682746611444, 0.4749549957643072, 0.47003100291792166, 0.46809846809111444, 0.46614534309111444, 0.4618317018072289, 0.4582710725715362, 0.455432570124247, 0.4502850268260542, 0.4499188158885542, 0.44797598597515065, 0.4441609210278614, 0.44563605986445776, 0.44108857304216864, 0.4426960772778614, 0.4390030826430723, 0.4365410862198795, 0.43876923710466864, 0.4349747623305723, 0.4332451877823795, 0.4341202701430723, 0.4312920627823795, 0.43154649849397586, 0.43056993599397586, 0.4305493458207832, 0.43020372505647586, 0.42886095161897586, 0.42797557417168675, 0.4284741505082832, 0.4275990681475903, 0.4278740940323795, 0.42651073042168675, 0.4265210255082832, 0.4259209690323795, 0.4256665333207832, 0.42382518354668675, 0.42370311323418675, 0.4243237598832832, 0.4237134083207832, 0.42197353868599397, 0.4238148884600903, 0.4232045368975903, 0.4245679005082832, 0.4228383259600903, 0.4229603962725903, 0.42223826948418675, 0.42233974962349397, 0.42150584760918675, 0.42295010118599397, 0.4224721150225903, 0.42138377729668675, 0.4219838337725903, 0.42038662462349397, 0.42087490587349397, 0.42013218891189763, 0.42099697618599397, 0.42013218891189763, 0.4213734822100903, 0.42063076524849397, 0.4201527790850903, 0.42162791792168675, 0.4199086384600903, 0.42014248399849397, 0.42148525743599397, 0.42124111681099397, 0.42126170698418675, 0.42065135542168675, 0.4213734822100903, 0.4210278614457832, 0.41965420274849397, 0.42113963667168675, 0.41965420274849397, 0.42028514448418675, 0.4197865681475903, 0.42028514448418675, 0.4211293415850903, 0.4192982868975903, 0.41976597797439763, 0.42040721479668675, 0.4207631306475903, 0.4213940723832832, 0.4191762165850903, 0.4211499317582832, 0.4195424275225903, 0.4206410603350903, 0.4204175098832832, 0.42016307417168675, 0.42026455431099397, 0.4202748493975903, 0.41953213243599397, 0.42065135542168675, 0.4210381565323795, 0.4191762165850903, 0.42052928510918675, 0.41892178087349397, 0.41989834337349397, 0.42038662462349397, 0.41976597797439763, 0.41941006212349397, 0.42138377729668675, 0.41891148578689763, 0.41867764024849397, 0.4216382130082832, 0.4211293415850903, 0.41915562641189763, 0.4228692112198795, 0.4211499317582832, 0.41892178087349397, 0.41831142931099397, 0.41953213243599397, 0.42016307417168675, 0.4179246282003012, 0.42113963667168675, 0.4199086384600903, 0.41939976703689763, 0.4202748493975903, 0.41989834337349397, 0.41930858198418675, 0.41818935899849397, 0.4201527790850903, 0.4184437947100903, 0.42038662462349397, 0.42040721479668675, 0.4192982868975903, 0.4209057911332832, 0.42004100385918675, 0.4206410603350903, 0.41977627306099397, 0.4190541462725903, 0.4195424275225903, 0.41965420274849397, 0.4190541462725903, 0.41891148578689763, 0.41915562641189763, 0.42052928510918675, 0.42077342573418675, 0.41943065229668675, 0.4190541462725903, 0.41964390766189763, 0.4199086384600903, 0.41989834337349397, 0.4200307087725903, 0.42113963667168675, 0.41991893354668675, 0.41979686323418675, 0.42077342573418675, 0.41977627306099397, 0.41854527484939763, 0.4194203572100903, 0.41782314806099397, 0.42016307417168675, 0.41843349962349397, 0.4200307087725903, 0.4200307087725903, 0.42026455431099397, 0.4199086384600903, 0.42077342573418675, 0.4188100056475903, 0.4213734822100903, 0.4200307087725903, 0.42077342573418675, 0.4201733692582832, 0.41892178087349397, 0.4221264942582832, 0.4202748493975903, 0.42002041368599397, 0.42028514448418675, 0.42028514448418675, 0.41904385118599397, 0.42016307417168675, 0.4197865681475903, 0.4185658650225903, 0.42049839984939763, 0.42040721479668675, 0.4207940159073795, 0.41915562641189763, 0.4202748493975903, 0.4195424275225903, 0.42077342573418675, 0.41867764024849397, 0.42002041368599397, 0.42089549604668675, 0.4196644978350903, 0.4200307087725903, 0.41965420274849397, 0.4210278614457832, 0.41941006212349397, 0.4213734822100903, 0.41904385118599397, 0.4201527790850903, 0.41830113422439763, 0.4201527790850903, 0.4201527790850903, 0.4205395801957832, 0.42050869493599397, 0.4192982868975903, 0.4202748493975903, 0.4190541462725903, 0.42113963667168675, 0.4201527790850903], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-3, 3], "momentum": 0.5235704480129004, "shear_range": [1, 1], "patience_check_each": 1, "learning_rate": 0.0003983753584505203, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "optimization": "nesterov_momentum", "nb_data_augmentation": 3, "learning_rate_decay_method": "exp", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 5.694955472660488e-10, "valid_ratio": 0.15, "rotation_range": [0, 0], "learning_rate_decay": 0.06650987492288275}, "accuracy_valid_max": 0.582176851939006, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='sqrt', interval=['exp', 'none', 'sqrt', 'lin'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'adadelta', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-3, 3)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.5798472209149097, "loss_train": [2.252213716506958, 2.0550684928894043, 1.9618316888809204, 1.896373987197876, 1.8442994356155396, 1.7973439693450928, 1.75365149974823, 1.7136727571487427, 1.6768581867218018, 1.642853021621704, 1.6158559322357178, 1.5895485877990723, 1.5663326978683472, 1.5459425449371338, 1.5273557901382446, 1.511525273323059, 1.4967106580734253, 1.484001636505127, 1.4720591306686401, 1.4615390300750732, 1.4505687952041626, 1.4423449039459229, 1.433855652809143, 1.4267170429229736, 1.4193159341812134, 1.4133356809616089, 1.4077332019805908, 1.4029216766357422, 1.3975515365600586, 1.3927267789840698, 1.3894131183624268, 1.3853706121444702, 1.381252646446228, 1.3771663904190063, 1.3742948770523071, 1.3714520931243896, 1.369061827659607, 1.3667116165161133, 1.3633785247802734, 1.361266016960144, 1.3600608110427856, 1.3580876588821411, 1.3567320108413696, 1.354032039642334, 1.3526986837387085, 1.350978970527649, 1.3497655391693115, 1.3491761684417725, 1.3482670783996582, 1.346596121788025, 1.3462313413619995, 1.3441755771636963, 1.3423066139221191, 1.3431565761566162, 1.3427462577819824, 1.3417158126831055, 1.3407130241394043, 1.339052677154541, 1.3404206037521362, 1.3400596380233765, 1.3384536504745483, 1.3382002115249634, 1.338740348815918, 1.3366488218307495, 1.3359428644180298, 1.336051106452942, 1.3362737894058228, 1.3361642360687256, 1.3352208137512207, 1.3356361389160156, 1.3355083465576172, 1.3349756002426147, 1.3342280387878418, 1.3350224494934082, 1.3336600065231323, 1.3344608545303345, 1.3339991569519043, 1.3338253498077393, 1.333855390548706, 1.3345330953598022, 1.3328876495361328, 1.3332915306091309, 1.3329728841781616, 1.334684133529663, 1.3334040641784668, 1.332518458366394, 1.3324639797210693, 1.3332157135009766, 1.3340667486190796, 1.3329039812088013, 1.3322408199310303, 1.3330415487289429, 1.331700325012207, 1.3315874338150024, 1.3320040702819824, 1.3322035074234009, 1.332262635231018, 1.3311859369277954, 1.332284688949585, 1.3325893878936768, 1.3312654495239258, 1.3320627212524414, 1.3321077823638916, 1.332403540611267, 1.3320947885513306, 1.3324540853500366, 1.3321521282196045, 1.3316280841827393, 1.332034707069397, 1.3313902616500854, 1.3321181535720825, 1.3328404426574707, 1.3312569856643677, 1.3321332931518555, 1.3309926986694336, 1.3318678140640259, 1.3317456245422363, 1.3311185836791992, 1.3320930004119873, 1.3328183889389038, 1.3313055038452148, 1.3326563835144043, 1.331002116203308, 1.3320187330245972, 1.3316012620925903, 1.330937385559082, 1.3317301273345947, 1.3317747116088867, 1.3315222263336182, 1.3313243389129639, 1.331800103187561, 1.332451343536377, 1.3328043222427368, 1.332830786705017, 1.3323750495910645, 1.3320040702819824, 1.3312920331954956, 1.3338744640350342, 1.3324062824249268, 1.332301378250122, 1.331654667854309, 1.3320692777633667, 1.3312050104141235, 1.3322458267211914, 1.3314317464828491, 1.331862211227417, 1.3316618204116821, 1.331011176109314, 1.3320155143737793, 1.3309258222579956, 1.3310084342956543, 1.3307346105575562, 1.3316500186920166, 1.3329310417175293, 1.3320047855377197, 1.3313099145889282, 1.3318350315093994, 1.3320692777633667, 1.3305851221084595, 1.3317298889160156, 1.3327322006225586, 1.332120656967163, 1.3314718008041382, 1.331939697265625, 1.3310227394104004, 1.3325600624084473, 1.332651972770691, 1.3324806690216064, 1.3313555717468262, 1.3315160274505615, 1.3322783708572388, 1.3309903144836426, 1.3319897651672363, 1.331406593322754, 1.330100178718567, 1.3322463035583496, 1.3317391872406006, 1.3321382999420166, 1.3326307535171509, 1.331260085105896, 1.3326348066329956, 1.3326388597488403, 1.3319637775421143, 1.331148386001587, 1.33130943775177, 1.3317947387695312, 1.3309135437011719, 1.3316928148269653, 1.3319681882858276, 1.3322407007217407, 1.3307205438613892, 1.3321385383605957, 1.3321800231933594, 1.3326027393341064, 1.3314675092697144, 1.3314611911773682, 1.3320515155792236, 1.3321677446365356, 1.3307862281799316, 1.3320844173431396, 1.3332598209381104, 1.3319259881973267, 1.332351565361023, 1.331653356552124, 1.331924319267273, 1.3329817056655884, 1.3318721055984497, 1.3315439224243164, 1.3322607278823853, 1.3317680358886719, 1.331545352935791, 1.3321235179901123, 1.3315528631210327, 1.3314173221588135, 1.3312880992889404, 1.3323581218719482], "accuracy_train_first": 0.22060975855943152, "model": "residualv5", "loss_std": [0.12524403631687164, 0.11022990942001343, 0.11900077760219574, 0.1277599185705185, 0.13835261762142181, 0.14706000685691833, 0.15592628717422485, 0.16313797235488892, 0.16932588815689087, 0.17514099180698395, 0.1808706670999527, 0.18490250408649445, 0.18920376896858215, 0.19273734092712402, 0.1955944448709488, 0.1979272961616516, 0.20171889662742615, 0.20252816379070282, 0.20542150735855103, 0.2069978266954422, 0.20662400126457214, 0.20863687992095947, 0.2097732275724411, 0.21088404953479767, 0.21146970987319946, 0.2130172997713089, 0.21271339058876038, 0.21385206282138824, 0.21279340982437134, 0.21506333351135254, 0.21527986228466034, 0.215003103017807, 0.21543031930923462, 0.21569854021072388, 0.21721777319908142, 0.21623684465885162, 0.21795864403247833, 0.21663373708724976, 0.21816983819007874, 0.21864832937717438, 0.2170298993587494, 0.218422070145607, 0.2181916981935501, 0.21861447393894196, 0.2183433324098587, 0.21864429116249084, 0.21840505301952362, 0.2192695587873459, 0.21810755133628845, 0.21783775091171265, 0.2190892994403839, 0.21863728761672974, 0.21915733814239502, 0.2201915979385376, 0.21997790038585663, 0.2185896784067154, 0.21921205520629883, 0.21965187788009644, 0.22127611935138702, 0.22042225301265717, 0.2196788787841797, 0.21970951557159424, 0.22026824951171875, 0.2193414568901062, 0.21965935826301575, 0.21901290118694305, 0.22069533169269562, 0.21936243772506714, 0.21890993416309357, 0.21976353228092194, 0.21960397064685822, 0.22013887763023376, 0.2200779765844345, 0.22057180106639862, 0.22017802298069, 0.21953149139881134, 0.21991321444511414, 0.2211393266916275, 0.22093333303928375, 0.22040118277072906, 0.22006630897521973, 0.21997275948524475, 0.21961116790771484, 0.22016564011573792, 0.21925488114356995, 0.2197864055633545, 0.22025567293167114, 0.2204810529947281, 0.22064979374408722, 0.219769686460495, 0.22037442028522491, 0.22060926258563995, 0.2204277068376541, 0.2193291038274765, 0.22005054354667664, 0.2199162095785141, 0.21925951540470123, 0.22077246010303497, 0.21993157267570496, 0.21987943351268768, 0.21957486867904663, 0.21977351605892181, 0.22006364166736603, 0.21986719965934753, 0.22045674920082092, 0.2191164493560791, 0.2198072075843811, 0.21987450122833252, 0.21942271292209625, 0.2212972342967987, 0.22052447497844696, 0.22143681347370148, 0.2205178290605545, 0.2200833112001419, 0.22030407190322876, 0.2202816754579544, 0.22082722187042236, 0.2183971405029297, 0.22026890516281128, 0.22034841775894165, 0.21970054507255554, 0.2201295793056488, 0.21971242129802704, 0.22046111524105072, 0.22096972167491913, 0.2201886624097824, 0.21998734772205353, 0.21931886672973633, 0.2202952802181244, 0.21951515972614288, 0.2188502997159958, 0.22059817612171173, 0.22054603695869446, 0.21962867677211761, 0.21955899894237518, 0.22156111896038055, 0.21953217685222626, 0.2200448364019394, 0.2202616035938263, 0.21979346871376038, 0.21953563392162323, 0.21967263519763947, 0.2200111299753189, 0.22030745446681976, 0.21921373903751373, 0.21906834840774536, 0.22116906940937042, 0.22003911435604095, 0.2192857563495636, 0.21951964497566223, 0.2204926609992981, 0.2190401703119278, 0.2200084775686264, 0.2202044427394867, 0.2205297350883484, 0.21982739865779877, 0.21979211270809174, 0.21965216100215912, 0.21984675526618958, 0.22055818140506744, 0.21911948919296265, 0.22019055485725403, 0.2195078730583191, 0.22013355791568756, 0.22001223266124725, 0.22108149528503418, 0.220676988363266, 0.21980658173561096, 0.2204582244157791, 0.22077997028827667, 0.22037968039512634, 0.2202439159154892, 0.21900556981563568, 0.22015723586082458, 0.2184332311153412, 0.2209232598543167, 0.22006939351558685, 0.2205152064561844, 0.2205507457256317, 0.2197299599647522, 0.2190815657377243, 0.22049498558044434, 0.2203202247619629, 0.2193642109632492, 0.2205590456724167, 0.2203257828950882, 0.21863555908203125, 0.2186024785041809, 0.2212028205394745, 0.21891671419143677, 0.22012782096862793, 0.22017230093479156, 0.22062210738658905, 0.22116386890411377, 0.21975110471248627, 0.2207828164100647, 0.22016426920890808, 0.2199680656194687, 0.2205638736486435, 0.22029413282871246, 0.219515860080719, 0.21949151158332825, 0.22100256383419037, 0.2190287709236145, 0.219665989279747, 0.22083206474781036, 0.219273641705513, 0.219781294465065, 0.22027939558029175, 0.22104708850383759, 0.22121351957321167, 0.22079011797904968, 0.22057519853115082, 0.22016839683055878, 0.22013050317764282, 0.2210892140865326]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:17 2016", "state": "available"}], "summary": "6c236c605d19854e5083fd67018480cf"}