{"content": {"hp_model": {"f0": 64, "f1": 64, "f2": 64, "f3": 32, "nonlin": "leaky_rectify", "nbg1": 5, "nbg3": 5, "nbg2": 5, "fs0": 3, "fs1": 3, "fs2": 3, "fs3": 3, "pg2": 2, "pg3": 2, "pg1": 2}, "loss_train": [1.4802751541137695, 1.062888264656067, 0.8322960734367371, 0.7301118969917297, 0.6677152514457703, 0.6241839528083801, 0.5895202159881592, 0.5648148059844971, 0.5420796871185303, 0.5244818925857544, 0.5086696147918701, 0.49364227056503296, 0.48055970668792725, 0.47064363956451416, 0.4603123068809509, 0.45119908452033997, 0.44430920481681824, 0.43531671166419983, 0.43081894516944885, 0.4234428405761719, 0.41573771834373474, 0.41133642196655273, 0.4051915109157562, 0.3993588387966156, 0.3947690427303314, 0.38922318816185, 0.38725706934928894, 0.38193339109420776, 0.37892892956733704, 0.37584415078163147, 0.3706135153770447, 0.3673757016658783, 0.3646184206008911, 0.3594094216823578, 0.36033299565315247, 0.356241911649704, 0.3517104387283325, 0.348919153213501, 0.3481827676296234, 0.3445022404193878, 0.34277114272117615, 0.34019261598587036, 0.3377941846847534, 0.33602046966552734, 0.3332749605178833, 0.3307000994682312, 0.32928329706192017, 0.3306586742401123, 0.32455477118492126, 0.3222612738609314, 0.32090526819229126, 0.3199623227119446, 0.3195200264453888, 0.3167630136013031, 0.3155926764011383, 0.3126818537712097, 0.3123442530632019, 0.3109821081161499, 0.30952367186546326, 0.30943843722343445, 0.30695202946662903, 0.3032248616218567, 0.3047903776168823, 0.3016462028026581, 0.301609605550766, 0.2997927963733673, 0.2971292734146118, 0.2975732386112213, 0.29558151960372925, 0.29615265130996704, 0.2919982075691223, 0.2898331880569458, 0.2926540970802307, 0.29032424092292786, 0.2890258729457855, 0.2865974009037018, 0.28744974732398987, 0.28811484575271606, 0.28533628582954407, 0.28499728441238403, 0.28399044275283813, 0.28231582045555115, 0.27911636233329773, 0.27886930108070374, 0.2790435552597046, 0.2770232558250427, 0.2770930826663971, 0.27693819999694824, 0.2763257324695587, 0.2754623293876648, 0.27395597100257874, 0.2733718156814575, 0.2701525390148163, 0.2734870910644531, 0.27202901244163513, 0.2683246433734894, 0.26854532957077026, 0.2686694264411926, 0.26930031180381775, 0.2661243677139282, 0.265930712223053, 0.26589998602867126, 0.2634119987487793, 0.26635217666625977, 0.2642125189304352, 0.26357871294021606, 0.2636115849018097, 0.2632375955581665, 0.259878933429718, 0.26160964369773865, 0.2590743601322174, 0.2600027024745941, 0.25742948055267334, 0.2599155008792877, 0.2569747865200043, 0.25792619585990906, 0.2545556128025055, 0.25649362802505493, 0.25490713119506836, 0.25318413972854614, 0.25376608967781067, 0.25182175636291504, 0.2544299066066742, 0.252623051404953, 0.25364363193511963, 0.24988992512226105, 0.2504817247390747, 0.24932433664798737, 0.2480212152004242, 0.2475670725107193, 0.24860438704490662, 0.2463996410369873, 0.24702772498130798, 0.24766720831394196, 0.24678078293800354, 0.24694035947322845, 0.24597319960594177, 0.24560080468654633, 0.24377653002738953, 0.24366824328899384, 0.2445860356092453, 0.2432299703359604, 0.2416345328092575, 0.2425248622894287, 0.24207505583763123, 0.24236759543418884, 0.24106930196285248, 0.23926778137683868, 0.24013076722621918, 0.23940330743789673, 0.24147360026836395, 0.23884522914886475, 0.23903967440128326, 0.23845969140529633, 0.23818950355052948, 0.23480631411075592, 0.2354041188955307, 0.23574841022491455, 0.23582036793231964, 0.23810313642024994, 0.23513877391815186], "moving_avg_accuracy_train": [0.057600893433923946, 0.11990252399409373, 0.18606358723237354, 0.24854037655154113, 0.30655026253633716, 0.3603585378642372, 0.40966700076524426, 0.4549441615748863, 0.4963493703654874, 0.5342929656805905, 0.5690372232665089, 0.6006697062104837, 0.6294994110231746, 0.6557437283533969, 0.6798031031244157, 0.7017680382611713, 0.7217898490068518, 0.7401046644303176, 0.7566089606995413, 0.7717439901037013, 0.7855283851305681, 0.7980366472023673, 0.8094591686324628, 0.8199625801076256, 0.8295504730174057, 0.8383539988457408, 0.8465305772626617, 0.8539428320652345, 0.8606650867589971, 0.8668964775905263, 0.8726325043769702, 0.877911149876427, 0.88261074150331, 0.8869845331936951, 0.8910441786019464, 0.8947956238658289, 0.8983019887413815, 0.9014322486389302, 0.9042308092586104, 0.9069656805579709, 0.9094898797940711, 0.9118315217173034, 0.9139783467338806, 0.9161036207952655, 0.9179464687909512, 0.9197283730204293, 0.9213925046471886, 0.9229437375827096, 0.9243443893758414, 0.9256353471194588, 0.9268808423482198, 0.9280809152112659, 0.9291725344344176, 0.9301688705304738, 0.9312400673240949, 0.9322041804871727, 0.9330368609065248, 0.9338584610434932, 0.9346351035477172, 0.9354247265562714, 0.9362028526282652, 0.9369705954085358, 0.937589484297684, 0.9382046490669742, 0.9387768264521742, 0.9393778166048065, 0.9399721861647946, 0.940541959952108, 0.9409943745892799, 0.941496770717469, 0.9419885268602384, 0.9424055307518262, 0.9428622144625886, 0.9432569898094267, 0.9436563573025059, 0.9441134803450958, 0.9444085254964942, 0.9447555184387237, 0.945109628716483, 0.9454887818355139, 0.9457951424104988, 0.9461057802089469, 0.9464504583942168, 0.9467327309264267, 0.9470821073066061, 0.9474058105951869, 0.9476738199691769, 0.9478964632641104, 0.9482178220652833, 0.9484978164887383, 0.9488565520221734, 0.9490214841273111, 0.949355934926697, 0.9495476586520968, 0.9497898923716047, 0.9500730068858285, 0.9503254127021828, 0.9506409696404824, 0.95093427148019, 0.9511703773990315, 0.9513968957164836, 0.9515635237724192, 0.9517761959429809, 0.9520001169310011, 0.9521783943321239, 0.9524063093574294, 0.952660152858748, 0.9528235799409057, 0.9530217454910196, 0.9531745899468549, 0.9534493337368686, 0.9536524613693186, 0.9538352401897049, 0.9539602856959283, 0.9541193296277197, 0.9543809436091615, 0.9546024813484207, 0.9547855532232685, 0.9548992367344595, 0.9551037863445131, 0.9553344200185704, 0.9555187027883081, 0.9556358012537094, 0.9558178476856474, 0.9559862676743731, 0.9560751387440067, 0.9562690910471623, 0.9563924948461928, 0.9565383994486445, 0.9566371975563364, 0.9568098572592206, 0.9569884303822741, 0.9572073109620791, 0.9573437775172186, 0.9575038358466151, 0.9576223117061674, 0.9577615281119164, 0.9578984125723193, 0.9580262228354824, 0.9581017245425674, 0.9582743077753723, 0.9583970806015634, 0.9585657048653735, 0.958759319381374, 0.9588824552207837, 0.9589863741274615, 0.9590984302363099, 0.9591945583390171, 0.9593044693148238, 0.9594846612549269, 0.9595678510391333, 0.9597170905580051, 0.9598909697035704, 0.959961394379808, 0.9601642494681746, 0.9602886903274664, 0.9603891334544189, 0.960565490676991, 0.9607265733749342, 0.960799432141169, 0.9608510180891046], "dataset": "Cifar10", "nb_examples_train": 42500, "moving_avg_accuracy_valid": [0.056795492516942755, 0.11738129765154365, 0.18094319612434107, 0.24036146593208768, 0.2951583271513789, 0.34487421624535247, 0.3906266995530462, 0.43192600484247046, 0.46930186962554266, 0.5035932976385156, 0.5347506106175104, 0.5630038298644943, 0.5888467662492797, 0.6122508638619271, 0.6331751268262614, 0.6521952165062106, 0.6694475745619148, 0.6850071998711601, 0.6993232449913482, 0.7122076855995176, 0.7237416174819604, 0.7341852503497283, 0.74374115231965, 0.7522814584449892, 0.7600643606991349, 0.7670201446028659, 0.7734034499373835, 0.7790131178860398, 0.7840862331023304, 0.7887995506806516, 0.7928421059665021, 0.7965424703886772, 0.8000070757123848, 0.8034090707484506, 0.8061270103885904, 0.8089027459084663, 0.8113764938138546, 0.8135896303887944, 0.8153861408062402, 0.8171637506055107, 0.8186628546488753, 0.8202836915013824, 0.8218045093335483, 0.8232709016324977, 0.8245031464654828, 0.8255623091815099, 0.8263080360946842, 0.8273495192886796, 0.8281616953247966, 0.8289170678198019, 0.8297688310114663, 0.8306198375940546, 0.8310785087198148, 0.8317385418839779, 0.8323081576692247, 0.8328208118759468, 0.8334042709744968, 0.833659799967032, 0.8340484674665638, 0.8343117894887327, 0.8348713096644829, 0.8353239906803388, 0.8356703684383591, 0.8359454873268274, 0.83597027923797, 0.8361655494128175, 0.8361896606518822, 0.8363619631766789, 0.8364671778153363, 0.8365109838478086, 0.8367854313967628, 0.837106705886981, 0.8370764110983582, 0.8372434287799381, 0.8373235910318388, 0.8374913342912302, 0.8375202329121825, 0.8376693414921992, 0.8377292675180545, 0.837573622392755, 0.8375587006184645, 0.8376052766691933, 0.8377082302710992, 0.8376523451204952, 0.8377638284171807, 0.8379007844779475, 0.8378877085715684, 0.8380102175995772, 0.8380848841396947, 0.8381775275969602, 0.8382975278022491, 0.8382813986571899, 0.8382923259977961, 0.8383499592206822, 0.8383041728712797, 0.8384725437053867, 0.8386352549786733, 0.8388671443433813, 0.838930389905278, 0.8388875956436659, 0.8389833581519649, 0.8390573373781841, 0.8391137706678506, 0.8390038102049812, 0.8388926387571487, 0.8390143700339188, 0.8390639225354215, 0.8389202667747258, 0.8389151059199189, 0.8390590045429119, 0.8392505779685153, 0.8394107870203084, 0.8395285020871028, 0.8394136895760582, 0.839383600503618, 0.8394328510519008, 0.8394242303857167, 0.8392567508712413, 0.8392545627005328, 0.8393746636593952, 0.8393596547012117, 0.8395068970624158, 0.8396150011249995, 0.8395891949601653, 0.839615827045474, 0.8395278736236826, 0.83947415911523, 0.8395000877537824, 0.8394634178808891, 0.8394283559779658, 0.8394954860239945, 0.8395538440481011, 0.8395076805111373, 0.8396024696889393, 0.8397142530287802, 0.8397395568298178, 0.8396758515233421, 0.8396917589350139, 0.8396694545117686, 0.8397480662895073, 0.8397944028269722, 0.8398238986794406, 0.8398260308841622, 0.8398055948232309, 0.8397973503823235, 0.8396180024393471, 0.8396030736656684, 0.8395774307381076, 0.8396031802283028, 0.8396009111983189, 0.8395988690713334, 0.8396835098844562, 0.8396132022412666, 0.8395763984422152, 0.839616517210569, 0.8396404170708374, 0.8396985480388289, 0.8397996940350213, 0.8398429268152541, 0.8397444704477347, 0.839717924381877], "moving_var_accuracy_train": [0.029860766319476373, 0.06180812822163169, 0.09502289199884546, 0.1206507456316458, 0.13887199291620256, 0.15104276806845018, 0.15782041188454507, 0.16048856231493025, 0.15986922791837666, 0.15683975295546587, 0.15202024857668914, 0.14582374951382793, 0.13872174147872723, 0.13104844506001914, 0.12315328218336823, 0.11518007934508831, 0.10726992756040214, 0.09956182698032252, 0.09205717044037107, 0.08491307543191703, 0.07813185380503573, 0.07172677800504346, 0.06572836616692802, 0.060148424423784444, 0.054960931195450695, 0.05016235667899709, 0.0457478289225699, 0.04166751972163729, 0.03790746612298389, 0.0344661915959429, 0.03131569046600182, 0.02843489830418207, 0.025790183926899076, 0.023383336017967108, 0.021193328903737027, 0.019200656087474482, 0.017391241830491615, 0.015740304390678275, 0.014236761425488673, 0.012880400972156395, 0.011649705110992519, 0.010534084181963017, 0.00952215548263293, 0.008610591042893596, 0.007780096737221062, 0.007030663707646245, 0.006352521343522243, 0.0057389261217522265, 0.005182689938587433, 0.004679420091790942, 0.004225439407895646, 0.003815857040995662, 0.003444996029651285, 0.003109430597232897, 0.0028088147006455847, 0.0025362988583020048, 0.0022889091825987564, 0.0020660935054044793, 0.0018649127170783364, 0.001684032985831247, 0.0015210790089033698, 0.0013742759688029507, 0.0012402955830366563, 0.0011196718739733744, 0.0010106511692172452, 0.00091283675476757, 0.0008247325558553777, 0.0007451810797882256, 0.0006725050828447503, 0.0006075261913868492, 0.0005489499891837284, 0.00049562002047575, 0.0004479350585332562, 0.0004045441808501717, 0.0003655252123159092, 0.0003308533443689186, 0.00029855147470430075, 0.00026977996415148445, 0.0002439305145356685, 0.00022083127687113957, 0.00019959286040117176, 0.00018050203693747673, 0.00016352106070633796, 0.000147886054677666, 0.00013419602390514485, 0.00012171947588597267, 0.00011019398951829396, 9.962072089747442e-05, 9.05880921195488e-05, 8.223485480208733e-05, 7.516958996841955e-05, 6.78974543653242e-05, 6.211442496368067e-05, 5.6233804349242866e-05, 5.113851848811842e-05, 4.6746051092784195e-05, 4.2644824248671554e-05, 3.92765274555857e-05, 3.612310843261077e-05, 3.301251163355745e-05, 3.0173055403473107e-05, 2.7405634044349775e-05, 2.5072135709097656e-05, 2.3016187618071e-05, 2.1000614342023998e-05, 1.936806023666172e-05, 1.8011182921451228e-05, 1.6450440329949284e-05, 1.5158822564222007e-05, 1.3853193156916073e-05, 1.3147231192584115e-05, 1.220385558890875e-05, 1.1284142904654287e-05, 1.0296456021828642e-05, 9.49446516980292e-06, 9.160995530394947e-06, 8.686606706600103e-06, 8.11958383818287e-06, 7.423940920815127e-06, 7.058111715490932e-06, 6.831027568424658e-06, 6.453566064581775e-06, 5.931617913517849e-06, 5.636724252598391e-06, 5.328339460760073e-06, 4.8665881178443895e-06, 4.71848676915429e-06, 4.383694570775395e-06, 4.1369184908468696e-06, 3.811076236513584e-06, 3.698270969862986e-06, 3.6154391153706585e-06, 3.685073577775882e-06, 3.4841743060429456e-06, 3.366324894721849e-06, 3.1560211689194056e-06, 3.0148499206947006e-06, 2.882001128123731e-06, 2.7408201856396034e-06, 2.5180427370301546e-06, 2.534303213535695e-06, 2.4165313938406437e-06, 2.4307855355659555e-06, 2.5250862092642675e-06, 2.4090395028619306e-06, 2.265327805061384e-06, 2.151804168327727e-06, 2.0197892606656147e-06, 1.9265341380240206e-06, 2.026102941724586e-06, 1.88577750931899e-06, 1.8976516643249365e-06, 1.9799921132551375e-06, 1.8266296171382199e-06, 2.014318337310236e-06, 1.9522562507307568e-06, 1.8478300214256684e-06, 1.942963848863346e-06, 1.982196184166979e-06, 1.831752164105724e-06, 1.6725269379148041e-06], "duration": 239309.338417, "accuracy_train": [0.5760089343392396, 0.680617199035622, 0.7815131563768919, 0.8108314804240495, 0.8286392363995018, 0.8446330158153378, 0.8534431668743078, 0.8624386088616648, 0.868996249480897, 0.875785323516519, 0.8817355415397747, 0.8853620527062569, 0.8889667543373938, 0.8919425843253967, 0.8963374760635843, 0.8994524544919711, 0.9019861457179772, 0.9049380032415099, 0.9051476271225545, 0.9079592547411407, 0.9095879403723699, 0.9106110058485604, 0.9122618615033223, 0.9144932833840901, 0.9158415092054264, 0.9175857313007567, 0.9201197830149501, 0.9206531252883905, 0.9211653790028608, 0.9229789950742894, 0.9242567454549648, 0.9254189593715393, 0.9249070661452565, 0.9263486584071613, 0.9275809872762089, 0.9285586312407714, 0.9298592726213547, 0.9296045877168696, 0.9294178548357327, 0.931579522252215, 0.9322076729189737, 0.9329062990263934, 0.933299771883075, 0.9352310873477298, 0.9345321007521227, 0.9357655110857327, 0.9363696892880213, 0.9369048340023993, 0.9369502555140274, 0.9372539668120154, 0.938090299407069, 0.9388815709786821, 0.9389971074427832, 0.9391358953949798, 0.940880838466685, 0.9408811989548725, 0.9405309846806941, 0.9412528622762089, 0.9416248860857327, 0.9425313336332595, 0.9432059872762089, 0.9438802804309707, 0.9431594843000184, 0.9437411319905868, 0.9439264229189737, 0.9447867279784975, 0.945321512204688, 0.9456699240379292, 0.9450661063238279, 0.9460183358711702, 0.9464143321451642, 0.9461585657761166, 0.9469723678594499, 0.9468099679309707, 0.9472506647402179, 0.9482275877284054, 0.9470639318590809, 0.9478784549187893, 0.9482966212163161, 0.9489011599067922, 0.9485523875853636, 0.9489015203949798, 0.9495525620616464, 0.9492731837163161, 0.9502264947282208, 0.9503191401924143, 0.9500859043350868, 0.9499002529185124, 0.9511100512758398, 0.9510177662998339, 0.9520851718230897, 0.9505058730735512, 0.9523659921211702, 0.9512731721806941, 0.9519699958471761, 0.9526210375138427, 0.9525970650493725, 0.9534809820851791, 0.95357398803756, 0.9532953306686047, 0.9534355605735512, 0.9530631762758398, 0.9536902454780363, 0.954015405823182, 0.9537828909422297, 0.9544575445851791, 0.9549447443706165, 0.9542944236803249, 0.9548052354420451, 0.9545501900493725, 0.9559220278469915, 0.9554806100613695, 0.955480249573182, 0.955085695251938, 0.9555507250138427, 0.9567354694421374, 0.9565963210017534, 0.9564332000968992, 0.9559223883351791, 0.9569447328349945, 0.9574101230850868, 0.9571772477159468, 0.956689687442322, 0.9574562655730897, 0.9575020475729051, 0.9568749783707088, 0.9580146617755629, 0.9575031290374677, 0.9578515408707088, 0.9575263805255629, 0.9583637945851791, 0.9585955884897563, 0.9591772361803249, 0.9585719765134736, 0.958944360811185, 0.9586885944421374, 0.9590144757636582, 0.9591303727159468, 0.9591765152039498, 0.9587812399063308, 0.9598275568706165, 0.9595020360372831, 0.960083323239664, 0.9605018500253784, 0.9599906777754706, 0.95992164428756, 0.9601069352159468, 0.9600597112633813, 0.9602936680970838, 0.9611063887158545, 0.9603165590969915, 0.9610602462278516, 0.9614558820136582, 0.9605952164659468, 0.9619899452634736, 0.9614086580610927, 0.9612931215969915, 0.9621527056801403, 0.9621763176564231, 0.9614551610372831, 0.9613152916205242], "end": "2016-02-06 06:15:51.890000", "epoch": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0, 114.0, 115.0, 116.0, 117.0, 118.0, 119.0, 120.0, 121.0, 122.0, 123.0, 124.0, 125.0, 126.0, 127.0, 128.0, 129.0, 130.0, 131.0, 132.0, 133.0, 134.0, 135.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 143.0, 144.0, 145.0, 146.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 153.0, 154.0, 155.0, 156.0, 157.0, 158.0, 159.0, 160.0], "moving_var_accuracy_valid": [0.029031551732178914, 0.05916415461323152, 0.08960877358910425, 0.11242267331270939, 0.12820466997681482, 0.13762922963477184, 0.14270591423068174, 0.1437860163641152, 0.14198001214224618, 0.13836512924454164, 0.13326561968872683, 0.12712325730021742, 0.12042164781908833, 0.11330924910274041, 0.10591874721787192, 0.09858274679898452, 0.09140326684542607, 0.08444185761876041, 0.07784221418783376, 0.07155207205711939, 0.06559414911342694, 0.06001635940937473, 0.054836560830566046, 0.05000933620594, 0.045553564692824275, 0.04143365459098049, 0.03765700841482557, 0.03417452294379064, 0.03098869913139141, 0.028089767481599272, 0.025427871012591766, 0.023008318183044683, 0.020815517775181782, 0.018838128129692346, 0.017020800079710106, 0.0153880624408258, 0.013904331055037938, 0.012557979711028174, 0.01133122878704528, 0.010226544977726917, 0.009224116296349715, 0.00832534867563675, 0.007513629789980781, 0.006781619568352466, 0.006117123457472987, 0.005515507542656884, 0.004968961766052488, 0.0044818277746376135, 0.004039581666396636, 0.003640758788212867, 0.003283212414203649, 0.002961409082615762, 0.002667161587168641, 0.0024043662224519334, 0.002166849759491961, 0.0019525301125637936, 0.0017603409219845401, 0.0015848944853803209, 0.001427764598669019, 0.0012856121851883495, 0.0011598685321131585, 0.00104572595982089, 0.0009422331618000618, 0.0008486910592451841, 0.0007638274850703886, 0.0006877879105340149, 0.0006190143516472564, 0.0005573801099229927, 0.0005017417300123838, 0.0004515848277274741, 0.0004071042380688688, 0.0003673227699445672, 0.0003305987529180698, 0.0002977899317799056, 0.00026806877248158325, 0.00024151513544306625, 0.00021737113807139612, 0.00019583412458196785, 0.00017628303228094444, 0.00015887275769811528, 0.00014298748586243556, 0.0001287082612327055, 0.00011593283010674368, 0.00010436765544659149, 9.404274663089096e-05, 8.480728463102887e-05, 7.63280949818747e-05, 6.883036164118026e-05, 6.199750130698042e-05, 5.5874996467849255e-05, 5.0417097264488776e-05, 4.537772888192298e-05, 4.084103065468522e-05, 3.67868218846388e-05, 3.3127007204299436e-05, 3.0069445123870366e-05, 2.7300775237574347e-05, 2.505465181099891e-05, 2.258518663979581e-05, 2.0343150115258577e-05, 1.8391369225694307e-05, 1.660148863633277e-05, 1.4970002218342767e-05, 1.3581823727058549e-05, 1.2334873171671156e-05, 1.1234752388200673e-05, 1.0133376203027183e-05, 9.305771380954195e-06, 8.375433952659817e-06, 7.724251880687486e-06, 7.282130089195484e-06, 6.784919542763784e-06, 6.231139121041235e-06, 5.72666242316844e-06, 5.162144351374375e-06, 4.667760464792325e-06, 4.201653261282204e-06, 4.033932425073873e-06, 3.63058227538593e-06, 3.397342210724327e-06, 3.0596354090836787e-06, 2.948794684572038e-06, 2.7590936112387826e-06, 2.4891778734059934e-06, 2.2466434977764403e-06, 2.091601387642103e-06, 1.9084084846426287e-06, 1.723618284852946e-06, 1.56335857256975e-06, 1.4180867486421626e-06, 1.3168360614963114e-06, 1.215803386145292e-06, 1.1134026968357757e-06, 1.0829273212075008e-06, 1.0870942246808514e-06, 9.841473433353426e-07, 9.22257903660345e-07, 8.323095250091793e-07, 7.53555958175038e-07, 7.338186667508507e-07, 6.797604724139276e-07, 6.196144729881009e-07, 5.576939423620645e-07, 5.056832414033424e-07, 4.557266545158923e-07, 6.996451509128547e-07, 6.316864503735153e-07, 5.744358429411698e-07, 5.229595848548966e-07, 4.7070996284301767e-07, 4.236764991023406e-07, 4.457854544067201e-07, 4.4569539118392377e-07, 4.133165286870439e-07, 3.864705159863597e-07, 3.5296429427536773e-07, 3.4808074980455853e-07, 4.05347287735899e-07, 3.816342185422325e-07, 4.307137034340183e-07, 3.939845756033594e-07], "accuracy_test": 0.8266501913265307, "start": "2016-02-03 11:47:22.551000", "learning_rate_per_epoch": [0.00022682789131067693, 0.00011341394565533847, 7.560929952887818e-05, 5.670697282766923e-05, 4.5365577534539625e-05, 3.780464976443909e-05, 3.240398291382007e-05, 2.8353486413834617e-05, 2.520309863029979e-05, 2.2682788767269813e-05, 2.0620716895791702e-05, 1.8902324882219546e-05, 1.7448299331590533e-05, 1.6201991456910037e-05, 1.5121859178179875e-05, 1.4176743206917308e-05, 1.3342816600925289e-05, 1.2601549315149896e-05, 1.1938310308323707e-05, 1.1341394383634906e-05, 1.0801328244269826e-05, 1.0310358447895851e-05, 9.862082151812501e-06, 9.451162441109773e-06, 9.073115506907925e-06, 8.724149665795267e-06, 8.401032573601697e-06, 8.100995728455018e-06, 7.821651706763078e-06, 7.5609295890899375e-06, 7.317028575926088e-06, 7.088371603458654e-06, 6.873572601762135e-06, 6.6714083004626445e-06, 6.480796855612425e-06, 6.300774657574948e-06, 6.13048359809909e-06, 5.969155154161854e-06, 5.816099928779295e-06, 5.670697191817453e-06, 5.532387604034739e-06, 5.400664122134913e-06, 5.275066996546229e-06, 5.1551792239479255e-06, 5.040619726059958e-06, 4.931041075906251e-06, 4.826125405088533e-06, 4.725581220554886e-06, 4.6291406761156395e-06, 4.5365577534539625e-06, 4.447605533641763e-06, 4.362074832897633e-06, 4.279771474102745e-06, 4.200516286800848e-06, 4.12414328820887e-06, 4.050497864227509e-06, 3.979436769441236e-06, 3.910825853381539e-06, 3.844540515274275e-06, 3.7804647945449688e-06, 3.718490006576758e-06, 3.658514287963044e-06, 3.6004425965074915e-06, 3.544185801729327e-06, 3.4896597753686365e-06, 3.4367863008810673e-06, 3.385490799701074e-06, 3.3357041502313223e-06, 3.2873606414796086e-06, 3.2403984278062126e-06, 3.1947590741765453e-06, 3.150387328787474e-06, 3.1072313504409976e-06, 3.065241799049545e-06, 3.024371835635975e-06, 2.984577577080927e-06, 2.945816731880768e-06, 2.9080499643896474e-06, 2.87123907583009e-06, 2.8353485959087266e-06, 2.800344418574241e-06, 2.7661938020173693e-06, 2.7328660507919267e-06, 2.7003320610674564e-06, 2.668563411134528e-06, 2.6375334982731147e-06, 2.6072170840052422e-06, 2.5775896119739627e-06, 2.548627890064381e-06, 2.520309863029979e-06, 2.4926141577452654e-06, 2.4655205379531253e-06, 2.4390096768911462e-06, 2.4130627025442664e-06, 2.3876621071394766e-06, 2.362790610277443e-06, 2.3384318410535343e-06, 2.3145703380578198e-06, 2.291190867254045e-06, 2.2682788767269813e-06, 2.2458207240561023e-06, 2.2238027668208815e-06, 2.2022124994691694e-06, 2.1810374164488167e-06, 2.1602656943287e-06, 2.1398857370513724e-06, 2.1198868580540875e-06, 2.100258143400424e-06, 2.0809898160223383e-06, 2.062071644104435e-06, 2.0434945326996967e-06, 2.0252489321137546e-06, 2.0073264295206172e-06, 1.989718384720618e-06, 1.972416384887765e-06, 1.9554129266907694e-06, 1.9386998246773146e-06, 1.9222702576371375e-06, 1.9061167222389486e-06, 1.8902323972724844e-06, 1.8746106889011571e-06, 1.859245003288379e-06, 1.8441292013449129e-06, 1.829257143981522e-06, 1.81462314685632e-06, 1.8002212982537458e-06, 1.7860463685792638e-06, 1.7720929008646635e-06, 1.7583557792022475e-06, 1.7448298876843182e-06, 1.7315105651505291e-06, 1.7183931504405336e-06, 1.7054728687071474e-06, 1.692745399850537e-06, 1.6802065374577069e-06, 1.6678520751156611e-06, 1.6556780337850796e-06, 1.6436803207398043e-06, 1.6318552980010281e-06, 1.6201992139031063e-06, 1.6087084304672317e-06, 1.5973795370882726e-06, 1.5862090094742598e-06, 1.575193664393737e-06, 1.564330318615248e-06, 1.5536156752204988e-06, 1.5430468920385465e-06, 1.5326208995247725e-06, 1.5223348555082339e-06, 1.5121859178179875e-06, 1.5021714716567658e-06, 1.4922887885404634e-06, 1.4825352536718128e-06, 1.472908365940384e-06, 1.463405737922585e-06, 1.4540249821948237e-06, 1.44476359764667e-06, 1.435619537915045e-06, 1.4265905292631942e-06, 1.4176742979543633e-06, 1.408868911312311e-06], "accuracy_train_first": 0.5760089343392396, "accuracy_train_last": 0.9613152916205242, "batch_size_eval": 1024, "accuracy_train_std": [0.017620082684812903, 0.018554467193351576, 0.017397000839259963, 0.015186748419445343, 0.014573875519944348, 0.0150732318689781, 0.014180729015301558, 0.014721204897800098, 0.01364636746432211, 0.014344448326500765, 0.012748896685541737, 0.012901151548448264, 0.012731130756107916, 0.012380790926758159, 0.010962998273702073, 0.010674442353145358, 0.010936543842323296, 0.010968887478504653, 0.010462047219293634, 0.009799621971780243, 0.009797443031206597, 0.010138135090618968, 0.009859781987715508, 0.009562610798177931, 0.010021970028955725, 0.009205087067440249, 0.009152176011615226, 0.009713441851045583, 0.009516393150927721, 0.009666133011227629, 0.00919955142262301, 0.009731634205554889, 0.009183070619514419, 0.00987813316329665, 0.009558659204314514, 0.008901588430266423, 0.008928632348436676, 0.008744743443492572, 0.008636716531458526, 0.00901272000022044, 0.008536771033861626, 0.008222856938772973, 0.00882131126235332, 0.007522022026737554, 0.0089313461291704, 0.00802518140163346, 0.008729097862716948, 0.00882288887757675, 0.00850767763168287, 0.00821607121560661, 0.00860982208150902, 0.008085984281722074, 0.008411910272506106, 0.008759439321024399, 0.0079099186894995, 0.007656994882119752, 0.008352420738957205, 0.008207353375396064, 0.007194421101322076, 0.0073792813565399734, 0.007321836683230804, 0.007374501451090401, 0.007477227435003001, 0.0077908252973128015, 0.007440622235662253, 0.007260585257811853, 0.0071806644407120725, 0.0072195612298784, 0.006564334990441031, 0.007566398385588276, 0.006915747535498186, 0.007587313688648577, 0.006600125224484057, 0.0072088053289115685, 0.007161705733721695, 0.006689631320905949, 0.007184348347435708, 0.0067713466775037754, 0.007048740139987509, 0.0075270037315953, 0.007034056002113466, 0.0071603605530455566, 0.006116288356526202, 0.0069666291179531865, 0.00637148670246683, 0.006575701295208905, 0.00676619014195363, 0.00674916310223792, 0.006669559702288361, 0.0064594959668342845, 0.006816240724377, 0.007501793093892501, 0.006621967282738575, 0.006517146762098086, 0.006560093168677131, 0.0063159689642613, 0.006679191807882916, 0.006572567355949505, 0.007114557609492067, 0.0064632813954165645, 0.0062644898246471846, 0.006680845711398939, 0.0067473170536217425, 0.006334339192339001, 0.006720223247657014, 0.006150269805603534, 0.0066124094816210725, 0.006399760052745741, 0.00650422085675954, 0.006884799061000637, 0.006476793804073484, 0.006609097085186278, 0.00628687284882989, 0.005699685869271095, 0.006207264147325171, 0.005931954702948674, 0.005913938025344243, 0.005994973140897369, 0.006065826875391111, 0.0059938782874989225, 0.006193394989337195, 0.00619028586855959, 0.006032066585602118, 0.006239578374845826, 0.006474721070537763, 0.005963978818953434, 0.006044460290972396, 0.005835717749578854, 0.006019764603402093, 0.006190690726020401, 0.005732790039093423, 0.005572908431057946, 0.005853181538607282, 0.006555825115391669, 0.0057089369070012505, 0.005887545845477854, 0.006041403565651625, 0.005817543482052727, 0.006362235453166354, 0.006093917774781579, 0.005779339089708857, 0.005635134616643642, 0.005690498969899138, 0.005511667084786291, 0.005432736153660883, 0.00577476482758685, 0.005839830231166624, 0.005970884052683703, 0.005781037568722966, 0.005720102146333578, 0.005791719002241699, 0.0059528881865739975, 0.005596613615887974, 0.005512237526435599, 0.005838900265912206, 0.005819431878932666, 0.005727479594605076, 0.005541092296128326, 0.005962561988703768, 0.006259276776073953, 0.006164814620915811], "accuracy_test_std": 0.008154050335924588, "error_valid": [0.4320450748305723, 0.33734645613704817, 0.2469997176204819, 0.22487410579819278, 0.211669921875, 0.20768278190888556, 0.19760095067771077, 0.19638024755271077, 0.19431534732680722, 0.18778385024472888, 0.1848335725715362, 0.18271719691265065, 0.17856680628765065, 0.17711225762424698, 0.17850650649472888, 0.17662397637424698, 0.17528120293674698, 0.17495617234563254, 0.17183234892695776, 0.17183234892695776, 0.1724529955760542, 0.17182205384036142, 0.1702557299510542, 0.17085578642695776, 0.1698895190135542, 0.1703778002635542, 0.16914680205195776, 0.1704998705760542, 0.1702557299510542, 0.16878059111445776, 0.17077489646084332, 0.17015424981174698, 0.16881147637424698, 0.16597297392695776, 0.16941153285015065, 0.16611563441265065, 0.16635977503765065, 0.16649214043674698, 0.16844526543674698, 0.1668377612010542, 0.16784520896084332, 0.1651287768260542, 0.16450813017695776, 0.16353156767695776, 0.16440665003765065, 0.16490522637424698, 0.16698042168674698, 0.16327713196536142, 0.16452872035015065, 0.16428457972515065, 0.1625653002635542, 0.16172110316265065, 0.16479345114834332, 0.1623211596385542, 0.1625653002635542, 0.1625653002635542, 0.1613445971385542, 0.16404043910015065, 0.16245352503765065, 0.16331831231174698, 0.1600930087537651, 0.16060188017695776, 0.16121223173945776, 0.16157844267695776, 0.16380659356174698, 0.1620770190135542, 0.1635933381965362, 0.16208731410015065, 0.16258589043674698, 0.16309476185993976, 0.16074454066265065, 0.1600018237010542, 0.16319624199924698, 0.16125341208584332, 0.1619549487010542, 0.16099897637424698, 0.16221967949924698, 0.16098868128765065, 0.16173139824924698, 0.16382718373493976, 0.16257559535015065, 0.16197553887424698, 0.16136518731174698, 0.16285062123493976, 0.16123282191265065, 0.16086661097515065, 0.16222997458584332, 0.16088720114834332, 0.16124311699924698, 0.16098868128765065, 0.16062247035015065, 0.16186376364834332, 0.16160932793674698, 0.16113134177334332, 0.16210790427334332, 0.16001211878765065, 0.15990034356174698, 0.15904585137424698, 0.16050040003765065, 0.16149755271084332, 0.16015477927334332, 0.16027684958584332, 0.16037832972515065, 0.16198583396084332, 0.16210790427334332, 0.15989004847515065, 0.1604901049510542, 0.1623726350715362, 0.16113134177334332, 0.15964590785015065, 0.1590252612010542, 0.1591473315135542, 0.15941206231174698, 0.16161962302334332, 0.16088720114834332, 0.1601238940135542, 0.16065335560993976, 0.1622505647590362, 0.16076513083584332, 0.15954442771084332, 0.16077542592243976, 0.15916792168674698, 0.15941206231174698, 0.16064306052334332, 0.16014448418674698, 0.16126370717243976, 0.16100927146084332, 0.16026655449924698, 0.16086661097515065, 0.16088720114834332, 0.15990034356174698, 0.15992093373493976, 0.1609077913215362, 0.15954442771084332, 0.15927969691265065, 0.16003270896084332, 0.16089749623493976, 0.16016507435993976, 0.16053128529743976, 0.15954442771084332, 0.15978856833584332, 0.15991063864834332, 0.16015477927334332, 0.16037832972515065, 0.16027684958584332, 0.16199612904743976, 0.16053128529743976, 0.16065335560993976, 0.16016507435993976, 0.1604195100715362, 0.1604195100715362, 0.15955472279743976, 0.16101956654743976, 0.16075483574924698, 0.16002241387424698, 0.16014448418674698, 0.15977827324924698, 0.15928999199924698, 0.15976797816265065, 0.16114163685993976, 0.16052099021084332], "tags": ["deepconvnets", "zoonormalized"], "hp": {"zoom_range": [1, 1], "translation_range": [-5, 5], "learning_rate_decay": 0.04367887323179581, "discrete_learning_divide": 10.0, "shear_range": [1, 1], "patience_check_each": 1, "discrete_learning_rate_epsilon": 0.0001, "patience_threshold": 1, "do_flip": true, "batch_size": 16, "valid_ratio": 0.15, "learning_rate": 0.00022682788970910058, "optimization": "rmsprop", "nb_data_augmentation": 2, "learning_rate_decay_method": "lin", "max_epochs": 1000, "patience_nb_epochs": 50, "weight_decay": 0.0, "l2_decay": 2.0372741336861268e-08, "rotation_range": [0, 0], "momentum": 0.6197813194242635}, "accuracy_valid_max": 0.8409747387989458, "code_": "from datetime import datetime\nimport matplotlib as mpl\nmpl.use('Agg')   # NOQA\nfrom lasagnekit.easy import BatchOptimizer, BatchIterator, get_batch_slice\nfrom lasagnekit.nnet.capsule import Capsule\nfrom lasagnekit.easy import iterate_minibatches\nfrom lasagne import updates\nfrom lasagnekit.updates import santa_sss\nupdates.santa_sss = santa_sss  # NOQA\nimport theano\nimport theano.tensor as T\n\nimport numpy as np\nimport json\n\nfrom skimage.io import imsave\nfrom lasagnekit.datasets.infinite_image_dataset import Transform\n\n\nclass MyBatchIterator(BatchIterator):\n\n    def __init__(self, nb_data_augmentation=1,  **transform_params):\n        super(MyBatchIterator, self).__init__()\n\n        self.nb_data_augmentation = nb_data_augmentation\n        self.transform_params = transform_params\n\n    def transform(self, batch_index, V):\n        assert self.batch_size is not None\n        assert self.nb_batches is not None\n\n        if isinstance(batch_index, T.TensorVariable):\n            batch_slice = get_batch_slice(batch_index,\n                                          self.batch_size)\n        else:\n            batch_slice = slice(batch_index * self.batch_size,\n                                (batch_index+1) * self.batch_size)\n\n        d = OrderedDict()\n        X = V[\"X\"][batch_slice]\n        y = V[\"y\"][batch_slice]\n\n        X_list = [X]\n        y_list = [y]\n        for i in range(self.nb_data_augmentation):\n            tr, _ = Transform(X.transpose(0, 2, 3, 1),\n                              np.random,\n                              **self.transform_params)\n            imsave(\"out.png\", (((tr[0] + 1) / 2.)))\n            X_transformed = tr.transpose((0, 3, 1, 2))\n            X_list.append(X_transformed)\n            y_list.append(y)\n        d[\"X\"] = np.concatenate(X_list, axis=0)\n        d[\"y\"] = np.concatenate(y_list, axis=0)\n        d[\"X\"], d[\"y\"] = shuffle(d[\"X\"], d[\"y\"])\n        return d\n\n\nif __name__ == \"__main__\":\n    from lasagnekit.datasets.cifar10 import Cifar10\n    from sklearn.utils import shuffle\n    from sklearn.cross_validation import train_test_split\n    from collections import OrderedDict\n\n    from lightexperiments.light import Light\n    from hp_toolkit.hp import (\n            Param, make_constant_param,\n            instantiate_random, instantiate_default\n    )\n    import argparse\n    import vgg  # NOQA\n    import vgg_small  # NOQA\n    import vgg_very_small  # NOQA\n    import spatially_sparse  # NOQA\n    import nin  # NOQA\n    import fully  # NOQA\n    import residual  # NOQA\n    import residualv2  # NOQA\n    import residualv3  # NOQA\n    import residualv4  # NOQA\n    import residualv5  # NOQA\n\n    parser = argparse.ArgumentParser(description='zoo')\n    parser.add_argument(\"--budget-hours\",\n                        default=np.inf,\n                        help=\"nb of maximum hours (defaut=inf)\")\n    parser.add_argument(\"--fast-test\", default=False, type=bool)\n    parser.add_argument(\"--model\", default=\"vgg\", type=str)\n    parser.add_argument(\"--default-model\", default=False, type=bool)\n\n    models = {\n        \"vgg\": vgg,\n        \"vgg_small\": vgg_small,\n        \"vgg_very_small\": vgg_very_small,\n        \"spatially_sparse\": spatially_sparse,\n        \"nin\": nin,\n        \"fully\": fully,\n        \"residual\": residual,\n        \"residualv2\": residualv2,\n        \"residualv3\": residualv3,\n        \"residualv4\": residualv4,\n        \"residualv5\": residualv5\n    }\n    args = parser.parse_args()\n    model_class = models[args.model]\n    budget_sec = args.budget_hours * 3600\n    begin = datetime.now()\n    seed = np.random.randint(0, 1000000000)\n    np.random.seed(seed)\n    fast_test = args.fast_test\n    np.random.seed(seed)\n    rng = np.random\n\n    if args.default_model is True:\n        instantiate = instantiate_default\n    else:\n        instantiate = instantiate_random\n\n    light = Light()\n    light.launch()\n    light.initials()\n    light.file_snapshot()\n    light.set_seed(seed)\n    light.tag(\"deepconvnets\")\n    light.tag(\"zoonormalized\")\n\n    data = Cifar10(batch_indexes=[1, 2, 3, 4, 5])\n    data.load()\n\n    data_test = Cifar10(batch_indexes=[6])\n    data_test.load()\n\n    light.set(\"dataset\", data.__class__.__name__)\n\n    hp = dict(\n        learning_rate=Param(initial=0.001, interval=[-4, -2], type='real', scale='log10'),\n        learning_rate_decay=Param(initial=0.05, interval=[0, 0.1], type='real'),\n        learning_rate_decay_method=Param(initial='discrete', interval=['exp', 'none', 'sqrt', 'lin', 'discrete'], type='choice'),\n        momentum=Param(initial=0.9, interval=[0.5, 0.99], type='real'),\n        #weight_decay=Param(initial=0, interval=[-10, -3], type='real', scale='log10'),\n        weight_decay=make_constant_param(0.),\n        discrete_learning_rate_epsilon=make_constant_param(1e-4),#NEW TO ADD\n        discrete_learning_divide=make_constant_param(10.),\n        l2_decay=Param(initial=0, interval=[-8, -4], type='real', scale='log10'),#NEW TO ADD\n        max_epochs=make_constant_param(1000),\n        batch_size=Param(initial=32,\n                         interval=[16, 32, 64, 128],\n                         type='choice'),\n        patience_nb_epochs=make_constant_param(50),\n        valid_ratio=make_constant_param(0.15),\n\n        patience_threshold=make_constant_param(1),\n        patience_check_each=make_constant_param(1),\n\n        optimization=Param(initial='adam',\n                           interval=['adam', 'nesterov_momentum', 'rmsprop'],\n                           type='choice'),\n        # data augmentation\n        nb_data_augmentation=Param(initial=1, interval=[0, 1, 2, 3, 4], type='choice'),\n        zoom_range=make_constant_param((1, 1)),\n        rotation_range=make_constant_param((0, 0)),\n        shear_range=make_constant_param((1, 1)),\n        translation_range=make_constant_param((-5, 5)),\n        do_flip=make_constant_param(True)\n\n    )\n\n    if fast_test is True:\n        instantiate = instantiate_default\n\n    default_params = {}\n    if fast_test is True:\n        default_params[\"max_epochs\"] = 1\n    hp = instantiate(hp, default_params=default_params)\n    light.set(\"hp\", hp)\n\n    hp_model = model_class.params\n    hp_model = instantiate(hp_model)\n    light.set(\"hp_model\", hp_model)\n\n    model = model_class.build_model(\n        input_width=data.img_dim[1],\n        input_height=data.img_dim[2],\n        output_dim=data.output_dim,\n        **hp_model)\n    light.set(\"model\", model_class.__name__)\n    print(model_class.__name__)\n    print(json.dumps(hp, indent=4))\n    print(json.dumps(hp_model, indent=4))\n\n    initial_lr = hp[\"learning_rate\"]\n\n    def evaluate(X, y, batch_size=None):\n        if batch_size is None:\n            batch_size = hp[\"batch_size\"]\n        accs = []\n        for mini_batch in iterate_minibatches(X.shape[0],\n                                              batch_size):\n            acc = (nnet.predict(X[mini_batch]) == y[mini_batch]).mean()\n            accs.append(acc)\n        return accs\n\n    class MyBatchOptimizer(BatchOptimizer):\n\n        def quitter(self, update_status):\n            quit = super(MyBatchOptimizer, self).quitter(update_status)\n            if (datetime.now() - begin).total_seconds() >= budget_sec:\n                print(\"Budget finished.quit.\")\n                quit = True\n            return quit\n\n        def iter_update(self, epoch, nb_batches, iter_update_batch):\n            start = datetime.now()\n            status = super(MyBatchOptimizer, self).iter_update(epoch,\n                                                               nb_batches,\n                                                               iter_update_batch)\n            duration = (datetime.now() - start).total_seconds()\n            status[\"duration\"] = duration\n            accs = evaluate(X_train, y_train, batch_size=self.batch_size_eval)\n            status[\"accuracy_train\"] = np.mean(accs)\n            status[\"accuracy_train_std\"] = np.std(accs)\n            accs = evaluate(X_valid, y_valid, batch_size=self.batch_size_eval)\n            status[\"accuracy_valid\"] = np.mean(accs)\n            status[\"accuracy_valid_std\"] = np.std(accs)\n\n            status[\"error_valid\"] = 1 - status[\"accuracy_valid\"]\n\n            status = self.add_moving_avg(\"accuracy_train\", status)\n            status = self.add_moving_var(\"accuracy_train\", status)\n            status = self.add_moving_avg(\"accuracy_valid\", status)\n            status = self.add_moving_var(\"accuracy_valid\", status)\n\n            for k, v in status.items():\n                light.append(k, float(v))\n\n            lr = self.learning_rate\n            lr_decay_method = hp[\"learning_rate_decay_method\"]\n            lr_decay = hp[\"learning_rate_decay\"]\n            cur_lr = lr.get_value()\n            t = status[\"epoch\"]\n\n            if lr_decay_method == \"exp\":\n                new_lr = cur_lr * (1 - lr_decay)\n            elif lr_decay_method == \"lin\":\n                new_lr = initial_lr / (1 + t)\n            elif lr_decay_method == \"sqrt\":\n                new_lr = initial_lr / np.sqrt(1 + t)\n            elif lr_decay_method == 'discrete':\n                eps = hp[\"discrete_learning_rate_epsilon\"]\n                div = hp[\"discrete_learning_divide\"]\n                if status[\"moving_var_accuracy_valid\"] <= eps:\n                    new_lr = cur_lr / div\n                else:\n                    new_lr = cur_lr\n            else:\n                new_lr = cur_lr\n\n            new_lr = np.array(new_lr, dtype=\"float32\")\n            lr.set_value(new_lr)\n\n            light.append(\"learning_rate_per_epoch\",\n                         float(self.learning_rate.get_value()))\n            return status\n\n        def add_moving_avg(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n            else:\n                old_avg = 0\n            avg = B * old_avg + (1 - B) * status[name]\n            status[\"moving_avg_\" + name] = avg\n            return status\n\n        def add_moving_var(self, name, status, B=0.9):\n            if len(self.stats) >= 2:\n                old_avg = self.stats[-2][\"moving_avg_\" + name]\n                old_var = self.stats[-2][\"moving_var_\" + name]\n            else:\n                old_avg = 0\n                old_var = 0\n            new_avg = B * old_avg + (1 - B) * status[name]\n            var = B * old_var + (1 - B) * (status[name] - old_avg) * (status[name] - new_avg)\n            status[\"moving_var_\" + name] = var\n            return status\n\n    learning_rate = theano.shared(np.array(hp[\"learning_rate\"],\n                                  dtype=\"float32\"))\n    momentum = hp[\"momentum\"]\n\n    optim_params = {\"learning_rate\": learning_rate}\n    if \"momentum\" in hp[\"optimization\"]:\n        optim_params[\"momentum\"] = hp[\"momentum\"]\n\n    batch_optimizer = MyBatchOptimizer(\n        verbose=1, max_nb_epochs=hp[\"max_epochs\"],\n        batch_size=hp[\"batch_size\"],\n        optimization_procedure=(getattr(updates, hp[\"optimization\"]),\n                                optim_params),\n        patience_stat=\"error_valid\",\n        patience_nb_epochs=hp[\"patience_nb_epochs\"],\n        patience_progression_rate_threshold=hp[\"patience_threshold\"],\n        patience_check_each=hp[\"patience_check_each\"],\n        verbose_stat_show=[\n            \"epoch\",\n            \"duration\",\n            \"accuracy_train\",\n            \"accuracy_train_std\",\n            \"accuracy_valid\",\n            \"accuracy_valid_std\",\n        ]\n    )\n    batch_size_eval = 1024\n    light.set(\"batch_size_eval\", batch_size_eval)\n    batch_optimizer.learning_rate = learning_rate\n    batch_optimizer.batch_size_eval = batch_size_eval\n\n    input_variables = OrderedDict()\n    input_variables[\"X\"] = dict(tensor_type=T.tensor4)\n    input_variables[\"y\"] = dict(tensor_type=T.ivector)\n    functions = dict(\n        predict=dict(\n            get_output=lambda model, X: (model.get_output(X, deterministic=True)[0]).argmax(axis=1),\n            params=[\"X\"]\n        )\n    )\n\n    def loss_function(model, tensors):\n        X = tensors[\"X\"]\n        y = tensors[\"y\"]\n        y_hat, = model.get_output(X)\n        if hp[\"weight_decay\"] > 0:\n            l1 = sum(T.abs_(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"weight_decay\"]\n        else:\n            l1 = 0\n\n        if hp[\"l2_decay\"] > 0:\n            l2 = sum(T.sqr(param).sum() for param in model.capsule.all_params_regularizable) * hp[\"l2_decay\"]\n        else:\n            l2 = 0\n\n        return T.nnet.categorical_crossentropy(y_hat, y).mean() + l1 + l2\n\n    batch_iterator = MyBatchIterator(hp[\"nb_data_augmentation\"],\n                                     zoom_range=hp[\"zoom_range\"],\n                                     rotation_range=hp[\"rotation_range\"],\n                                     shear_range=hp[\"shear_range\"],\n                                     translation_range=hp[\"translation_range\"],\n                                     do_flip=hp[\"do_flip\"])\n\n    nnet = Capsule(\n        input_variables, model,\n        loss_function,\n        functions=functions,\n        batch_optimizer=batch_optimizer,\n        batch_iterator=batch_iterator,\n    )\n\n    from sklearn.preprocessing import LabelEncoder\n\n    imshape = ([data.X.shape[0]] +\n               list(data.img_dim))\n    X = data.X.reshape(imshape).astype(np.float32)\n    y = data.y\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    y = y.astype(np.int32)\n\n    # rescaling to [-1, 1]\n    X_min = X.min(axis=(0, 2, 3))[None, :, None, None]\n    X_max = X.max(axis=(0, 2, 3))[None, :, None, None]\n    X = 2 * ((X - X_min) / (X_max - X_min)) - 1\n    X, y = shuffle(X, y)\n\n    if fast_test is True:\n        X = X[0:100]\n        y = y[0:100]\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=hp[\"valid_ratio\"])\n    light.set(\"nb_examples_train\", X_train.shape[0])\n    light.set(\"nb_examples_valid\", X_valid.shape[0])\n    try:\n        nnet.fit(X=X_train, y=y_train)\n    except KeyboardInterrupt:\n        print(\"interruption...\")\n\n    imshape = ([data_test.X.shape[0]] +\n               list(data_test.img_dim))\n    X_test = data_test.X.reshape(imshape).astype(np.float32)\n    X_test = 2 * ((X_test - X_min) / (X_max - X_min)) - 1\n    y_test = data_test.y\n    y_test = label_encoder.transform(y_test)\n    y_test = y_test.astype(np.int32)\n\n    accs = evaluate(X_test, y_test, batch_size_eval)\n    m, s = np.mean(accs), np.std(accs)\n    light.set(\"accuracy_test\", m)\n    light.set(\"accuracy_test_std\", s)\n    print(\"Test accuracy : {}+-{}\".format(m, s))\n\n    light.endings()  # save the duration\n\n    if fast_test is False:\n        light.store_experiment()  # update the DB\n    light.close()\n", "nb_examples_valid": 7500, "accuracy_valid_last": 0.8394790097891567, "accuracy_valid_std": [0.01392559792911133, 0.02002875606521396, 0.012498076943419084, 0.016224740783861852, 0.017583210527373865, 0.024083049037546614, 0.01977795148803893, 0.020243710629635366, 0.022034472711332277, 0.017850013971717697, 0.01524157333724468, 0.012282729977108822, 0.015262265322578388, 0.016043618760896484, 0.021364701761520787, 0.01523919462431561, 0.016109478067990877, 0.019367231295874685, 0.013445230825062942, 0.014988103632344894, 0.015129801310198487, 0.01518340933077745, 0.015968554787259087, 0.013731669314855305, 0.016153835748829688, 0.01555328006290173, 0.014435391166763806, 0.014628825831546875, 0.015818545142068655, 0.014415198111521806, 0.017833338390635684, 0.0170670185747793, 0.017829186757736707, 0.016320034366773675, 0.01704707073981938, 0.01814346487803609, 0.01814704656394077, 0.018826509919486256, 0.017892768387421466, 0.016731010686381616, 0.018771031298427422, 0.017066347548147016, 0.017219109354522928, 0.01750508510142836, 0.018611696894671115, 0.019451125908749128, 0.018946739001829548, 0.016105085999567784, 0.0183282604450535, 0.018997070963147105, 0.018428982962081352, 0.01990241827442174, 0.02104228568119144, 0.01817944178308408, 0.018512883474623966, 0.018312178422919996, 0.01859267859457094, 0.01898580260988891, 0.019710636073197896, 0.020687545207396886, 0.015874388779254203, 0.0185302831634324, 0.017999373407348446, 0.016583985190293478, 0.01928002157914057, 0.01778287925566943, 0.022735335375167406, 0.01898353300499219, 0.01996128734491501, 0.021495348481895438, 0.020092101931014288, 0.01865874868170096, 0.01972396816047838, 0.02217672417915229, 0.0192098336619755, 0.021157367161831055, 0.020719668910475355, 0.021074042827320474, 0.020848443772557686, 0.022591512719657814, 0.019090552245649248, 0.020289306540150678, 0.020367021005770196, 0.022144220948047303, 0.019682033207606378, 0.02010891588877602, 0.021665609185888404, 0.02219449886050812, 0.021658661139730456, 0.020518008648864685, 0.02092927018785437, 0.02203284127862429, 0.0213347986676328, 0.022168987562507538, 0.021707171691309358, 0.020982313888775334, 0.021265295469671118, 0.02256853200808179, 0.020757203715820963, 0.02231339255275938, 0.02351031075640461, 0.022112060054548913, 0.020629256116360534, 0.022396080269683635, 0.021827654742922264, 0.02160534837144404, 0.020196642148506572, 0.02464230487986717, 0.023013278388853337, 0.02071641319037718, 0.020542528242705055, 0.019855540856621105, 0.022453876093105857, 0.022653069030516743, 0.02277766062110567, 0.020143231860648675, 0.02400615740685692, 0.024490454790955483, 0.022022959146247866, 0.02313684202968246, 0.024070271694045478, 0.02245507540008102, 0.022832929932443256, 0.022878170007448646, 0.02210243799010938, 0.022696157269861614, 0.022802187823187752, 0.02159750026951136, 0.020785212373166887, 0.022646442521789795, 0.022342364500997264, 0.023695294115123885, 0.024788137509076713, 0.023555546747906508, 0.02150344410117309, 0.0230310076742508, 0.023609248448002124, 0.024164718196433514, 0.024030708261760746, 0.023721964414929055, 0.023393056794480904, 0.022918469680434792, 0.02338830190424856, 0.021154212752824017, 0.02287000342517467, 0.0235617688172779, 0.02362547427364432, 0.023966398244416678, 0.0242090762468327, 0.025373133778327343, 0.025344928610242456, 0.024497794587564653, 0.0240825604349583, 0.02250330242304582, 0.022244506822553443, 0.02227971225615317, 0.023222347076793628, 0.022462769429520114, 0.021639337033462307, 0.02405558483377204, 0.022626341084398264], "accuracy_valid": [0.5679549251694277, 0.6626535438629518, 0.7530002823795181, 0.7751258942018072, 0.788330078125, 0.7923172180911144, 0.8023990493222892, 0.8036197524472892, 0.8056846526731928, 0.8122161497552711, 0.8151664274284638, 0.8172828030873494, 0.8214331937123494, 0.822887742375753, 0.8214934935052711, 0.823376023625753, 0.824718797063253, 0.8250438276543675, 0.8281676510730422, 0.8281676510730422, 0.8275470044239458, 0.8281779461596386, 0.8297442700489458, 0.8291442135730422, 0.8301104809864458, 0.8296221997364458, 0.8308531979480422, 0.8295001294239458, 0.8297442700489458, 0.8312194088855422, 0.8292251035391567, 0.829845750188253, 0.831188523625753, 0.8340270260730422, 0.8305884671498494, 0.8338843655873494, 0.8336402249623494, 0.833507859563253, 0.831554734563253, 0.8331622387989458, 0.8321547910391567, 0.8348712231739458, 0.8354918698230422, 0.8364684323230422, 0.8355933499623494, 0.835094773625753, 0.833019578313253, 0.8367228680346386, 0.8354712796498494, 0.8357154202748494, 0.8374346997364458, 0.8382788968373494, 0.8352065488516567, 0.8376788403614458, 0.8374346997364458, 0.8374346997364458, 0.8386554028614458, 0.8359595608998494, 0.8375464749623494, 0.836681687688253, 0.8399069912462349, 0.8393981198230422, 0.8387877682605422, 0.8384215573230422, 0.836193406438253, 0.8379229809864458, 0.8364066618034638, 0.8379126858998494, 0.837414109563253, 0.8369052381400602, 0.8392554593373494, 0.8399981762989458, 0.836803758000753, 0.8387465879141567, 0.8380450512989458, 0.839001023625753, 0.837780320500753, 0.8390113187123494, 0.838268601750753, 0.8361728162650602, 0.8374244046498494, 0.838024461125753, 0.838634812688253, 0.8371493787650602, 0.8387671780873494, 0.8391333890248494, 0.8377700254141567, 0.8391127988516567, 0.838756883000753, 0.8390113187123494, 0.8393775296498494, 0.8381362363516567, 0.838390672063253, 0.8388686582266567, 0.8378920957266567, 0.8399878812123494, 0.840099656438253, 0.840954148625753, 0.8394995999623494, 0.8385024472891567, 0.8398452207266567, 0.8397231504141567, 0.8396216702748494, 0.8380141660391567, 0.8378920957266567, 0.8401099515248494, 0.8395098950489458, 0.8376273649284638, 0.8388686582266567, 0.8403540921498494, 0.8409747387989458, 0.8408526684864458, 0.840587937688253, 0.8383803769766567, 0.8391127988516567, 0.8398761059864458, 0.8393466443900602, 0.8377494352409638, 0.8392348691641567, 0.8404555722891567, 0.8392245740775602, 0.840832078313253, 0.840587937688253, 0.8393569394766567, 0.839855515813253, 0.8387362928275602, 0.8389907285391567, 0.839733445500753, 0.8391333890248494, 0.8391127988516567, 0.840099656438253, 0.8400790662650602, 0.8390922086784638, 0.8404555722891567, 0.8407203030873494, 0.8399672910391567, 0.8391025037650602, 0.8398349256400602, 0.8394687147025602, 0.8404555722891567, 0.8402114316641567, 0.8400893613516567, 0.8398452207266567, 0.8396216702748494, 0.8397231504141567, 0.8380038709525602, 0.8394687147025602, 0.8393466443900602, 0.8398349256400602, 0.8395804899284638, 0.8395804899284638, 0.8404452772025602, 0.8389804334525602, 0.839245164250753, 0.839977586125753, 0.839855515813253, 0.840221726750753, 0.840710008000753, 0.8402320218373494, 0.8388583631400602, 0.8394790097891567], "seed": 332270844, "model": "residualv3", "loss_std": [0.3675500154495239, 0.262725830078125, 0.24700991809368134, 0.24112407863140106, 0.23339058458805084, 0.2289274036884308, 0.22290074825286865, 0.22093936800956726, 0.21848224103450775, 0.21483658254146576, 0.2122296690940857, 0.20698530972003937, 0.20570944249629974, 0.20404988527297974, 0.2014087736606598, 0.19837558269500732, 0.19818879663944244, 0.1949235200881958, 0.1956183910369873, 0.194200798869133, 0.19124585390090942, 0.19138552248477936, 0.18872275948524475, 0.18862895667552948, 0.18607564270496368, 0.18377754092216492, 0.18567100167274475, 0.18155305087566376, 0.1821921020746231, 0.18128792941570282, 0.18091022968292236, 0.17580272257328033, 0.17859935760498047, 0.1757555902004242, 0.177864670753479, 0.17443135380744934, 0.17570877075195312, 0.17230483889579773, 0.1753777712583542, 0.1739734709262848, 0.17246206104755402, 0.17063237726688385, 0.16842570900917053, 0.16970661282539368, 0.16812962293624878, 0.1686141937971115, 0.1676921546459198, 0.16782592236995697, 0.16523784399032593, 0.1658133715391159, 0.1647322028875351, 0.16401387751102448, 0.16387344896793365, 0.16367915272712708, 0.16138358414173126, 0.16174136102199554, 0.16036999225616455, 0.16121399402618408, 0.16085705161094666, 0.16043347120285034, 0.16103093326091766, 0.16006167232990265, 0.1599661260843277, 0.1601131111383438, 0.1594046652317047, 0.16052426397800446, 0.15635907649993896, 0.1567780077457428, 0.15515469014644623, 0.15639165043830872, 0.15595576167106628, 0.15478383004665375, 0.15713132917881012, 0.15491199493408203, 0.15437652170658112, 0.15273967385292053, 0.1535332202911377, 0.1554420292377472, 0.15417730808258057, 0.15100187063217163, 0.15303146839141846, 0.1524498611688614, 0.1488368660211563, 0.1521574854850769, 0.1485183835029602, 0.15104743838310242, 0.15129990875720978, 0.1493738889694214, 0.1515798419713974, 0.15070949494838715, 0.14770665764808655, 0.149881049990654, 0.14573723077774048, 0.15097932517528534, 0.14937248826026917, 0.146898090839386, 0.1454872190952301, 0.14882603287696838, 0.14732789993286133, 0.1452777236700058, 0.146563321352005, 0.14614498615264893, 0.14553123712539673, 0.1448689103126526, 0.1447516530752182, 0.14559975266456604, 0.14620666205883026, 0.144032284617424, 0.14405284821987152, 0.14407891035079956, 0.14190538227558136, 0.1448925882577896, 0.1433049440383911, 0.1443977952003479, 0.14069607853889465, 0.14410780370235443, 0.14020222425460815, 0.14287810027599335, 0.14300619065761566, 0.14009875059127808, 0.14147432148456573, 0.14021247625350952, 0.1415046900510788, 0.1403655707836151, 0.1431916505098343, 0.14141401648521423, 0.1397790163755417, 0.13954852521419525, 0.13875630497932434, 0.13862718641757965, 0.13940802216529846, 0.13961122930049896, 0.13839976489543915, 0.14110855758190155, 0.1364530771970749, 0.1398017555475235, 0.14023888111114502, 0.13787591457366943, 0.13829688727855682, 0.1386723667383194, 0.14044761657714844, 0.13656075298786163, 0.1372414380311966, 0.1373523771762848, 0.13845883309841156, 0.1362314522266388, 0.13742990791797638, 0.136996790766716, 0.13659822940826416, 0.13736817240715027, 0.1367895007133484, 0.13487306237220764, 0.1340806931257248, 0.1343275010585785, 0.13408803939819336, 0.13406765460968018, 0.13615457713603973, 0.1321321576833725, 0.13384340703487396, 0.13500477373600006, 0.13574616611003876]}, "state": "available", "life": [{"dt": "Sun May 15 22:05:41 2016", "state": "available"}], "summary": "25880dbae5fe0933fd4b89ba47134790"}